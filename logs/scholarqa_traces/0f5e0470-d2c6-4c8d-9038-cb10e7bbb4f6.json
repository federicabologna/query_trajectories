{
    "query": "Provide a taxonomy and detailed list of the most commonly used retrieval-augmented language model architectures in NLP (such as kNN-LM, RETRO, REALM, RAG), highlighting their main architectural designs and use cases.",
    "user_id": "lib_user",
    "task_id": "0f5e0470-d2c6-4c8d-9038-cb10e7bbb4f6",
    "timestamp": "2025-06-23T23:29:13.623326",
    "n_retrieval": 256,
    "n_retrieved": 256,
    "n_candidates": 45,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.601056,
    "decomposed_query": {
        "rewritten_query": "Taxonomy and detailed list of the most commonly used retrieval-augmented language model architectures in NLP (such as kNN-LM, RETRO, REALM, RAG), highlighting their main architectural designs and use cases.",
        "keyword_query": "retrieval-augmented language model architectures NLP kNN-LM RETRO REALM RAG architectural designs use cases",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010542,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 251,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.10677, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1491233507",
                    "name": "Mingyue Cheng"
                },
                {
                    "authorId": "2208917508",
                    "name": "Yucong Luo"
                },
                {
                    "authorId": "2322501286",
                    "name": "Ouyang Jie"
                },
                {
                    "authorId": "2332691115",
                    "name": "Qi Liu"
                },
                {
                    "authorId": "2312648865",
                    "name": "Huijie Liu"
                },
                {
                    "authorId": "2291070758",
                    "name": "Li Li"
                },
                {
                    "authorId": "2322429208",
                    "name": "Shuo Yu"
                },
                {
                    "authorId": "2351226328",
                    "name": "Bohou Zhang"
                },
                {
                    "authorId": "2350426005",
                    "name": "Jiawei Cao"
                },
                {
                    "authorId": "2350427710",
                    "name": "Jie Ma"
                },
                {
                    "authorId": "2322524150",
                    "name": "Daoyu Wang"
                },
                {
                    "authorId": "2258714945",
                    "name": "Enhong Chen"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has gained significant attention in recent years for its potential to enhance natural language understanding and generation by combining large-scale retrieval systems with generative models. RAG leverages external knowledge sources, such as documents, databases, or structured data, to improve model performance and generate more accurate and contextually relevant outputs. This survey aims to provide a comprehensive overview of RAG by examining its fundamental components, including retrieval mechanisms, generation processes, and the integration between the two. We discuss the key characteristics of RAG, such as its ability to augment generative models with dynamic external knowledge, and the challenges associated with aligning retrieved information with generative objectives. We also present a taxonomy that categorizes RAG methods, ranging from basic retrieval-augmented approaches to more advanced models incorporating multi-modal data and reasoning capabilities. Additionally, we review the evaluation benchmarks and datasets commonly used to assess RAG systems, along with a detailed exploration of its applications in fields such as question answering, summarization, and information retrieval. Finally, we highlight emerging research directions and opportunities for improving RAG systems, such as enhanced retrieval efficiency, model interpretability, and domain-specific adaptations. This paper concludes by outlining the prospects for RAG in addressing real-world challenges and its potential to drive further advancements in natural language processing.",
            "corpus_id": 277043297,
            "sentences": [
                {
                    "corpus_id": "277043297",
                    "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
                    "text": "Retrieval-Augmented Generation (RAG) has gained significant attention in recent years for its potential to enhance natural language understanding and generation by combining large-scale retrieval systems with generative models. RAG leverages external knowledge sources, such as documents, databases, or structured data, to improve model performance and generate more accurate and contextually relevant outputs. This survey aims to provide a comprehensive overview of RAG by examining its fundamental components, including retrieval mechanisms, generation processes, and the integration between the two. We discuss the key characteristics of RAG, such as its ability to augment generative models with dynamic external knowledge, and the challenges associated with aligning retrieved information with generative objectives. We also present a taxonomy that categorizes RAG methods, ranging from basic retrieval-augmented approaches to more advanced models incorporating multi-modal data and reasoning capabilities. Additionally, we review the evaluation benchmarks and datasets commonly used to assess RAG systems, along with a detailed exploration of its applications in fields such as question answering, summarization, and information retrieval. Finally, we highlight emerging research directions and opportunities for improving RAG systems, such as enhanced retrieval efficiency, model interpretability, and domain-specific adaptations. This paper concludes by outlining the prospects for RAG in addressing real-world challenges and its potential to drive further advancements in natural language processing.",
                    "score": 0.5034804059108214,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9296875
                },
                {
                    "corpus_id": "277043297",
                    "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
                    "text": "Retrieval-Augmented Generation (RAG) [127] has emerged as a key approach that integrates information retrieval with generative models to enhance natural language processing tasks. By leveraging external knowledge sources, RAG systems can generate more accurate and contextually relevant outputs, addressing complex challenges in areas like question answering [119], summarization [85], and open-domain dialogue. In recent years, a variety of RAG methods have been proposed, ranging from basic retrieval-augmented models to more advanced architectures incorporating multi-hop [190] reasoning and memory-augmented techniques [67]. These developments have highlighted the potential of RAG to improve the performance of NLP systems by dynamically combining retrieval and generation in a unified framework. 1 https://github.com/USTCAGI/Awesome-Papers-Retrieval-Augmented-Generation Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \n\nRAG models augment traditional language models by incorporating external knowledge sources, such as documents, databases, or structured data [93,103]., during the generation process. Unlike conventional models that rely solely on pre-trained parameters, RAG systems dynamically retrieve relevant information at generation time, allowing them to produce more informed and contextually accurate outputs. This approach addresses key limitations of traditional language models, such as their inability to access real-time or domain-specific knowledge, and mitigates the challenge of handling out-of-vocabulary or rare entities. For example, in question answering tasks [62,192], RAG models retrieve relevant passages from large corpora to generate more precise and informative answers, while in summarization [85,172], they leverage external documents to provide richer and more comprehensive summaries.",
                    "score": 0.49681754670135403,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 411
                        },
                        {
                            "start": 412,
                            "end": 628
                        },
                        {
                            "start": 629,
                            "end": 803
                        },
                        {
                            "start": 804,
                            "end": 876
                        },
                        {
                            "start": 877,
                            "end": 1157
                        },
                        {
                            "start": 1158,
                            "end": 1248
                        },
                        {
                            "start": 1249,
                            "end": 1286
                        },
                        {
                            "start": 1287,
                            "end": 1416
                        },
                        {
                            "start": 1417,
                            "end": 1462
                        },
                        {
                            "start": 1465,
                            "end": 1616
                        },
                        {
                            "start": 1617,
                            "end": 1647
                        },
                        {
                            "start": 1648,
                            "end": 1866
                        },
                        {
                            "start": 1867,
                            "end": 2088
                        },
                        {
                            "start": 2089,
                            "end": 2364
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 37,
                            "end": 42,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 359,
                            "end": 364,
                            "matchedPaperCorpusId": "86611921"
                        },
                        {
                            "start": 380,
                            "end": 384,
                            "matchedPaperCorpusId": "226965071"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.818359375
                }
            ],
            "relevance_judgement": 0.9296875,
            "relevance_judgment_input_expanded": "# Title: A Survey on Knowledge-Oriented Retrieval-Augmented Generation\n# Venue: arXiv.org\n# Authors: Mingyue Cheng, Yucong Luo, Ouyang Jie, Qi Liu, Huijie Liu, Li Li, Shuo Yu, Bohou Zhang, Jiawei Cao, Jie Ma, Daoyu Wang, Enhong Chen\n## Abstract\nRetrieval-Augmented Generation (RAG) has gained significant attention in recent years for its potential to enhance natural language understanding and generation by combining large-scale retrieval systems with generative models. RAG leverages external knowledge sources, such as documents, databases, or structured data, to improve model performance and generate more accurate and contextually relevant outputs. This survey aims to provide a comprehensive overview of RAG by examining its fundamental components, including retrieval mechanisms, generation processes, and the integration between the two. We discuss the key characteristics of RAG, such as its ability to augment generative models with dynamic external knowledge, and the challenges associated with aligning retrieved information with generative objectives. We also present a taxonomy that categorizes RAG methods, ranging from basic retrieval-augmented approaches to more advanced models incorporating multi-modal data and reasoning capabilities. Additionally, we review the evaluation benchmarks and datasets commonly used to assess RAG systems, along with a detailed exploration of its applications in fields such as question answering, summarization, and information retrieval. Finally, we highlight emerging research directions and opportunities for improving RAG systems, such as enhanced retrieval efficiency, model interpretability, and domain-specific adaptations. This paper concludes by outlining the prospects for RAG in addressing real-world challenges and its potential to drive further advancements in natural language processing.\n## INTRODUCTION\nRetrieval-Augmented Generation (RAG) [127] has emerged as a key approach that integrates information retrieval with generative models to enhance natural language processing tasks. By leveraging external knowledge sources, RAG systems can generate more accurate and contextually relevant outputs, addressing complex challenges in areas like question answering [119], summarization [85], and open-domain dialogue. In recent years, a variety of RAG methods have been proposed, ranging from basic retrieval-augmented models to more advanced architectures incorporating multi-hop [190] reasoning and memory-augmented techniques [67]. These developments have highlighted the potential of RAG to improve the performance of NLP systems by dynamically combining retrieval and generation in a unified framework. 1 https://github.com/USTCAGI/Awesome-Papers-Retrieval-Augmented-Generation Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \n\nRAG models augment traditional language models by incorporating external knowledge sources, such as documents, databases, or structured data [93,103]., during the generation process. Unlike conventional models that rely solely on pre-trained parameters, RAG systems dynamically retrieve relevant information at generation time, allowing them to produce more informed and contextually accurate outputs. This approach addresses key limitations of traditional language models, such as their inability to access real-time or domain-specific knowledge, and mitigates the challenge of handling out-of-vocabulary or rare entities. For example, in question answering tasks [62,192], RAG models retrieve relevant passages from large corpora to generate more precise and informative answers, while in summarization [85,172], they leverage external documents to provide richer and more comprehensive summaries.",
            "reference_string": "[277043297 | Cheng et al. | 2025 | Citations: 6]"
        },
        {
            "title": "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 136,
            "citation_count": 24,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.19543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2299263700",
                    "name": "Yucheng Hu"
                },
                {
                    "authorId": "2220675861",
                    "name": "Yuxing Lu"
                }
            ],
            "abstract": "Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks. This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications. The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment. It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: https://github.com/2471023025/RALM_Survey.",
            "corpus_id": 269457256,
            "sentences": [
                {
                    "corpus_id": "269457256",
                    "title": "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing",
                    "text": "Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks. This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications. The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment. It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: https://github.com/2471023025/RALM_Survey.",
                    "score": 0.4227106823263234,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9248046875
                }
            ],
            "relevance_judgement": 0.9248046875,
            "relevance_judgment_input_expanded": "# Title: RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing\n# Venue: arXiv.org\n# Authors: Yucheng Hu, Yuxing Lu\n## Abstract\nLarge Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks. This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications. The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment. It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: https://github.com/2471023025/RALM_Survey.\n",
            "reference_string": "[269457256 | Hu et al. | 2024 | Citations: 24]"
        },
        {
            "title": "Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 34,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.09136, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109435730",
                    "name": "Aditi Singh"
                },
                {
                    "authorId": "2284079039",
                    "name": "Abul Ehtesham"
                },
                {
                    "authorId": "2310654424",
                    "name": "Saket Kumar"
                },
                {
                    "authorId": "2418738",
                    "name": "T. T. Khoei"
                }
            ],
            "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human like text generation and natural language understanding. However, their reliance on static training data limits their ability to respond to dynamic, real time queries, resulting in outdated or inaccurate outputs. Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by integrating real time data retrieval to provide contextually relevant and up-to-date responses. Despite its promise, traditional RAG systems are constrained by static workflows and lack the adaptability required for multistep reasoning and complex task management. Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embedding autonomous AI agents into the RAG pipeline. These agents leverage agentic design patterns reflection, planning, tool use, and multiagent collaboration to dynamically manage retrieval strategies, iteratively refine contextual understanding, and adapt workflows to meet complex task requirements. This integration enables Agentic RAG systems to deliver unparalleled flexibility, scalability, and context awareness across diverse applications. This survey provides a comprehensive exploration of Agentic RAG, beginning with its foundational principles and the evolution of RAG paradigms. It presents a detailed taxonomy of Agentic RAG architectures, highlights key applications in industries such as healthcare, finance, and education, and examines practical implementation strategies. Additionally, it addresses challenges in scaling these systems, ensuring ethical decision making, and optimizing performance for real-world applications, while providing detailed insights into frameworks and tools for implementing Agentic RAG.",
            "corpus_id": 275570331,
            "sentences": [
                {
                    "corpus_id": "275570331",
                    "title": "Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG",
                    "text": "Large Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human like text generation and natural language understanding. However, their reliance on static training data limits their ability to respond to dynamic, real time queries, resulting in outdated or inaccurate outputs. Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by integrating real time data retrieval to provide contextually relevant and up-to-date responses. Despite its promise, traditional RAG systems are constrained by static workflows and lack the adaptability required for multistep reasoning and complex task management. Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embedding autonomous AI agents into the RAG pipeline. These agents leverage agentic design patterns reflection, planning, tool use, and multiagent collaboration to dynamically manage retrieval strategies, iteratively refine contextual understanding, and adapt workflows to meet complex task requirements. This integration enables Agentic RAG systems to deliver unparalleled flexibility, scalability, and context awareness across diverse applications. This survey provides a comprehensive exploration of Agentic RAG, beginning with its foundational principles and the evolution of RAG paradigms. It presents a detailed taxonomy of Agentic RAG architectures, highlights key applications in industries such as healthcare, finance, and education, and examines practical implementation strategies. Additionally, it addresses challenges in scaling these systems, ensuring ethical decision making, and optimizing performance for real-world applications, while providing detailed insights into frameworks and tools for implementing Agentic RAG.",
                    "score": 0.46077352306505437,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9189453125
                }
            ],
            "relevance_judgement": 0.9189453125,
            "relevance_judgment_input_expanded": "# Title: Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG\n# Venue: arXiv.org\n# Authors: Aditi Singh, Abul Ehtesham, Saket Kumar, T. T. Khoei\n## Abstract\nLarge Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human like text generation and natural language understanding. However, their reliance on static training data limits their ability to respond to dynamic, real time queries, resulting in outdated or inaccurate outputs. Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by integrating real time data retrieval to provide contextually relevant and up-to-date responses. Despite its promise, traditional RAG systems are constrained by static workflows and lack the adaptability required for multistep reasoning and complex task management. Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embedding autonomous AI agents into the RAG pipeline. These agents leverage agentic design patterns reflection, planning, tool use, and multiagent collaboration to dynamically manage retrieval strategies, iteratively refine contextual understanding, and adapt workflows to meet complex task requirements. This integration enables Agentic RAG systems to deliver unparalleled flexibility, scalability, and context awareness across diverse applications. This survey provides a comprehensive exploration of Agentic RAG, beginning with its foundational principles and the evolution of RAG paradigms. It presents a detailed taxonomy of Agentic RAG architectures, highlights key applications in industries such as healthcare, finance, and education, and examines practical implementation strategies. Additionally, it addresses challenges in scaling these systems, ensuring ethical decision making, and optimizing performance for real-world applications, while providing detailed insights into frameworks and tools for implementing Agentic RAG.\n",
            "reference_string": "[275570331 | Singh et al. | 2025 | Citations: 34]"
        },
        {
            "title": "Formal Language Knowledge Corpus for Retrieval Augmented Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.16689, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2336913948",
                    "name": "Majd Zayyad"
                },
                {
                    "authorId": "2727584",
                    "name": "Yossi Adi"
                }
            ],
            "abstract": "The integration of retrieval-augmented techniques with LLMs has shown promise in improving performance across various domains. However, their utility in tasks requiring advanced reasoning, such as generating and evaluating mathematical statements and proofs, remains underexplored. This study explores the use of Lean, a programming language for writing mathematical proofs, to populate the knowledge corpus used by RAG systems. We hope for this to lay the foundation to exploring different methods of using RAGs to improve the performance of LLMs in advanced logical reasoning tasks.",
            "corpus_id": 274982275,
            "sentences": [
                {
                    "corpus_id": "274982275",
                    "title": "Formal Language Knowledge Corpus for Retrieval Augmented Generation",
                    "text": "RAG is a sophisticated framework designed to enhance language models by coupling them with external retrieval systems, addressing limitations inherent in static, solely parameter-based language models. RAG integrates a dual-component architecture where a retriever dynamically searches a structured external corpus for relevant information based on the input query, and a generator LLM uses the retrieved content as context to generate accurate and contextually enriched responses [Gao et al., 2023, Mialon et al., 2023]. This setup mitigates common issues such as hallucinations and factual inaccuracies in language models by grounding generated text in real-world, verified information. In practice, RAG systems employ dense vector embeddings to ensure retrieval relevance, capturing semantic relationships within documents beyond mere keyword matching. The retrieved information is subsequently fed into the generator, allowing it to synthesize data with pre-existing knowledge for enhanced coherence and contextual accuracy. \n\nRAGs can employ two main types of retrieval mechanisms: dense and sparse [Mialon et al., 2023]. Sparse retrievers rely on bag-of-words representations, excelling at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities, enhancing the model's comprehension of related concepts. By appending retrieved documents directly to the model's context, these retrievers allow the language model to ground its responses in a broader context, thereby increasing accuracy and factual consistency across complex tasks. \n\nThe success of retrieval-augmented models in various domains has catalyzed interest in their application to more demanding reasoning tasks. A great representative of such tasks is the construction and verification of mathematical proofs, which requires solving problems step-by-step, and generating precise mathematical statements. Recent approaches, such as chain-of-thought (CoT) prompting [Lewkowycz et al., 2022] combined with retrieval, highlight the potential for retrieval-augmented models to provide sequential reasoning support. These models can generate reasoning paths interspersed with retrieval steps to guide complex problem-solving processes, such as multi-step question answering, enabling models to leverage external information dynamically at each reasoning stage.",
                    "score": 0.43899103315585686,
                    "section_title": "A. Retrieval-Augmented Language Models",
                    "char_start_offset": 3315,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 201
                        },
                        {
                            "start": 202,
                            "end": 521
                        },
                        {
                            "start": 522,
                            "end": 688
                        },
                        {
                            "start": 689,
                            "end": 855
                        },
                        {
                            "start": 856,
                            "end": 1028
                        },
                        {
                            "start": 1031,
                            "end": 1126
                        },
                        {
                            "start": 1127,
                            "end": 1398
                        },
                        {
                            "start": 1399,
                            "end": 1626
                        },
                        {
                            "start": 1629,
                            "end": 1768
                        },
                        {
                            "start": 1769,
                            "end": 1960
                        },
                        {
                            "start": 1961,
                            "end": 2166
                        },
                        {
                            "start": 2167,
                            "end": 2411
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90283203125
                }
            ],
            "relevance_judgement": 0.90283203125,
            "relevance_judgment_input_expanded": "# Title: Formal Language Knowledge Corpus for Retrieval Augmented Generation\n# Venue: arXiv.org\n# Authors: Majd Zayyad, Yossi Adi\n## Abstract\nThe integration of retrieval-augmented techniques with LLMs has shown promise in improving performance across various domains. However, their utility in tasks requiring advanced reasoning, such as generating and evaluating mathematical statements and proofs, remains underexplored. This study explores the use of Lean, a programming language for writing mathematical proofs, to populate the knowledge corpus used by RAG systems. We hope for this to lay the foundation to exploring different methods of using RAGs to improve the performance of LLMs in advanced logical reasoning tasks.\n## A. Retrieval-Augmented Language Models\nRAG is a sophisticated framework designed to enhance language models by coupling them with external retrieval systems, addressing limitations inherent in static, solely parameter-based language models. RAG integrates a dual-component architecture where a retriever dynamically searches a structured external corpus for relevant information based on the input query, and a generator LLM uses the retrieved content as context to generate accurate and contextually enriched responses [Gao et al., 2023, Mialon et al., 2023]. This setup mitigates common issues such as hallucinations and factual inaccuracies in language models by grounding generated text in real-world, verified information. In practice, RAG systems employ dense vector embeddings to ensure retrieval relevance, capturing semantic relationships within documents beyond mere keyword matching. The retrieved information is subsequently fed into the generator, allowing it to synthesize data with pre-existing knowledge for enhanced coherence and contextual accuracy. \n\nRAGs can employ two main types of retrieval mechanisms: dense and sparse [Mialon et al., 2023]. Sparse retrievers rely on bag-of-words representations, excelling at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities, enhancing the model's comprehension of related concepts. By appending retrieved documents directly to the model's context, these retrievers allow the language model to ground its responses in a broader context, thereby increasing accuracy and factual consistency across complex tasks. \n\nThe success of retrieval-augmented models in various domains has catalyzed interest in their application to more demanding reasoning tasks. A great representative of such tasks is the construction and verification of mathematical proofs, which requires solving problems step-by-step, and generating precise mathematical statements. Recent approaches, such as chain-of-thought (CoT) prompting [Lewkowycz et al., 2022] combined with retrieval, highlight the potential for retrieval-augmented models to provide sequential reasoning support. These models can generate reasoning paths interspersed with retrieval steps to guide complex problem-solving processes, such as multi-step question answering, enabling models to leverage external information dynamically at each reasoning stage.",
            "reference_string": "[274982275 | Zayyad et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Reliable, Adaptable, and Attributable Language Models with Retrieval",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 140,
            "citation_count": 61,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.03187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "49164966",
                    "name": "Zexuan Zhong"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                },
                {
                    "authorId": "2303396379",
                    "name": "Pang Wei Koh"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                },
                {
                    "authorId": "2072801764",
                    "name": "Wen-tau Yih"
                }
            ],
            "abstract": "Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose retrieval-augmented LMs. This involves a reconsideration of datastores and retrievers, the exploration of pipelines with improved retriever-LM interaction, and significant investment in infrastructure for efficient training and inference.",
            "corpus_id": 268248911,
            "sentences": [
                {
                    "corpus_id": "268248911",
                    "title": "Reliable, Adaptable, and Attributable Language Models with Retrieval",
                    "text": "Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose retrieval-augmented LMs. This involves a reconsideration of datastores and retrievers, the exploration of pipelines with improved retriever-LM interaction, and significant investment in infrastructure for efficient training and inference.",
                    "score": 0.3685763966626367,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86865234375
                },
                {
                    "corpus_id": "268248911",
                    "title": "Reliable, Adaptable, and Attributable Language Models with Retrieval",
                    "text": "Address These Issues? \n\nIn this section, we discuss how retrieval-augmented LMs can alleviate the aforementioned issues in parametric LMs. \n\nDefinition. A retrieval-augmented LM (Figure 1, bottom; detailed in Figure 2) typically consists of two key components: a retriever R and a parametric LM \u03b8. The retriever builds a search index I1 based on documents in the datastore D. During inference time, given an input sequence x, the retriever finds relevant text z 2 from the inference datastore, leveraging an index I: z = f R,I (x). Subsequently, the LM \u03b8 uses both the original prompt and the retrieved text to predict the output y: y = f \u03b8 (x, z). \n\nOrigins, progress, and recent shift. \n\nThe concept of retrieval augmentation has been extensively explored across various machine learning domains (Tian et al., 2019). In NLP, earlier efforts have been applied to specific tasks such as QA and machine translation. Chen et al. (2017) introduce DrQA, which combines a term-based information retrieval (IR) system with a neural QA model to answer knowledgeintensive questions. While IR and such task LMs were initially studied separately, several work explores more organic combinations of retrieval and LM by pre-training the two components jointly or sequentially, including REALM (Guu et al., 2020), RAG (Lewis et al., 2020a), RETRO (Borgeaud et al., 2022), etc. \n\nSuch earlier work designed special architectures and training objectives for the retrieval-augmented LM. Most recently, there has been a shift of view of retrieval-augmented LMsinstead of training retrieval-augmented LMs from scratch, some work supplementary integrate retrieval on top of existing powerful parametric LMs (e.g., GPT-3;Black et al. 2022) without any additional training. Such methods-often referred to simply as Retrieval-Augmented Generation or RAG-concatenate the original input sequence x with retrieved text z when prompting, yielding significant improvements over the base parametric LMs on certain knowledgeintensive tasks (Ram et al., 2023;Shi et al., 2023c).",
                    "score": 0.5038035324767401,
                    "section_title": "How Can Retrieval-Augmented LMs",
                    "char_start_offset": 9288,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 21
                        },
                        {
                            "start": 24,
                            "end": 138
                        },
                        {
                            "start": 141,
                            "end": 152
                        },
                        {
                            "start": 153,
                            "end": 297
                        },
                        {
                            "start": 298,
                            "end": 531
                        },
                        {
                            "start": 532,
                            "end": 648
                        },
                        {
                            "start": 651,
                            "end": 687
                        },
                        {
                            "start": 690,
                            "end": 818
                        },
                        {
                            "start": 819,
                            "end": 914
                        },
                        {
                            "start": 915,
                            "end": 1074
                        },
                        {
                            "start": 1075,
                            "end": 1363
                        },
                        {
                            "start": 1366,
                            "end": 1470
                        },
                        {
                            "start": 1471,
                            "end": 1752
                        },
                        {
                            "start": 1753,
                            "end": 2048
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 798,
                            "end": 817,
                            "matchedPaperCorpusId": "57759353"
                        },
                        {
                            "start": 915,
                            "end": 933,
                            "matchedPaperCorpusId": "3618568"
                        },
                        {
                            "start": 1281,
                            "end": 1299,
                            "matchedPaperCorpusId": "211204736"
                        },
                        {
                            "start": 1334,
                            "end": 1357,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 1701,
                            "end": 1719,
                            "matchedPaperCorpusId": "248177957"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8642578125
                }
            ],
            "relevance_judgement": 0.86865234375,
            "relevance_judgment_input_expanded": "# Title: Reliable, Adaptable, and Attributable Language Models with Retrieval\n# Venue: arXiv.org\n# Authors: Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke S. Zettlemoyer, Hanna Hajishirzi, Wen-tau Yih\n## Abstract\nParametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose retrieval-augmented LMs. This involves a reconsideration of datastores and retrievers, the exploration of pipelines with improved retriever-LM interaction, and significant investment in infrastructure for efficient training and inference.\n## How Can Retrieval-Augmented LMs\nAddress These Issues? \n\nIn this section, we discuss how retrieval-augmented LMs can alleviate the aforementioned issues in parametric LMs. \n\nDefinition. A retrieval-augmented LM (Figure 1, bottom; detailed in Figure 2) typically consists of two key components: a retriever R and a parametric LM \u03b8. The retriever builds a search index I1 based on documents in the datastore D. During inference time, given an input sequence x, the retriever finds relevant text z 2 from the inference datastore, leveraging an index I: z = f R,I (x). Subsequently, the LM \u03b8 uses both the original prompt and the retrieved text to predict the output y: y = f \u03b8 (x, z). \n\nOrigins, progress, and recent shift. \n\nThe concept of retrieval augmentation has been extensively explored across various machine learning domains (Tian et al., 2019). In NLP, earlier efforts have been applied to specific tasks such as QA and machine translation. Chen et al. (2017) introduce DrQA, which combines a term-based information retrieval (IR) system with a neural QA model to answer knowledgeintensive questions. While IR and such task LMs were initially studied separately, several work explores more organic combinations of retrieval and LM by pre-training the two components jointly or sequentially, including REALM (Guu et al., 2020), RAG (Lewis et al., 2020a), RETRO (Borgeaud et al., 2022), etc. \n\nSuch earlier work designed special architectures and training objectives for the retrieval-augmented LM. Most recently, there has been a shift of view of retrieval-augmented LMsinstead of training retrieval-augmented LMs from scratch, some work supplementary integrate retrieval on top of existing powerful parametric LMs (e.g., GPT-3;Black et al. 2022) without any additional training. Such methods-often referred to simply as Retrieval-Augmented Generation or RAG-concatenate the original input sequence x with retrieved text z when prompting, yielding significant improvements over the base parametric LMs on certain knowledgeintensive tasks (Ram et al., 2023;Shi et al., 2023c).",
            "reference_string": "[268248911 | Asai et al. | 2024 | Citations: 61]"
        },
        {
            "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
            "venue": "Knowledge-Based Systems",
            "year": 2025,
            "reference_count": 137,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.13947, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2342276561",
                    "name": "Lilian Some"
                },
                {
                    "authorId": "2341600949",
                    "name": "Wenli Yang"
                },
                {
                    "authorId": "2342277330",
                    "name": "Michael Bain"
                },
                {
                    "authorId": "2341910700",
                    "name": "Byeong Kang"
                }
            ],
            "abstract": null,
            "corpus_id": 275906690,
            "sentences": [
                {
                    "corpus_id": "275906690",
                    "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
                    "text": "Modern LLMs are primarily based on the transformer architecture, which can be easily adapted for specific tasks with robust performance, leading to large gains on downstream tasks like text classification, language understanding, reference resolution, common-sense inference, summarization, and machine translation [7]. Building upon the transformer, LLMs are typically designed using one of three main architectures: encoder, decoder and encoder-decoder [24,25]. The Table 1 summarized these three main architectural design, highlighting their examples, primary use cases, and knowledge integration benefits. \n\nEncoder LLMs: These models, such as BERT, RoBERTa, focus on text classification and retrieval tasks. They process input text bidirectionally, allowing them to generate high-quality embeddings that improve retrieval-augmented systems. By learning contextual relationships, encoder models enhance semantic search, information extraction, and retrieval-based NLP applications. \n\nDecoder LLMs: Examples include GPT-4, LLaMA-3, DeepSeek LLM, and OpenAI o1, which are mainly designed for tasks such as text generation, dialogue systems, code synthesis, and creative writing. These models generate text auto-regressively, predicting each token based on prior context. DeepSeek LLM exemplifies efficient open-source performance across both understanding and generation tasks, while OpenAI o1 introduces a novel reasoning-optimized architecture that differentiates it from conventional decoder models. Instead of allocating a fixed amount of computational resources uniformly across all inputs, o1 dynamically adjusts its internal computation in response to task complexity. Although highly capable in generative tasks, decoder models often benefit from retrieval-augmented generation (RAG), which grounds outputs in external knowledge to reduce hallucinations and improve factual accuracy in knowledge-intensive domains. \n\nEncoder-Decoder LLMs: Models like T5 and BART leverage both an encoder and a decoder to transform input text into output sequences, making them well-suited for machine translation, text summarization, and paraphrasing. Their dual-component structure allows them to integrate effectively with structured knowledge bases (KBs) and semantic reasoning systems, enhancing their ability to provide factually accurate and contextually relevant outputs.",
                    "score": 0.4892592503714181,
                    "section_title": "LLM architecture",
                    "char_start_offset": 8250,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 463
                        },
                        {
                            "start": 464,
                            "end": 609
                        },
                        {
                            "start": 612,
                            "end": 712
                        },
                        {
                            "start": 713,
                            "end": 845
                        },
                        {
                            "start": 846,
                            "end": 985
                        },
                        {
                            "start": 988,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1272
                        },
                        {
                            "start": 1273,
                            "end": 1504
                        },
                        {
                            "start": 1505,
                            "end": 1677
                        },
                        {
                            "start": 1678,
                            "end": 1924
                        },
                        {
                            "start": 1927,
                            "end": 2145
                        },
                        {
                            "start": 2146,
                            "end": 2372
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 315,
                            "end": 318,
                            "matchedPaperCorpusId": "267675587"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8525390625
                }
            ],
            "relevance_judgement": 0.8525390625,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods\n# Venue: Knowledge-Based Systems\n# Authors: Lilian Some, Wenli Yang, Michael Bain, Byeong Kang\n## Abstract\nNone\n## LLM architecture\nModern LLMs are primarily based on the transformer architecture, which can be easily adapted for specific tasks with robust performance, leading to large gains on downstream tasks like text classification, language understanding, reference resolution, common-sense inference, summarization, and machine translation [7]. Building upon the transformer, LLMs are typically designed using one of three main architectures: encoder, decoder and encoder-decoder [24,25]. The Table 1 summarized these three main architectural design, highlighting their examples, primary use cases, and knowledge integration benefits. \n\nEncoder LLMs: These models, such as BERT, RoBERTa, focus on text classification and retrieval tasks. They process input text bidirectionally, allowing them to generate high-quality embeddings that improve retrieval-augmented systems. By learning contextual relationships, encoder models enhance semantic search, information extraction, and retrieval-based NLP applications. \n\nDecoder LLMs: Examples include GPT-4, LLaMA-3, DeepSeek LLM, and OpenAI o1, which are mainly designed for tasks such as text generation, dialogue systems, code synthesis, and creative writing. These models generate text auto-regressively, predicting each token based on prior context. DeepSeek LLM exemplifies efficient open-source performance across both understanding and generation tasks, while OpenAI o1 introduces a novel reasoning-optimized architecture that differentiates it from conventional decoder models. Instead of allocating a fixed amount of computational resources uniformly across all inputs, o1 dynamically adjusts its internal computation in response to task complexity. Although highly capable in generative tasks, decoder models often benefit from retrieval-augmented generation (RAG), which grounds outputs in external knowledge to reduce hallucinations and improve factual accuracy in knowledge-intensive domains. \n\nEncoder-Decoder LLMs: Models like T5 and BART leverage both an encoder and a decoder to transform input text into output sequences, making them well-suited for machine translation, text summarization, and paraphrasing. Their dual-component structure allows them to integrate effectively with structured knowledge bases (KBs) and semantic reasoning systems, enhancing their ability to provide factually accurate and contextually relevant outputs.",
            "reference_string": "[275906690 | Some et al. | 2025 | Citations: 0]"
        },
        {
            "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
            "venue": "Knowledge Discovery and Data Mining",
            "year": 2024,
            "reference_count": 183,
            "citation_count": 248,
            "influential_citation_count": 12,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.06211, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2291324376",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2301103404",
                    "name": "Yujuan Ding"
                },
                {
                    "authorId": "2301015154",
                    "name": "Liang-bo Ning"
                },
                {
                    "authorId": "2266567589",
                    "name": "Shijie Wang"
                },
                {
                    "authorId": "2301167804",
                    "name": "Hengyun Li"
                },
                {
                    "authorId": "2297846971",
                    "name": "Dawei Yin"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                },
                {
                    "authorId": "2297955888",
                    "name": "Qing Li"
                }
            ],
            "abstract": "As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at: https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/",
            "corpus_id": 269740933,
            "sentences": [
                {
                    "corpus_id": "269740933",
                    "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
                    "text": "Retrieval-augmented generation (RAG), a cutting-edge AI technique, has achieved remarkable success across various applications, including recommendation, molecule generation, protein representation, and software engineering, owing to the potent capabilities of retrieval in providing supplementary information to enhance generation performance.Recently, increasing efforts have been made to alleviate the limitations of large language models (LLMs), such as hallucination and out-of-date internal knowledge, by leveraging retrieval to provide the latest auxiliary information and teaching LLMs to harness the retrieved external knowledge.With the rapid advancements in retrieval-augmented large language models (RA-LLMs), there is a pressing need for a comprehensive and systematic overview.To bridge this gap, in this paper, we comprehensively review the RA-LLMs from the perspectives of morel architecture, training strategy, and application area, providing researchers with an in-depth understanding.Moreover, since the studies of RA-LLMs are still in the early stage, we also discuss the current limitations and several potential research directions for future research.",
                    "score": 0.38704866882156796,
                    "section_title": "CONCLUSION",
                    "char_start_offset": 60360,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 344
                        },
                        {
                            "start": 344,
                            "end": 638
                        },
                        {
                            "start": 638,
                            "end": 791
                        },
                        {
                            "start": 791,
                            "end": 1003
                        },
                        {
                            "start": 1003,
                            "end": 1174
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84130859375
                }
            ],
            "relevance_judgement": 0.84130859375,
            "relevance_judgment_input_expanded": "# Title: A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\n# Venue: Knowledge Discovery and Data Mining\n# Authors: Wenqi Fan, Yujuan Ding, Liang-bo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, Qing Li\n## Abstract\nAs one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at: https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/\n## CONCLUSION\nRetrieval-augmented generation (RAG), a cutting-edge AI technique, has achieved remarkable success across various applications, including recommendation, molecule generation, protein representation, and software engineering, owing to the potent capabilities of retrieval in providing supplementary information to enhance generation performance.Recently, increasing efforts have been made to alleviate the limitations of large language models (LLMs), such as hallucination and out-of-date internal knowledge, by leveraging retrieval to provide the latest auxiliary information and teaching LLMs to harness the retrieved external knowledge.With the rapid advancements in retrieval-augmented large language models (RA-LLMs), there is a pressing need for a comprehensive and systematic overview.To bridge this gap, in this paper, we comprehensively review the RA-LLMs from the perspectives of morel architecture, training strategy, and application area, providing researchers with an in-depth understanding.Moreover, since the studies of RA-LLMs are still in the early stage, we also discuss the current limitations and several potential research directions for future research.",
            "reference_string": "[269740933 | Fan et al. | 2024 | Citations: 248]"
        },
        {
            "title": "ExpertRAG: Efficient RAG with Mixture of Experts - Optimizing Context Retrieval for Adaptive LLM Responses",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 94,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.08744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2354181125",
                    "name": "Esmail Gumaan"
                }
            ],
            "abstract": "ExpertRAG is a novel theoretical framework that integrates Mixture-of-Experts (MoE) architectures with Retrieval Augmented Generation (RAG) to advance the efficiency and accuracy of knowledge-intensive language modeling. We propose a dynamic retrieval gating mechanism coupled with expert routing, enabling the model to selectively consult an external knowledge store or rely on specialized internal experts based on the query's needs. The paper lays out the theoretical foundations of ExpertRAG, including a probabilistic formulation that treats retrieval and expert selection as latent decisions, and mathematical justifications for its efficiency in both computation and knowledge utilization. We derive formulae to quantify the expected computational cost savings from selective retrieval and the capacity gains from sparse expert utilization. A comparative analysis positions ExpertRAG against standard RAG (with always-on retrieval) and pure MoE models (e.g., Switch Transformer, Mixtral) to highlight its unique balance between parametric knowledge and non-parametric retrieval. We also outline an experimental validation strategy, proposing benchmarks and evaluation protocols to test ExpertRAG's performance on factual recall, generalization, and inference efficiency. The proposed framework, although presented theoretically, is supported by insights from prior work in RAG and MoE, and is poised to provide more factual, efficient, and adaptive generation by leveraging the best of both paradigms. In summary, ExpertRAG contributes a new perspective on scaling and augmenting language models, backed by a thorough analysis and a roadmap for empirical validation.",
            "corpus_id": 277780258,
            "sentences": [
                {
                    "corpus_id": "277780258",
                    "title": "ExpertRAG: Efficient RAG with Mixture of Experts - Optimizing Context Retrieval for Adaptive LLM Responses",
                    "text": "Large language models (LLMs) have achieved remarkable success in many NLP tasks, yet they face persistent challenges in knowledge-intensive applications. A key limitation is the reliance on storing factual knowledge purely in model parameters. As models grow, their ability to recall or update specific facts becomes problematic [1]. Retrieval-Augmented Generation (RAG) was introduced to address this by equipping models with access to external nonparametric memory (e.g. a text corpus or database), allowing them to fetch relevant information on-the-fly [2]. RAG combines a parametric neural generator with a retrieval module, producing outputs that are more specific and factual than those of parametric-only models [1]. However, standard RAG pipelines retrieve documents for every query, which can be inefficient when the model's internal knowledge is already sufficient. Unnecessary retrieval incurs extra latency and may introduce distractors, highlighting a need for dynamic retrieval strategies that invoke external lookup only when needed. \n\nIn parallel, Mixture-of-Experts (MoE) architectures have emerged as a solution for scaling model capacity without proportional increases in computation [3]. In an MoE model, multiple expert subnetworks (e.g. feed-forward layers) are trained, and a learned gating function routes each input token or example to a subset of these experts. This sparse activation means only a few experts process each token, allowing the total parameter count to increase (potentially to trillions) while keeping the compute per token comparable to a much smaller model [4]. Notable MoE instances include 2 Literature Review",
                    "score": 0.44760771816579314,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 153
                        },
                        {
                            "start": 154,
                            "end": 243
                        },
                        {
                            "start": 244,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 560
                        },
                        {
                            "start": 561,
                            "end": 723
                        },
                        {
                            "start": 724,
                            "end": 875
                        },
                        {
                            "start": 876,
                            "end": 1048
                        },
                        {
                            "start": 1051,
                            "end": 1207
                        },
                        {
                            "start": 1208,
                            "end": 1258
                        },
                        {
                            "start": 1259,
                            "end": 1387
                        },
                        {
                            "start": 1388,
                            "end": 1605
                        },
                        {
                            "start": 1606,
                            "end": 1655
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 329,
                            "end": 332,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 719,
                            "end": 722,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1203,
                            "end": 1206,
                            "matchedPaperCorpusId": "247951931"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83447265625
                },
                {
                    "corpus_id": "277780258",
                    "title": "ExpertRAG: Efficient RAG with Mixture of Experts - Optimizing Context Retrieval for Adaptive LLM Responses",
                    "text": "We have presented ExpertRAG, a new conceptual framework that marries the Mixture-of-Experts architecture with Retrieval-Augmented Generation. Through this integration, ExpertRAG brings together two powerful ideas in modern NLP: (1) sparse expert models for efficient scaling of language model capacity, and (2) dynamic retrieval of external knowledge for enhanced factual accuracy and up-to-date information. We detailed how ExpertRAG is constructedintroducing a retrieval gating mechanism that decides when to consult external memory and an expert routing mechanism that distributes the query and context across specialized sub-networks. Our theoretical analysis provided insights into why this approach is compelling: we showed that selective retrieval can yield significant efficiency gains without sacrificing performance, and we quantified how MoE routing keeps computation scalable even as we increase model parameters. We also formulated the balance between retrieved and parametric knowledge in a probabilistic mixture form, giving a principled view of ExpertRAG's decision-making. Comparatively, ExpertRAG addresses limitations of prior models. It improves upon standard RAG by avoiding needless retrieval and leveraging more parametric knowledge when appropriate, and it extends MoE models by providing a way to handle queries beyond the model's stored knowledge. In our discussion, we positioned ExpertRAG as a middle ground between oneshot retrieval models and agentic multi-step systems, aiming to capture much of the benefit of the latter within the efficiency of the former. The proposed experiments will evaluate these claims rigorously, and we expect to confirm improvements in both accuracy and speed against strong baselines on knowledge-intensive benchmarks. \n\nComparatively, ExpertRAG addresses limitations of prior models. It improves upon standard RAG by avoiding needless retrieval and leveraging more parametric knowledge when appropriate, and it extends MoE models by providing a way to handle queries beyond the model's stored knowledge. In our discussion, we positioned ExpertRAG as a middle ground between one-shot retrieval models and agentic multi-step systems, aiming to capture much of the benefit of the latter within the efficiency of the former. The proposed experiments will evaluate these claims rigorously, and we expect to confirm improvements in both accuracy and speed against strong baselines on knowledge-intensive benchmarks. \n\nThis work opens several future research directions and challenges to be addressed:",
                    "score": 0.3673091203261256,
                    "section_title": "Conclusion",
                    "char_start_offset": 79816,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 141
                        },
                        {
                            "start": 142,
                            "end": 408
                        },
                        {
                            "start": 409,
                            "end": 638
                        },
                        {
                            "start": 639,
                            "end": 925
                        },
                        {
                            "start": 926,
                            "end": 1089
                        },
                        {
                            "start": 1090,
                            "end": 1153
                        },
                        {
                            "start": 1154,
                            "end": 1373
                        },
                        {
                            "start": 1374,
                            "end": 1589
                        },
                        {
                            "start": 1590,
                            "end": 1778
                        },
                        {
                            "start": 1781,
                            "end": 1844
                        },
                        {
                            "start": 1845,
                            "end": 2064
                        },
                        {
                            "start": 2065,
                            "end": 2281
                        },
                        {
                            "start": 2282,
                            "end": 2470
                        },
                        {
                            "start": 2473,
                            "end": 2555
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7255859375
                }
            ],
            "relevance_judgement": 0.83447265625,
            "relevance_judgment_input_expanded": "# Title: ExpertRAG: Efficient RAG with Mixture of Experts - Optimizing Context Retrieval for Adaptive LLM Responses\n# Venue: arXiv.org\n# Authors: Esmail Gumaan\n## Abstract\nExpertRAG is a novel theoretical framework that integrates Mixture-of-Experts (MoE) architectures with Retrieval Augmented Generation (RAG) to advance the efficiency and accuracy of knowledge-intensive language modeling. We propose a dynamic retrieval gating mechanism coupled with expert routing, enabling the model to selectively consult an external knowledge store or rely on specialized internal experts based on the query's needs. The paper lays out the theoretical foundations of ExpertRAG, including a probabilistic formulation that treats retrieval and expert selection as latent decisions, and mathematical justifications for its efficiency in both computation and knowledge utilization. We derive formulae to quantify the expected computational cost savings from selective retrieval and the capacity gains from sparse expert utilization. A comparative analysis positions ExpertRAG against standard RAG (with always-on retrieval) and pure MoE models (e.g., Switch Transformer, Mixtral) to highlight its unique balance between parametric knowledge and non-parametric retrieval. We also outline an experimental validation strategy, proposing benchmarks and evaluation protocols to test ExpertRAG's performance on factual recall, generalization, and inference efficiency. The proposed framework, although presented theoretically, is supported by insights from prior work in RAG and MoE, and is poised to provide more factual, efficient, and adaptive generation by leveraging the best of both paradigms. In summary, ExpertRAG contributes a new perspective on scaling and augmenting language models, backed by a thorough analysis and a roadmap for empirical validation.\n## Introduction\nLarge language models (LLMs) have achieved remarkable success in many NLP tasks, yet they face persistent challenges in knowledge-intensive applications. A key limitation is the reliance on storing factual knowledge purely in model parameters. As models grow, their ability to recall or update specific facts becomes problematic [1]. Retrieval-Augmented Generation (RAG) was introduced to address this by equipping models with access to external nonparametric memory (e.g. a text corpus or database), allowing them to fetch relevant information on-the-fly [2]. RAG combines a parametric neural generator with a retrieval module, producing outputs that are more specific and factual than those of parametric-only models [1]. However, standard RAG pipelines retrieve documents for every query, which can be inefficient when the model's internal knowledge is already sufficient. Unnecessary retrieval incurs extra latency and may introduce distractors, highlighting a need for dynamic retrieval strategies that invoke external lookup only when needed. \n\nIn parallel, Mixture-of-Experts (MoE) architectures have emerged as a solution for scaling model capacity without proportional increases in computation [3]. In an MoE model, multiple expert subnetworks (e.g. feed-forward layers) are trained, and a learned gating function routes each input token or example to a subset of these experts. This sparse activation means only a few experts process each token, allowing the total parameter count to increase (potentially to trillions) while keeping the compute per token comparable to a much smaller model [4]. Notable MoE instances include 2 Literature Review\n\n## Conclusion\nWe have presented ExpertRAG, a new conceptual framework that marries the Mixture-of-Experts architecture with Retrieval-Augmented Generation. Through this integration, ExpertRAG brings together two powerful ideas in modern NLP: (1) sparse expert models for efficient scaling of language model capacity, and (2) dynamic retrieval of external knowledge for enhanced factual accuracy and up-to-date information. We detailed how ExpertRAG is constructedintroducing a retrieval gating mechanism that decides when to consult external memory and an expert routing mechanism that distributes the query and context across specialized sub-networks. Our theoretical analysis provided insights into why this approach is compelling: we showed that selective retrieval can yield significant efficiency gains without sacrificing performance, and we quantified how MoE routing keeps computation scalable even as we increase model parameters. We also formulated the balance between retrieved and parametric knowledge in a probabilistic mixture form, giving a principled view of ExpertRAG's decision-making. Comparatively, ExpertRAG addresses limitations of prior models. It improves upon standard RAG by avoiding needless retrieval and leveraging more parametric knowledge when appropriate, and it extends MoE models by providing a way to handle queries beyond the model's stored knowledge. In our discussion, we positioned ExpertRAG as a middle ground between oneshot retrieval models and agentic multi-step systems, aiming to capture much of the benefit of the latter within the efficiency of the former. The proposed experiments will evaluate these claims rigorously, and we expect to confirm improvements in both accuracy and speed against strong baselines on knowledge-intensive benchmarks. \n\nComparatively, ExpertRAG addresses limitations of prior models. It improves upon standard RAG by avoiding needless retrieval and leveraging more parametric knowledge when appropriate, and it extends MoE models by providing a way to handle queries beyond the model's stored knowledge. In our discussion, we positioned ExpertRAG as a middle ground between one-shot retrieval models and agentic multi-step systems, aiming to capture much of the benefit of the latter within the efficiency of the former. The proposed experiments will evaluate these claims rigorously, and we expect to confirm improvements in both accuracy and speed against strong baselines on knowledge-intensive benchmarks. \n\nThis work opens several future research directions and challenges to be addressed:",
            "reference_string": "[277780258 | Gumaan | 2025 | Citations: 0]"
        },
        {
            "title": "HIRO: Hierarchical Information Retrieval Optimization",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 27,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.09979, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306783450",
                    "name": "Krish Goel"
                },
                {
                    "authorId": "2306782235",
                    "name": "Mahek Chandak"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has revolutionized natural language processing by dynamically integrating external knowledge into Large Language Models (LLMs), addressing their limitation of static training datasets. Recent implementations of RAG leverage hierarchical data structures, which organize documents at various levels of summarization and information density. This complexity, however, can cause LLMs to\"choke\"on information overload, necessitating more sophisticated querying mechanisms. In this context, we introduce Hierarchical Information Retrieval Optimization (HIRO), a novel querying approach that employs a Depth-First Search (DFS)-based recursive similarity score calculation and branch pruning. This method uniquely minimizes the context delivered to the LLM without informational loss, effectively managing the challenge of excessive data. HIRO's refined approach is validated by a 10.85% improvement in performance on the NarrativeQA dataset.",
            "corpus_id": 270521566,
            "sentences": [
                {
                    "corpus_id": "270521566",
                    "title": "HIRO: Hierarchical Information Retrieval Optimization",
                    "text": "The advent of Large Language Models (LLMs) has brought about significant transformation in artificial intelligence and natural language processing, enabling advancements in tasks such as text generation, translation, and question-answering [3]. As these models increase in size and complexity, they have evolved into highly effective standalone knowledge repositories, embedding vast amounts of factual information within their parameters [12,19]. However, LLMs are inherently constrained by the static nature of their training datasets, which limits their adaptability to continuously evolving real-world information, where the breadth and depth of knowledge required far exceed any static training corpus [10]. This limitation underscores a critical gap in LLM design, where fine-tuning falls short, particularly when updates and domain-specific information are necessary [8]. \n\nIn response to these challenges, the field has witnessed the emergence of Retrieval-Augmented Generation (RAG) models, evolving into what are now known as Retrieval-Augmented Language Models (RALMs). These new paradigms enhance LLMs by integrating dynamic external knowledge through sophisticated retrieval mechanisms, effectively addressing the inherent limitations of static knowledge bases [8]. Inspired by the success of open-domain question answering systems, RALMs utilize indexed text databases to enhance model responses by presenting retrieved information alongside input queries [1,13]. This integration not only boosts the models' ability to provide current, domain-specific knowledge but also enhances interpretability and source traceability, offering a stark contrast to the opaque parametric knowledge of traditional LLMs. \n\nAmidst this evolving landscape, novel approaches to information retrieval, such as the use of hierarchical data structures for organizing documents, represent significant advancements. Techniques like RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval, which clusters and summarizes texts into a tree structure, demonstrate the potential to overcome traditional limitations by providing both granular and high-level insights [4,5,15,20]. However, the adoption of hierarchical data structures poses challenges, particularly the overwhelming amount of context they can return, which may not always align with the query requirements [15]. For queries demanding extensive information, this can result in insufficient data being returned, whereas for simpler inquiries, it might overwhelm LLMs with excessive context.",
                    "score": 0.375857175152596,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 244
                        },
                        {
                            "start": 245,
                            "end": 447
                        },
                        {
                            "start": 448,
                            "end": 712
                        },
                        {
                            "start": 713,
                            "end": 878
                        },
                        {
                            "start": 881,
                            "end": 1080
                        },
                        {
                            "start": 1081,
                            "end": 1278
                        },
                        {
                            "start": 1279,
                            "end": 1477
                        },
                        {
                            "start": 1478,
                            "end": 1718
                        },
                        {
                            "start": 1721,
                            "end": 1905
                        },
                        {
                            "start": 1906,
                            "end": 2177
                        },
                        {
                            "start": 2178,
                            "end": 2375
                        },
                        {
                            "start": 2376,
                            "end": 2552
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 443,
                            "end": 446,
                            "matchedPaperCorpusId": "209515274"
                        },
                        {
                            "start": 874,
                            "end": 877,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 1274,
                            "end": 1277,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 1473,
                            "end": 1476,
                            "matchedPaperCorpusId": "256459451"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83447265625
                },
                {
                    "corpus_id": "270521566",
                    "title": "HIRO: Hierarchical Information Retrieval Optimization",
                    "text": "Final generated response from the augmented prompt Augmented prompt returned to the LM Figure 1: Architectural Overview of a Retrieval-Augmented Language Model (RALM). A query is inputted, processed through an embedding model to generate a vector representation, which is then matched in a vector database to retrieve relevant contexts. These contexts are fed, along with the original query, into the LLM, resulting in an informed response. \n\nThe adoption of hierarchical structures in RAG models, exemplified by RAPTOR [15], represents a pivotal shift towards more organized and efficient information storage and retrieval. RAPTOR's approach, through recursive summarisation to create a hierarchical tree, facilitates nuanced access to information at various levels of abstraction. This innovation not only improves coherence and information density tailored to specific tasks but also encourages exploration into other hierarchical frameworks like graphs for enhanced document interrelation preservation. These advancements signal a new phase in computational linguistics, focusing on sophisticated, hierarchical data use for better knowledge representation and retrieval. However, existing hierarchical models often face challenges in balancing information density and retrieval efficiency, leading to potential information overload or loss. \n\nDynamic Querying Mechanisms in RAG Models address the evolving nature of queries where traditional static retrieval methods, such as TF-IDF and BM25, have been complemented by more adaptive techniques. The Tree Traversal method progressively selects the top-k most relevant nodes from each layer of the hierarchical data based on similarity, allowing for adjustments in depth and number of nodes to control the information's specificity and breadth. Conversely, the Collapsed Tree approach flattens the hierarchy to allow for simultaneous comparison of all nodes. Figure 5 and Figure 6 illustrate the workings of these algorithms, providing visual representations. \n\nDespite these innovations, a limitation arises from the static quantity of data retrieved, which may not always align with the query's needs, leading to potential information overload for LLMs. This static k value, irrespective of query complexity, can hamper LLMs' performance by either providing insufficient data for complex queries or overwhelming them with excessive context for simpler ones. Additionally, returning both parent and child nodes due to similar matching levels often results in inefficient, redundant information. Optimizing the retrieval process to avoid such duplication is crucial for improving LLM performance in hierarchical document structures.",
                    "score": 0.4024982055363106,
                    "section_title": "Language Model",
                    "char_start_offset": 4240,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 168,
                            "end": 336
                        },
                        {
                            "start": 337,
                            "end": 440
                        },
                        {
                            "start": 443,
                            "end": 624
                        },
                        {
                            "start": 625,
                            "end": 782
                        },
                        {
                            "start": 783,
                            "end": 1006
                        },
                        {
                            "start": 1007,
                            "end": 1174
                        },
                        {
                            "start": 1175,
                            "end": 1344
                        },
                        {
                            "start": 1347,
                            "end": 1548
                        },
                        {
                            "start": 1549,
                            "end": 1796
                        },
                        {
                            "start": 1797,
                            "end": 1910
                        },
                        {
                            "start": 1911,
                            "end": 2011
                        },
                        {
                            "start": 2014,
                            "end": 2207
                        },
                        {
                            "start": 2208,
                            "end": 2411
                        },
                        {
                            "start": 2412,
                            "end": 2547
                        },
                        {
                            "start": 2548,
                            "end": 2684
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7529296875
                }
            ],
            "relevance_judgement": 0.83447265625,
            "relevance_judgment_input_expanded": "# Title: HIRO: Hierarchical Information Retrieval Optimization\n# Venue: arXiv.org\n# Authors: Krish Goel, Mahek Chandak\n## Abstract\nRetrieval-Augmented Generation (RAG) has revolutionized natural language processing by dynamically integrating external knowledge into Large Language Models (LLMs), addressing their limitation of static training datasets. Recent implementations of RAG leverage hierarchical data structures, which organize documents at various levels of summarization and information density. This complexity, however, can cause LLMs to\"choke\"on information overload, necessitating more sophisticated querying mechanisms. In this context, we introduce Hierarchical Information Retrieval Optimization (HIRO), a novel querying approach that employs a Depth-First Search (DFS)-based recursive similarity score calculation and branch pruning. This method uniquely minimizes the context delivered to the LLM without informational loss, effectively managing the challenge of excessive data. HIRO's refined approach is validated by a 10.85% improvement in performance on the NarrativeQA dataset.\n## Introduction\nThe advent of Large Language Models (LLMs) has brought about significant transformation in artificial intelligence and natural language processing, enabling advancements in tasks such as text generation, translation, and question-answering [3]. As these models increase in size and complexity, they have evolved into highly effective standalone knowledge repositories, embedding vast amounts of factual information within their parameters [12,19]. However, LLMs are inherently constrained by the static nature of their training datasets, which limits their adaptability to continuously evolving real-world information, where the breadth and depth of knowledge required far exceed any static training corpus [10]. This limitation underscores a critical gap in LLM design, where fine-tuning falls short, particularly when updates and domain-specific information are necessary [8]. \n\nIn response to these challenges, the field has witnessed the emergence of Retrieval-Augmented Generation (RAG) models, evolving into what are now known as Retrieval-Augmented Language Models (RALMs). These new paradigms enhance LLMs by integrating dynamic external knowledge through sophisticated retrieval mechanisms, effectively addressing the inherent limitations of static knowledge bases [8]. Inspired by the success of open-domain question answering systems, RALMs utilize indexed text databases to enhance model responses by presenting retrieved information alongside input queries [1,13]. This integration not only boosts the models' ability to provide current, domain-specific knowledge but also enhances interpretability and source traceability, offering a stark contrast to the opaque parametric knowledge of traditional LLMs. \n\nAmidst this evolving landscape, novel approaches to information retrieval, such as the use of hierarchical data structures for organizing documents, represent significant advancements. Techniques like RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval, which clusters and summarizes texts into a tree structure, demonstrate the potential to overcome traditional limitations by providing both granular and high-level insights [4,5,15,20]. However, the adoption of hierarchical data structures poses challenges, particularly the overwhelming amount of context they can return, which may not always align with the query requirements [15]. For queries demanding extensive information, this can result in insufficient data being returned, whereas for simpler inquiries, it might overwhelm LLMs with excessive context.\n\n## Language Model\nFinal generated response from the augmented prompt Augmented prompt returned to the LM Figure 1: Architectural Overview of a Retrieval-Augmented Language Model (RALM). A query is inputted, processed through an embedding model to generate a vector representation, which is then matched in a vector database to retrieve relevant contexts. These contexts are fed, along with the original query, into the LLM, resulting in an informed response. \n\nThe adoption of hierarchical structures in RAG models, exemplified by RAPTOR [15], represents a pivotal shift towards more organized and efficient information storage and retrieval. RAPTOR's approach, through recursive summarisation to create a hierarchical tree, facilitates nuanced access to information at various levels of abstraction. This innovation not only improves coherence and information density tailored to specific tasks but also encourages exploration into other hierarchical frameworks like graphs for enhanced document interrelation preservation. These advancements signal a new phase in computational linguistics, focusing on sophisticated, hierarchical data use for better knowledge representation and retrieval. However, existing hierarchical models often face challenges in balancing information density and retrieval efficiency, leading to potential information overload or loss. \n\nDynamic Querying Mechanisms in RAG Models address the evolving nature of queries where traditional static retrieval methods, such as TF-IDF and BM25, have been complemented by more adaptive techniques. The Tree Traversal method progressively selects the top-k most relevant nodes from each layer of the hierarchical data based on similarity, allowing for adjustments in depth and number of nodes to control the information's specificity and breadth. Conversely, the Collapsed Tree approach flattens the hierarchy to allow for simultaneous comparison of all nodes. Figure 5 and Figure 6 illustrate the workings of these algorithms, providing visual representations. \n\nDespite these innovations, a limitation arises from the static quantity of data retrieved, which may not always align with the query's needs, leading to potential information overload for LLMs. This static k value, irrespective of query complexity, can hamper LLMs' performance by either providing insufficient data for complex queries or overwhelming them with excessive context for simpler ones. Additionally, returning both parent and child nodes due to similar matching levels often results in inefficient, redundant information. Optimizing the retrieval process to avoid such duplication is crucial for improving LLM performance in hierarchical document structures.",
            "reference_string": "[270521566 | Goel et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Retrieval-Enhanced Machine Learning: Synthesis and Opportunities",
            "venue": "SIGIR-AP",
            "year": 2024,
            "reference_count": 258,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3673791.3698439",
                "status": "HYBRID",
                "license": "CCBYND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.12982, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2175557537",
                    "name": "To Eun Kim"
                },
                {
                    "authorId": "2073044451",
                    "name": "Alireza Salemi"
                },
                {
                    "authorId": "32573794",
                    "name": "Andrew Drozdov"
                },
                {
                    "authorId": "2311888401",
                    "name": "Fernando Diaz"
                },
                {
                    "authorId": "2295731593",
                    "name": "Hamed Zamani"
                }
            ],
            "abstract": "Retrieval-enhanced machine learning (REML) refers to the use of information retrieval methods to support reasoning and inference in machine learning tasks. Although relatively recent, these approaches can substantially improve model performance. This includes improved generalization, knowledge grounding, scalability, freshness, attribution, interpretability and on-device learning. To date, despite being influenced by work in the information retrieval community, REML research has predominantly been presented in natural language processing (NLP) conferences. Our tutorial addresses this disconnect by introducing core REML concepts and synthesizing the literature from various domains in machine learning (ML), including, but beyond NLP. What is unique to our approach is that we used consistent notations, to provide researchers with a unified and expandable framework. The tutorial will be presented in lecture format based on an existing manuscript, with supporting materials and a comprehensive reading list available at https://retrieval-enhanced-ml.github.io/SIGIR-AP2024-tutorial.",
            "corpus_id": 271270159,
            "sentences": [
                {
                    "corpus_id": "271270159",
                    "title": "Retrieval-Enhanced Machine Learning: Synthesis and Opportunities",
                    "text": "Background. In recent years, the research landscape surrounding large language models (LLMs) has witnessed substantial growth, underscored by the profound potential these models hold for various natural language processing (NLP) tasks. One of the significant advancements that has propelled this field forward is the scaling of the number of parameters of LLMs, which has enabled the training of models with unprecedented size and complexity [267]. We witness a similar trend in other fields adjacent to machine learning, for example, large vision foundation models for representing images and videos [5,40]. Concurrently, the notion of in-context learning (ICL) [39] has emerged as a transformative capability, allowing LLMs to dynamically adapt and incorporate new information during its inference. In parallel, the information retrieval (IR) community has been actively exploring techniques aimed at improving the efficiency, effectiveness, and robustness of accessing information from large-scale collections. \n\nThe convergence of these two domains has given rise to a new trend in research, where models are equipped with retrieval capabilities to access external knowledge during both training and inference stages [148,257]. This integration of retrieval mechanisms into the prediction pipeline started to gain significant traction, as it allows models to ground their predictions in external knowledge without necessitating an increase in model capacity. Methods presented by Hashemi et al. [68] and Lewis et al. [123] are among the earliest work in this space; the former focuses on retrieval-augmented representation learning by extending the transformer network, while the latter studies the paradigm of retrieval-augmented generation (RAG) for knowledge-intensive language tasks. That said, using retrieval results to improve a machine learning systems is not new. Pseudo-relevance feedback methods-methods for representing search queries using the top retrieved documents-are perhaps the first set of methods in this category [10,30]. The ICL ability inherent in the LLMs has played a pivotal role in facilitating the dissemination and adoption of these retrieval-augmented approaches. By integrating retrieved documents into the prompt of the LLMs, researchers have been able to harness the external knowledge sources without fundamentally altering the underlying model architecture. \n\nMotivation.",
                    "score": 0.4068739431190821,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 11
                        },
                        {
                            "start": 12,
                            "end": 235
                        },
                        {
                            "start": 236,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 608
                        },
                        {
                            "start": 609,
                            "end": 800
                        },
                        {
                            "start": 801,
                            "end": 1013
                        },
                        {
                            "start": 1016,
                            "end": 1231
                        },
                        {
                            "start": 1232,
                            "end": 1462
                        },
                        {
                            "start": 1463,
                            "end": 1791
                        },
                        {
                            "start": 1792,
                            "end": 1876
                        },
                        {
                            "start": 1877,
                            "end": 2047
                        },
                        {
                            "start": 2048,
                            "end": 2198
                        },
                        {
                            "start": 2199,
                            "end": 2397
                        },
                        {
                            "start": 2400,
                            "end": 2411
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 601,
                            "end": 604,
                            "matchedPaperCorpusId": "232417054"
                        },
                        {
                            "start": 604,
                            "end": 607,
                            "matchedPaperCorpusId": "225039882"
                        },
                        {
                            "start": 1221,
                            "end": 1226,
                            "matchedPaperCorpusId": "256868474"
                        },
                        {
                            "start": 1226,
                            "end": 1230,
                            "matchedPaperCorpusId": "248506020"
                        },
                        {
                            "start": 1499,
                            "end": 1503,
                            "matchedPaperCorpusId": "219687568"
                        },
                        {
                            "start": 1521,
                            "end": 1526,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 2039,
                            "end": 2043,
                            "matchedPaperCorpusId": "553561"
                        },
                        {
                            "start": 2043,
                            "end": 2046,
                            "matchedPaperCorpusId": "33071665"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83056640625
                }
            ],
            "relevance_judgement": 0.83056640625,
            "relevance_judgment_input_expanded": "# Title: Retrieval-Enhanced Machine Learning: Synthesis and Opportunities\n# Venue: SIGIR-AP\n# Authors: To Eun Kim, Alireza Salemi, Andrew Drozdov, Fernando Diaz, Hamed Zamani\n## Abstract\nRetrieval-enhanced machine learning (REML) refers to the use of information retrieval methods to support reasoning and inference in machine learning tasks. Although relatively recent, these approaches can substantially improve model performance. This includes improved generalization, knowledge grounding, scalability, freshness, attribution, interpretability and on-device learning. To date, despite being influenced by work in the information retrieval community, REML research has predominantly been presented in natural language processing (NLP) conferences. Our tutorial addresses this disconnect by introducing core REML concepts and synthesizing the literature from various domains in machine learning (ML), including, but beyond NLP. What is unique to our approach is that we used consistent notations, to provide researchers with a unified and expandable framework. The tutorial will be presented in lecture format based on an existing manuscript, with supporting materials and a comprehensive reading list available at https://retrieval-enhanced-ml.github.io/SIGIR-AP2024-tutorial.\n## Introduction\nBackground. In recent years, the research landscape surrounding large language models (LLMs) has witnessed substantial growth, underscored by the profound potential these models hold for various natural language processing (NLP) tasks. One of the significant advancements that has propelled this field forward is the scaling of the number of parameters of LLMs, which has enabled the training of models with unprecedented size and complexity [267]. We witness a similar trend in other fields adjacent to machine learning, for example, large vision foundation models for representing images and videos [5,40]. Concurrently, the notion of in-context learning (ICL) [39] has emerged as a transformative capability, allowing LLMs to dynamically adapt and incorporate new information during its inference. In parallel, the information retrieval (IR) community has been actively exploring techniques aimed at improving the efficiency, effectiveness, and robustness of accessing information from large-scale collections. \n\nThe convergence of these two domains has given rise to a new trend in research, where models are equipped with retrieval capabilities to access external knowledge during both training and inference stages [148,257]. This integration of retrieval mechanisms into the prediction pipeline started to gain significant traction, as it allows models to ground their predictions in external knowledge without necessitating an increase in model capacity. Methods presented by Hashemi et al. [68] and Lewis et al. [123] are among the earliest work in this space; the former focuses on retrieval-augmented representation learning by extending the transformer network, while the latter studies the paradigm of retrieval-augmented generation (RAG) for knowledge-intensive language tasks. That said, using retrieval results to improve a machine learning systems is not new. Pseudo-relevance feedback methods-methods for representing search queries using the top retrieved documents-are perhaps the first set of methods in this category [10,30]. The ICL ability inherent in the LLMs has played a pivotal role in facilitating the dissemination and adoption of these retrieval-augmented approaches. By integrating retrieved documents into the prompt of the LLMs, researchers have been able to harness the external knowledge sources without fundamentally altering the underlying model architecture. \n\nMotivation.",
            "reference_string": "[271270159 | Kim et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "venue": "Neural Information Processing Systems",
            "year": 2020,
            "reference_count": 67,
            "citation_count": 6476,
            "influential_citation_count": 662,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.11401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145222654",
                    "name": "Patrick Lewis"
                },
                {
                    "authorId": "3439053",
                    "name": "Ethan Perez"
                },
                {
                    "authorId": "1716179427",
                    "name": "Aleksandara Piktus"
                },
                {
                    "authorId": "40052301",
                    "name": "F. Petroni"
                },
                {
                    "authorId": "2067091563",
                    "name": "Vladimir Karpukhin"
                },
                {
                    "authorId": "39589154",
                    "name": "Naman Goyal"
                },
                {
                    "authorId": "103131985",
                    "name": "Heinrich Kuttler"
                },
                {
                    "authorId": "35084211",
                    "name": "M. Lewis"
                },
                {
                    "authorId": "144105277",
                    "name": "Wen-tau Yih"
                },
                {
                    "authorId": "2620211",
                    "name": "Tim Rockt\u00e4schel"
                },
                {
                    "authorId": "48662861",
                    "name": "Sebastian Riedel"
                },
                {
                    "authorId": "1743722",
                    "name": "Douwe Kiela"
                }
            ],
            "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
            "corpus_id": 218869575,
            "sentences": [
                {
                    "corpus_id": "218869575",
                    "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                    "text": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                    "score": 0.457190172091137,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82568359375
                },
                {
                    "corpus_id": "218869575",
                    "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                    "text": "Single-Task Retrieval Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [5,25], fact checking [50], fact completion [42], long-form question answering [12], Wikipedia article generation [32], dialogue [36,59,9,13], translation [16], and language modeling [17,23]. Our work unifies previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks. \n\nGeneral-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classification tasks in the GLUE benchmarks [54,55] after fine-tuning [43,8]. GPT-2 [44] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [28] and T5 [45,46] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, unified architecture, by learning a retrieval module to augment pre-trained, generative language models. \n\nLearned Retrieval There is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [39,22] similar to ours. Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search [40], reinforcement learning [6,57,56], or a latent variable approach [27,18] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be fine-tuned for strong performance on a variety of tasks.",
                    "score": 0.4376638787058579,
                    "section_title": "Related Work",
                    "char_start_offset": 28170,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 138
                        },
                        {
                            "start": 139,
                            "end": 380
                        },
                        {
                            "start": 381,
                            "end": 582
                        },
                        {
                            "start": 585,
                            "end": 734
                        },
                        {
                            "start": 735,
                            "end": 905
                        },
                        {
                            "start": 906,
                            "end": 1068
                        },
                        {
                            "start": 1069,
                            "end": 1282
                        },
                        {
                            "start": 1283,
                            "end": 1454
                        },
                        {
                            "start": 1457,
                            "end": 1640
                        },
                        {
                            "start": 1641,
                            "end": 1854
                        },
                        {
                            "start": 1855,
                            "end": 2116
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 189,
                            "end": 192,
                            "matchedPaperCorpusId": "3618568"
                        },
                        {
                            "start": 192,
                            "end": 195,
                            "matchedPaperCorpusId": "86611921"
                        },
                        {
                            "start": 211,
                            "end": 215,
                            "matchedPaperCorpusId": "4711425"
                        },
                        {
                            "start": 233,
                            "end": 237,
                            "matchedPaperCorpusId": "212411919"
                        },
                        {
                            "start": 268,
                            "end": 272,
                            "matchedPaperCorpusId": "196170479"
                        },
                        {
                            "start": 303,
                            "end": 307,
                            "matchedPaperCorpusId": "3608234"
                        },
                        {
                            "start": 318,
                            "end": 322,
                            "matchedPaperCorpusId": "52333947"
                        },
                        {
                            "start": 322,
                            "end": 325,
                            "matchedPaperCorpusId": "52006529"
                        },
                        {
                            "start": 325,
                            "end": 327,
                            "matchedPaperCorpusId": "53218829"
                        },
                        {
                            "start": 344,
                            "end": 348,
                            "matchedPaperCorpusId": "19206366"
                        },
                        {
                            "start": 372,
                            "end": 376,
                            "matchedPaperCorpusId": "2318481"
                        },
                        {
                            "start": 376,
                            "end": 379,
                            "matchedPaperCorpusId": "207870430"
                        },
                        {
                            "start": 872,
                            "end": 876,
                            "matchedPaperCorpusId": "5034059"
                        },
                        {
                            "start": 876,
                            "end": 879,
                            "matchedPaperCorpusId": "143424870"
                        },
                        {
                            "start": 902,
                            "end": 904,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1761,
                            "end": 1765,
                            "matchedPaperCorpusId": "202572744"
                        },
                        {
                            "start": 1790,
                            "end": 1793,
                            "matchedPaperCorpusId": "17476563"
                        },
                        {
                            "start": 1793,
                            "end": 1796,
                            "matchedPaperCorpusId": "13764176"
                        },
                        {
                            "start": 1831,
                            "end": 1835,
                            "matchedPaperCorpusId": "173990818"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75830078125
                }
            ],
            "relevance_judgement": 0.82568359375,
            "relevance_judgment_input_expanded": "# Title: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n# Venue: Neural Information Processing Systems\n# Authors: Patrick Lewis, Ethan Perez, Aleksandara Piktus, F. Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, M. Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, Douwe Kiela\n## Abstract\nLarge pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\n## Related Work\nSingle-Task Retrieval Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [5,25], fact checking [50], fact completion [42], long-form question answering [12], Wikipedia article generation [32], dialogue [36,59,9,13], translation [16], and language modeling [17,23]. Our work unifies previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks. \n\nGeneral-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classification tasks in the GLUE benchmarks [54,55] after fine-tuning [43,8]. GPT-2 [44] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [28] and T5 [45,46] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, unified architecture, by learning a retrieval module to augment pre-trained, generative language models. \n\nLearned Retrieval There is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [39,22] similar to ours. Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search [40], reinforcement learning [6,57,56], or a latent variable approach [27,18] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be fine-tuned for strong performance on a variety of tasks.",
            "reference_string": "[218869575 | Lewis et al. | 2020 | Citations: 6476]"
        },
        {
            "title": "ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 27,
            "citation_count": 8,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.12289, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2258957941",
                    "name": "Sai Munikoti"
                },
                {
                    "authorId": "145536102",
                    "name": "Anurag Acharya"
                },
                {
                    "authorId": "2054838317",
                    "name": "S. Wagle"
                },
                {
                    "authorId": "24029613",
                    "name": "Sameera Horawalavithana"
                }
            ],
            "abstract": "Large language models record impressive performance on many natural language processing tasks. However, their knowledge capacity is limited to the pretraining corpus. Retrieval augmentation offers an effective solution by retrieving context from external knowledge sources to complement the language model. However, existing retrieval augmentation techniques ignore the structural relationships between these documents. Furthermore, retrieval models are not explored much in scientific tasks, especially in regard to the faithfulness of retrieved documents. In this paper, we propose a novel structure-aware retrieval augmented language model that accommodates document structure during retrieval augmentation. We create a heterogeneous document graph capturing multiple types of relationships (e.g., citation, co-authorship, etc.) that connect documents from more than 15 scientific disciplines (e.g., Physics, Medicine, Chemistry, etc.). We train a graph neural network on the curated document graph to act as a structural encoder for the corresponding passages retrieved during the model pretraining. Particularly, along with text embeddings of the retrieved passages, we obtain structural embeddings of the documents (passages) and fuse them together before feeding them to the language model. We evaluate our model extensively on various scientific benchmarks that include science question-answering and scientific document classification tasks. Experimental results demonstrate that structure-aware retrieval improves retrieving more coherent, faithful and contextually relevant passages, while showing a comparable performance in the overall accuracy.",
            "corpus_id": 265308533,
            "sentences": [
                {
                    "corpus_id": "265308533",
                    "title": "ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science",
                    "text": "The continuous advancement in natural language processing (NLP) has led to the development of various novel model architectures that overcome existing limitations and demonstrate state-of-the-art performances. The retrieval augmented language models (RALM) primarily address the grounding and scalability challenges in standard language models (LM). RALM aims to address these limitations by combining a LM with an external knowledge base. In this framework, the LM generates text conditioned not only on the input query but also on relevant knowledge retrieved from the knowledge base. The retrieved knowledge is usually the text chunks or passages from documents that provide factual grounding to contextualize the model's pre-Preprint version dictions. In other words, this approach decentralizes model knowledge into parameters and external knowledge sources, thereby addressing the challenges of scalability and adaptability. \n\nTypically in RALM, text data from an external knowledge base is segmented and encoded into vectors (also known as vector databases). The retriever component of RALM retrieves relevant documents based on the similarity between the query and vectors corresponding to documents in the database. Many existing RALMs rely solely on semantic/lexical information of the documents for retrieval. However, in certain scenarios, the structural relationship between documents can further support the retriever in retrieving contextually relevant documents. For instance, a scientific paper in materials science might reference papers that describe relevant advances in nuclear physics, and viceversa. Having such relational information explicitly present in the scientific documents would allow the model to draw on the interdisciplinary scientific knowledge in a similar way to how scientists do. Thus, it would be beneficial to learn about the relationships between documents (e.g., citations, co-authorship, etc.) in a corpus of scientific publications and connect different scientific concepts. \n\nTo address the challenges of adequate structural component in RALM and retrieval faithfulness in science-focused tasks, we propose a novel model architecture (ATLANTIC) in this work that systematically incorporates structural and textual information into the RALM. We develop AT-LANTIC on top of the standard RALM, ATLAS (Izacard et al. 2022) architecture.",
                    "score": 0.38591610178735913,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 209
                        },
                        {
                            "start": 210,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 586
                        },
                        {
                            "start": 587,
                            "end": 755
                        },
                        {
                            "start": 756,
                            "end": 930
                        },
                        {
                            "start": 933,
                            "end": 1065
                        },
                        {
                            "start": 1066,
                            "end": 1224
                        },
                        {
                            "start": 1225,
                            "end": 1320
                        },
                        {
                            "start": 1321,
                            "end": 1478
                        },
                        {
                            "start": 1479,
                            "end": 1622
                        },
                        {
                            "start": 1623,
                            "end": 1819
                        },
                        {
                            "start": 1820,
                            "end": 2020
                        },
                        {
                            "start": 2023,
                            "end": 2287
                        },
                        {
                            "start": 2288,
                            "end": 2379
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82080078125
                }
            ],
            "relevance_judgement": 0.82080078125,
            "relevance_judgment_input_expanded": "# Title: ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science\n# Venue: arXiv.org\n# Authors: Sai Munikoti, Anurag Acharya, S. Wagle, Sameera Horawalavithana\n## Abstract\nLarge language models record impressive performance on many natural language processing tasks. However, their knowledge capacity is limited to the pretraining corpus. Retrieval augmentation offers an effective solution by retrieving context from external knowledge sources to complement the language model. However, existing retrieval augmentation techniques ignore the structural relationships between these documents. Furthermore, retrieval models are not explored much in scientific tasks, especially in regard to the faithfulness of retrieved documents. In this paper, we propose a novel structure-aware retrieval augmented language model that accommodates document structure during retrieval augmentation. We create a heterogeneous document graph capturing multiple types of relationships (e.g., citation, co-authorship, etc.) that connect documents from more than 15 scientific disciplines (e.g., Physics, Medicine, Chemistry, etc.). We train a graph neural network on the curated document graph to act as a structural encoder for the corresponding passages retrieved during the model pretraining. Particularly, along with text embeddings of the retrieved passages, we obtain structural embeddings of the documents (passages) and fuse them together before feeding them to the language model. We evaluate our model extensively on various scientific benchmarks that include science question-answering and scientific document classification tasks. Experimental results demonstrate that structure-aware retrieval improves retrieving more coherent, faithful and contextually relevant passages, while showing a comparable performance in the overall accuracy.\n## Introduction\nThe continuous advancement in natural language processing (NLP) has led to the development of various novel model architectures that overcome existing limitations and demonstrate state-of-the-art performances. The retrieval augmented language models (RALM) primarily address the grounding and scalability challenges in standard language models (LM). RALM aims to address these limitations by combining a LM with an external knowledge base. In this framework, the LM generates text conditioned not only on the input query but also on relevant knowledge retrieved from the knowledge base. The retrieved knowledge is usually the text chunks or passages from documents that provide factual grounding to contextualize the model's pre-Preprint version dictions. In other words, this approach decentralizes model knowledge into parameters and external knowledge sources, thereby addressing the challenges of scalability and adaptability. \n\nTypically in RALM, text data from an external knowledge base is segmented and encoded into vectors (also known as vector databases). The retriever component of RALM retrieves relevant documents based on the similarity between the query and vectors corresponding to documents in the database. Many existing RALMs rely solely on semantic/lexical information of the documents for retrieval. However, in certain scenarios, the structural relationship between documents can further support the retriever in retrieving contextually relevant documents. For instance, a scientific paper in materials science might reference papers that describe relevant advances in nuclear physics, and viceversa. Having such relational information explicitly present in the scientific documents would allow the model to draw on the interdisciplinary scientific knowledge in a similar way to how scientists do. Thus, it would be beneficial to learn about the relationships between documents (e.g., citations, co-authorship, etc.) in a corpus of scientific publications and connect different scientific concepts. \n\nTo address the challenges of adequate structural component in RALM and retrieval faithfulness in science-focused tasks, we propose a novel model architecture (ATLANTIC) in this work that systematically incorporates structural and textual information into the RALM. We develop AT-LANTIC on top of the standard RALM, ATLAS (Izacard et al. 2022) architecture.",
            "reference_string": "[265308533 | Munikoti et al. | 2023 | Citations: 8]"
        },
        {
            "title": "Survey on Memory-Augmented Neural Networks: Cognitive Insights to AI Applications",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 35,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.06141, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2056070459",
                    "name": "Savya Khosla"
                },
                {
                    "authorId": "2273562657",
                    "name": "Zhen Zhu"
                },
                {
                    "authorId": "2273661082",
                    "name": "Yifie He"
                }
            ],
            "abstract": "This paper explores Memory-Augmented Neural Networks (MANNs), delving into how they blend human-like memory processes into AI. It covers different memory types, like sensory, short-term, and long-term memory, linking psychological theories with AI applications. The study investigates advanced architectures such as Hopfield Networks, Neural Turing Machines, Correlation Matrix Memories, Memformer, and Neural Attention Memory, explaining how they work and where they excel. It dives into real-world uses of MANNs across Natural Language Processing, Computer Vision, Multimodal Learning, and Retrieval Models, showing how memory boosters enhance accuracy, efficiency, and reliability in AI tasks. Overall, this survey provides a comprehensive view of MANNs, offering insights for future research in memory-based AI systems.",
            "corpus_id": 266163023,
            "sentences": [
                {
                    "corpus_id": "266163023",
                    "title": "Survey on Memory-Augmented Neural Networks: Cognitive Insights to AI Applications",
                    "text": "Retrieval augmentation proves to be highly beneficial in various knowledge-intensive NLP applications, especially those where factuality is a crucial requirement. This approach enriches the applications by providing access to a broader range of factual information, enhancing their accuracy and reliability. Retrieval Augmented Language Model (REALM) [27] is the first method to jointly train a knowledge retriever and a knowledge-augmented language encoder in an unsupervised manner. Retrieval augmented generation (RAG) [6] fine-tunes a pre-trained retriever (e.g., DPR [28]) and a pre-trained sequence-to-sequence model (e.g., BART [29]). RAG achieves superior performance on various knowledge-intensive tasks, including question answering, question generation and fact verification. Retrieval Augmented Translation (RAT) [30] improves neural machine translation by treating the external knowledge base as a dictionary. More recently, Chain-of-Noting (CoN) [31] proposes to enhance the robustness of retrieval augmented models by first generating sequential reading notes based on the retrieved documents, then formulating the final answer. This approach enables a comprehensive evaluation of the relevance and factuality of the retrieval results. \n\nBeyond the benefit of providing truthful information for knowledge-intensive tasks, retrieval augmentation also enhances the capability of language models without increasing trainable parameters. Retrieval Enhanced Transformers (RETRO) [5] augments a language model with an external knowledge base consisting of 2 trillion tokens, and achieves performance comparable to GPT-3 [32] and Jurassic-1 [33], which has 25x more parameters. Similarly, Atlas [34] achieves state-of-the-art performance the few-shot learning capabilities in language models while still maintains a relatively small parameter size. In-Context Retrieval Augmented Language Modeling (RALM) [35] further reduces the training cost by directly prepending retrieved documents to the input and freezing the weights of language models.",
                    "score": 0.38704866882156796,
                    "section_title": "Natural Language Processing",
                    "char_start_offset": 22318,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 307
                        },
                        {
                            "start": 308,
                            "end": 484
                        },
                        {
                            "start": 485,
                            "end": 641
                        },
                        {
                            "start": 642,
                            "end": 786
                        },
                        {
                            "start": 787,
                            "end": 922
                        },
                        {
                            "start": 923,
                            "end": 1143
                        },
                        {
                            "start": 1144,
                            "end": 1250
                        },
                        {
                            "start": 1253,
                            "end": 1448
                        },
                        {
                            "start": 1449,
                            "end": 1685
                        },
                        {
                            "start": 1686,
                            "end": 1856
                        },
                        {
                            "start": 1857,
                            "end": 2052
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 825,
                            "end": 829,
                            "matchedPaperCorpusId": "252815975"
                        },
                        {
                            "start": 1629,
                            "end": 1633,
                            "matchedPaperCorpusId": "218971783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81494140625
                }
            ],
            "relevance_judgement": 0.81494140625,
            "relevance_judgment_input_expanded": "# Title: Survey on Memory-Augmented Neural Networks: Cognitive Insights to AI Applications\n# Venue: arXiv.org\n# Authors: Savya Khosla, Zhen Zhu, Yifie He\n## Abstract\nThis paper explores Memory-Augmented Neural Networks (MANNs), delving into how they blend human-like memory processes into AI. It covers different memory types, like sensory, short-term, and long-term memory, linking psychological theories with AI applications. The study investigates advanced architectures such as Hopfield Networks, Neural Turing Machines, Correlation Matrix Memories, Memformer, and Neural Attention Memory, explaining how they work and where they excel. It dives into real-world uses of MANNs across Natural Language Processing, Computer Vision, Multimodal Learning, and Retrieval Models, showing how memory boosters enhance accuracy, efficiency, and reliability in AI tasks. Overall, this survey provides a comprehensive view of MANNs, offering insights for future research in memory-based AI systems.\n## Natural Language Processing\nRetrieval augmentation proves to be highly beneficial in various knowledge-intensive NLP applications, especially those where factuality is a crucial requirement. This approach enriches the applications by providing access to a broader range of factual information, enhancing their accuracy and reliability. Retrieval Augmented Language Model (REALM) [27] is the first method to jointly train a knowledge retriever and a knowledge-augmented language encoder in an unsupervised manner. Retrieval augmented generation (RAG) [6] fine-tunes a pre-trained retriever (e.g., DPR [28]) and a pre-trained sequence-to-sequence model (e.g., BART [29]). RAG achieves superior performance on various knowledge-intensive tasks, including question answering, question generation and fact verification. Retrieval Augmented Translation (RAT) [30] improves neural machine translation by treating the external knowledge base as a dictionary. More recently, Chain-of-Noting (CoN) [31] proposes to enhance the robustness of retrieval augmented models by first generating sequential reading notes based on the retrieved documents, then formulating the final answer. This approach enables a comprehensive evaluation of the relevance and factuality of the retrieval results. \n\nBeyond the benefit of providing truthful information for knowledge-intensive tasks, retrieval augmentation also enhances the capability of language models without increasing trainable parameters. Retrieval Enhanced Transformers (RETRO) [5] augments a language model with an external knowledge base consisting of 2 trillion tokens, and achieves performance comparable to GPT-3 [32] and Jurassic-1 [33], which has 25x more parameters. Similarly, Atlas [34] achieves state-of-the-art performance the few-shot learning capabilities in language models while still maintains a relatively small parameter size. In-Context Retrieval Augmented Language Modeling (RALM) [35] further reduces the training cost by directly prepending retrieved documents to the input and freezing the weights of language models.",
            "reference_string": "[266163023 | Khosla et al. | 2023 | Citations: 2]"
        },
        {
            "title": "FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion",
            "venue": "International Symposium on Software Testing and Analysis",
            "year": 2024,
            "reference_count": 47,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2404.01554",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.01554, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2290464625",
                    "name": "Qi Guo"
                },
                {
                    "authorId": "2118890600",
                    "name": "Xiaohong Li"
                },
                {
                    "authorId": "2288741802",
                    "name": "Xiaofei Xie"
                },
                {
                    "authorId": "2290359321",
                    "name": "Shangqing Liu"
                },
                {
                    "authorId": "2109915677",
                    "name": "Ze Tang"
                },
                {
                    "authorId": "1758019",
                    "name": "Ruitao Feng"
                },
                {
                    "authorId": "2294667814",
                    "name": "Junjie Wang"
                },
                {
                    "authorId": "2248015856",
                    "name": "Jidong Ge"
                },
                {
                    "authorId": "2279752248",
                    "name": "Lei Bu"
                }
            ],
            "abstract": "The rise of code pre-trained models has significantly enhanced various coding tasks, such as code completion, and tools like GitHub Copilot. However, the substantial size of these models, especially large models, poses a significant challenge when it comes to fine-tuning them for specific downstream tasks. As an alternative approach, retrieval-based methods have emerged as a promising solution, augmenting model predictions without the need for fine-tuning. Despite their potential, a significant challenge is that the designs of these methods often rely on heuristics, leaving critical questions about what information should be stored or retrieved and how to interpolate such information for augmenting predictions. To tackle this challenge, we first perform a theoretical analysis of the fine-tuning process, highlighting the importance of delta logits as a catalyst for improving model predictions. Building on this insight, we develop a novel retrieval-based method, FT2Ra, which aims to mimic genuine fine-tuning. While FT2Ra adopts a retrieval-based mechanism, it uniquely adopts a paradigm with a learning rate and multi-epoch retrievals, which is similar to fine-tuning. We conducted a comprehensive evaluation of FT2Ra in both token-level and line-level code completions. Our findings demonstrate the remarkable effectiveness of FT2Ra when compared to state-of-the-art methods and its potential to genuine fine-tuning. In token-level completion, which represents a relatively easier task, FT2Ra achieves a 4.29% improvement in accuracy compared to the best baseline method on UniXcoder. In the more challenging line-level completion task, we observe a substantial more than twice increase in Exact Match (EM) performance, indicating the significant advantages of our theoretical analysis. Notably, even when operating without actual fine-tuning, FT2Ra exhibits competitive performance compared to the models with real fine-tuning.",
            "corpus_id": 268856642,
            "sentences": [
                {
                    "corpus_id": "268856642",
                    "title": "FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion",
                    "text": "Recently, a series of retrieval-augmented language models [14,24,47] have been proposed to augment language models with external knowledge [9,17,53].Retrieval-augmented techniques can generally be divided into two types.The first type is at the input layer [14,20,42], where the retrieved information is text chunks.The second type is at the output layer [7,24,47], where the retrieved information is tokens.By combining the retrieved tokens with the tokens generated by the original model, the accuracy of the retrievalaugmented model's generation for each token can be improved.\n\nThe first type of method can provide the model with more external knowledge, making it adept at handling tasks in the NLP field such as knowledge-based question answering [27,45,49].The second type of method can refer to the retrieved information to correct the generated tokens, making it more suited for handling strictly structured generative tasks, such as code completion [7,10,11].In this work, we mainly focus on the second category.\n\nTo better understand the mechanism, we take kNN-LM [24] as an example for a detailed explanation.Given a context sequence   = ( 1 , . . .,   \u22121 ), the language models (LMs) estimate   (  |  ), i.e., the probability distribution over the next token   .kNN-LM is designed to augment a pre-trained language model with a set of nearest neighbours retrieved from an external text collection, which can be the training set . Different from fine-tuning, retrieval augmentation does not need any retraining.In particular, RaLM includes two tasks, i.e., building a datastore and retrieval-augmented inference.Datastore: The datastore is a retrieval set, which can be built with a forward pass by LM on the prepared text collection to store the context-target pairs as the subject of a query.We denote a function  (\u2022) to map a context  to a fixed-length vector representation computed by a pre-trained LM.Given an example (  ,   ) \u2208 , we can pass   to a LM to get its vector representation, i.e.,   =  (  ).",
                    "score": 0.37955109953353117,
                    "section_title": "BACKGROUND AND PROBLEM 2.1 Retrieval-Augmented Language Models",
                    "char_start_offset": 7770,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 149
                        },
                        {
                            "start": 149,
                            "end": 220
                        },
                        {
                            "start": 220,
                            "end": 316
                        },
                        {
                            "start": 316,
                            "end": 408
                        },
                        {
                            "start": 408,
                            "end": 580
                        },
                        {
                            "start": 582,
                            "end": 764
                        },
                        {
                            "start": 764,
                            "end": 969
                        },
                        {
                            "start": 969,
                            "end": 1022
                        },
                        {
                            "start": 1024,
                            "end": 1121
                        },
                        {
                            "start": 1121,
                            "end": 1161
                        },
                        {
                            "start": 1161,
                            "end": 1275
                        },
                        {
                            "start": 1275,
                            "end": 1523
                        },
                        {
                            "start": 1523,
                            "end": 1624
                        },
                        {
                            "start": 1624,
                            "end": 1806
                        },
                        {
                            "start": 1806,
                            "end": 1919
                        },
                        {
                            "start": 1919,
                            "end": 2021
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 58,
                            "end": 62,
                            "matchedPaperCorpusId": "211204736"
                        },
                        {
                            "start": 62,
                            "end": 65,
                            "matchedPaperCorpusId": "207870430"
                        },
                        {
                            "start": 139,
                            "end": 142,
                            "matchedPaperCorpusId": "249191271"
                        },
                        {
                            "start": 142,
                            "end": 145,
                            "matchedPaperCorpusId": "252568176"
                        },
                        {
                            "start": 257,
                            "end": 261,
                            "matchedPaperCorpusId": "211204736"
                        },
                        {
                            "start": 355,
                            "end": 358,
                            "matchedPaperCorpusId": "246431219"
                        },
                        {
                            "start": 358,
                            "end": 361,
                            "matchedPaperCorpusId": "207870430"
                        },
                        {
                            "start": 753,
                            "end": 757,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 959,
                            "end": 962,
                            "matchedPaperCorpusId": "246431219"
                        },
                        {
                            "start": 1075,
                            "end": 1079,
                            "matchedPaperCorpusId": "207870430"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81396484375
                }
            ],
            "relevance_judgement": 0.81396484375,
            "relevance_judgment_input_expanded": "# Title: FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion\n# Venue: International Symposium on Software Testing and Analysis\n# Authors: Qi Guo, Xiaohong Li, Xiaofei Xie, Shangqing Liu, Ze Tang, Ruitao Feng, Junjie Wang, Jidong Ge, Lei Bu\n## Abstract\nThe rise of code pre-trained models has significantly enhanced various coding tasks, such as code completion, and tools like GitHub Copilot. However, the substantial size of these models, especially large models, poses a significant challenge when it comes to fine-tuning them for specific downstream tasks. As an alternative approach, retrieval-based methods have emerged as a promising solution, augmenting model predictions without the need for fine-tuning. Despite their potential, a significant challenge is that the designs of these methods often rely on heuristics, leaving critical questions about what information should be stored or retrieved and how to interpolate such information for augmenting predictions. To tackle this challenge, we first perform a theoretical analysis of the fine-tuning process, highlighting the importance of delta logits as a catalyst for improving model predictions. Building on this insight, we develop a novel retrieval-based method, FT2Ra, which aims to mimic genuine fine-tuning. While FT2Ra adopts a retrieval-based mechanism, it uniquely adopts a paradigm with a learning rate and multi-epoch retrievals, which is similar to fine-tuning. We conducted a comprehensive evaluation of FT2Ra in both token-level and line-level code completions. Our findings demonstrate the remarkable effectiveness of FT2Ra when compared to state-of-the-art methods and its potential to genuine fine-tuning. In token-level completion, which represents a relatively easier task, FT2Ra achieves a 4.29% improvement in accuracy compared to the best baseline method on UniXcoder. In the more challenging line-level completion task, we observe a substantial more than twice increase in Exact Match (EM) performance, indicating the significant advantages of our theoretical analysis. Notably, even when operating without actual fine-tuning, FT2Ra exhibits competitive performance compared to the models with real fine-tuning.\n## BACKGROUND AND PROBLEM 2.1 Retrieval-Augmented Language Models\nRecently, a series of retrieval-augmented language models [14,24,47] have been proposed to augment language models with external knowledge [9,17,53].Retrieval-augmented techniques can generally be divided into two types.The first type is at the input layer [14,20,42], where the retrieved information is text chunks.The second type is at the output layer [7,24,47], where the retrieved information is tokens.By combining the retrieved tokens with the tokens generated by the original model, the accuracy of the retrievalaugmented model's generation for each token can be improved.\n\nThe first type of method can provide the model with more external knowledge, making it adept at handling tasks in the NLP field such as knowledge-based question answering [27,45,49].The second type of method can refer to the retrieved information to correct the generated tokens, making it more suited for handling strictly structured generative tasks, such as code completion [7,10,11].In this work, we mainly focus on the second category.\n\nTo better understand the mechanism, we take kNN-LM [24] as an example for a detailed explanation.Given a context sequence   = ( 1 , . . .,   \u22121 ), the language models (LMs) estimate   (  |  ), i.e., the probability distribution over the next token   .kNN-LM is designed to augment a pre-trained language model with a set of nearest neighbours retrieved from an external text collection, which can be the training set . Different from fine-tuning, retrieval augmentation does not need any retraining.In particular, RaLM includes two tasks, i.e., building a datastore and retrieval-augmented inference.Datastore: The datastore is a retrieval set, which can be built with a forward pass by LM on the prepared text collection to store the context-target pairs as the subject of a query.We denote a function  (\u2022) to map a context  to a fixed-length vector representation computed by a pre-trained LM.Given an example (  ,   ) \u2208 , we can pass   to a LM to get its vector representation, i.e.,   =  (  ).",
            "reference_string": "[268856642 | Guo et al. | 2024 | Citations: 11]"
        },
        {
            "title": "RAVEN: Multitask Retrieval Augmented Vision-Language Learning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 5,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.19150, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2300845194",
                    "name": "Varun Nagaraj Rao"
                },
                {
                    "authorId": "2308471326",
                    "name": "Siddharth Choudhary"
                },
                {
                    "authorId": "2308471915",
                    "name": "Aditya Deshpande"
                },
                {
                    "authorId": "1710219",
                    "name": "R. Satzoda"
                },
                {
                    "authorId": "2261492087",
                    "name": "Srikar Appalaraju"
                }
            ],
            "abstract": "The scaling of large language models to encode all the world's knowledge in model parameters is unsustainable and has exacerbated resource barriers. Retrieval-Augmented Generation (RAG) presents a potential solution, yet its application to vision-language models (VLMs) is under explored. Existing methods focus on models designed for single tasks. Furthermore, they're limited by the need for resource intensive pre training, additional parameter requirements, unaddressed modality prioritization and lack of clear benefit over non-retrieval baselines. This paper introduces RAVEN, a multitask retrieval augmented VLM framework that enhances base VLMs through efficient, task specific fine-tuning. By integrating retrieval augmented samples without the need for additional retrieval-specific parameters, we show that the model acquires retrieval properties that are effective across multiple tasks. Our results and extensive ablations across retrieved modalities for the image captioning and VQA tasks indicate significant performance improvements compared to non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a +3\\% accuracy on specific VQA question types. This underscores the efficacy of applying RAG approaches to VLMs, marking a stride toward more efficient and accessible multimodal learning.",
            "corpus_id": 270764768,
            "sentences": [
                {
                    "corpus_id": "270764768",
                    "title": "RAVEN: Multitask Retrieval Augmented Vision-Language Learning",
                    "text": "Retrieval augmentation has become an important technique for improving natural language processing models.One of the first works in this area was kNN-LM by Khandelwal et al. (Khandelwal et al., 2020) who showed how interpolating over nearest neighbors from any text collection could improve generalization.This was followed by RETRO (Borgeaud et al., 2021), which scaled up the retrieval corpus to trillions of tokens.Another line of work has focused on integrating Wikipedia passages directly into models like REALM (Guu et al., 2020), RAG (Lewis et al., 2020), and FiD (Izacard and Grave, 2021).By retrieving and conditioning on relevant Wikipedia passages, these models can better perform knowledge-intensive downstream tasks like question answering.Overall, retrieval augmentation has proven to be a highly effective way of injecting knowledge into language models to improve their capabilities.The techniques have progressed from simple corpus retrieval to integrated and scalable architectures that retrieve from large knowledge bases like Wikipedia.\n\nFigure 1: Illustration of our RAVEN framework.Given an input image, we retrieve image-text pairs from an external memory.Subsequently, we use a multitask pretrained base vision-language model (VLM) to encode the retrieved samples along with the query and decode to generate an output by attending over both the query and retrieved samples.",
                    "score": 0.3721991141151649,
                    "section_title": "Retrieval Augmented Generation in NLP",
                    "char_start_offset": 5638,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 106
                        },
                        {
                            "start": 106,
                            "end": 306
                        },
                        {
                            "start": 306,
                            "end": 418
                        },
                        {
                            "start": 418,
                            "end": 597
                        },
                        {
                            "start": 597,
                            "end": 753
                        },
                        {
                            "start": 753,
                            "end": 899
                        },
                        {
                            "start": 899,
                            "end": 1056
                        },
                        {
                            "start": 1058,
                            "end": 1104
                        },
                        {
                            "start": 1104,
                            "end": 1179
                        },
                        {
                            "start": 1179,
                            "end": 1397
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 156,
                            "end": 199,
                            "matchedPaperCorpusId": "207870430"
                        },
                        {
                            "start": 333,
                            "end": 356,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 541,
                            "end": 561,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 571,
                            "end": 596,
                            "matchedPaperCorpusId": "220302360"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8125
                }
            ],
            "relevance_judgement": 0.8125,
            "relevance_judgment_input_expanded": "# Title: RAVEN: Multitask Retrieval Augmented Vision-Language Learning\n# Venue: arXiv.org\n# Authors: Varun Nagaraj Rao, Siddharth Choudhary, Aditya Deshpande, R. Satzoda, Srikar Appalaraju\n## Abstract\nThe scaling of large language models to encode all the world's knowledge in model parameters is unsustainable and has exacerbated resource barriers. Retrieval-Augmented Generation (RAG) presents a potential solution, yet its application to vision-language models (VLMs) is under explored. Existing methods focus on models designed for single tasks. Furthermore, they're limited by the need for resource intensive pre training, additional parameter requirements, unaddressed modality prioritization and lack of clear benefit over non-retrieval baselines. This paper introduces RAVEN, a multitask retrieval augmented VLM framework that enhances base VLMs through efficient, task specific fine-tuning. By integrating retrieval augmented samples without the need for additional retrieval-specific parameters, we show that the model acquires retrieval properties that are effective across multiple tasks. Our results and extensive ablations across retrieved modalities for the image captioning and VQA tasks indicate significant performance improvements compared to non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a +3\\% accuracy on specific VQA question types. This underscores the efficacy of applying RAG approaches to VLMs, marking a stride toward more efficient and accessible multimodal learning.\n## Retrieval Augmented Generation in NLP\nRetrieval augmentation has become an important technique for improving natural language processing models.One of the first works in this area was kNN-LM by Khandelwal et al. (Khandelwal et al., 2020) who showed how interpolating over nearest neighbors from any text collection could improve generalization.This was followed by RETRO (Borgeaud et al., 2021), which scaled up the retrieval corpus to trillions of tokens.Another line of work has focused on integrating Wikipedia passages directly into models like REALM (Guu et al., 2020), RAG (Lewis et al., 2020), and FiD (Izacard and Grave, 2021).By retrieving and conditioning on relevant Wikipedia passages, these models can better perform knowledge-intensive downstream tasks like question answering.Overall, retrieval augmentation has proven to be a highly effective way of injecting knowledge into language models to improve their capabilities.The techniques have progressed from simple corpus retrieval to integrated and scalable architectures that retrieve from large knowledge bases like Wikipedia.\n\nFigure 1: Illustration of our RAVEN framework.Given an input image, we retrieve image-text pairs from an external memory.Subsequently, we use a multitask pretrained base vision-language model (VLM) to encode the retrieved samples along with the query and decode to generate an output by attending over both the query and retrieved samples.",
            "reference_string": "[270764768 | Rao et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Large Language Models and Information Retrieval",
            "venue": "International Journal For Multidisciplinary Research",
            "year": 2023,
            "reference_count": 13,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ijfmr.com/papers/2023/6/8841.pdf",
                "status": "HYBRID",
                "license": "CCBYSA",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.36948/ijfmr.2023.v05i06.8841?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.36948/ijfmr.2023.v05i06.8841, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2245395211",
                    "name": "Kalyani Pakhale"
                }
            ],
            "abstract": "This research article explores the synergistic integration of Optical Character Recognition (OCR) technology and Large Language Models (LLMs) to advance Information Retrieval (IR) processes. In a data-centric society, efficient IR is imperative, and the combination of OCR and LLMs presents a powerful solution. OCR transforms diverse document types into machine-readable formats, while LLMs excel in language understanding and generation. The article delves into the technical intricacies of these technologies, their seamless integration, and their potential to revolutionize information retrieval. By investigating their collaborative capabilities, this research contributes to the evolving landscape of natural language processing and information retrieval systems.",
            "corpus_id": 265329916,
            "sentences": [
                {
                    "corpus_id": "265329916",
                    "title": "Large Language Models and Information Retrieval",
                    "text": "The research investigates fundamental architectural designs integral to Large Language Models (LLMs): the encoder-decoder architecture and the decoder-only architecture. These structural frameworks have played a pivotal role in advancing language understanding and generation, laying the foundation for innovative applications across diverse domains. \n\nThe encoder-decoder architecture, a widely embraced structure in Natural Language Processing (NLP) tasks, comprises two essential components: an encoder and a decoder. The encoder processes the input sequence, generating a fixed-length context vector. Conversely, the decoder receives this context vector and produces the output sequence. Typically, recurrent layers such as LSTM or GRUs construct the encoder, processing the input sequentially and capturing contextual information. Attention mechanisms further enhance this architecture by enabling the decoder to selectively focus on different parts of the input during decoding. \n\nIn contrast, the decoder-only architecture revolves around a solitary transformer-based decoder, devoid of an explicit encoder. This architectural model generates the output sequence based on a learned positional representation of the input sequence. Initially introduced in encoder-decoder models, transformer-based architectures have been adapted for decoder-only models. They incorporate selfattention mechanisms, allowing the model to weigh the significance of each input token when generating the output token. The transformer decoder processes tokens in parallel, enhancing computational efficiency and significantly accelerating training and inference processes. These architectural paradigms have significantly propelled the field of language modeling. Encoder-decoder architectures have notably augmented language understanding by efficiently capturing contextual relationships in both input and output sequences, crucial for translation and summarization tasks. On the other hand, the decoder-only architecture, particularly with transformer models, has immensely elevated generation capabilities. The self-attention mechanism's ability to consider all input tokens simultaneously results in coherent and contextually appropriate text generation. Understanding these architectural designs is fundamental in the development of sophisticated LLMs adept at both understanding and generating human-like text. Their flexibility and effectiveness continue to drive innovation, paving the way for a multitude of applications in the realm of natural language processing.",
                    "score": 0.4124818366967099,
                    "section_title": "LLM Architecture",
                    "char_start_offset": 13306,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 169
                        },
                        {
                            "start": 170,
                            "end": 350
                        },
                        {
                            "start": 353,
                            "end": 520
                        },
                        {
                            "start": 521,
                            "end": 604
                        },
                        {
                            "start": 605,
                            "end": 691
                        },
                        {
                            "start": 692,
                            "end": 835
                        },
                        {
                            "start": 836,
                            "end": 984
                        },
                        {
                            "start": 987,
                            "end": 1114
                        },
                        {
                            "start": 1115,
                            "end": 1237
                        },
                        {
                            "start": 1238,
                            "end": 1360
                        },
                        {
                            "start": 1361,
                            "end": 1502
                        },
                        {
                            "start": 1503,
                            "end": 1656
                        },
                        {
                            "start": 1657,
                            "end": 1747
                        },
                        {
                            "start": 1748,
                            "end": 1958
                        },
                        {
                            "start": 1959,
                            "end": 2094
                        },
                        {
                            "start": 2095,
                            "end": 2243
                        },
                        {
                            "start": 2244,
                            "end": 2401
                        },
                        {
                            "start": 2402,
                            "end": 2559
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8017578125
                }
            ],
            "relevance_judgement": 0.8017578125,
            "relevance_judgment_input_expanded": "# Title: Large Language Models and Information Retrieval\n# Venue: International Journal For Multidisciplinary Research\n# Authors: Kalyani Pakhale\n## Abstract\nThis research article explores the synergistic integration of Optical Character Recognition (OCR) technology and Large Language Models (LLMs) to advance Information Retrieval (IR) processes. In a data-centric society, efficient IR is imperative, and the combination of OCR and LLMs presents a powerful solution. OCR transforms diverse document types into machine-readable formats, while LLMs excel in language understanding and generation. The article delves into the technical intricacies of these technologies, their seamless integration, and their potential to revolutionize information retrieval. By investigating their collaborative capabilities, this research contributes to the evolving landscape of natural language processing and information retrieval systems.\n## LLM Architecture\nThe research investigates fundamental architectural designs integral to Large Language Models (LLMs): the encoder-decoder architecture and the decoder-only architecture. These structural frameworks have played a pivotal role in advancing language understanding and generation, laying the foundation for innovative applications across diverse domains. \n\nThe encoder-decoder architecture, a widely embraced structure in Natural Language Processing (NLP) tasks, comprises two essential components: an encoder and a decoder. The encoder processes the input sequence, generating a fixed-length context vector. Conversely, the decoder receives this context vector and produces the output sequence. Typically, recurrent layers such as LSTM or GRUs construct the encoder, processing the input sequentially and capturing contextual information. Attention mechanisms further enhance this architecture by enabling the decoder to selectively focus on different parts of the input during decoding. \n\nIn contrast, the decoder-only architecture revolves around a solitary transformer-based decoder, devoid of an explicit encoder. This architectural model generates the output sequence based on a learned positional representation of the input sequence. Initially introduced in encoder-decoder models, transformer-based architectures have been adapted for decoder-only models. They incorporate selfattention mechanisms, allowing the model to weigh the significance of each input token when generating the output token. The transformer decoder processes tokens in parallel, enhancing computational efficiency and significantly accelerating training and inference processes. These architectural paradigms have significantly propelled the field of language modeling. Encoder-decoder architectures have notably augmented language understanding by efficiently capturing contextual relationships in both input and output sequences, crucial for translation and summarization tasks. On the other hand, the decoder-only architecture, particularly with transformer models, has immensely elevated generation capabilities. The self-attention mechanism's ability to consider all input tokens simultaneously results in coherent and contextually appropriate text generation. Understanding these architectural designs is fundamental in the development of sophisticated LLMs adept at both understanding and generating human-like text. Their flexibility and effectiveness continue to drive innovation, paving the way for a multitude of applications in the realm of natural language processing.",
            "reference_string": "[265329916 | Pakhale | 2023 | Citations: 3]"
        },
        {
            "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 46,
            "citation_count": 641,
            "influential_citation_count": 57,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2301.12652",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.12652, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3040379",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "48872685",
                    "name": "Sewon Min"
                },
                {
                    "authorId": "19168196",
                    "name": "Michihiro Yasunaga"
                },
                {
                    "authorId": "4418074",
                    "name": "Minjoon Seo"
                },
                {
                    "authorId": "2191899140",
                    "name": "Rich James"
                },
                {
                    "authorId": "35084211",
                    "name": "M. Lewis"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "2072801764",
                    "name": "Wen-tau Yih"
                }
            ],
            "abstract": "We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%. Code is publicly released at github.com/swj0419/REPLUG.",
            "corpus_id": 256389797,
            "sentences": [
                {
                    "corpus_id": "256389797",
                    "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                    "text": "Retrieval-augmented Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language modeling (Min et al., 2022;Borgeaud et al., 2022;Khandelwal et al., 2020) and open-domain question answering (Lewis et al., 2020;Izacard et al., 2022b;Hu et al., 2022). Specifically, using the input as query, (1) a retriever first retrieves a set of documents (i.e., sequences of tokens) from a corpus and then (2) a language model incorporates the retrieved documents as additional information to make a final prediction. This style of retrieval can be added to both encoderdecoder (Yu, 2022;Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2020;Borgeaud et al., 2022;Shi et al., 2022;Rubin et al., 2022). For example, Atlas (Izacard et al., 2022b) finetunes an encoder-decoder model jointly with the retriever by modeling documents as latent variables, while RETRO (Borgeaud et al., 2022) changes the decoderonly architecture to incorporate retrieved texts and pretrains the language model from scratch. Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs. Another line of retrieval-augmented LMs such as kNN-LM (Khandelwal et al., 2020;Zhong et al., 2022) retrieves a set of tokens and interpolates between the LM's next token distribution and kNN distributions computed from the retrieved tokens at inference. Although kNN-LM does not require additional training, it requires access to internal LM representations to compute the kNN distribution, which are not always available for large LMs such as GPT-3. In this work, we investigate ways to improve large black-box language models with retrieval.",
                    "score": 0.4353487993929209,
                    "section_title": "Background and Related Work",
                    "char_start_offset": 5661,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 380
                        },
                        {
                            "start": 381,
                            "end": 634
                        },
                        {
                            "start": 635,
                            "end": 836
                        },
                        {
                            "start": 837,
                            "end": 1135
                        },
                        {
                            "start": 1136,
                            "end": 1254
                        },
                        {
                            "start": 1255,
                            "end": 1509
                        },
                        {
                            "start": 1510,
                            "end": 1706
                        },
                        {
                            "start": 1707,
                            "end": 1799
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 239,
                            "end": 261,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 261,
                            "end": 285,
                            "matchedPaperCorpusId": "207870430"
                        },
                        {
                            "start": 695,
                            "end": 705,
                            "matchedPaperCorpusId": "250391000"
                        },
                        {
                            "start": 752,
                            "end": 777,
                            "matchedPaperCorpusId": "207870430"
                        },
                        {
                            "start": 777,
                            "end": 799,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 816,
                            "end": 835,
                            "matchedPaperCorpusId": "245218561"
                        },
                        {
                            "start": 997,
                            "end": 1020,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 1310,
                            "end": 1335,
                            "matchedPaperCorpusId": "207870430"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7978515625
                }
            ],
            "relevance_judgement": 0.7978515625,
            "relevance_judgment_input_expanded": "# Title: REPLUG: Retrieval-Augmented Black-Box Language Models\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, M. Lewis, Luke Zettlemoyer, Wen-tau Yih\n## Abstract\nWe introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%. Code is publicly released at github.com/swj0419/REPLUG.\n## Background and Related Work\nRetrieval-augmented Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language modeling (Min et al., 2022;Borgeaud et al., 2022;Khandelwal et al., 2020) and open-domain question answering (Lewis et al., 2020;Izacard et al., 2022b;Hu et al., 2022). Specifically, using the input as query, (1) a retriever first retrieves a set of documents (i.e., sequences of tokens) from a corpus and then (2) a language model incorporates the retrieved documents as additional information to make a final prediction. This style of retrieval can be added to both encoderdecoder (Yu, 2022;Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2020;Borgeaud et al., 2022;Shi et al., 2022;Rubin et al., 2022). For example, Atlas (Izacard et al., 2022b) finetunes an encoder-decoder model jointly with the retriever by modeling documents as latent variables, while RETRO (Borgeaud et al., 2022) changes the decoderonly architecture to incorporate retrieved texts and pretrains the language model from scratch. Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs. Another line of retrieval-augmented LMs such as kNN-LM (Khandelwal et al., 2020;Zhong et al., 2022) retrieves a set of tokens and interpolates between the LM's next token distribution and kNN distributions computed from the retrieved tokens at inference. Although kNN-LM does not require additional training, it requires access to internal LM representations to compute the kNN distribution, which are not always available for large LMs such as GPT-3. In this work, we investigate ways to improve large black-box language models with retrieval.",
            "reference_string": "[256389797 | Shi et al. | 2023 | Citations: 641]"
        },
        {
            "title": "Zero-Indexing Internet Search Augmented Generation for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 53,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.19478, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2333982900",
                    "name": "Guangxin He"
                },
                {
                    "authorId": "83732784",
                    "name": "Zonghong Dai"
                },
                {
                    "authorId": "2290357307",
                    "name": "Jiangcheng Zhu"
                },
                {
                    "authorId": "2333537636",
                    "name": "Binqiang Zhao"
                },
                {
                    "authorId": "2313032000",
                    "name": "Chenyue Li"
                },
                {
                    "authorId": "2333318691",
                    "name": "You Peng"
                },
                {
                    "authorId": "2333373998",
                    "name": "Chen Wang"
                },
                {
                    "authorId": "2312922261",
                    "name": "Binhang Yuan"
                }
            ],
            "abstract": "Retrieval augmented generation has emerged as an effective method to enhance large language model performance. This approach typically relies on an internal retrieval module that uses various indexing mechanisms to manage a static pre-processed corpus. However, such a paradigm often falls short when it is necessary to integrate the most up-to-date information that has not been updated into the corpus during generative inference time. In this paper, we explore an alternative approach that leverages standard search engine APIs to dynamically integrate the latest online information (without maintaining any index for any fixed corpus), thereby improving the quality of generated content. We design a collaborative LLM-based paradigm, where we include: (i) a parser-LLM that determines if the Internet augmented generation is demanded and extracts the search keywords if so with a single inference; (ii) a mixed ranking strategy that re-ranks the retrieved HTML files to eliminate bias introduced from the search engine API; and (iii) an extractor-LLM that can accurately and efficiently extract relevant information from the fresh content in each HTML file. We conduct extensive empirical studies to evaluate the performance of this Internet search augmented generation paradigm. The experimental results demonstrate that our method generates content with significantly improved quality. Our system has been successfully deployed in a production environment to serve 01.AI's generative inference requests.",
            "corpus_id": 274423377,
            "sentences": [
                {
                    "corpus_id": "274423377",
                    "title": "Zero-Indexing Internet Search Augmented Generation for Large Language Models",
                    "text": "Large language models (LLM) have demonstrated remarkable capabilities, fueling a new wave of innovative AI applications [1]. To incorporate information not included during their training phase, retrieval augmented generation (RAG) has been developed as an efficient method to enhance LLM performance by integrating externally retrieved information [2,3]. Usually, RAG systems use an internal retrieval module that employs various indexing mechanisms to manage a static corpus. In this paper, we study an alternative approach, exploring the design and implementation of a RAG paradigm that leverages standard search engine APIs (such as Google and Bing), which allows for the flexible and dynamic integration of the most update-to-date online information, thereby improving the quality of content generated by LLMs. \n\nWhile RAG systems with a static retrieval component, which leverages a fixed corpus of data, are effective for tasks within well-defined knowledge domains, Internet search augmented generation [4,5,6,7,8,9] offers distinct advantages. By accessing the most update-to-date information available online, internet search augmented generation is particularly beneficial for tasks that require up-to-date data, such as news updates, market trends, or recent scientific discoveries, enabling the model to produce more relevant and timely responses [9]. Moreover, Internet search augmented systems can retrieve information from a vast array of sources across the web, encompassing a broader range of topics and perspectives, thereby enhancing the comprehensiveness and diversity of the generated content [6]. In fact, OpenAI has also released its toolkit CHATGPT-SEARCH to integrate information from the Internet to improve the user experience very recently [10].",
                    "score": 0.39857759446045876,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 124
                        },
                        {
                            "start": 125,
                            "end": 354
                        },
                        {
                            "start": 355,
                            "end": 476
                        },
                        {
                            "start": 477,
                            "end": 814
                        },
                        {
                            "start": 817,
                            "end": 1051
                        },
                        {
                            "start": 1052,
                            "end": 1363
                        },
                        {
                            "start": 1364,
                            "end": 1618
                        },
                        {
                            "start": 1619,
                            "end": 1773
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1013,
                            "end": 1015,
                            "matchedPaperCorpusId": "236034557"
                        },
                        {
                            "start": 1019,
                            "end": 1021,
                            "matchedPaperCorpusId": "258190866"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7958984375
                }
            ],
            "relevance_judgement": 0.7958984375,
            "relevance_judgment_input_expanded": "# Title: Zero-Indexing Internet Search Augmented Generation for Large Language Models\n# Venue: arXiv.org\n# Authors: Guangxin He, Zonghong Dai, Jiangcheng Zhu, Binqiang Zhao, Chenyue Li, You Peng, Chen Wang, Binhang Yuan\n## Abstract\nRetrieval augmented generation has emerged as an effective method to enhance large language model performance. This approach typically relies on an internal retrieval module that uses various indexing mechanisms to manage a static pre-processed corpus. However, such a paradigm often falls short when it is necessary to integrate the most up-to-date information that has not been updated into the corpus during generative inference time. In this paper, we explore an alternative approach that leverages standard search engine APIs to dynamically integrate the latest online information (without maintaining any index for any fixed corpus), thereby improving the quality of generated content. We design a collaborative LLM-based paradigm, where we include: (i) a parser-LLM that determines if the Internet augmented generation is demanded and extracts the search keywords if so with a single inference; (ii) a mixed ranking strategy that re-ranks the retrieved HTML files to eliminate bias introduced from the search engine API; and (iii) an extractor-LLM that can accurately and efficiently extract relevant information from the fresh content in each HTML file. We conduct extensive empirical studies to evaluate the performance of this Internet search augmented generation paradigm. The experimental results demonstrate that our method generates content with significantly improved quality. Our system has been successfully deployed in a production environment to serve 01.AI's generative inference requests.\n## Introduction\nLarge language models (LLM) have demonstrated remarkable capabilities, fueling a new wave of innovative AI applications [1]. To incorporate information not included during their training phase, retrieval augmented generation (RAG) has been developed as an efficient method to enhance LLM performance by integrating externally retrieved information [2,3]. Usually, RAG systems use an internal retrieval module that employs various indexing mechanisms to manage a static corpus. In this paper, we study an alternative approach, exploring the design and implementation of a RAG paradigm that leverages standard search engine APIs (such as Google and Bing), which allows for the flexible and dynamic integration of the most update-to-date online information, thereby improving the quality of content generated by LLMs. \n\nWhile RAG systems with a static retrieval component, which leverages a fixed corpus of data, are effective for tasks within well-defined knowledge domains, Internet search augmented generation [4,5,6,7,8,9] offers distinct advantages. By accessing the most update-to-date information available online, internet search augmented generation is particularly beneficial for tasks that require up-to-date data, such as news updates, market trends, or recent scientific discoveries, enabling the model to produce more relevant and timely responses [9]. Moreover, Internet search augmented systems can retrieve information from a vast array of sources across the web, encompassing a broader range of topics and perspectives, thereby enhancing the comprehensiveness and diversity of the generated content [6]. In fact, OpenAI has also released its toolkit CHATGPT-SEARCH to integrate information from the Internet to improve the user experience very recently [10].",
            "reference_string": "[274423377 | He et al. | 2024 | Citations: 1]"
        },
        {
            "title": "R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models",
            "venue": "European Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 51,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.02659, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2146342371",
                    "name": "Taolin Zhang"
                },
                {
                    "authorId": "2257089368",
                    "name": "Dongyang Li"
                },
                {
                    "authorId": "2300139675",
                    "name": "Qizhou Chen"
                },
                {
                    "authorId": "50097294",
                    "name": "Chengyu Wang"
                },
                {
                    "authorId": "2292090586",
                    "name": "Longtao Huang"
                },
                {
                    "authorId": "2292128230",
                    "name": "Hui Xue"
                },
                {
                    "authorId": "2257159827",
                    "name": "Xiaofeng He"
                },
                {
                    "authorId": "2272790856",
                    "name": "Junyuan Huang"
                }
            ],
            "abstract": "Retrieval-augmented large language models (LLMs) leverage relevant content retrieved by information retrieval systems to generate correct responses, aiming to alleviate the hallucination problem. However, existing retriever-responder methods typically append relevant documents to the prompt of LLMs to perform text generation tasks without considering the interaction of fine-grained structural semantics between the retrieved documents and the LLMs. This issue is particularly important for accurate response generation as LLMs tend to\"lose in the middle\"when dealing with input prompts augmented with lengthy documents. In this work, we propose a new pipeline named\"Reinforced Retriever-Reorder-Responder\"(R$^4$) to learn document orderings for retrieval-augmented LLMs, thereby further enhancing their generation abilities while the large numbers of parameters of LLMs remain frozen. The reordering learning process is divided into two steps according to the quality of the generated responses: document order adjustment and document representation enhancement. Specifically, document order adjustment aims to organize retrieved document orderings into beginning, middle, and end positions based on graph attention learning, which maximizes the reinforced reward of response quality. Document representation enhancement further refines the representations of retrieved documents for responses of poor quality via document-level gradient adversarial learning. Extensive experiments demonstrate that our proposed pipeline achieves better factual question-answering performance on knowledge-intensive tasks compared to strong baselines across various public datasets. The source codes and trained models will be released upon paper acceptance.",
            "corpus_id": 269605025,
            "sentences": [
                {
                    "corpus_id": "269605025",
                    "title": "R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models",
                    "text": "Recently, large language models (LLMs) have attracted extensive attention, which are typically pre-trained on large datasets and implicitly store substantial amounts of world or domain knowledge [33,45]. However, LLMs are also prone to the hallucination problem, and thus, they may generate erroneous responses [49]. In contrast, retrieval-augmented LLMs [17,10,47,35] retrieve knowledge from an external datastore when needed, thereby reducing hallucinations and increasing the knowledge coverage in responses. \n\nIn the literature, there are two major research aspects in this field: \n\n(1) Datastore Indexing [17,10,44,48] and (2) Document Retrieval [35,27]. For Datastore Indexing, these approaches utilize pre-trained models to generate static embeddings for documents, which are viewed as mounted external memory, and they leverage various semantic similarities to enhance indexing. For Document Retrieval, the system initially retrieves a collection of relevant documents based on the semantic relevance between the user query and the documents. Then, the LLMs concatenate these highly related documents in an unordered manner to the prompt input [4], which makes LLMs better at answering factual questions. These methods essentially organize the information related to the user query from the perspective of coarse-grained memory , ignoring the fine-grained relationships between retrieved documents and the knowledge mastery characteristics of LLMs [14,22]. For instance, the ordering of the top-K retrieved documents can be further adjusted to enhance the performance of retrieval-augmented LLMs in answering questions more accurately, as illustrated in Figure 1. \n\nIn this paper, we propose the Reinforced Retriever-Reorder-Responder framework (R 4 ) to formalize a new retrieval-augmented generation (RAG) pipeline. To reorder the retrieved top-K documents and enhance the response effectiveness of the LLMs, we divide the reorder learning process into the following two steps: Document Order Adjustment: Prior research indicates that LLMs have a better recall of information at the beginning and the ending positions of retrieved documents in prompts [14,22].",
                    "score": 0.39857759446045876,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 316
                        },
                        {
                            "start": 317,
                            "end": 511
                        },
                        {
                            "start": 514,
                            "end": 584
                        },
                        {
                            "start": 587,
                            "end": 659
                        },
                        {
                            "start": 660,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1050
                        },
                        {
                            "start": 1051,
                            "end": 1212
                        },
                        {
                            "start": 1213,
                            "end": 1464
                        },
                        {
                            "start": 1465,
                            "end": 1671
                        },
                        {
                            "start": 1674,
                            "end": 1825
                        },
                        {
                            "start": 1826,
                            "end": 2170
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 199,
                            "end": 202,
                            "matchedPaperCorpusId": "246411621"
                        },
                        {
                            "start": 355,
                            "end": 359,
                            "matchedPaperCorpusId": "207870430"
                        },
                        {
                            "start": 362,
                            "end": 365,
                            "matchedPaperCorpusId": "253802096"
                        },
                        {
                            "start": 610,
                            "end": 614,
                            "matchedPaperCorpusId": "207870430"
                        },
                        {
                            "start": 617,
                            "end": 620,
                            "matchedPaperCorpusId": "249674500"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7939453125
                }
            ],
            "relevance_judgement": 0.7939453125,
            "relevance_judgment_input_expanded": "# Title: R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models\n# Venue: European Conference on Artificial Intelligence\n# Authors: Taolin Zhang, Dongyang Li, Qizhou Chen, Chengyu Wang, Longtao Huang, Hui Xue, Xiaofeng He, Junyuan Huang\n## Abstract\nRetrieval-augmented large language models (LLMs) leverage relevant content retrieved by information retrieval systems to generate correct responses, aiming to alleviate the hallucination problem. However, existing retriever-responder methods typically append relevant documents to the prompt of LLMs to perform text generation tasks without considering the interaction of fine-grained structural semantics between the retrieved documents and the LLMs. This issue is particularly important for accurate response generation as LLMs tend to\"lose in the middle\"when dealing with input prompts augmented with lengthy documents. In this work, we propose a new pipeline named\"Reinforced Retriever-Reorder-Responder\"(R$^4$) to learn document orderings for retrieval-augmented LLMs, thereby further enhancing their generation abilities while the large numbers of parameters of LLMs remain frozen. The reordering learning process is divided into two steps according to the quality of the generated responses: document order adjustment and document representation enhancement. Specifically, document order adjustment aims to organize retrieved document orderings into beginning, middle, and end positions based on graph attention learning, which maximizes the reinforced reward of response quality. Document representation enhancement further refines the representations of retrieved documents for responses of poor quality via document-level gradient adversarial learning. Extensive experiments demonstrate that our proposed pipeline achieves better factual question-answering performance on knowledge-intensive tasks compared to strong baselines across various public datasets. The source codes and trained models will be released upon paper acceptance.\n## Introduction\nRecently, large language models (LLMs) have attracted extensive attention, which are typically pre-trained on large datasets and implicitly store substantial amounts of world or domain knowledge [33,45]. However, LLMs are also prone to the hallucination problem, and thus, they may generate erroneous responses [49]. In contrast, retrieval-augmented LLMs [17,10,47,35] retrieve knowledge from an external datastore when needed, thereby reducing hallucinations and increasing the knowledge coverage in responses. \n\nIn the literature, there are two major research aspects in this field: \n\n(1) Datastore Indexing [17,10,44,48] and (2) Document Retrieval [35,27]. For Datastore Indexing, these approaches utilize pre-trained models to generate static embeddings for documents, which are viewed as mounted external memory, and they leverage various semantic similarities to enhance indexing. For Document Retrieval, the system initially retrieves a collection of relevant documents based on the semantic relevance between the user query and the documents. Then, the LLMs concatenate these highly related documents in an unordered manner to the prompt input [4], which makes LLMs better at answering factual questions. These methods essentially organize the information related to the user query from the perspective of coarse-grained memory , ignoring the fine-grained relationships between retrieved documents and the knowledge mastery characteristics of LLMs [14,22]. For instance, the ordering of the top-K retrieved documents can be further adjusted to enhance the performance of retrieval-augmented LLMs in answering questions more accurately, as illustrated in Figure 1. \n\nIn this paper, we propose the Reinforced Retriever-Reorder-Responder framework (R 4 ) to formalize a new retrieval-augmented generation (RAG) pipeline. To reorder the retrieved top-K documents and enhance the response effectiveness of the LLMs, we divide the reorder learning process into the following two steps: Document Order Adjustment: Prior research indicates that LLMs have a better recall of information at the beginning and the ending positions of retrieved documents in prompts [14,22].",
            "reference_string": "[269605025 | Zhang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 20,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.21059, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280046531",
                    "name": "Yunfan Gao"
                },
                {
                    "authorId": "2275320371",
                    "name": "Yun Xiong"
                },
                {
                    "authorId": "2291409458",
                    "name": "Meng Wang"
                },
                {
                    "authorId": "2256769434",
                    "name": "Haofen Wang"
                }
            ],
            "abstract": "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of\"retrieve-then-generate\". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies.",
            "corpus_id": 271571401,
            "sentences": [
                {
                    "corpus_id": "271571401",
                    "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
                    "text": "The development of RAG technology can be summarized in three stages. Initially, retrieval-augmented techniques were introduced to improve the performance of pre-trained language models on knowledge-intensive tasks [19], [20]. In specific implementations, Retro [21] optimized pre-trained autoregressive models through retrieval augmentation, while Atlas [22] utilized a retrieval-augmented few-shot fine-tuning method, enabling language models to adapt to diverse tasks. IRCOT [23] further enriched the reasoning process during the inference phase by combining chain-of-thought and multistep retrieval processes. Entering the second stage, as the language processing capabilities of LLMs significantly improved, retrieval-augmented techniques began to serve as a means of supplementing additional knowledge and providing references, aiming to reduce the hallucination. For instance, RRR [24] improved the rewriting phase, and LLMlingua [25] removed redundant tokens in retrieved document chunks. With the continuous progress of RAG technology, research has become more refined and focused, while also achieving innovative integration with other technologies such as graph neural networks [26] and fine-tuning techniques [27]. The overall pipeline has also become more flexible, such as using LLMs to proactively determine the timing of retrieval and generation [14], [28]. \n\nThe development of RAG technology has been accelerated by LLM technology and practical application needs. Researchers are examining and organizing the RAG framework and development pathways from different perspectives. Building upon the enhanced stages of RAG, Gao et al., [2] subdivided RAG into enhancement during pre-training, inference, and fine-tuning stages. Based on the main processes of RAG, relevant works on RAG were organized from the perspectives of retrieval, generation, and augmentation methods. Huang et al., [29] categorize RAG methods into four main classes: pre-retrieval, retrieval, post-retrieval, generation, and provide a detailed discussion of the methods and techniques within each class. Hu et al., [30] discuss Retrieval-Augmented Language Models (RALMs) form three key components, including retrievers, language models, augmentations, and how their interactions lead to different model structures and applications.",
                    "score": 0.37955109953353117,
                    "section_title": "II. RELATED WORK",
                    "char_start_offset": 8318,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 68
                        },
                        {
                            "start": 69,
                            "end": 225
                        },
                        {
                            "start": 226,
                            "end": 470
                        },
                        {
                            "start": 471,
                            "end": 612
                        },
                        {
                            "start": 613,
                            "end": 868
                        },
                        {
                            "start": 869,
                            "end": 995
                        },
                        {
                            "start": 996,
                            "end": 1225
                        },
                        {
                            "start": 1226,
                            "end": 1372
                        },
                        {
                            "start": 1375,
                            "end": 1480
                        },
                        {
                            "start": 1481,
                            "end": 1593
                        },
                        {
                            "start": 1594,
                            "end": 1739
                        },
                        {
                            "start": 1740,
                            "end": 1886
                        },
                        {
                            "start": 1887,
                            "end": 2089
                        },
                        {
                            "start": 2090,
                            "end": 2318
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 220,
                            "end": 224,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 261,
                            "end": 265,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 936,
                            "end": 940,
                            "matchedPaperCorpusId": "252186384"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7841796875
                }
            ],
            "relevance_judgement": 0.7841796875,
            "relevance_judgment_input_expanded": "# Title: Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n# Venue: arXiv.org\n# Authors: Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n## Abstract\nRetrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of\"retrieve-then-generate\". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies.\n## II. RELATED WORK\nThe development of RAG technology can be summarized in three stages. Initially, retrieval-augmented techniques were introduced to improve the performance of pre-trained language models on knowledge-intensive tasks [19], [20]. In specific implementations, Retro [21] optimized pre-trained autoregressive models through retrieval augmentation, while Atlas [22] utilized a retrieval-augmented few-shot fine-tuning method, enabling language models to adapt to diverse tasks. IRCOT [23] further enriched the reasoning process during the inference phase by combining chain-of-thought and multistep retrieval processes. Entering the second stage, as the language processing capabilities of LLMs significantly improved, retrieval-augmented techniques began to serve as a means of supplementing additional knowledge and providing references, aiming to reduce the hallucination. For instance, RRR [24] improved the rewriting phase, and LLMlingua [25] removed redundant tokens in retrieved document chunks. With the continuous progress of RAG technology, research has become more refined and focused, while also achieving innovative integration with other technologies such as graph neural networks [26] and fine-tuning techniques [27]. The overall pipeline has also become more flexible, such as using LLMs to proactively determine the timing of retrieval and generation [14], [28]. \n\nThe development of RAG technology has been accelerated by LLM technology and practical application needs. Researchers are examining and organizing the RAG framework and development pathways from different perspectives. Building upon the enhanced stages of RAG, Gao et al., [2] subdivided RAG into enhancement during pre-training, inference, and fine-tuning stages. Based on the main processes of RAG, relevant works on RAG were organized from the perspectives of retrieval, generation, and augmentation methods. Huang et al., [29] categorize RAG methods into four main classes: pre-retrieval, retrieval, post-retrieval, generation, and provide a detailed discussion of the methods and techniques within each class. Hu et al., [30] discuss Retrieval-Augmented Language Models (RALMs) form three key components, including retrievers, language models, augmentations, and how their interactions lead to different model structures and applications.",
            "reference_string": "[271571401 | Gao et al. | 2024 | Citations: 20]"
        },
        {
            "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 45,
            "citation_count": 179,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00530/2067834/tacl_a_00530.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.02627, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51516859",
                    "name": "Shamane Siriwardhana"
                },
                {
                    "authorId": "52001535",
                    "name": "Rivindu Weerasekera"
                },
                {
                    "authorId": "2114425044",
                    "name": "Elliott Wen"
                },
                {
                    "authorId": "1992921690",
                    "name": "Tharindu Kaluarachchi"
                },
                {
                    "authorId": "1814487",
                    "name": "R. Rana"
                },
                {
                    "authorId": "1486464114",
                    "name": "Suranga Nanayakkara"
                }
            ],
            "abstract": "Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work\u2019s credibility and technical consistency.",
            "corpus_id": 252735056,
            "sentences": [
                {
                    "corpus_id": "252735056",
                    "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering",
                    "text": "Recently, Retrieval Augmented Architectures (Lewis et al., 2020b;Guu et al., 2020) have drawn a lot of attention due to their explainable, scalable, and adaptable nature. Unlike other open-domain QA architectures, RAG (Lewis et al., 2020b) combines the information retrieval stage and answer generation stage in a differentiable manner. It uses a combination of parametric and non-parametric memory, where the parametric memory consists of a pre-trained seq2seq BART (Lewis et al., 2019) generator, and the non-parametric memory consists of dense vector representations of Wikipedia articles indexed with the FAISS library (Johnson et al., 2017). RAG first encodes a question into a dense representation, retrieves the relevant passages from an indexed Wikipedia knowledge base, and then feeds them into the generator. The loss function can finetune both the generator and the question encoder at the same time. Lewis et al. (Lewis et al., 2020b) highlight RAG's ability to perform well in Wikipedia-based general question-answering datasets like Natural Questions (Kwiatkowski et al., 2019). Other recent work also highlights how the outputs generated from RAG models are much more factual due to RAG being conditioned on the retrieved documents, possibly providing an answer to the hallucination problem of generative language models. Shuster, Kurt, et al. (Shuster et al., 2021) also highlight how RAG reduces hallucinations in knowledge-grounded conversational tasks, where the task is to generate responses to dialogues based on a large Wikipedia knowledge base. Xu et al. (2021) illustrate the effectiveness of RAG in chat-bot frameworks and highlight how RAG models are able to recall and summarize conversations compared to standard seq2seq models with only parametric memory. This paper aims to understand how RAG could be extended to an end2end model and adapted to specific domains. To the best of our knowledge, this is the first time RAG is being investigated on domain adaptation for the task of ODQA systems. (Guu et al., 2020) is a similar Retrieval Augmented model to RAG. REALM introduced a novel masked language pre-training step that involves an end-to-end trainable retriever.",
                    "score": 0.511998214682212,
                    "section_title": "Retrieval Augmented Architecture",
                    "char_start_offset": 6206,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 170
                        },
                        {
                            "start": 171,
                            "end": 336
                        },
                        {
                            "start": 337,
                            "end": 646
                        },
                        {
                            "start": 647,
                            "end": 818
                        },
                        {
                            "start": 819,
                            "end": 911
                        },
                        {
                            "start": 912,
                            "end": 1092
                        },
                        {
                            "start": 1093,
                            "end": 1336
                        },
                        {
                            "start": 1337,
                            "end": 1567
                        },
                        {
                            "start": 1568,
                            "end": 1784
                        },
                        {
                            "start": 1785,
                            "end": 1893
                        },
                        {
                            "start": 1894,
                            "end": 2023
                        },
                        {
                            "start": 2024,
                            "end": 2089
                        },
                        {
                            "start": 2090,
                            "end": 2197
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77880859375
                }
            ],
            "relevance_judgement": 0.77880859375,
            "relevance_judgment_input_expanded": "# Title: Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering\n# Venue: Transactions of the Association for Computational Linguistics\n# Authors: Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, R. Rana, Suranga Nanayakkara\n## Abstract\nRetrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work\u2019s credibility and technical consistency.\n## Retrieval Augmented Architecture\nRecently, Retrieval Augmented Architectures (Lewis et al., 2020b;Guu et al., 2020) have drawn a lot of attention due to their explainable, scalable, and adaptable nature. Unlike other open-domain QA architectures, RAG (Lewis et al., 2020b) combines the information retrieval stage and answer generation stage in a differentiable manner. It uses a combination of parametric and non-parametric memory, where the parametric memory consists of a pre-trained seq2seq BART (Lewis et al., 2019) generator, and the non-parametric memory consists of dense vector representations of Wikipedia articles indexed with the FAISS library (Johnson et al., 2017). RAG first encodes a question into a dense representation, retrieves the relevant passages from an indexed Wikipedia knowledge base, and then feeds them into the generator. The loss function can finetune both the generator and the question encoder at the same time. Lewis et al. (Lewis et al., 2020b) highlight RAG's ability to perform well in Wikipedia-based general question-answering datasets like Natural Questions (Kwiatkowski et al., 2019). Other recent work also highlights how the outputs generated from RAG models are much more factual due to RAG being conditioned on the retrieved documents, possibly providing an answer to the hallucination problem of generative language models. Shuster, Kurt, et al. (Shuster et al., 2021) also highlight how RAG reduces hallucinations in knowledge-grounded conversational tasks, where the task is to generate responses to dialogues based on a large Wikipedia knowledge base. Xu et al. (2021) illustrate the effectiveness of RAG in chat-bot frameworks and highlight how RAG models are able to recall and summarize conversations compared to standard seq2seq models with only parametric memory. This paper aims to understand how RAG could be extended to an end2end model and adapted to specific domains. To the best of our knowledge, this is the first time RAG is being investigated on domain adaptation for the task of ODQA systems. (Guu et al., 2020) is a similar Retrieval Augmented model to RAG. REALM introduced a novel masked language pre-training step that involves an end-to-end trainable retriever.",
            "reference_string": "[252735056 | Siriwardhana et al. | 2022 | Citations: 179]"
        },
        {
            "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 153,
            "citation_count": 51,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.10981, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2260272949",
                    "name": "Yizheng Huang"
                },
                {
                    "authorId": "2259653248",
                    "name": "Jimmy X. Huang"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.",
            "corpus_id": 269188036,
            "sentences": [
                {
                    "corpus_id": "269188036",
                    "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
                    "text": "To provide clarity and structure, this paper is organized as follows: Section 2 outlines the overall RAG workflow, dividing the methodologies into pre-retrieval, retrieval, post-retrieval, and generation phases. Sections 3 through 6 explore the core techniques within each phase. Section 7 focuses on the evaluation methodologies for RAG. Section 8 summarizes the reviewed studies, detailing the retrievers and generators used, while Section 9 discusses challenges and future research directions, extending beyond text-based studies to include multimodal data applications. The paper concludes with Section 10. \n\nOther related surveys provide valuable insights into the evolving RAG landscape from different angles. Gao et al. [38] identified three key stages in RAG development: pre-training enhancement, inference, and fine-tuning. Zhao et al. [162] focused on the diverse applications of RAG, including text, code, image, and video generation, emphasizing augmented intelligence in generative tasks. Meanwhile, Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how interactions between retrievers, language models, and augmentations influence model architectures and applications. \n\nIn this paper, we aim to offer a comprehensive and unified framework for understanding RAG from an information retrieval (IR) perspective, identifying key challenges and areas for improvement. We delve into the core technologies that drive RAG, assessing their effectiveness in addressing retrieval and generation tasks. Additionally, this survey introduces the evaluation methods employed in RAG research, highlights current limitations, and proposes promising avenues for future exploration.",
                    "score": 0.42433399003964706,
                    "section_title": "Introduction",
                    "char_start_offset": 2330,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 279
                        },
                        {
                            "start": 280,
                            "end": 338
                        },
                        {
                            "start": 339,
                            "end": 573
                        },
                        {
                            "start": 574,
                            "end": 610
                        },
                        {
                            "start": 613,
                            "end": 715
                        },
                        {
                            "start": 716,
                            "end": 833
                        },
                        {
                            "start": 834,
                            "end": 1002
                        },
                        {
                            "start": 1003,
                            "end": 1212
                        },
                        {
                            "start": 1215,
                            "end": 1407
                        },
                        {
                            "start": 1408,
                            "end": 1535
                        },
                        {
                            "start": 1536,
                            "end": 1708
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77880859375
                }
            ],
            "relevance_judgement": 0.77880859375,
            "relevance_judgment_input_expanded": "# Title: A Survey on Retrieval-Augmented Text Generation for Large Language Models\n# Venue: arXiv.org\n# Authors: Yizheng Huang, Jimmy X. Huang\n## Abstract\nRetrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.\n## Introduction\nTo provide clarity and structure, this paper is organized as follows: Section 2 outlines the overall RAG workflow, dividing the methodologies into pre-retrieval, retrieval, post-retrieval, and generation phases. Sections 3 through 6 explore the core techniques within each phase. Section 7 focuses on the evaluation methodologies for RAG. Section 8 summarizes the reviewed studies, detailing the retrievers and generators used, while Section 9 discusses challenges and future research directions, extending beyond text-based studies to include multimodal data applications. The paper concludes with Section 10. \n\nOther related surveys provide valuable insights into the evolving RAG landscape from different angles. Gao et al. [38] identified three key stages in RAG development: pre-training enhancement, inference, and fine-tuning. Zhao et al. [162] focused on the diverse applications of RAG, including text, code, image, and video generation, emphasizing augmented intelligence in generative tasks. Meanwhile, Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how interactions between retrievers, language models, and augmentations influence model architectures and applications. \n\nIn this paper, we aim to offer a comprehensive and unified framework for understanding RAG from an information retrieval (IR) perspective, identifying key challenges and areas for improvement. We delve into the core technologies that drive RAG, assessing their effectiveness in addressing retrieval and generation tasks. Additionally, this survey introduces the evaluation methods employed in RAG research, highlights current limitations, and proposes promising avenues for future exploration.",
            "reference_string": "[269188036 | Huang et al. | 2024 | Citations: 51]"
        },
        {
            "title": "Domain Adaptive Code Completion via Language Models and Decoupled Domain Databases",
            "venue": "International Conference on Automated Software Engineering",
            "year": 2023,
            "reference_count": 57,
            "citation_count": 26,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.09313",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.09313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109915677",
                    "name": "Ze Tang"
                },
                {
                    "authorId": "2669512",
                    "name": "Jidong Ge"
                },
                {
                    "authorId": "13877308",
                    "name": "Shangqing Liu"
                },
                {
                    "authorId": "3274600",
                    "name": "Tingwei Zhu"
                },
                {
                    "authorId": "2118717147",
                    "name": "Tongtong Xu"
                },
                {
                    "authorId": "1482584966",
                    "name": "LiGuo Huang"
                },
                {
                    "authorId": "2075400450",
                    "name": "Bin Luo"
                }
            ],
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in code completion. However, due to the lack of domain-specific knowledge, they may not be optimal in completing code that requires intensive domain knowledge for example completing the library names. Although there are several works that have confirmed the effectiveness of fine-tuning techniques to adapt language models for code completion in specific domains. They are limited by the need for constant fine-tuning of the model when the project is in constant iteration. To address this limitation, in this paper, we propose $k$ NM-LM, a retrieval-augmented language model (R-LM), that integrates domain knowledge into language models without fine-tuning. Different from previous techniques, our approach is able to automatically adapt to different language models and domains. Specifically, it utilizes the in-domain code to build the retrieval-based database decoupled from LM, and then combines it with LM through Bayesian inference to complete the code. The extensive experiments on the completion of intra-project and intra-scenario have confirmed that $k$ NM-LM brings about appreciable enhancements when compared to CodeGPT and UnixCoder. A deep analysis of our tool including the responding speed, storage usage, specific type code completion, and API invocation completion has confirmed that $k$ NM-LM provides satisfactory performance, which renders it highly appropriate for domain adaptive code completion. Furthermore, our approach operates without the requirement for direct access to the language model's parameters. As a result, it can seamlessly integrate with black-box code completion models, making it easy to integrate our approach as a plugin to further enhance the performance of these models.",
            "corpus_id": 261030382,
            "sentences": [
                {
                    "corpus_id": "261030382",
                    "title": "Domain Adaptive Code Completion via Language Models and Decoupled Domain Databases",
                    "text": "Retrieval-augmented language models (R-LMs) utilize retrieval-based techniques to improve the performance of LMs and can be divided into two main categories: block R-LMs and token R-LMs. Block R-LMs [12], [13], [48] are similar to one-shot or few-shot learning [49], where one or a few examples are retrieved from a database instead of being randomly selected. Token R-LMs [15], [24], [50] retrieve tokens from database and then combine the retrieved results into the LM. Compared with block R-LMs, token R-LMs can update retrieval results at the same time of generating new tokens, hence our approach uses the architecture of token R-LM. However, token R-LMs suffer from high storage costs and require hyper-parameters selection to combine the inference results from the database and language model. \n\nVarious approaches have been proposed to address the limitations of token R-LMs. For example, kNN-Adapter [51] uses a trained network to determine the combination weights. To reduce the search cost, RetoMaton [52] uses the automaton states to save search time, while AdaptRet [53] uses a trained network to decide whether to use the retrieval module. GNN-LM [50] selects similar texts and builds a contextual graph to incorporate into the language model. In contrast, our proposed approach does not require training or the addition of an additional module. By decoupling the datastore and language model, we can save the storage cost and utilize Bayesian inference to select suitable hyper-parameters at the same time.",
                    "score": 0.39857759446045876,
                    "section_title": "B. Retrieval-augment Language Model",
                    "char_start_offset": 37070,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 186
                        },
                        {
                            "start": 187,
                            "end": 360
                        },
                        {
                            "start": 361,
                            "end": 471
                        },
                        {
                            "start": 472,
                            "end": 638
                        },
                        {
                            "start": 639,
                            "end": 800
                        },
                        {
                            "start": 803,
                            "end": 883
                        },
                        {
                            "start": 884,
                            "end": 974
                        },
                        {
                            "start": 975,
                            "end": 1153
                        },
                        {
                            "start": 1154,
                            "end": 1257
                        },
                        {
                            "start": 1258,
                            "end": 1359
                        },
                        {
                            "start": 1360,
                            "end": 1521
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 199,
                            "end": 203,
                            "matchedPaperCorpusId": "211204736"
                        },
                        {
                            "start": 205,
                            "end": 209,
                            "matchedPaperCorpusId": "247450969"
                        },
                        {
                            "start": 373,
                            "end": 377,
                            "matchedPaperCorpusId": "207870430"
                        },
                        {
                            "start": 379,
                            "end": 383,
                            "matchedPaperCorpusId": "222125236"
                        },
                        {
                            "start": 385,
                            "end": 389,
                            "matchedPaperCorpusId": "239016943"
                        },
                        {
                            "start": 1012,
                            "end": 1016,
                            "matchedPaperCorpusId": "246431219"
                        },
                        {
                            "start": 1079,
                            "end": 1083,
                            "matchedPaperCorpusId": "237452184"
                        },
                        {
                            "start": 1161,
                            "end": 1165,
                            "matchedPaperCorpusId": "239016943"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7783203125
                }
            ],
            "relevance_judgement": 0.7783203125,
            "relevance_judgment_input_expanded": "# Title: Domain Adaptive Code Completion via Language Models and Decoupled Domain Databases\n# Venue: International Conference on Automated Software Engineering\n# Authors: Ze Tang, Jidong Ge, Shangqing Liu, Tingwei Zhu, Tongtong Xu, LiGuo Huang, Bin Luo\n## Abstract\nLarge Language Models (LLMs) have demonstrated remarkable performance in code completion. However, due to the lack of domain-specific knowledge, they may not be optimal in completing code that requires intensive domain knowledge for example completing the library names. Although there are several works that have confirmed the effectiveness of fine-tuning techniques to adapt language models for code completion in specific domains. They are limited by the need for constant fine-tuning of the model when the project is in constant iteration. To address this limitation, in this paper, we propose $k$ NM-LM, a retrieval-augmented language model (R-LM), that integrates domain knowledge into language models without fine-tuning. Different from previous techniques, our approach is able to automatically adapt to different language models and domains. Specifically, it utilizes the in-domain code to build the retrieval-based database decoupled from LM, and then combines it with LM through Bayesian inference to complete the code. The extensive experiments on the completion of intra-project and intra-scenario have confirmed that $k$ NM-LM brings about appreciable enhancements when compared to CodeGPT and UnixCoder. A deep analysis of our tool including the responding speed, storage usage, specific type code completion, and API invocation completion has confirmed that $k$ NM-LM provides satisfactory performance, which renders it highly appropriate for domain adaptive code completion. Furthermore, our approach operates without the requirement for direct access to the language model's parameters. As a result, it can seamlessly integrate with black-box code completion models, making it easy to integrate our approach as a plugin to further enhance the performance of these models.\n## B. Retrieval-augment Language Model\nRetrieval-augmented language models (R-LMs) utilize retrieval-based techniques to improve the performance of LMs and can be divided into two main categories: block R-LMs and token R-LMs. Block R-LMs [12], [13], [48] are similar to one-shot or few-shot learning [49], where one or a few examples are retrieved from a database instead of being randomly selected. Token R-LMs [15], [24], [50] retrieve tokens from database and then combine the retrieved results into the LM. Compared with block R-LMs, token R-LMs can update retrieval results at the same time of generating new tokens, hence our approach uses the architecture of token R-LM. However, token R-LMs suffer from high storage costs and require hyper-parameters selection to combine the inference results from the database and language model. \n\nVarious approaches have been proposed to address the limitations of token R-LMs. For example, kNN-Adapter [51] uses a trained network to determine the combination weights. To reduce the search cost, RetoMaton [52] uses the automaton states to save search time, while AdaptRet [53] uses a trained network to decide whether to use the retrieval module. GNN-LM [50] selects similar texts and builds a contextual graph to incorporate into the language model. In contrast, our proposed approach does not require training or the addition of an additional module. By decoupling the datastore and language model, we can save the storage cost and utilize Bayesian inference to select suitable hyper-parameters at the same time.",
            "reference_string": "[261030382 | Tang et al. | 2023 | Citations: 26]"
        },
        {
            "title": "Knowledge Retrieval Based on Generative AI",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 11,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.04635, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2191368257",
                    "name": "Te-Lun Yang"
                },
                {
                    "authorId": "2253878746",
                    "name": "Jyi-Shane Liu"
                },
                {
                    "authorId": "40130996",
                    "name": "Yuen-Hsien Tseng"
                },
                {
                    "authorId": "2262396644",
                    "name": "Jyh-Shing Roger Jang"
                }
            ],
            "abstract": "This study develops a question-answering system based on Retrieval-Augmented Generation (RAG) using Chinese Wikipedia and Lawbank as retrieval sources. Using TTQA and TMMLU+ as evaluation datasets, the system employs BGE-M3 for dense vector retrieval to obtain highly relevant search results and BGE-reranker to reorder these results based on query relevance. The most pertinent retrieval outcomes serve as reference knowledge for a Large Language Model (LLM), enhancing its ability to answer questions and establishing a knowledge retrieval system grounded in generative AI. The system's effectiveness is assessed through a two-stage evaluation: automatic and assisted performance evaluations. The automatic evaluation calculates accuracy by comparing the model's auto-generated labels with ground truth answers, measuring performance under standardized conditions without human intervention. The assisted performance evaluation involves 20 finance-related multiple-choice questions answered by 20 participants without financial backgrounds. Initially, participants answer independently. Later, they receive system-generated reference information to assist in answering, examining whether the system improves accuracy when assistance is provided. The main contributions of this research are: (1) Enhanced LLM Capability: By integrating BGE-M3 and BGE-reranker, the system retrieves and reorders highly relevant results, reduces hallucinations, and dynamically accesses authorized or public knowledge sources. (2) Improved Data Privacy: A customized RAG architecture enables local operation of the LLM, eliminating the need to send private data to external servers. This approach enhances data security, reduces reliance on commercial services, lowers operational costs, and mitigates privacy risks.",
            "corpus_id": 275358357,
            "sentences": [
                {
                    "corpus_id": "275358357",
                    "title": "Knowledge Retrieval Based on Generative AI",
                    "text": "Retrieval-Augmented Generation (RAG) [4] overcomes the limitations of Large Language Models (LLMs), which rely on static, pre-trained datasets that can become outdated and lack domain-specific information. This restricts LLMs' ability to generate accurate and up-to-date responses. RAG integrates Information Retrieval (IR) systems with LLMs, enabling them to query external knowledge sources and access real-time, domain-relevant data. In a typical RAG framework, a retriever processes user queries and retrieves relevant documents based on semantic similarity. These documents are then combined with the original query and passed to the LLM to generate a more accurate and comprehensive response. RAG also addresses the issue of model obsolescence by dynamically accessing updated information without retraining. However, balancing precision and recall during retrieval can be challenging, and re-ranking is required to prioritize relevant information, ensuring that LLM responses are accurate and contextually appropriate in complex queries.",
                    "score": 0.4227106823263234,
                    "section_title": "C. Retrieval-Augmented Generation",
                    "char_start_offset": 3961,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 205
                        },
                        {
                            "start": 206,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 436
                        },
                        {
                            "start": 437,
                            "end": 562
                        },
                        {
                            "start": 563,
                            "end": 698
                        },
                        {
                            "start": 699,
                            "end": 814
                        },
                        {
                            "start": 815,
                            "end": 1044
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 37,
                            "end": 40,
                            "matchedPaperCorpusId": "218869575"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77734375
                }
            ],
            "relevance_judgement": 0.77734375,
            "relevance_judgment_input_expanded": "# Title: Knowledge Retrieval Based on Generative AI\n# Venue: arXiv.org\n# Authors: Te-Lun Yang, Jyi-Shane Liu, Yuen-Hsien Tseng, Jyh-Shing Roger Jang\n## Abstract\nThis study develops a question-answering system based on Retrieval-Augmented Generation (RAG) using Chinese Wikipedia and Lawbank as retrieval sources. Using TTQA and TMMLU+ as evaluation datasets, the system employs BGE-M3 for dense vector retrieval to obtain highly relevant search results and BGE-reranker to reorder these results based on query relevance. The most pertinent retrieval outcomes serve as reference knowledge for a Large Language Model (LLM), enhancing its ability to answer questions and establishing a knowledge retrieval system grounded in generative AI. The system's effectiveness is assessed through a two-stage evaluation: automatic and assisted performance evaluations. The automatic evaluation calculates accuracy by comparing the model's auto-generated labels with ground truth answers, measuring performance under standardized conditions without human intervention. The assisted performance evaluation involves 20 finance-related multiple-choice questions answered by 20 participants without financial backgrounds. Initially, participants answer independently. Later, they receive system-generated reference information to assist in answering, examining whether the system improves accuracy when assistance is provided. The main contributions of this research are: (1) Enhanced LLM Capability: By integrating BGE-M3 and BGE-reranker, the system retrieves and reorders highly relevant results, reduces hallucinations, and dynamically accesses authorized or public knowledge sources. (2) Improved Data Privacy: A customized RAG architecture enables local operation of the LLM, eliminating the need to send private data to external servers. This approach enhances data security, reduces reliance on commercial services, lowers operational costs, and mitigates privacy risks.\n## C. Retrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) [4] overcomes the limitations of Large Language Models (LLMs), which rely on static, pre-trained datasets that can become outdated and lack domain-specific information. This restricts LLMs' ability to generate accurate and up-to-date responses. RAG integrates Information Retrieval (IR) systems with LLMs, enabling them to query external knowledge sources and access real-time, domain-relevant data. In a typical RAG framework, a retriever processes user queries and retrieves relevant documents based on semantic similarity. These documents are then combined with the original query and passed to the LLM to generate a more accurate and comprehensive response. RAG also addresses the issue of model obsolescence by dynamically accessing updated information without retraining. However, balancing precision and recall during retrieval can be challenging, and re-ranking is required to prioritize relevant information, ensuring that LLM responses are accurate and contextually appropriate in complex queries.",
            "reference_string": "[275358357 | Yang et al. | 2025 | Citations: 2]"
        },
        {
            "title": "CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 25,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.19878, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2352004975",
                    "name": "Nengbo Wang"
                },
                {
                    "authorId": "2346134041",
                    "name": "Xiaotian Han"
                },
                {
                    "authorId": "2352427030",
                    "name": "Jagdip Singh"
                },
                {
                    "authorId": "2352917796",
                    "name": "Jing Ma"
                },
                {
                    "authorId": "2346129602",
                    "name": "Vipin Chaudhary"
                }
            ],
            "abstract": "Large language models (LLMs) have revolutionized natural language processing (NLP), particularly through Retrieval-Augmented Generation (RAG), which enhances LLM capabilities by integrating external knowledge. However, traditional RAG systems face critical limitations, including disrupted contextual integrity due to text chunking, and over-reliance on semantic similarity for retrieval. To address these issues, we propose CausalRAG, a novel framework that incorporates causal graphs into the retrieval process. By constructing and tracing causal relationships, CausalRAG preserves contextual continuity and improves retrieval precision, leading to more accurate and interpretable responses. We evaluate CausalRAG against regular RAG and graph-based RAG approaches, demonstrating its superiority across several metrics. Our findings suggest that grounding retrieval in causal reasoning provides a promising approach to knowledge-intensive tasks.",
            "corpus_id": 277313225,
            "sentences": [
                {
                    "corpus_id": "277313225",
                    "title": "CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation",
                    "text": "Large language models (LLMs) have transformed natural language processing (NLP), enabling a wide range of applications (Anthropic, 2024;Google, 2024;OpenAI, 2024). However, their reliance on static, pre-trained knowledge limits their ability to incorporate and reason over dynamically updated external information, particularly in knowledge-intensive domains. Retrieval-Augmented Generation (RAG) addresses this limitation by combining external retrieval with generative modeling to improve contextual understanding and response quality (Lewis et al., 2021). \n\nRecent efforts to improve RAG focus on two fronts: 1) enhancing retrieval efficiency through adaptive and modular frameworks (Gan et al., 2024;Ravuru et al., 2024;Zhang et al., 2024a); and 2) better structuring external knowledge, with graphbased RAGs emerging as a dominant approach (Edge et al., 2024;Guo et al., 2024;Potts, 2024). Despite these advancements, existing RAG architectures still face critical limitations that impact retrieval quality and response accuracy, primarily due to three key issues: 1) disruption of contextual integrity caused by the text chunking design; 2) reliance on semantic similarity rather than causal relevance for retrieval; and 3) a lack of accuracy in selecting truly relevant documents. \n\nThrough theoretical and empirical analysis, we rethink the limitations of current RAG systems by introducing a novel perspective grounded in context recall and precision metrics. Using this lens, we find that both regular and graph-based RAGs often fail to retrieve causally grounded content or accurately align it with the user query. We identify this fundamental issue as a primary reason why LLMs in RAG frameworks frequently produce seemingly relevant yet shallow responses that lack essential details. \n\nTo address these gaps, we propose CausalRAG, a novel RAG framework that integrates causal graphs to guide retrieval. By identifying cause-effect relationships within external knowledge, CausalRAG preserves contextual coherence and improves reasoning fidelity. This leads to more accurate and causally grounded responses, while reducing hallucinations and enhancing answer faithfulness. \n\nWe evaluate CausalRAG across datasets and varying context lengths, comparing it with regular and other competitive graph-based RAGs over multiple metrics. Results demonstrate that Causal-RAG achieves superior performance across different contexts.",
                    "score": 0.4292505159464041,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 359
                        },
                        {
                            "start": 360,
                            "end": 558
                        },
                        {
                            "start": 561,
                            "end": 894
                        },
                        {
                            "start": 895,
                            "end": 1287
                        },
                        {
                            "start": 1290,
                            "end": 1468
                        },
                        {
                            "start": 1469,
                            "end": 1625
                        },
                        {
                            "start": 1626,
                            "end": 1796
                        },
                        {
                            "start": 1799,
                            "end": 1915
                        },
                        {
                            "start": 1916,
                            "end": 2058
                        },
                        {
                            "start": 2059,
                            "end": 2184
                        },
                        {
                            "start": 2187,
                            "end": 2341
                        },
                        {
                            "start": 2342,
                            "end": 2434
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.775390625
                }
            ],
            "relevance_judgement": 0.775390625,
            "relevance_judgment_input_expanded": "# Title: CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation\n# Venue: arXiv.org\n# Authors: Nengbo Wang, Xiaotian Han, Jagdip Singh, Jing Ma, Vipin Chaudhary\n## Abstract\nLarge language models (LLMs) have revolutionized natural language processing (NLP), particularly through Retrieval-Augmented Generation (RAG), which enhances LLM capabilities by integrating external knowledge. However, traditional RAG systems face critical limitations, including disrupted contextual integrity due to text chunking, and over-reliance on semantic similarity for retrieval. To address these issues, we propose CausalRAG, a novel framework that incorporates causal graphs into the retrieval process. By constructing and tracing causal relationships, CausalRAG preserves contextual continuity and improves retrieval precision, leading to more accurate and interpretable responses. We evaluate CausalRAG against regular RAG and graph-based RAG approaches, demonstrating its superiority across several metrics. Our findings suggest that grounding retrieval in causal reasoning provides a promising approach to knowledge-intensive tasks.\n## Introduction\nLarge language models (LLMs) have transformed natural language processing (NLP), enabling a wide range of applications (Anthropic, 2024;Google, 2024;OpenAI, 2024). However, their reliance on static, pre-trained knowledge limits their ability to incorporate and reason over dynamically updated external information, particularly in knowledge-intensive domains. Retrieval-Augmented Generation (RAG) addresses this limitation by combining external retrieval with generative modeling to improve contextual understanding and response quality (Lewis et al., 2021). \n\nRecent efforts to improve RAG focus on two fronts: 1) enhancing retrieval efficiency through adaptive and modular frameworks (Gan et al., 2024;Ravuru et al., 2024;Zhang et al., 2024a); and 2) better structuring external knowledge, with graphbased RAGs emerging as a dominant approach (Edge et al., 2024;Guo et al., 2024;Potts, 2024). Despite these advancements, existing RAG architectures still face critical limitations that impact retrieval quality and response accuracy, primarily due to three key issues: 1) disruption of contextual integrity caused by the text chunking design; 2) reliance on semantic similarity rather than causal relevance for retrieval; and 3) a lack of accuracy in selecting truly relevant documents. \n\nThrough theoretical and empirical analysis, we rethink the limitations of current RAG systems by introducing a novel perspective grounded in context recall and precision metrics. Using this lens, we find that both regular and graph-based RAGs often fail to retrieve causally grounded content or accurately align it with the user query. We identify this fundamental issue as a primary reason why LLMs in RAG frameworks frequently produce seemingly relevant yet shallow responses that lack essential details. \n\nTo address these gaps, we propose CausalRAG, a novel RAG framework that integrates causal graphs to guide retrieval. By identifying cause-effect relationships within external knowledge, CausalRAG preserves contextual coherence and improves reasoning fidelity. This leads to more accurate and causally grounded responses, while reducing hallucinations and enhancing answer faithfulness. \n\nWe evaluate CausalRAG across datasets and varying context lengths, comparing it with regular and other competitive graph-based RAGs over multiple metrics. Results demonstrate that Causal-RAG achieves superior performance across different contexts.",
            "reference_string": "[277313225 | Wang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "DuetRAG: Collaborative Retrieval-Augmented Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 21,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.13002, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2302798653",
                    "name": "Dian Jiao"
                },
                {
                    "authorId": "2303434387",
                    "name": "Li Cai"
                },
                {
                    "authorId": "2303044665",
                    "name": "Jingsheng Huang"
                },
                {
                    "authorId": "2108125912",
                    "name": "Wenqiao Zhang"
                },
                {
                    "authorId": "2118071462",
                    "name": "Siliang Tang"
                },
                {
                    "authorId": "2253660817",
                    "name": "Yueting Zhuang"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) methods augment the input of Large Language Models (LLMs) with relevant retrieved passages, reducing factual errors in knowledge-intensive tasks. However, contemporary RAG approaches suffer from irrelevant knowledge retrieval issues in complex domain questions (e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to low-quality generations. To address this issue, we propose a novel Collaborative Retrieval-Augmented Generation framework, DuetRAG. Our bootstrapping philosophy is to simultaneously integrate the domain fintuning and RAG models to improve the knowledge retrieval quality, thereby enhancing generation quality. Finally, we demonstrate DuetRAG' s matches with expert human researchers on HotPot QA.",
            "corpus_id": 269983737,
            "sentences": [
                {
                    "corpus_id": "269983737",
                    "title": "DuetRAG: Collaborative Retrieval-Augmented Generation",
                    "text": "Retrieval-Augmented Language Models Augmenting language models with relevant information obtained from various external knowledge bases has been shown to significantly improve the performance of various NLP tasks, including language modeling (Guu et al., 2020;Borgeaud et al., 2022;Shi et al., 2023;Lin et al., 2023) and open domain question answering (Izacard et al., 2022;Zhang et al., 2024).RAG mainly adopts the \"retrieve then read\" paradigm.Specifically, the input question is first used as the query, then the retrieval module retrieves relevant documents from the external knowledge base, and finally the retrieved documents and questions are merged into a complete input to generate final output.For example, RETRO (Borgeaud et al., 2022) modifies the autoregressive LM to focus on relevant documents through chunked cross-attention, thereby introducing new parameters to the model.REPLUG (Shi et al., 2023) assumes black-box access to LM and optimizes it by fine-tuning the retriever.RAFT (Zhang et al., 2024) proposes a fine-tuned data that additionally contains relevant documents and answers with reasoning chains to train language models for domain-specific open-book settings.Finetuning for RAG Recently, related work has studied how to improve the overall performance by fine-tuning the LLM or retriever in the RAG framework.For example, RADIT (Lin et al., 2023) proposes a dual-instruction fine-tuning framework to fine-tune both the LLM and the retriever simultaneously.InstructRetro (Wang et al., 2023) pre-trains a larger autoregressive large-scale language model with retrieval function and performs instruction fine-tuning based on it.ChatQA (Liu et al., 2024) additionally proposes a context-enhanced instruction fine-tuning stage, specifically to enhance the model's ability to perform context awareness in conversational QA.RAFT (Zhang et al., 2024) proposes a kind of fine-tuning data that additionally contains related documents and answers with reasoning chains to fine-tune LLM and improve LLM's ability to understand the retrieved documents under the RAG framework.",
                    "score": 0.3908534054751986,
                    "section_title": "Related Work",
                    "char_start_offset": 2461,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 394
                        },
                        {
                            "start": 394,
                            "end": 446
                        },
                        {
                            "start": 446,
                            "end": 704
                        },
                        {
                            "start": 704,
                            "end": 890
                        },
                        {
                            "start": 890,
                            "end": 993
                        },
                        {
                            "start": 993,
                            "end": 1190
                        },
                        {
                            "start": 1190,
                            "end": 1340
                        },
                        {
                            "start": 1340,
                            "end": 1487
                        },
                        {
                            "start": 1487,
                            "end": 1656
                        },
                        {
                            "start": 1656,
                            "end": 1848
                        },
                        {
                            "start": 1848,
                            "end": 2094
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 242,
                            "end": 260,
                            "matchedPaperCorpusId": "211204736"
                        },
                        {
                            "start": 260,
                            "end": 282,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 723,
                            "end": 746,
                            "matchedPaperCorpusId": "244954723"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77490234375
                }
            ],
            "relevance_judgement": 0.77490234375,
            "relevance_judgment_input_expanded": "# Title: DuetRAG: Collaborative Retrieval-Augmented Generation\n# Venue: arXiv.org\n# Authors: Dian Jiao, Li Cai, Jingsheng Huang, Wenqiao Zhang, Siliang Tang, Yueting Zhuang\n## Abstract\nRetrieval-Augmented Generation (RAG) methods augment the input of Large Language Models (LLMs) with relevant retrieved passages, reducing factual errors in knowledge-intensive tasks. However, contemporary RAG approaches suffer from irrelevant knowledge retrieval issues in complex domain questions (e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to low-quality generations. To address this issue, we propose a novel Collaborative Retrieval-Augmented Generation framework, DuetRAG. Our bootstrapping philosophy is to simultaneously integrate the domain fintuning and RAG models to improve the knowledge retrieval quality, thereby enhancing generation quality. Finally, we demonstrate DuetRAG' s matches with expert human researchers on HotPot QA.\n## Related Work\nRetrieval-Augmented Language Models Augmenting language models with relevant information obtained from various external knowledge bases has been shown to significantly improve the performance of various NLP tasks, including language modeling (Guu et al., 2020;Borgeaud et al., 2022;Shi et al., 2023;Lin et al., 2023) and open domain question answering (Izacard et al., 2022;Zhang et al., 2024).RAG mainly adopts the \"retrieve then read\" paradigm.Specifically, the input question is first used as the query, then the retrieval module retrieves relevant documents from the external knowledge base, and finally the retrieved documents and questions are merged into a complete input to generate final output.For example, RETRO (Borgeaud et al., 2022) modifies the autoregressive LM to focus on relevant documents through chunked cross-attention, thereby introducing new parameters to the model.REPLUG (Shi et al., 2023) assumes black-box access to LM and optimizes it by fine-tuning the retriever.RAFT (Zhang et al., 2024) proposes a fine-tuned data that additionally contains relevant documents and answers with reasoning chains to train language models for domain-specific open-book settings.Finetuning for RAG Recently, related work has studied how to improve the overall performance by fine-tuning the LLM or retriever in the RAG framework.For example, RADIT (Lin et al., 2023) proposes a dual-instruction fine-tuning framework to fine-tune both the LLM and the retriever simultaneously.InstructRetro (Wang et al., 2023) pre-trains a larger autoregressive large-scale language model with retrieval function and performs instruction fine-tuning based on it.ChatQA (Liu et al., 2024) additionally proposes a context-enhanced instruction fine-tuning stage, specifically to enhance the model's ability to perform context awareness in conversational QA.RAFT (Zhang et al., 2024) proposes a kind of fine-tuning data that additionally contains related documents and answers with reasoning chains to fine-tune LLM and improve LLM's ability to understand the retrieved documents under the RAG framework.",
            "reference_string": "[269983737 | Jiao et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Knowledge Pyramid Construction for Multi-Level Retrieval-Augmented Generation",
            "venue": "",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.21276, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2314169880",
                    "name": "Rubing Chen"
                },
                {
                    "authorId": "2108162240",
                    "name": "Xulu Zhang"
                },
                {
                    "authorId": "2313746412",
                    "name": "Jiaxin Wu"
                },
                {
                    "authorId": "2291324376",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2115493866",
                    "name": "Xiao Wei"
                },
                {
                    "authorId": "2293397899",
                    "name": "Qing Li"
                }
            ],
            "abstract": "This paper addresses the need for improved precision in existing knowledge-enhanced question-answering frameworks, specifically Retrieval-Augmented Generation (RAG) methods that primarily focus on enhancing recall. We propose a multi-layer knowledge pyramid approach within the RAG framework to achieve a better balance between precision and recall. The knowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs), and chunk-based raw text. We employ cross-layer augmentation techniques for comprehensive knowledge coverage and dynamic updates of the Ontology schema and instances. To ensure compactness, we utilize cross-layer filtering methods for knowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall model for retrieval, starting from the top of the pyramid and progressing down until a confident answer is obtained. We introduce two benchmarks for domain-specific knowledge retrieval, one in the academic domain and the other in the financial domain. The effectiveness of the methods has been validated through comprehensive experiments by outperforming 19 SOTA methods. An encouraging observation is that the proposed method has augmented the GPT-4, providing 395% F1 gain by improving its performance from 0.1636 to 0.8109.",
            "corpus_id": 271570928,
            "sentences": [
                {
                    "corpus_id": "271570928",
                    "title": "Knowledge Pyramid Construction for Multi-Level Retrieval-Augmented Generation",
                    "text": "Retrieval-Augmented Generation (RAG) enhances the generative capabilities of language models by incorporating retrieved knowledge for in-context learning [22], [26]. While general language models excel in producing responses for Fig. 2. The framework of PolyRAG includes: (1) Knowledge Pyramid Construction, starting from extracting knowledge from corpora data to form the initial layers, then performing the layer interactions, which are knowledge completion and condensation. (2) Given the refined Knowledge Pyramid, the LLM inference obtains the contexts through a multi-level query, which retrieves the knowledge from the higher layer down to the lower layer in a waterfall pattern. \n\ngeneral queries, they are prone to generating hallucinations when tasked with domain-specific knowledge usage. RAG addresses this issue by retrieving established knowledge corpora and providing this information as context to the language model [22]. NaiveRAG represents the most basic architecture within this framework, in which the system retrieves the top-k documents that are most relevant to the query and integrate them into the prompt, thereby grounding the responses in more relevant information [25]. \n\nExpanding on NaiveRAG, advanced RAG incorporates additional modules or structures to improve retrieval precision. Reranking is a notable example, where a reranker is employed to refine the initial ranked list (e.g., Re2G [37] and bgereranker [38], both are based on BERT [39]). Furthermore, studies have indicated that excessive noise and lengthy context can have a negative impact on inference performance. To address this, prompt compression methods such as Selective Context [40] and LLMLingua [41] have been developed. These methods emphasize key information while reducing noise and context length, as discussed in [22]. While current knowledge retrieval methods that rely on single queries can offer some contextual assistance for domain-specific knowledge, they heavily rely on the expressive abilities of the original collections of information and consistently require a trade-off between more detailed knowledge and less noisy information [42], [43]. These techniques do not effectively tackle the issues of retrieving responses that require integrating information from numerous sources, nor do they fulfill the need for dense knowledge in question-and-answer interactions. To address these constraints, we suggest employing varied knowledge representations.",
                    "score": 0.41650522710216176,
                    "section_title": "B. Retrieval-Augmented Generation",
                    "char_start_offset": 7859,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 165
                        },
                        {
                            "start": 166,
                            "end": 477
                        },
                        {
                            "start": 478,
                            "end": 686
                        },
                        {
                            "start": 689,
                            "end": 799
                        },
                        {
                            "start": 800,
                            "end": 938
                        },
                        {
                            "start": 939,
                            "end": 1198
                        },
                        {
                            "start": 1201,
                            "end": 1314
                        },
                        {
                            "start": 1315,
                            "end": 1478
                        },
                        {
                            "start": 1479,
                            "end": 1608
                        },
                        {
                            "start": 1609,
                            "end": 1723
                        },
                        {
                            "start": 1724,
                            "end": 1826
                        },
                        {
                            "start": 1827,
                            "end": 2161
                        },
                        {
                            "start": 2162,
                            "end": 2385
                        },
                        {
                            "start": 2386,
                            "end": 2470
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 160,
                            "end": 164,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 1193,
                            "end": 1197,
                            "matchedPaperCorpusId": "258841283"
                        },
                        {
                            "start": 1679,
                            "end": 1683,
                            "matchedPaperCorpusId": "214641123"
                        },
                        {
                            "start": 1698,
                            "end": 1702,
                            "matchedPaperCorpusId": "252186384"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77001953125
                }
            ],
            "relevance_judgement": 0.77001953125,
            "relevance_judgment_input_expanded": "# Title: Knowledge Pyramid Construction for Multi-Level Retrieval-Augmented Generation\n# Venue: \n# Authors: Rubing Chen, Xulu Zhang, Jiaxin Wu, Wenqi Fan, Xiao Wei, Qing Li\n## Abstract\nThis paper addresses the need for improved precision in existing knowledge-enhanced question-answering frameworks, specifically Retrieval-Augmented Generation (RAG) methods that primarily focus on enhancing recall. We propose a multi-layer knowledge pyramid approach within the RAG framework to achieve a better balance between precision and recall. The knowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs), and chunk-based raw text. We employ cross-layer augmentation techniques for comprehensive knowledge coverage and dynamic updates of the Ontology schema and instances. To ensure compactness, we utilize cross-layer filtering methods for knowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall model for retrieval, starting from the top of the pyramid and progressing down until a confident answer is obtained. We introduce two benchmarks for domain-specific knowledge retrieval, one in the academic domain and the other in the financial domain. The effectiveness of the methods has been validated through comprehensive experiments by outperforming 19 SOTA methods. An encouraging observation is that the proposed method has augmented the GPT-4, providing 395% F1 gain by improving its performance from 0.1636 to 0.8109.\n## B. Retrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) enhances the generative capabilities of language models by incorporating retrieved knowledge for in-context learning [22], [26]. While general language models excel in producing responses for Fig. 2. The framework of PolyRAG includes: (1) Knowledge Pyramid Construction, starting from extracting knowledge from corpora data to form the initial layers, then performing the layer interactions, which are knowledge completion and condensation. (2) Given the refined Knowledge Pyramid, the LLM inference obtains the contexts through a multi-level query, which retrieves the knowledge from the higher layer down to the lower layer in a waterfall pattern. \n\ngeneral queries, they are prone to generating hallucinations when tasked with domain-specific knowledge usage. RAG addresses this issue by retrieving established knowledge corpora and providing this information as context to the language model [22]. NaiveRAG represents the most basic architecture within this framework, in which the system retrieves the top-k documents that are most relevant to the query and integrate them into the prompt, thereby grounding the responses in more relevant information [25]. \n\nExpanding on NaiveRAG, advanced RAG incorporates additional modules or structures to improve retrieval precision. Reranking is a notable example, where a reranker is employed to refine the initial ranked list (e.g., Re2G [37] and bgereranker [38], both are based on BERT [39]). Furthermore, studies have indicated that excessive noise and lengthy context can have a negative impact on inference performance. To address this, prompt compression methods such as Selective Context [40] and LLMLingua [41] have been developed. These methods emphasize key information while reducing noise and context length, as discussed in [22]. While current knowledge retrieval methods that rely on single queries can offer some contextual assistance for domain-specific knowledge, they heavily rely on the expressive abilities of the original collections of information and consistently require a trade-off between more detailed knowledge and less noisy information [42], [43]. These techniques do not effectively tackle the issues of retrieving responses that require integrating information from numerous sources, nor do they fulfill the need for dense knowledge in question-and-answer interactions. To address these constraints, we suggest employing varied knowledge representations.",
            "reference_string": "[271570928 | Chen et al. | 2024 | Citations: 2]"
        },
        {
            "title": "An Empirical Study of Retrieval-Augmented Code Generation: Challenges and Opportunities",
            "venue": "ACM Transactions on Software Engineering and Methodology",
            "year": 2025,
            "reference_count": 80,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2501.13742",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.13742, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2155450982",
                    "name": "Zezhou Yang"
                },
                {
                    "authorId": "2210301796",
                    "name": "Sirong Chen"
                },
                {
                    "authorId": "2296784064",
                    "name": "Cuiyun Gao"
                },
                {
                    "authorId": "2293350478",
                    "name": "Zhenhao Li"
                },
                {
                    "authorId": "2265809330",
                    "name": "Xing Hu"
                },
                {
                    "authorId": "2303514300",
                    "name": "Kui Liu"
                },
                {
                    "authorId": "2303567076",
                    "name": "Xin Xia"
                }
            ],
            "abstract": "Code generation aims to automatically generate code snippets of specific programming language according to natural language descriptions. The continuous advancements in deep learning, particularly pre-trained models, have empowered the code generation task to achieve remarkable performance. One main challenge of pre-trained models for code generation is the semantic gap between developers\u2019 natural language requirements and source code. To address the issue, prior studies typically adopt a retrieval-augmented framework for the task, where the similar code snippets collected by a retrieval process can be leveraged to help understand the requirements and provide guidance for the generation process. In a retrieval-augmented framework, similar data can be retrieved from the database using a retrieval algorithm, and original input data can be fused with retrieved data by different fusion strategies. However, there is a lack of systematic study on the application of this framework for code generation, including the impact of the final generated results and the specific usage of the framework. In this paper, we choose three popular pre-trained code models, namely CodeGen, UniXcoder, and CodeT5, to assess the impact of the quality and utilization of retrieved code on the retrieval-augmented framework. Our analysis shows that the retrieval-augmented framework is beneficial for improving the performance of the existing pre-trained models. We also provide suggestions on the utilization of the retrieval-augmented code generation framework: BM25 and Sequential Integration Fusion are recommended due to their convenience and superior performance. Sketch Filling Fusion, which extracts a sketch of relevant code, could help the model improve its performance further. Additionally, we conduct experiments to investigate the influence of the retrieval-augmented framework on large language models for code generation, showing the effectiveness of the framework, and we discuss the trade-off between performance improvement and computational costs in each phase within the framework.",
            "corpus_id": 275820707,
            "sentences": [
                {
                    "corpus_id": "275820707",
                    "title": "An Empirical Study of Retrieval-Augmented Code Generation: Challenges and Opportunities",
                    "text": "Retrieval-augmented generation refers to improving the generation performance with the retrieved results provided by retrieval techniques. For language models, the knowledge learned from training data is all stored in the parameters of the neural network. The model might encounter challenges in generating the correct answer due to numerous parameters [9]. Furthermore, when confronted with knowledge that has never been learned during pre-training, the model might fail to provide the accurate response. The retrieved results can be regarded as a supplement to the implicit knowledge stored in the parameters of language models, encouraging the model to produce more accurate outputs [24]. In addition, database can be modified and constantly updated, enabling the trained models to adapt to a broader range of new data [78]. In other words, retrieval-augmented generation achieves scalability of the modification or replacement of retrieval sources without the need to alter the models. \n\nThe k-Nearest Neighbor Language Model (kNN-LM) [36] retrieve the k most similar training contexts for test context according to the distance in the embedding space of the pre-trained language model. In fact, the k training contexts correspond to k training targets. By normalizing and aggregating k training targets, kNN-LM can get a target distribution from k nearest neighbors, and the pre-trained language model can generate another target distribution directly according to current input. kNN-LM can merge the two distributions above by weighted sum to get the final target distribution. Different from kNN-LM, Retrieval-Augmented Language Model (REALM) [24], whose workflow can be summarized as the retriever-and-reader, has two components that are both trained. One is a neural knowledge retriever, which retrieves similar text with input using a dense inner product model. The other is the knowledge-augmented encoder, which predicts the final results based on input and the retrieved text in the last step. Actually, the prediction cannot generate texts using the encoder but extract a contiguous sequence from the retrieved text as the result. A similar workflow has been proposed and developed [8,49] before REALM occurs.",
                    "score": 0.47108735754461806,
                    "section_title": "Retrieval-Augmented Generation",
                    "char_start_offset": 15164,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 138
                        },
                        {
                            "start": 139,
                            "end": 255
                        },
                        {
                            "start": 256,
                            "end": 357
                        },
                        {
                            "start": 358,
                            "end": 505
                        },
                        {
                            "start": 506,
                            "end": 691
                        },
                        {
                            "start": 692,
                            "end": 827
                        },
                        {
                            "start": 828,
                            "end": 989
                        },
                        {
                            "start": 992,
                            "end": 1190
                        },
                        {
                            "start": 1191,
                            "end": 1257
                        },
                        {
                            "start": 1258,
                            "end": 1484
                        },
                        {
                            "start": 1485,
                            "end": 1583
                        },
                        {
                            "start": 1584,
                            "end": 1759
                        },
                        {
                            "start": 1760,
                            "end": 1871
                        },
                        {
                            "start": 1872,
                            "end": 2006
                        },
                        {
                            "start": 2007,
                            "end": 2144
                        },
                        {
                            "start": 2145,
                            "end": 2223
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 686,
                            "end": 690,
                            "matchedPaperCorpusId": "211204736"
                        },
                        {
                            "start": 822,
                            "end": 826,
                            "matchedPaperCorpusId": "252734952"
                        },
                        {
                            "start": 1039,
                            "end": 1043,
                            "matchedPaperCorpusId": "207870430"
                        },
                        {
                            "start": 1650,
                            "end": 1654,
                            "matchedPaperCorpusId": "211204736"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7685546875
                }
            ],
            "relevance_judgement": 0.7685546875,
            "relevance_judgment_input_expanded": "# Title: An Empirical Study of Retrieval-Augmented Code Generation: Challenges and Opportunities\n# Venue: ACM Transactions on Software Engineering and Methodology\n# Authors: Zezhou Yang, Sirong Chen, Cuiyun Gao, Zhenhao Li, Xing Hu, Kui Liu, Xin Xia\n## Abstract\nCode generation aims to automatically generate code snippets of specific programming language according to natural language descriptions. The continuous advancements in deep learning, particularly pre-trained models, have empowered the code generation task to achieve remarkable performance. One main challenge of pre-trained models for code generation is the semantic gap between developers\u2019 natural language requirements and source code. To address the issue, prior studies typically adopt a retrieval-augmented framework for the task, where the similar code snippets collected by a retrieval process can be leveraged to help understand the requirements and provide guidance for the generation process. In a retrieval-augmented framework, similar data can be retrieved from the database using a retrieval algorithm, and original input data can be fused with retrieved data by different fusion strategies. However, there is a lack of systematic study on the application of this framework for code generation, including the impact of the final generated results and the specific usage of the framework. In this paper, we choose three popular pre-trained code models, namely CodeGen, UniXcoder, and CodeT5, to assess the impact of the quality and utilization of retrieved code on the retrieval-augmented framework. Our analysis shows that the retrieval-augmented framework is beneficial for improving the performance of the existing pre-trained models. We also provide suggestions on the utilization of the retrieval-augmented code generation framework: BM25 and Sequential Integration Fusion are recommended due to their convenience and superior performance. Sketch Filling Fusion, which extracts a sketch of relevant code, could help the model improve its performance further. Additionally, we conduct experiments to investigate the influence of the retrieval-augmented framework on large language models for code generation, showing the effectiveness of the framework, and we discuss the trade-off between performance improvement and computational costs in each phase within the framework.\n## Retrieval-Augmented Generation\nRetrieval-augmented generation refers to improving the generation performance with the retrieved results provided by retrieval techniques. For language models, the knowledge learned from training data is all stored in the parameters of the neural network. The model might encounter challenges in generating the correct answer due to numerous parameters [9]. Furthermore, when confronted with knowledge that has never been learned during pre-training, the model might fail to provide the accurate response. The retrieved results can be regarded as a supplement to the implicit knowledge stored in the parameters of language models, encouraging the model to produce more accurate outputs [24]. In addition, database can be modified and constantly updated, enabling the trained models to adapt to a broader range of new data [78]. In other words, retrieval-augmented generation achieves scalability of the modification or replacement of retrieval sources without the need to alter the models. \n\nThe k-Nearest Neighbor Language Model (kNN-LM) [36] retrieve the k most similar training contexts for test context according to the distance in the embedding space of the pre-trained language model. In fact, the k training contexts correspond to k training targets. By normalizing and aggregating k training targets, kNN-LM can get a target distribution from k nearest neighbors, and the pre-trained language model can generate another target distribution directly according to current input. kNN-LM can merge the two distributions above by weighted sum to get the final target distribution. Different from kNN-LM, Retrieval-Augmented Language Model (REALM) [24], whose workflow can be summarized as the retriever-and-reader, has two components that are both trained. One is a neural knowledge retriever, which retrieves similar text with input using a dense inner product model. The other is the knowledge-augmented encoder, which predicts the final results based on input and the retrieved text in the last step. Actually, the prediction cannot generate texts using the encoder but extract a contiguous sequence from the retrieved text as the result. A similar workflow has been proposed and developed [8,49] before REALM occurs.",
            "reference_string": "[275820707 | Yang et al. | 2025 | Citations: 4]"
        },
        {
            "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 52,
            "citation_count": 35,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.07922",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.07922, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1490651934",
                    "name": "Jie Huang"
                },
                {
                    "authorId": "2056440915",
                    "name": "Wei Ping"
                },
                {
                    "authorId": "145011005",
                    "name": "Peng Xu"
                },
                {
                    "authorId": "1911755",
                    "name": "M. Shoeybi"
                },
                {
                    "authorId": "143922493",
                    "name": "K. Chang"
                },
                {
                    "authorId": "2301680",
                    "name": "Bryan Catanzaro"
                }
            ],
            "abstract": "In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of existing models and identify their limitations in in-context learning, primarily due to a mismatch between pretraining and inference, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training. Through extensive experiments, we demonstrate that our simple yet effective design significantly improves performance, achieving results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder language models for in-context learning and encourages further research in this direction.",
            "corpus_id": 260900354,
            "sentences": [
                {
                    "corpus_id": "260900354",
                    "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models",
                    "text": "Retrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2023;Lewis et al., 2020) and decoderonly (Khandelwal et al., 2020;Borgeaud et al., 2022;Shi et al., 2022) architectures. For decoder-only LMs, the computational cost typically increases quadratically with the input length, as well as with the number of retrieval passages. In contrast, for encoder-decoder LMs with a Fusion-in-Decoder architecture, the computation cost grows linearly with the number of retrieved passages, as they only perform self-attention over one passage at a time (Izacard & Grave, 2021). This concept is also investigated by Ye et al. (2023) for more efficient in-context learning. \n\nWhile there has been some research on in-context learning with retrieval-augmented decoder-only LMs, which can be straightforwardly implemented by concatenating retrieved passages with the query as the input of the LM (Mallen et al., 2022;Shi et al., 2023;Khattab et al., 2022), in-context learning with retrieval-augmented encoder-decoder LMs remains unexplored to the best of our knowledge. This is despite the fact that encoder-decoder LMs can be more efficient at incorporating multiple (e.g., 40) retrieved passages.",
                    "score": 0.4848914463148757,
                    "section_title": "Background and Related Work",
                    "char_start_offset": 4627,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 141
                        },
                        {
                            "start": 142,
                            "end": 320
                        },
                        {
                            "start": 321,
                            "end": 519
                        },
                        {
                            "start": 520,
                            "end": 671
                        },
                        {
                            "start": 672,
                            "end": 910
                        },
                        {
                            "start": 911,
                            "end": 1004
                        },
                        {
                            "start": 1007,
                            "end": 1399
                        },
                        {
                            "start": 1400,
                            "end": 1528
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 382,
                            "end": 404,
                            "matchedPaperCorpusId": "251371732"
                        },
                        {
                            "start": 404,
                            "end": 423,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 440,
                            "end": 465,
                            "matchedPaperCorpusId": "207870430"
                        },
                        {
                            "start": 487,
                            "end": 504,
                            "matchedPaperCorpusId": "249152130"
                        },
                        {
                            "start": 886,
                            "end": 909,
                            "matchedPaperCorpusId": "220302360"
                        },
                        {
                            "start": 948,
                            "end": 964,
                            "matchedPaperCorpusId": "259370780"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.767578125
                }
            ],
            "relevance_judgement": 0.767578125,
            "relevance_judgment_input_expanded": "# Title: RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models\n# Venue: arXiv.org\n# Authors: Jie Huang, Wei Ping, Peng Xu, M. Shoeybi, K. Chang, Bryan Catanzaro\n## Abstract\nIn this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of existing models and identify their limitations in in-context learning, primarily due to a mismatch between pretraining and inference, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training. Through extensive experiments, we demonstrate that our simple yet effective design significantly improves performance, achieving results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder language models for in-context learning and encourages further research in this direction.\n## Background and Related Work\nRetrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2023;Lewis et al., 2020) and decoderonly (Khandelwal et al., 2020;Borgeaud et al., 2022;Shi et al., 2022) architectures. For decoder-only LMs, the computational cost typically increases quadratically with the input length, as well as with the number of retrieval passages. In contrast, for encoder-decoder LMs with a Fusion-in-Decoder architecture, the computation cost grows linearly with the number of retrieved passages, as they only perform self-attention over one passage at a time (Izacard & Grave, 2021). This concept is also investigated by Ye et al. (2023) for more efficient in-context learning. \n\nWhile there has been some research on in-context learning with retrieval-augmented decoder-only LMs, which can be straightforwardly implemented by concatenating retrieved passages with the query as the input of the LM (Mallen et al., 2022;Shi et al., 2023;Khattab et al., 2022), in-context learning with retrieval-augmented encoder-decoder LMs remains unexplored to the best of our knowledge. This is despite the fact that encoder-decoder LMs can be more efficient at incorporating multiple (e.g., 40) retrieved passages.",
            "reference_string": "[260900354 | Huang et al. | 2023 | Citations: 35]"
        },
        {
            "title": "Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.17783, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327211339",
                    "name": "Salman Rakin"
                },
                {
                    "authorId": "2327211291",
                    "name": "Md. A.R. Shibly"
                },
                {
                    "authorId": "2327211491",
                    "name": "Zahin M. Hossain"
                },
                {
                    "authorId": "2327258807",
                    "name": "Zeeshan Khan"
                },
                {
                    "authorId": "2327212434",
                    "name": "Md. Mostofa Akbar"
                }
            ],
            "abstract": "While ongoing advancements in Large Language Models have demonstrated remarkable success across various NLP tasks, Retrieval Augmented Generation Model stands out to be highly effective on downstream applications like Question Answering. Recently, RAG-end2end model further optimized the architecture and achieved notable performance improvements on domain adaptation. However, the effectiveness of these RAG-based architectures remains relatively unexplored when fine-tuned on specialized domains such as customer service for building a reliable conversational AI system. Furthermore, a critical challenge persists in reducing the occurrence of hallucinations while maintaining high domain-specific accuracy. In this paper, we investigated the performance of diverse RAG and RAG-like architectures through domain adaptation and evaluated their ability to generate accurate and relevant response grounded in the contextual knowledge base. To facilitate the evaluation of the models, we constructed a novel dataset HotelConvQA, sourced from wide range of hotel-related conversations and fine-tuned all the models on our domain specific dataset. We also addressed a critical research gap on determining the impact of domain adaptation on reducing hallucinations across different RAG architectures, an aspect that was not properly measured in prior work. Our evaluation shows positive results in all metrics by employing domain adaptation, demonstrating strong performance on QA tasks and providing insights into their efficacy in reducing hallucinations. Our findings clearly indicate that domain adaptation not only enhances the models' performance on QA tasks but also significantly reduces hallucination across all evaluated RAG architectures.",
            "corpus_id": 273532207,
            "sentences": [
                {
                    "corpus_id": "273532207",
                    "title": "Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination",
                    "text": "While ongoing advancements in Large Language Models have demonstrated remarkable success across various NLP tasks, Retrieval Augmented Generation Model stands out to be highly effective on downstream applications like Question Answering. Recently, RAG-end2end model further optimized the architecture and achieved notable performance improvements on domain adaptation. However, the effectiveness of these RAG-based architectures remains relatively unexplored when fine-tuned on specialized domains such as customer service for building a reliable conversational AI system. Furthermore, a critical challenge persists in reducing the occurrence of hallucinations while maintaining high domain-specific accuracy. In this paper, we investigated the performance of diverse RAG and RAG-like architectures through domain adaptation and evaluated their ability to generate accurate and relevant response grounded in the contextual knowledge base. To facilitate the evaluation of the models, we constructed a novel dataset HotelConvQA, sourced from wide range of hotel-related conversations and fine-tuned all the models on our domain specific dataset. We also addressed a critical research gap on determining the impact of domain adaptation on reducing hallucinations across different RAG architectures, an aspect that was not properly measured in prior work. Our evaluation shows positive results in all metrics by employing domain adaptation, demonstrating strong performance on QA tasks and providing insights into their efficacy in reducing hallucinations. Our findings clearly indicate that domain adaptation not only enhances the models' performance on QA tasks but also significantly reduces hallucination across all evaluated RAG architectures.",
                    "score": 0.42907892905864997,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.767578125
                }
            ],
            "relevance_judgement": 0.767578125,
            "relevance_judgment_input_expanded": "# Title: Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination\n# Venue: arXiv.org\n# Authors: Salman Rakin, Md. A.R. Shibly, Zahin M. Hossain, Zeeshan Khan, Md. Mostofa Akbar\n## Abstract\nWhile ongoing advancements in Large Language Models have demonstrated remarkable success across various NLP tasks, Retrieval Augmented Generation Model stands out to be highly effective on downstream applications like Question Answering. Recently, RAG-end2end model further optimized the architecture and achieved notable performance improvements on domain adaptation. However, the effectiveness of these RAG-based architectures remains relatively unexplored when fine-tuned on specialized domains such as customer service for building a reliable conversational AI system. Furthermore, a critical challenge persists in reducing the occurrence of hallucinations while maintaining high domain-specific accuracy. In this paper, we investigated the performance of diverse RAG and RAG-like architectures through domain adaptation and evaluated their ability to generate accurate and relevant response grounded in the contextual knowledge base. To facilitate the evaluation of the models, we constructed a novel dataset HotelConvQA, sourced from wide range of hotel-related conversations and fine-tuned all the models on our domain specific dataset. We also addressed a critical research gap on determining the impact of domain adaptation on reducing hallucinations across different RAG architectures, an aspect that was not properly measured in prior work. Our evaluation shows positive results in all metrics by employing domain adaptation, demonstrating strong performance on QA tasks and providing insights into their efficacy in reducing hallucinations. Our findings clearly indicate that domain adaptation not only enhances the models' performance on QA tasks but also significantly reduces hallucination across all evaluated RAG architectures.\n",
            "reference_string": "[273532207 | Rakin et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.07021, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2330357180",
                    "name": "Ziwei Liu"
                },
                {
                    "authorId": "2290883726",
                    "name": "Liangyin Zhang"
                },
                {
                    "authorId": "2342500515",
                    "name": "Qian Li"
                },
                {
                    "authorId": "2333407955",
                    "name": "Jianghua Wu"
                },
                {
                    "authorId": "2330382982",
                    "name": "Guangxu Zhu"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) has shown impressive capability in providing reliable answer predictions and addressing hallucination problems. A typical RAG implementation uses powerful retrieval models to extract external information and large language models (LLMs) to generate answers. In contrast, recent LLM-based retrieval has gained attention for its substantial improvements in information retrieval (IR) due to the LLMs' semantic understanding capability. However, directly applying LLM to RAG systems presents challenges. This may cause feature locality problems as massive parametric knowledge can hinder effective usage of global information across the corpus; for example, an LLM-based retriever often inputs document summaries instead of full documents. Moreover, various pre-trained tasks in LLMs introduce variance, further weakening performance as a retriever. To address these issues, we propose a novel two-stage fine-tuning architecture called Invar-RAG. In the retrieval stage, an LLM-based retriever is constructed by integrating LoRA-based representation learning to tackle feature locality issues. To enhance retrieval performance, we develop two patterns (invariant and variant patterns) and an invariance loss to reduce LLM variance. In the generation stage, a refined fine-tuning method is employed to improve LLM accuracy in generating answers based on retrieved information. Experimental results show that Invar-RAG significantly outperforms existing baselines across three open-domain question answering (ODQA) datasets. Code is available in the Supplementary Material for reproducibility.",
            "corpus_id": 273962778,
            "sentences": [
                {
                    "corpus_id": "273962778",
                    "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
                    "text": "Information Retrieval: Advancements in deep learning have revolutionized information retrieval systems, enhancing their personalization and accuracy in retrieving relevant documents. Early information retrieval frameworks employed sparse retrievers [6] or dense retrievers [4,8] to represent large corpora but struggled to capture deep semantic relationships [13]. LLM-based retrievers (generative retrieval) have since emerged as notable methods, leveraging the rich prior knowledge of LLMs to significantly improve performance by converting documents into parametric knowledge and generating them instead of computing similarity scores [38]. However, the frequent encoding and decoding processes in LLMs severely hinder efficiency [38]. To address the trade-off between effectiveness and efficiency, we propose invar-retrieval in our architecture, enabling the model to efficiently retrieve the most relevant documents without introducing variance. \n\nRetrieval-augmented Language Model: Currently, retrieval-augmented language models have proven effective in answering questions by leveraging external information through the integration of novel retrievers and LLMs [38]. However, the architectural gap between retrieval and generation continues to hinder unified optimization across the entire retrieval-augmented generation system [2]. To address the isolation between retrieval and generation, a novel architecture called RA-DIT was introduced [18]. By aligning retriever scoring with LSR scoring [27], it has been shown to deliver state-of-the-art performance across various tasks. However, it still employs dense retrievers like DRAGON+ [17] in the retrieval stage, which fails to eliminate the problem at its source and introduces inefficiencies throughout the process. Correspondingly, we introduce a representation learning method and invariance loss in our Invar-RAG architecture, which partially addresses these issues and explores a novel approach to using a single LLM for multiple roles within the RAG system.",
                    "score": 0.5333996140338054,
                    "section_title": "Related Work",
                    "char_start_offset": 21968,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 364
                        },
                        {
                            "start": 365,
                            "end": 643
                        },
                        {
                            "start": 644,
                            "end": 738
                        },
                        {
                            "start": 739,
                            "end": 950
                        },
                        {
                            "start": 953,
                            "end": 1174
                        },
                        {
                            "start": 1175,
                            "end": 1340
                        },
                        {
                            "start": 1341,
                            "end": 1455
                        },
                        {
                            "start": 1456,
                            "end": 1588
                        },
                        {
                            "start": 1589,
                            "end": 1778
                        },
                        {
                            "start": 1779,
                            "end": 2025
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 638,
                            "end": 642,
                            "matchedPaperCorpusId": "230433817"
                        },
                        {
                            "start": 733,
                            "end": 737,
                            "matchedPaperCorpusId": "230433817"
                        },
                        {
                            "start": 1169,
                            "end": 1173,
                            "matchedPaperCorpusId": "230433817"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.765625
                }
            ],
            "relevance_judgement": 0.765625,
            "relevance_judgment_input_expanded": "# Title: Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation\n# Venue: arXiv.org\n# Authors: Ziwei Liu, Liangyin Zhang, Qian Li, Jianghua Wu, Guangxu Zhu\n## Abstract\nRetrieval-augmented generation (RAG) has shown impressive capability in providing reliable answer predictions and addressing hallucination problems. A typical RAG implementation uses powerful retrieval models to extract external information and large language models (LLMs) to generate answers. In contrast, recent LLM-based retrieval has gained attention for its substantial improvements in information retrieval (IR) due to the LLMs' semantic understanding capability. However, directly applying LLM to RAG systems presents challenges. This may cause feature locality problems as massive parametric knowledge can hinder effective usage of global information across the corpus; for example, an LLM-based retriever often inputs document summaries instead of full documents. Moreover, various pre-trained tasks in LLMs introduce variance, further weakening performance as a retriever. To address these issues, we propose a novel two-stage fine-tuning architecture called Invar-RAG. In the retrieval stage, an LLM-based retriever is constructed by integrating LoRA-based representation learning to tackle feature locality issues. To enhance retrieval performance, we develop two patterns (invariant and variant patterns) and an invariance loss to reduce LLM variance. In the generation stage, a refined fine-tuning method is employed to improve LLM accuracy in generating answers based on retrieved information. Experimental results show that Invar-RAG significantly outperforms existing baselines across three open-domain question answering (ODQA) datasets. Code is available in the Supplementary Material for reproducibility.\n## Related Work\nInformation Retrieval: Advancements in deep learning have revolutionized information retrieval systems, enhancing their personalization and accuracy in retrieving relevant documents. Early information retrieval frameworks employed sparse retrievers [6] or dense retrievers [4,8] to represent large corpora but struggled to capture deep semantic relationships [13]. LLM-based retrievers (generative retrieval) have since emerged as notable methods, leveraging the rich prior knowledge of LLMs to significantly improve performance by converting documents into parametric knowledge and generating them instead of computing similarity scores [38]. However, the frequent encoding and decoding processes in LLMs severely hinder efficiency [38]. To address the trade-off between effectiveness and efficiency, we propose invar-retrieval in our architecture, enabling the model to efficiently retrieve the most relevant documents without introducing variance. \n\nRetrieval-augmented Language Model: Currently, retrieval-augmented language models have proven effective in answering questions by leveraging external information through the integration of novel retrievers and LLMs [38]. However, the architectural gap between retrieval and generation continues to hinder unified optimization across the entire retrieval-augmented generation system [2]. To address the isolation between retrieval and generation, a novel architecture called RA-DIT was introduced [18]. By aligning retriever scoring with LSR scoring [27], it has been shown to deliver state-of-the-art performance across various tasks. However, it still employs dense retrievers like DRAGON+ [17] in the retrieval stage, which fails to eliminate the problem at its source and introduces inefficiencies throughout the process. Correspondingly, we introduce a representation learning method and invariance loss in our Invar-RAG architecture, which partially addresses these issues and explores a novel approach to using a single LLM for multiple roles within the RAG system.",
            "reference_string": "[273962778 | Liu et al. | 2024 | Citations: 2]"
        },
        {
            "title": "CorpusLM: Towards a Unified Language Model on Corpus for Knowledge-Intensive Tasks",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 17,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.01176, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2144456832",
                    "name": "Xiaoxi Li"
                },
                {
                    "authorId": "2257039188",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "2118788278",
                    "name": "Yujia Zhou"
                },
                {
                    "authorId": "2282524575",
                    "name": "Fangchao Liu"
                }
            ],
            "abstract": "Large language models (LLMs) have gained significant attention in various fields but prone to hallucination, especially in knowledge-intensive (KI) tasks. To address this, retrieval-augmented generation (RAG) has emerged as a popular solution to enhance factual accuracy. However, traditional retrieval modules often rely on large document index and disconnect with generative tasks. With the advent of generative retrieval (GR), language models can retrieve by directly generating document identifiers (DocIDs), offering superior performance in retrieval tasks. However, the potential relationship between GR and downstream tasks remains unexplored. In this paper, we propose CorpusLM, a unified language model that leverages external corpus to tackle various knowledge-intensive tasks by integrating generative retrieval, closed-book generation, and RAG through a unified greedy decoding process. We design the following mechanisms to facilitate effective retrieval and generation, and improve the end-to-end effectiveness of KI tasks: (1) We develop a ranking-oriented DocID list generation strategy, which refines GR by directly learning from a DocID ranking list, to improve retrieval quality. (2) We design a continuous DocIDs-References-Answer generation strategy, which facilitates effective and efficient RAG. (3) We employ well-designed unsupervised DocID understanding tasks, to comprehend DocID semantics and their relevance to downstream tasks. We evaluate our approach on the widely used KILT benchmark with two variants of backbone models, i.e., T5 and Llama2. Experimental results demonstrate the superior performance of our models in both retrieval and downstream tasks.",
            "corpus_id": 267406766,
            "sentences": [
                {
                    "corpus_id": "267406766",
                    "title": "CorpusLM: Towards a Unified Language Model on Corpus for Knowledge-Intensive Tasks",
                    "text": "CorpusLM is a multi-task learning architecture designed to handle various types of knowledge-intensive tasks. It is a unified language model capable of performing generative retrieval, closed-book generation, and retrieval-augmented generation through the same autoregressive greedy generation. The model identifies different tasks using specific prefixes. \n\nThe overview of CorpusLM is illustrated in Figure 1. The training of CorpusLM involves the following three basic tasks: \n\n\u2022 Generative Retrieval: retrieving relevant documents to a given query by generating a ranked DocID list, facilitating the model's ranking ability, and can be achieved through greedy decoding. \n\n\u2022 Closed-book Generation: generating answers solely based on the query input, without relying on external information, similar to classic auto-regressive language models. \u2022 Retrieval-Augmented Generation: generating answers by first retrieving relevant content using DocID list generation, and then generate references and final response through continuous greedy decoding, enhancing effective and efficient RAG. \n\nMoreover, in order to effectively integrate generative retrieval and RAG, it is necessary to improve the model's understanding of Do-cIDs and the relationship between DocIDs and their corresponding knowledge. Therefore, we incorporate a group of unsupervised Do-cID understanding tasks into the multi-task learning framework to enhance the model's understanding of the meaning behind DocIDs: \n\n\u2022 DocID Understanding: The model is equipped with auxiliary tasks that deepen its understanding of DocIDs' structure and semantic meaning. \n\nBy training on the above four types of tasks that share common patterns and connections, CorpusLM develops a more comprehensive understanding of the relationships between retrieval and downstream tasks, as well as the meaning behind the DocIDs, thereby gaining a more robust grasp of each individual task. Formally, the training of CorpusLM aims to optimize objectives with a combined loss function as below: \n\nwhere Lrank, L gen , and Lrag are corresponding loss functions for generative retrieval, closed-book generation, and retrieval-augmented generation represented by Equations ( 1)-( 3). Laux is the loss function for the DocID Understanding task. The specific forms of these loss functions will be explained in subsequent sections.  1 ,  2 ,  3 , and  4 are weighting coefficients for each task's loss.",
                    "score": 0.38888023795084137,
                    "section_title": "CorpusLM: the Unified Language Model",
                    "char_start_offset": 10571,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 109
                        },
                        {
                            "start": 110,
                            "end": 294
                        },
                        {
                            "start": 295,
                            "end": 356
                        },
                        {
                            "start": 359,
                            "end": 411
                        },
                        {
                            "start": 412,
                            "end": 478
                        },
                        {
                            "start": 481,
                            "end": 673
                        },
                        {
                            "start": 676,
                            "end": 846
                        },
                        {
                            "start": 847,
                            "end": 1088
                        },
                        {
                            "start": 1091,
                            "end": 1299
                        },
                        {
                            "start": 1300,
                            "end": 1482
                        },
                        {
                            "start": 1485,
                            "end": 1623
                        },
                        {
                            "start": 1626,
                            "end": 1931
                        },
                        {
                            "start": 1932,
                            "end": 2034
                        },
                        {
                            "start": 2037,
                            "end": 2220
                        },
                        {
                            "start": 2221,
                            "end": 2280
                        },
                        {
                            "start": 2281,
                            "end": 2365
                        },
                        {
                            "start": 2366,
                            "end": 2436
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76416015625
                }
            ],
            "relevance_judgement": 0.76416015625,
            "relevance_judgment_input_expanded": "# Title: CorpusLM: Towards a Unified Language Model on Corpus for Knowledge-Intensive Tasks\n# Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\n# Authors: Xiaoxi Li, Zhicheng Dou, Yujia Zhou, Fangchao Liu\n## Abstract\nLarge language models (LLMs) have gained significant attention in various fields but prone to hallucination, especially in knowledge-intensive (KI) tasks. To address this, retrieval-augmented generation (RAG) has emerged as a popular solution to enhance factual accuracy. However, traditional retrieval modules often rely on large document index and disconnect with generative tasks. With the advent of generative retrieval (GR), language models can retrieve by directly generating document identifiers (DocIDs), offering superior performance in retrieval tasks. However, the potential relationship between GR and downstream tasks remains unexplored. In this paper, we propose CorpusLM, a unified language model that leverages external corpus to tackle various knowledge-intensive tasks by integrating generative retrieval, closed-book generation, and RAG through a unified greedy decoding process. We design the following mechanisms to facilitate effective retrieval and generation, and improve the end-to-end effectiveness of KI tasks: (1) We develop a ranking-oriented DocID list generation strategy, which refines GR by directly learning from a DocID ranking list, to improve retrieval quality. (2) We design a continuous DocIDs-References-Answer generation strategy, which facilitates effective and efficient RAG. (3) We employ well-designed unsupervised DocID understanding tasks, to comprehend DocID semantics and their relevance to downstream tasks. We evaluate our approach on the widely used KILT benchmark with two variants of backbone models, i.e., T5 and Llama2. Experimental results demonstrate the superior performance of our models in both retrieval and downstream tasks.\n## CorpusLM: the Unified Language Model\nCorpusLM is a multi-task learning architecture designed to handle various types of knowledge-intensive tasks. It is a unified language model capable of performing generative retrieval, closed-book generation, and retrieval-augmented generation through the same autoregressive greedy generation. The model identifies different tasks using specific prefixes. \n\nThe overview of CorpusLM is illustrated in Figure 1. The training of CorpusLM involves the following three basic tasks: \n\n\u2022 Generative Retrieval: retrieving relevant documents to a given query by generating a ranked DocID list, facilitating the model's ranking ability, and can be achieved through greedy decoding. \n\n\u2022 Closed-book Generation: generating answers solely based on the query input, without relying on external information, similar to classic auto-regressive language models. \u2022 Retrieval-Augmented Generation: generating answers by first retrieving relevant content using DocID list generation, and then generate references and final response through continuous greedy decoding, enhancing effective and efficient RAG. \n\nMoreover, in order to effectively integrate generative retrieval and RAG, it is necessary to improve the model's understanding of Do-cIDs and the relationship between DocIDs and their corresponding knowledge. Therefore, we incorporate a group of unsupervised Do-cID understanding tasks into the multi-task learning framework to enhance the model's understanding of the meaning behind DocIDs: \n\n\u2022 DocID Understanding: The model is equipped with auxiliary tasks that deepen its understanding of DocIDs' structure and semantic meaning. \n\nBy training on the above four types of tasks that share common patterns and connections, CorpusLM develops a more comprehensive understanding of the relationships between retrieval and downstream tasks, as well as the meaning behind the DocIDs, thereby gaining a more robust grasp of each individual task. Formally, the training of CorpusLM aims to optimize objectives with a combined loss function as below: \n\nwhere Lrank, L gen , and Lrag are corresponding loss functions for generative retrieval, closed-book generation, and retrieval-augmented generation represented by Equations ( 1)-( 3). Laux is the loss function for the DocID Understanding task. The specific forms of these loss functions will be explained in subsequent sections.  1 ,  2 ,  3 , and  4 are weighting coefficients for each task's loss.",
            "reference_string": "[267406766 | Li et al. | 2024 | Citations: 17]"
        },
        {
            "title": "Retrieval is Accurate Generation",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 63,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2209367631",
                    "name": "Bowen Cao"
                },
                {
                    "authorId": "2266753374",
                    "name": "Deng Cai"
                },
                {
                    "authorId": "2279792419",
                    "name": "Leyang Cui"
                },
                {
                    "authorId": null,
                    "name": "Xuxin Cheng"
                },
                {
                    "authorId": "2237804371",
                    "name": "Wei Bi"
                },
                {
                    "authorId": "2260859476",
                    "name": "Yuexian Zou"
                },
                {
                    "authorId": "2257446263",
                    "name": "Shuming Shi"
                }
            ],
            "abstract": "Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects context-aware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents. To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement. Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation. For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from 42.61% to 81.58% in open-ended text generation. Remarkably, our model also achieves the best performance and the lowest latency among several retrieval-augmented baselines. In conclusion, we assert that retrieval is more accurate generation and hope that our work will encourage further research on this new paradigm shift.",
            "corpus_id": 268031947,
            "sentences": [
                {
                    "corpus_id": "268031947",
                    "title": "Retrieval is Accurate Generation",
                    "text": "We compare the proposed method with standard LM in the zero-shot setting, also drawing the following state-of-the-art retrieval-augmented methods as baselines: \n\nBase LM is the standard token-level language model using the Transformer (Vaswani et al., 2017) architecture. We fine-tune the pre-trained GPT-25 (Radford et al., 2019). \n\nkNN-LM (Khandelwal et al., 2020) is a retrieval-augmented LM that interpolates the next-token distribution of the base LM with a k-nearest neighbors (kNN) model. \n\nRETRO (Borgeaud et al., 2022) 6 is a retrieval-augmented LM incorporated with a pre-trained document retriever, a document encoder and a cross-attention mechanism. \n\nCoG (Lan et al., 2023) 7 is another retrieval-augmented LM that adopts a two-stage search pipeline. It first retrieves semantically-relevant documents, and then considers all n-grams within them as candidate phrases.",
                    "score": 0.4310919816026011,
                    "section_title": "BASELINES",
                    "char_start_offset": 17622,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 159
                        },
                        {
                            "start": 162,
                            "end": 271
                        },
                        {
                            "start": 272,
                            "end": 331
                        },
                        {
                            "start": 334,
                            "end": 495
                        },
                        {
                            "start": 498,
                            "end": 661
                        },
                        {
                            "start": 664,
                            "end": 763
                        },
                        {
                            "start": 764,
                            "end": 880
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 235,
                            "end": 257,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 308,
                            "end": 330,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 341,
                            "end": 365,
                            "matchedPaperCorpusId": "207870430"
                        },
                        {
                            "start": 504,
                            "end": 527,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 668,
                            "end": 686,
                            "matchedPaperCorpusId": "259298789"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.763671875
                }
            ],
            "relevance_judgement": 0.763671875,
            "relevance_judgment_input_expanded": "# Title: Retrieval is Accurate Generation\n# Venue: International Conference on Learning Representations\n# Authors: Bowen Cao, Deng Cai, Leyang Cui, Xuxin Cheng, Wei Bi, Yuexian Zou, Shuming Shi\n## Abstract\nStandard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects context-aware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents. To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement. Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation. For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from 42.61% to 81.58% in open-ended text generation. Remarkably, our model also achieves the best performance and the lowest latency among several retrieval-augmented baselines. In conclusion, we assert that retrieval is more accurate generation and hope that our work will encourage further research on this new paradigm shift.\n## BASELINES\nWe compare the proposed method with standard LM in the zero-shot setting, also drawing the following state-of-the-art retrieval-augmented methods as baselines: \n\nBase LM is the standard token-level language model using the Transformer (Vaswani et al., 2017) architecture. We fine-tune the pre-trained GPT-25 (Radford et al., 2019). \n\nkNN-LM (Khandelwal et al., 2020) is a retrieval-augmented LM that interpolates the next-token distribution of the base LM with a k-nearest neighbors (kNN) model. \n\nRETRO (Borgeaud et al., 2022) 6 is a retrieval-augmented LM incorporated with a pre-trained document retriever, a document encoder and a cross-attention mechanism. \n\nCoG (Lan et al., 2023) 7 is another retrieval-augmented LM that adopts a two-stage search pipeline. It first retrieves semantically-relevant documents, and then considers all n-grams within them as candidate phrases.",
            "reference_string": "[268031947 | Cao et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Improving Retrieval-Augmented Large Language Models via Data Importance Learning",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 30,
            "citation_count": 18,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.03027",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.03027, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "35308280",
                    "name": "Xiaozhong Lyu"
                },
                {
                    "authorId": "1393463989",
                    "name": "Stefan Grafberger"
                },
                {
                    "authorId": "2086970867",
                    "name": "Samantha Biegel"
                },
                {
                    "authorId": "1409866591",
                    "name": "Shaopeng Wei"
                },
                {
                    "authorId": "2057073725",
                    "name": "Meng Cao"
                },
                {
                    "authorId": "2180399",
                    "name": "Sebastian Schelter"
                },
                {
                    "authorId": "1776014",
                    "name": "Ce Zhang"
                }
            ],
            "abstract": "Retrieval augmentation enables large language models to take advantage of external knowledge, for example on tasks like question answering and data imputation. However, the performance of such retrieval-augmented models is limited by the data quality of their underlying retrieval corpus. In this paper, we propose an algorithm based on multilinear extension for evaluating the data importance of retrieved data points. There are exponentially many terms in the multilinear extension, and one key contribution of this paper is a polynomial time algorithm that computes exactly, given a retrieval-augmented model with an additive utility function and a validation set, the data importance of data points in the retrieval corpus using the multilinear extension of the model's utility function. We further proposed an even more efficient ({\\epsilon}, {\\delta})-approximation algorithm. Our experimental results illustrate that we can enhance the performance of large language models by only pruning or reweighting the retrieval corpus, without requiring further training. For some tasks, this even allows a small model (e.g., GPT-JT), augmented with a search engine API, to outperform GPT-3.5 (without retrieval augmentation). Moreover, we show that weights based on multilinear extension can be computed efficiently in practice (e.g., in less than ten minutes for a corpus with 100 million elements).",
            "corpus_id": 259360590,
            "sentences": [
                {
                    "corpus_id": "259360590",
                    "title": "Improving Retrieval-Augmented Large Language Models via Data Importance Learning",
                    "text": "Large language models (LLMs) consisting of neural networks with billions of parameters and trained on vast quantities of unlabelled text are the basis of unprecented progress in natural language processing tasks [6,20,21,13]. With zero-shot or few-shot prompting, LLMs can be adopted for a wide range of diverse tasks, such as question answering [15] summarization [15,2] and data imputation [17]. \n\nDrawbacks of large language models. LLMs, however, have two widely acknowledged disadvantages [1,22]. Firstly, despite their impressive capabilities, LLMs actually perform badly on tail entities [1], which they have not seen at training time or cannot remember due to limitations of the network capacity. The second drawback is that with the ever-growing number of model parameters, training, and fine-tuning costs are exploding as well. As a rough estimate, it costs $80k -$1.6m to train a 1.5 billion parameter language model [25,22,29]. This makes it difficult to leverage LLMs for tasks that require regularly updated data or that regularly need to remove privacy-sensitive or copyright-protected data [3]. \n\nRetrieval-augmented models. To address such problems, retrieval-augmented (RAG) models have recently been proposed [12,14,8]. A typical retrieval-augmented model consists of two parts, a retriever f ret and a generator f gen . Given a retrieval corpus D ret = {d 1 , \u2022 \u2022 \u2022 , d M }, the retriever f ret retrieves K data points for an input x i as f ret (x i , D ret ) = {d \u03b11 , d \u03b12 , ..., d \u03b1 K }. Here, \u03b1 k denotes the rank of each data point in the retrieval corpus assigned by the retriever. The generator Figure 1: Data importance evaluation for retrieval-augmented models: The retriever f ret retrieves K data points from the retrieval corpus D ret and provides them to the answer generator f gen . Our data importance evaluator learns weights for the data sources in the retrieval corpus based on the performance on a validation set D val .",
                    "score": 0.375857175152596,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 225
                        },
                        {
                            "start": 226,
                            "end": 397
                        },
                        {
                            "start": 400,
                            "end": 435
                        },
                        {
                            "start": 436,
                            "end": 501
                        },
                        {
                            "start": 502,
                            "end": 704
                        },
                        {
                            "start": 705,
                            "end": 837
                        },
                        {
                            "start": 838,
                            "end": 939
                        },
                        {
                            "start": 940,
                            "end": 1110
                        },
                        {
                            "start": 1113,
                            "end": 1140
                        },
                        {
                            "start": 1141,
                            "end": 1238
                        },
                        {
                            "start": 1239,
                            "end": 1339
                        },
                        {
                            "start": 1340,
                            "end": 1510
                        },
                        {
                            "start": 1511,
                            "end": 1607
                        },
                        {
                            "start": 1608,
                            "end": 1816
                        },
                        {
                            "start": 1817,
                            "end": 1959
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 218,
                            "end": 221,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 935,
                            "end": 938,
                            "matchedPaperCorpusId": "249375466"
                        },
                        {
                            "start": 1232,
                            "end": 1235,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 1235,
                            "end": 1237,
                            "matchedPaperCorpusId": "211204736"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.763671875
                }
            ],
            "relevance_judgement": 0.763671875,
            "relevance_judgment_input_expanded": "# Title: Improving Retrieval-Augmented Large Language Models via Data Importance Learning\n# Venue: arXiv.org\n# Authors: Xiaozhong Lyu, Stefan Grafberger, Samantha Biegel, Shaopeng Wei, Meng Cao, Sebastian Schelter, Ce Zhang\n## Abstract\nRetrieval augmentation enables large language models to take advantage of external knowledge, for example on tasks like question answering and data imputation. However, the performance of such retrieval-augmented models is limited by the data quality of their underlying retrieval corpus. In this paper, we propose an algorithm based on multilinear extension for evaluating the data importance of retrieved data points. There are exponentially many terms in the multilinear extension, and one key contribution of this paper is a polynomial time algorithm that computes exactly, given a retrieval-augmented model with an additive utility function and a validation set, the data importance of data points in the retrieval corpus using the multilinear extension of the model's utility function. We further proposed an even more efficient ({\\epsilon}, {\\delta})-approximation algorithm. Our experimental results illustrate that we can enhance the performance of large language models by only pruning or reweighting the retrieval corpus, without requiring further training. For some tasks, this even allows a small model (e.g., GPT-JT), augmented with a search engine API, to outperform GPT-3.5 (without retrieval augmentation). Moreover, we show that weights based on multilinear extension can be computed efficiently in practice (e.g., in less than ten minutes for a corpus with 100 million elements).\n## Introduction\nLarge language models (LLMs) consisting of neural networks with billions of parameters and trained on vast quantities of unlabelled text are the basis of unprecented progress in natural language processing tasks [6,20,21,13]. With zero-shot or few-shot prompting, LLMs can be adopted for a wide range of diverse tasks, such as question answering [15] summarization [15,2] and data imputation [17]. \n\nDrawbacks of large language models. LLMs, however, have two widely acknowledged disadvantages [1,22]. Firstly, despite their impressive capabilities, LLMs actually perform badly on tail entities [1], which they have not seen at training time or cannot remember due to limitations of the network capacity. The second drawback is that with the ever-growing number of model parameters, training, and fine-tuning costs are exploding as well. As a rough estimate, it costs $80k -$1.6m to train a 1.5 billion parameter language model [25,22,29]. This makes it difficult to leverage LLMs for tasks that require regularly updated data or that regularly need to remove privacy-sensitive or copyright-protected data [3]. \n\nRetrieval-augmented models. To address such problems, retrieval-augmented (RAG) models have recently been proposed [12,14,8]. A typical retrieval-augmented model consists of two parts, a retriever f ret and a generator f gen . Given a retrieval corpus D ret = {d 1 , \u2022 \u2022 \u2022 , d M }, the retriever f ret retrieves K data points for an input x i as f ret (x i , D ret ) = {d \u03b11 , d \u03b12 , ..., d \u03b1 K }. Here, \u03b1 k denotes the rank of each data point in the retrieval corpus assigned by the retriever. The generator Figure 1: Data importance evaluation for retrieval-augmented models: The retriever f ret retrieves K data points from the retrieval corpus D ret and provides them to the answer generator f gen . Our data importance evaluator learns weights for the data sources in the retrieval corpus based on the performance on a validation set D val .",
            "reference_string": "[259360590 | Lyu et al. | 2023 | Citations: 18]"
        },
        {
            "title": "MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 42,
            "citation_count": 159,
            "influential_citation_count": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.02928",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.02928, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2928777",
                    "name": "Wenhu Chen"
                },
                {
                    "authorId": "2804000",
                    "name": "Hexiang Hu"
                },
                {
                    "authorId": "2145309103",
                    "name": "Xi Chen"
                },
                {
                    "authorId": "2986975",
                    "name": "Pat Verga"
                },
                {
                    "authorId": "50056360",
                    "name": "William W. Cohen"
                }
            ],
            "abstract": "While language Models store a massive amount of world knowledge implicitly in their parameters, even very large models often fail to encode information about rare entities and events, while incurring huge computational costs. Recently, retrieval-augmented models, such as REALM, RAG, and RETRO, have incorporated world knowledge into language generation by leveraging an external non-parametric index and have demonstrated impressive performance with constrained model sizes. However, these methods are restricted to retrieving only textual knowledge, neglecting the ubiquitous amount of knowledge in other modalities like images \u2013 much of which contains information not covered by any text. To address this limitation, we propose the first Multimodal Retrieval-Augmented Transformer (MuRAG), which accesses an external non-parametric multimodal memory to augment language generation. MuRAG is pre-trained with a mixture of large-scale image-text and text-only corpora using a joint contrastive and generative loss. We perform experiments on two different datasets that require retrieving and reasoning over both images and text to answer a given query: WebQA, and MultimodalQA. Our results show that MuRAG achieves state-of-the-art accuracy, outperforming existing models by 10-20% absolute on both datasets and under both distractor and full-wiki settings.",
            "corpus_id": 252735160,
            "sentences": [
                {
                    "corpus_id": "252735160",
                    "title": "MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text",
                    "text": "Retrieval Augmented Models Retrieval augmented models are hybrid models containing both parameterized sequence models and a nonparametric memory, infusing world knowledge into existing language models. Among them, KNN-LM (Khandelwal et al., 2019) was first proposed to retrieve instances from a text training corpus to help language modeling. Later, RETRO (Borgeaud et al., 2021) was proposed to scale up the text corpus to trillions of tokens, enabling the model to achieve similar perplexity to GPT-3 (Brown et al., 2020) with 25x fewer model parameters. Another family of models, such as REALM (Guu et al., 2020), RAG (Lewis et al., 2020), and FiD (Izacard and Grave, 2021), integrate Wikipedia passages as a datastore to benefit downstream knowledge intensive tasks (e.g. Question Answering). REALM is an encoder-only model trained with masked lan-guage modeling, while RAG and FiD adopt an encoder-decoder model with a generative language modeling objective. Compared to them, MuRAG is the first retrieval-augmented model that is capable of using knowledge presented in multiple modalities (i.e. visual and textual knowledge data), whereas all prior methods are restricted to using text-only knowledge.",
                    "score": 0.4353487993929209,
                    "section_title": "Related Work",
                    "char_start_offset": 4933,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 221,
                            "end": 246,
                            "matchedPaperCorpusId": "207870430"
                        },
                        {
                            "start": 503,
                            "end": 522,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 597,
                            "end": 615,
                            "matchedPaperCorpusId": "211204736"
                        },
                        {
                            "start": 621,
                            "end": 641,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 651,
                            "end": 676,
                            "matchedPaperCorpusId": "220302360"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76220703125
                }
            ],
            "relevance_judgement": 0.76220703125,
            "relevance_judgment_input_expanded": "# Title: MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen\n## Abstract\nWhile language Models store a massive amount of world knowledge implicitly in their parameters, even very large models often fail to encode information about rare entities and events, while incurring huge computational costs. Recently, retrieval-augmented models, such as REALM, RAG, and RETRO, have incorporated world knowledge into language generation by leveraging an external non-parametric index and have demonstrated impressive performance with constrained model sizes. However, these methods are restricted to retrieving only textual knowledge, neglecting the ubiquitous amount of knowledge in other modalities like images \u2013 much of which contains information not covered by any text. To address this limitation, we propose the first Multimodal Retrieval-Augmented Transformer (MuRAG), which accesses an external non-parametric multimodal memory to augment language generation. MuRAG is pre-trained with a mixture of large-scale image-text and text-only corpora using a joint contrastive and generative loss. We perform experiments on two different datasets that require retrieving and reasoning over both images and text to answer a given query: WebQA, and MultimodalQA. Our results show that MuRAG achieves state-of-the-art accuracy, outperforming existing models by 10-20% absolute on both datasets and under both distractor and full-wiki settings.\n## Related Work\nRetrieval Augmented Models Retrieval augmented models are hybrid models containing both parameterized sequence models and a nonparametric memory, infusing world knowledge into existing language models. Among them, KNN-LM (Khandelwal et al., 2019) was first proposed to retrieve instances from a text training corpus to help language modeling. Later, RETRO (Borgeaud et al., 2021) was proposed to scale up the text corpus to trillions of tokens, enabling the model to achieve similar perplexity to GPT-3 (Brown et al., 2020) with 25x fewer model parameters. Another family of models, such as REALM (Guu et al., 2020), RAG (Lewis et al., 2020), and FiD (Izacard and Grave, 2021), integrate Wikipedia passages as a datastore to benefit downstream knowledge intensive tasks (e.g. Question Answering). REALM is an encoder-only model trained with masked lan-guage modeling, while RAG and FiD adopt an encoder-decoder model with a generative language modeling objective. Compared to them, MuRAG is the first retrieval-augmented model that is capable of using knowledge presented in multiple modalities (i.e. visual and textual knowledge data), whereas all prior methods are restricted to using text-only knowledge.",
            "reference_string": "[252735160 | Chen et al. | 2022 | Citations: 159]"
        },
        {
            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
            "venue": "",
            "year": 2025,
            "reference_count": 31,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.10792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2362089035",
                    "name": "Zhan Peng Lee"
                },
                {
                    "authorId": "2362188632",
                    "name": "Andre Lin"
                },
                {
                    "authorId": "2363425126",
                    "name": "Calvin Tan"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to improve factuality in large language models (LLMs) by grounding their outputs in retrieved documents. However, ensuring perfect retrieval of relevant information remains challenging, and when irrelevant content is passed downstream to an LLM, it can lead to hallucinations. In this work, we propose Finetune-RAG, a simple and effective fine-tuning approach that features the first-of-its-kind RAG training dataset constructed to mimic real-world imperfections. Experimental results show that Finetune-RAG improves factual accuracy by 21.2% over the base model. We also propose Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests models under realistic imperfect retrieval scenarios. Our codebase and dataset are fully open sourced for community use.",
            "corpus_id": 278714952,
            "sentences": [
                {
                    "corpus_id": "278714952",
                    "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
                    "text": "Retrieval-Augmented Generation (RAG) augments large language models by incorporating external documents into the generation process. Rather than relying solely on the model's internal parameters, RAG retrieves relevant passages from a knowledge base and feeds them, along with the user query, into the model to guide its response (Zhou et al., 2024). \n\nA standard RAG system operates in two phases: \n\n\u2022 Retrieval. A retriever model selects the top-k most relevant documents for a given query. \n\n\u2022 Generation. A language model generates a response conditioned on both the query and the retrieved documents. \n\nThe appeal of RAG lies in its ability to dynamically access up-to-date or domain-specific information, which is especially useful in fast-changing or specialized fields. However, it also introduces new failure modes, particularly when the retrieval quality is imperfect (Barnett et al., 2024).",
                    "score": 0.3721991141151649,
                    "section_title": "Retrieval-Augmented Generation",
                    "char_start_offset": 4504,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 132
                        },
                        {
                            "start": 133,
                            "end": 350
                        },
                        {
                            "start": 353,
                            "end": 398
                        },
                        {
                            "start": 401,
                            "end": 413
                        },
                        {
                            "start": 414,
                            "end": 492
                        },
                        {
                            "start": 495,
                            "end": 508
                        },
                        {
                            "start": 509,
                            "end": 605
                        },
                        {
                            "start": 608,
                            "end": 777
                        },
                        {
                            "start": 778,
                            "end": 901
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76171875
                }
            ],
            "relevance_judgement": 0.76171875,
            "relevance_judgment_input_expanded": "# Title: Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation\n# Venue: \n# Authors: Zhan Peng Lee, Andre Lin, Calvin Tan\n## Abstract\nRetrieval-Augmented Generation (RAG) has emerged as a powerful framework to improve factuality in large language models (LLMs) by grounding their outputs in retrieved documents. However, ensuring perfect retrieval of relevant information remains challenging, and when irrelevant content is passed downstream to an LLM, it can lead to hallucinations. In this work, we propose Finetune-RAG, a simple and effective fine-tuning approach that features the first-of-its-kind RAG training dataset constructed to mimic real-world imperfections. Experimental results show that Finetune-RAG improves factual accuracy by 21.2% over the base model. We also propose Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests models under realistic imperfect retrieval scenarios. Our codebase and dataset are fully open sourced for community use.\n## Retrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) augments large language models by incorporating external documents into the generation process. Rather than relying solely on the model's internal parameters, RAG retrieves relevant passages from a knowledge base and feeds them, along with the user query, into the model to guide its response (Zhou et al., 2024). \n\nA standard RAG system operates in two phases: \n\n\u2022 Retrieval. A retriever model selects the top-k most relevant documents for a given query. \n\n\u2022 Generation. A language model generates a response conditioned on both the query and the retrieved documents. \n\nThe appeal of RAG lies in its ability to dynamically access up-to-date or domain-specific information, which is especially useful in fast-changing or specialized fields. However, it also introduces new failure modes, particularly when the retrieval quality is imperfect (Barnett et al., 2024).",
            "reference_string": "[278714952 | Lee et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Enhancing Retrieval Processes for Language Generation with Augmented Queries",
            "venue": "International Conference on Knowledge-Based Intelligent Information & Engineering Systems",
            "year": 2024,
            "reference_count": 10,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.16874, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2272359478",
                    "name": "Julien Pierre Edmond Ghali"
                },
                {
                    "authorId": "2066228428",
                    "name": "Kosuke Shima"
                },
                {
                    "authorId": "2064946036",
                    "name": "Koichi Moriyama"
                },
                {
                    "authorId": "1755863",
                    "name": "Atsuko Mutoh"
                },
                {
                    "authorId": "50663362",
                    "name": "N. Inuzuka"
                }
            ],
            "abstract": null,
            "corpus_id": 268032392,
            "sentences": [
                {
                    "corpus_id": "268032392",
                    "title": "Enhancing Retrieval Processes for Language Generation with Augmented Queries",
                    "text": "Natural Language Processing (NLP) has witnessed significant advancements with the emergence of LLMs. Despite their prowess in downstream tasks, these models face challenges in accessing and manipulating knowledge, leading to suboptimal performance on knowledgeintensive tasks. A groundbreaking approach to address these limitations is Retrieval-Augmented Generation (RAG). In Lewis et al.'s seminal work (2020) [2], the authors introduce RAG models that combine pre-trained parametric and non-parametric memory for language generation. By leveraging a pre-trained seq2seq model as parametric memory and a dense vector index of Wikipedia as non-parametric memory, RAG achieves superior performance on a spectrum of knowledge-intensive NLP tasks. It outshines parametric seq2seq models and task-specific architectures, setting new benchmarks in open-domain question answering (QA) tasks. Furthermore, RAG models exhibit enhanced language generation capabilities, producing more specific, diverse, and factual language compared to state-of-the-art parametric-only seq2seq baselines. \n\nIn the realm of information retrieval (IR), recent research has made remarkable strides. Hambarde and Proenca (2023) [7] provide an extensive overview of IR models, discussing the state-of-the-art methods, including those based on terms, semantic retrieval, and neural approaches. Karpukhin et al. (2020) [8] revolutionize open-domain QA by introducing dense passage retrieval, demonstrating its practical implementation using only dense representations. This approach outperforms traditional sparse vector space models, leading to state-of-the-art performance on multiple open-domain QA benchmarks. Qu et al. (2020) [9] further optimize dense passage retrieval with RocketQA, addressing challenges such as training-inference discrepancy and limited training data. RocketQA significantly outperforms previous state-of-theart models on well-established datasets, showcasing its effectiveness in improving end-to-end QA systems. Advancements in similarity search, as demonstrated by Johnson et al.'s (2019) [10], their work, optimizing k-selection and addressing memory hierarchy challenges, sets new benchmarks in various similarity search scenarios.",
                    "score": 0.556512978173041,
                    "section_title": "Related works",
                    "char_start_offset": 2269,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 100
                        },
                        {
                            "start": 101,
                            "end": 276
                        },
                        {
                            "start": 277,
                            "end": 372
                        },
                        {
                            "start": 373,
                            "end": 535
                        },
                        {
                            "start": 536,
                            "end": 744
                        },
                        {
                            "start": 745,
                            "end": 885
                        },
                        {
                            "start": 886,
                            "end": 1079
                        },
                        {
                            "start": 1082,
                            "end": 1170
                        },
                        {
                            "start": 1171,
                            "end": 1362
                        },
                        {
                            "start": 1363,
                            "end": 1536
                        },
                        {
                            "start": 1537,
                            "end": 1681
                        },
                        {
                            "start": 1682,
                            "end": 1846
                        },
                        {
                            "start": 1847,
                            "end": 2008
                        },
                        {
                            "start": 2009,
                            "end": 2231
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 411,
                            "end": 414,
                            "matchedPaperCorpusId": "218869575"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75830078125
                }
            ],
            "relevance_judgement": 0.75830078125,
            "relevance_judgment_input_expanded": "# Title: Enhancing Retrieval Processes for Language Generation with Augmented Queries\n# Venue: International Conference on Knowledge-Based Intelligent Information & Engineering Systems\n# Authors: Julien Pierre Edmond Ghali, Kosuke Shima, Koichi Moriyama, Atsuko Mutoh, N. Inuzuka\n## Abstract\nNone\n## Related works\nNatural Language Processing (NLP) has witnessed significant advancements with the emergence of LLMs. Despite their prowess in downstream tasks, these models face challenges in accessing and manipulating knowledge, leading to suboptimal performance on knowledgeintensive tasks. A groundbreaking approach to address these limitations is Retrieval-Augmented Generation (RAG). In Lewis et al.'s seminal work (2020) [2], the authors introduce RAG models that combine pre-trained parametric and non-parametric memory for language generation. By leveraging a pre-trained seq2seq model as parametric memory and a dense vector index of Wikipedia as non-parametric memory, RAG achieves superior performance on a spectrum of knowledge-intensive NLP tasks. It outshines parametric seq2seq models and task-specific architectures, setting new benchmarks in open-domain question answering (QA) tasks. Furthermore, RAG models exhibit enhanced language generation capabilities, producing more specific, diverse, and factual language compared to state-of-the-art parametric-only seq2seq baselines. \n\nIn the realm of information retrieval (IR), recent research has made remarkable strides. Hambarde and Proenca (2023) [7] provide an extensive overview of IR models, discussing the state-of-the-art methods, including those based on terms, semantic retrieval, and neural approaches. Karpukhin et al. (2020) [8] revolutionize open-domain QA by introducing dense passage retrieval, demonstrating its practical implementation using only dense representations. This approach outperforms traditional sparse vector space models, leading to state-of-the-art performance on multiple open-domain QA benchmarks. Qu et al. (2020) [9] further optimize dense passage retrieval with RocketQA, addressing challenges such as training-inference discrepancy and limited training data. RocketQA significantly outperforms previous state-of-theart models on well-established datasets, showcasing its effectiveness in improving end-to-end QA systems. Advancements in similarity search, as demonstrated by Johnson et al.'s (2019) [10], their work, optimizing k-selection and addressing memory hierarchy challenges, sets new benchmarks in various similarity search scenarios.",
            "reference_string": "[268032392 | Ghali et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Artefact Retrieval: Overview of NLP Models with Knowledge Base Access",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 91,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.09651, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1429837660",
                    "name": "Vil\u00e9m Zouhar"
                },
                {
                    "authorId": "2140208311",
                    "name": "Marius Mosbach"
                },
                {
                    "authorId": "113817427",
                    "name": "Debanjali Biswas"
                },
                {
                    "authorId": "2561225",
                    "name": "D. Klakow"
                }
            ],
            "abstract": "Many NLP models gain performance by having access to a knowledge base. A lot of research has been devoted to devising and improving the way the knowledge base is accessed and incorporated into the model, resulting in a number of mechanisms and pipelines. Despite the diversity of proposed mechanisms, there are patterns in the designs of such systems. In this paper, we systematically describe the typology of artefacts (items retrieved from a knowledge base), retrieval mechanisms and the way these artefacts are fused into the model. This further allows us to uncover combinations of design decisions that had not yet been tried. Most of the focus is given to language models, though we also show how question answering, fact-checking and knowledgable dialogue models fit into this system as well. Having an abstract model which can describe the architecture of specific models also helps with transferring these architectures between multiple NLP tasks.",
            "corpus_id": 246240957,
            "sentences": [
                {
                    "corpus_id": "246240957",
                    "title": "Artefact Retrieval: Overview of NLP Models with Knowledge Base Access",
                    "text": "Many NLP models gain performance by having access to a knowledge base. A lot of research has been devoted to devising and improving the way the knowledge base is accessed and incorporated into the model, resulting in a number of mechanisms and pipelines. Despite the diversity of proposed mechanisms, there are patterns in the designs of such systems. In this paper, we systematically describe the typology of artefacts (items retrieved from a knowledge base), retrieval mechanisms and the way these artefacts are fused into the model. This further allows us to uncover combinations of design decisions that had not yet been tried. Most of the focus is given to language models, though we also show how question answering, fact-checking and knowledgable dialogue models fit into this system as well. Having an abstract model which can describe the architecture of specific models also helps with transferring these architectures between multiple NLP tasks.",
                    "score": 0.420401177688313,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75244140625
                }
            ],
            "relevance_judgement": 0.75244140625,
            "relevance_judgment_input_expanded": "# Title: Artefact Retrieval: Overview of NLP Models with Knowledge Base Access\n# Venue: arXiv.org\n# Authors: Vil\u00e9m Zouhar, Marius Mosbach, Debanjali Biswas, D. Klakow\n## Abstract\nMany NLP models gain performance by having access to a knowledge base. A lot of research has been devoted to devising and improving the way the knowledge base is accessed and incorporated into the model, resulting in a number of mechanisms and pipelines. Despite the diversity of proposed mechanisms, there are patterns in the designs of such systems. In this paper, we systematically describe the typology of artefacts (items retrieved from a knowledge base), retrieval mechanisms and the way these artefacts are fused into the model. This further allows us to uncover combinations of design decisions that had not yet been tried. Most of the focus is given to language models, though we also show how question answering, fact-checking and knowledgable dialogue models fit into this system as well. Having an abstract model which can describe the architecture of specific models also helps with transferring these architectures between multiple NLP tasks.\n",
            "reference_string": "[246240957 | Zouhar et al. | 2022 | Citations: 4]"
        },
        {
            "title": "Accelerating Retrieval-Augmented Generation",
            "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems",
            "year": 2024,
            "reference_count": 117,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.15246, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2271710074",
                    "name": "Derrick Quinn"
                },
                {
                    "authorId": "2292049291",
                    "name": "Mohammad Nouri"
                },
                {
                    "authorId": "2273672376",
                    "name": "Neel Patel"
                },
                {
                    "authorId": "2336731280",
                    "name": "John Salihu"
                },
                {
                    "authorId": "2073044451",
                    "name": "Alireza Salemi"
                },
                {
                    "authorId": "2336831464",
                    "name": "Sukhan Lee"
                },
                {
                    "authorId": "2295731593",
                    "name": "Hamed Zamani"
                },
                {
                    "authorId": "2273183386",
                    "name": "Mohammad Alian"
                }
            ],
            "abstract": "An evolving solution to address hallucination and enhance accuracy in large language models (LLMs) is Retrieval-Augmented Generation (RAG), which involves augmenting LLMs with information retrieved from an external knowledge source, such as the web. This paper profiles several RAG execution pipelines and demystifies the complex interplay between their retrieval and generation phases. We demonstrate that while exact retrieval schemes are expensive, they can reduce inference time compared to approximate retrieval variants because an exact retrieval model can send a smaller but more accurate list of documents to the generative model while maintaining the same end-to-end accuracy. This observation motivates the acceleration of the exact nearest neighbor search for RAG. In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL device that implements a scale-out near-memory acceleration architecture with a novel cache-coherent interface between the host CPU and near-memory accelerators. IKS offers 13.4--27.9\u00d7 faster exact nearest neighbor search over a 512GB vector database compared with executing the search on Intel Sapphire Rapids CPUs. This higher search performance translates to 1.7--26.3\u00d7 lower end-to-end inference time for representative RAG applications. IKS is inherently a memory expander; its internal DRAM can be disaggregated and used for other applications running on the server to prevent DRAM -- which is the most expensive component in today's servers -- from being stranded.",
            "corpus_id": 274965455,
            "sentences": [
                {
                    "corpus_id": "274965455",
                    "title": "Accelerating Retrieval-Augmented Generation",
                    "text": "An evolving solution to address hallucination and enhance accuracy in large language models (LLMs) is Retrieval-Augmented Generation (RAG), which involves augmenting LLMs with information retrieved from an external knowledge source, such as the web. This paper profiles several RAG execution pipelines and demystifies the complex interplay between their retrieval and generation phases. We demonstrate that while exact retrieval schemes are expensive, they can reduce inference time compared to approximate retrieval variants because an exact retrieval model can send a smaller but more accurate list of documents to the generative model while maintaining the same end-to-end accuracy. This observation motivates the acceleration of the exact nearest neighbor search for RAG. In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL device that implements a scale-out near-memory acceleration architecture with a novel cache-coherent interface between the host CPU and near-memory accelerators. IKS offers 13.4--27.9\u00d7 faster exact nearest neighbor search over a 512GB vector database compared with executing the search on Intel Sapphire Rapids CPUs. This higher search performance translates to 1.7--26.3\u00d7 lower end-to-end inference time for representative RAG applications. IKS is inherently a memory expander; its internal DRAM can be disaggregated and used for other applications running on the server to prevent DRAM -- which is the most expensive component in today's servers -- from being stranded.",
                    "score": 0.42392820410598686,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75146484375
                }
            ],
            "relevance_judgement": 0.75146484375,
            "relevance_judgment_input_expanded": "# Title: Accelerating Retrieval-Augmented Generation\n# Venue: International Conference on Architectural Support for Programming Languages and Operating Systems\n# Authors: Derrick Quinn, Mohammad Nouri, Neel Patel, John Salihu, Alireza Salemi, Sukhan Lee, Hamed Zamani, Mohammad Alian\n## Abstract\nAn evolving solution to address hallucination and enhance accuracy in large language models (LLMs) is Retrieval-Augmented Generation (RAG), which involves augmenting LLMs with information retrieved from an external knowledge source, such as the web. This paper profiles several RAG execution pipelines and demystifies the complex interplay between their retrieval and generation phases. We demonstrate that while exact retrieval schemes are expensive, they can reduce inference time compared to approximate retrieval variants because an exact retrieval model can send a smaller but more accurate list of documents to the generative model while maintaining the same end-to-end accuracy. This observation motivates the acceleration of the exact nearest neighbor search for RAG. In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL device that implements a scale-out near-memory acceleration architecture with a novel cache-coherent interface between the host CPU and near-memory accelerators. IKS offers 13.4--27.9\u00d7 faster exact nearest neighbor search over a 512GB vector database compared with executing the search on Intel Sapphire Rapids CPUs. This higher search performance translates to 1.7--26.3\u00d7 lower end-to-end inference time for representative RAG applications. IKS is inherently a memory expander; its internal DRAM can be disaggregated and used for other applications running on the server to prevent DRAM -- which is the most expensive component in today's servers -- from being stranded.\n",
            "reference_string": "[274965455 | Quinn et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 84,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.11295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2220096705",
                    "name": "Peng Shu"
                },
                {
                    "authorId": "2325897685",
                    "name": "Junhao Chen"
                },
                {
                    "authorId": "2145977326",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "2273568534",
                    "name": "Hui Wang"
                },
                {
                    "authorId": "2263593041",
                    "name": "Zihao Wu"
                },
                {
                    "authorId": "2215167446",
                    "name": "Tianyang Zhong"
                },
                {
                    "authorId": "2257102397",
                    "name": "Yiwei Li"
                },
                {
                    "authorId": "2276747984",
                    "name": "Huaqin Zhao"
                },
                {
                    "authorId": "2273631049",
                    "name": "Hanqi Jiang"
                },
                {
                    "authorId": "2221032216",
                    "name": "Yi Pan"
                },
                {
                    "authorId": "2325891087",
                    "name": "Yifan Zhou"
                },
                {
                    "authorId": "2331328795",
                    "name": "Constance Owl"
                },
                {
                    "authorId": "2249626607",
                    "name": "Xiaoming Zhai"
                },
                {
                    "authorId": "2238404369",
                    "name": "Ninghao Liu"
                },
                {
                    "authorId": "2331321061",
                    "name": "Claudio Saunt"
                },
                {
                    "authorId": "2254792886",
                    "name": "Tianming Liu"
                }
            ],
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable success across a wide range of tasks and domains. However, their performance in low-resource language translation, particularly when translating into these languages, remains underexplored. This gap poses significant challenges, as linguistic barriers hinder the cultural preservation and development of minority communities. To address this issue, this paper introduces a novel retrieval-based method that enhances translation quality for low-resource languages by focusing on key terms, which involves translating keywords and retrieving corresponding examples from existing data. To evaluate the effectiveness of this method, we conducted experiments translating from English into three low-resource languages: Cherokee, a critically endangered indigenous language of North America; Tibetan, a historically and culturally significant language in Asia; and Manchu, a language with few remaining speakers. Our comparison with the zero-shot performance of GPT-4o and LLaMA 3.1 405B, highlights the significant challenges these models face when translating into low-resource languages. In contrast, our retrieval-based method shows promise in improving both word-level accuracy and overall semantic understanding by leveraging existing resources more effectively.",
            "corpus_id": 274131235,
            "sentences": [
                {
                    "corpus_id": "274131235",
                    "title": "Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation",
                    "text": "We implement a RAG LLM low resource language translator by combining retrieval-based techniques with LLMs to ensure accurate and context-aware translations. The overall architecture is shown in Figure 1. The system utilizes dictionary entries, which are indexed through two complementary approaches: keyword-to-document mappings and vector embeddings. Key-to-document mappings in systems refer to a process where keywords are linked directly to the documents or data entries that contain or are relevant to those keywords. The keyword retriever will retrieve corresponding documents according to the key-to-document mapping, if the keyword is inside our storage. The vector embedding indexing process organizes raw linguistic data into retrievable units by associating words with dictionary definitions and using text-embedding-ada-002 model to encode the text into high-dimensional vectors that capture semantic relationships beyond mere surface forms. \n\nFigure 1: Illustrating a retrieval-augmented generation (RAG) architecture: Documents are indexed using both keyword and embedding vector methods, stored in separate databases. A retrieval agent accesses these indexes to provide relevant information, which is then processed by a GPT-4 model to deliver responses to users. \n\nOur model operates with this dual retrieval mechanism. First, a keyword-based index allows for fast and efficient lookup by identifying exact matches between query terms and dictionary entries. This method ensures that direct translations of words or phrases are retrieved whenever possible. Second, in cases where no exact keyword matches are found, the system employs a vector-based retrieval method using cosine similarity. This approach encodes the query into a semantic vector and calculates similarity scores between the query vector and all indexed vectors. The top K most similar entries are then retrieved, ensuring that semantically related content such as synonyms or conceptually similar expressions are identified, even in the absence of exact matches. \n\nAfter retrieval, the system uses a GPT-based model to synthesize the retrieved content into a coherent and fluent translation. This generative step integrates the context from retrieved entries to resolve ambiguities and produce a natural, human-readable translation. By balancing lightweight keyword-based retrieval with deeper semantic understanding through vector-based retrieval, this hybrid approach enhances both the accuracy and relevance of the translation output.",
                    "score": 0.44256305877079244,
                    "section_title": "Methodology",
                    "char_start_offset": 40068,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 156
                        },
                        {
                            "start": 157,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 351
                        },
                        {
                            "start": 352,
                            "end": 522
                        },
                        {
                            "start": 523,
                            "end": 662
                        },
                        {
                            "start": 663,
                            "end": 953
                        },
                        {
                            "start": 956,
                            "end": 1132
                        },
                        {
                            "start": 1133,
                            "end": 1278
                        },
                        {
                            "start": 1281,
                            "end": 1335
                        },
                        {
                            "start": 1336,
                            "end": 1474
                        },
                        {
                            "start": 1475,
                            "end": 1572
                        },
                        {
                            "start": 1573,
                            "end": 1707
                        },
                        {
                            "start": 1708,
                            "end": 1845
                        },
                        {
                            "start": 1846,
                            "end": 2046
                        },
                        {
                            "start": 2049,
                            "end": 2175
                        },
                        {
                            "start": 2176,
                            "end": 2316
                        },
                        {
                            "start": 2317,
                            "end": 2521
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7470703125
                }
            ],
            "relevance_judgement": 0.7470703125,
            "relevance_judgment_input_expanded": "# Title: Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation\n# Venue: arXiv.org\n# Authors: Peng Shu, Junhao Chen, Zheng Liu, Hui Wang, Zihao Wu, Tianyang Zhong, Yiwei Li, Huaqin Zhao, Hanqi Jiang, Yi Pan, Yifan Zhou, Constance Owl, Xiaoming Zhai, Ninghao Liu, Claudio Saunt, Tianming Liu\n## Abstract\nLarge Language Models (LLMs) have demonstrated remarkable success across a wide range of tasks and domains. However, their performance in low-resource language translation, particularly when translating into these languages, remains underexplored. This gap poses significant challenges, as linguistic barriers hinder the cultural preservation and development of minority communities. To address this issue, this paper introduces a novel retrieval-based method that enhances translation quality for low-resource languages by focusing on key terms, which involves translating keywords and retrieving corresponding examples from existing data. To evaluate the effectiveness of this method, we conducted experiments translating from English into three low-resource languages: Cherokee, a critically endangered indigenous language of North America; Tibetan, a historically and culturally significant language in Asia; and Manchu, a language with few remaining speakers. Our comparison with the zero-shot performance of GPT-4o and LLaMA 3.1 405B, highlights the significant challenges these models face when translating into low-resource languages. In contrast, our retrieval-based method shows promise in improving both word-level accuracy and overall semantic understanding by leveraging existing resources more effectively.\n## Methodology\nWe implement a RAG LLM low resource language translator by combining retrieval-based techniques with LLMs to ensure accurate and context-aware translations. The overall architecture is shown in Figure 1. The system utilizes dictionary entries, which are indexed through two complementary approaches: keyword-to-document mappings and vector embeddings. Key-to-document mappings in systems refer to a process where keywords are linked directly to the documents or data entries that contain or are relevant to those keywords. The keyword retriever will retrieve corresponding documents according to the key-to-document mapping, if the keyword is inside our storage. The vector embedding indexing process organizes raw linguistic data into retrievable units by associating words with dictionary definitions and using text-embedding-ada-002 model to encode the text into high-dimensional vectors that capture semantic relationships beyond mere surface forms. \n\nFigure 1: Illustrating a retrieval-augmented generation (RAG) architecture: Documents are indexed using both keyword and embedding vector methods, stored in separate databases. A retrieval agent accesses these indexes to provide relevant information, which is then processed by a GPT-4 model to deliver responses to users. \n\nOur model operates with this dual retrieval mechanism. First, a keyword-based index allows for fast and efficient lookup by identifying exact matches between query terms and dictionary entries. This method ensures that direct translations of words or phrases are retrieved whenever possible. Second, in cases where no exact keyword matches are found, the system employs a vector-based retrieval method using cosine similarity. This approach encodes the query into a semantic vector and calculates similarity scores between the query vector and all indexed vectors. The top K most similar entries are then retrieved, ensuring that semantically related content such as synonyms or conceptually similar expressions are identified, even in the absence of exact matches. \n\nAfter retrieval, the system uses a GPT-based model to synthesize the retrieved content into a coherent and fluent translation. This generative step integrates the context from retrieved entries to resolve ambiguities and produce a natural, human-readable translation. By balancing lightweight keyword-based retrieval with deeper semantic understanding through vector-based retrieval, this hybrid approach enhances both the accuracy and relevance of the translation output.",
            "reference_string": "[274131235 | Shu et al. | 2024 | Citations: 8]"
        },
        {
            "title": "From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 222,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.19678, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2864573",
                    "name": "M. Ferrag"
                },
                {
                    "authorId": "2283303502",
                    "name": "Norbert Tihanyi"
                },
                {
                    "authorId": "2065834880",
                    "name": "M. Debbah"
                }
            ],
            "abstract": "Large language models and autonomous AI agents have evolved rapidly, resulting in a diverse array of evaluation benchmarks, frameworks, and collaboration protocols. However, the landscape remains fragmented and lacks a unified taxonomy or comprehensive survey. Therefore, we present a side-by-side comparison of benchmarks developed between 2019 and 2025 that evaluate these models and agents across multiple domains. In addition, we propose a taxonomy of approximately 60 benchmarks that cover general and academic knowledge reasoning, mathematical problem-solving, code generation and software engineering, factual grounding and retrieval, domain-specific evaluations, multimodal and embodied tasks, task orchestration, and interactive assessments. Furthermore, we review AI-agent frameworks introduced between 2023 and 2025 that integrate large language models with modular toolkits to enable autonomous decision-making and multi-step reasoning. Moreover, we present real-world applications of autonomous AI agents in materials science, biomedical research, academic ideation, software engineering, synthetic data generation, chemical reasoning, mathematical problem-solving, geographic information systems, multimedia, healthcare, and finance. We then survey key agent-to-agent collaboration protocols, namely the Agent Communication Protocol (ACP), the Model Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally, we discuss recommendations for future research, focusing on advanced reasoning strategies, failure modes in multi-agent LLM systems, automated scientific discovery, dynamic tool integration via reinforcement learning, integrated search capabilities, and security vulnerabilities in agent protocols.",
            "corpus_id": 278165282,
            "sentences": [
                {
                    "corpus_id": "278165282",
                    "title": "From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review",
                    "text": "Singh et al. [49] delves into Agentic Retrieval-Augmented Generation (Agentic RAG), a sophisticated evolution of traditional Retrieval-Augmented Generation systems that enhances the capabilities of large language models (LLMs). While LLMs have transformed AI through human-like text generation and language understanding, their dependence on static training data often results in outdated or imprecise responses. The paper addresses these limitations by embedding autonomous agents within the RAG framework, enabling dynamic, real-time data retrieval and adaptive workflows. It details how agentic design patterns such as reflection, planning, tool utilization, and multi-agent collaboration equip these systems to manage complex tasks and support multi-step reasoning. The survey offers a comprehensive taxonomy of Agentic RAG architectures, highlights key applications across various sectors, including healthcare, finance, and education, and outlines practical implementation strategies. \n\nComplementing this architectural perspective, Yehudai et al. [50] mark a significant milestone in artificial intelligence by surveying evaluation methodologies for agents powered by large language models (LLMs). It thoroughly reviews the capabilities of these agents, focusing on core functions such as planning, tool utilization, self-reflection, and memory, while assessing specialized applications ranging from web interactions to software engineering and conversational tasks. The authors uncover a clear trend toward developing more rigorous, dynamically updated evaluation frameworks by examining both targeted benchmarks for domain-specific applications and those designed for more generalist agents. \n\nMoreover, the paper critically highlights existing deficiencies in the field, notably the need for metrics that more effectively capture cost efficiency, safety, and robustness. In doing so, it maps the current landscape of agent evaluation and sets forth compelling directions for future inquiry, underscoring the importance of scalable and fine-grained evaluation techniques in the rapidly evolving AI domain. \n\nSimilarly, Chen et al. [51] focus on Role-Playing Agents (RPAs), a growing class of LLM-based agents that mimic human behavior across various tasks. Recognizing the inherent challenges in evaluating such diverse systems, the authors systematically reviewed 1,676 papers published between January 2021 and December 2024. Their extensive analysis identifies six key agent attributes, seven task attributes, and seven evaluation metrics that are prevalent in the current literature.",
                    "score": 0.39380885161687496,
                    "section_title": "B. Agent Architectures and Evaluation Frameworks",
                    "char_start_offset": 8695,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 227
                        },
                        {
                            "start": 228,
                            "end": 412
                        },
                        {
                            "start": 413,
                            "end": 574
                        },
                        {
                            "start": 575,
                            "end": 769
                        },
                        {
                            "start": 770,
                            "end": 990
                        },
                        {
                            "start": 993,
                            "end": 1204
                        },
                        {
                            "start": 1205,
                            "end": 1473
                        },
                        {
                            "start": 1474,
                            "end": 1700
                        },
                        {
                            "start": 1703,
                            "end": 1880
                        },
                        {
                            "start": 1881,
                            "end": 2114
                        },
                        {
                            "start": 2117,
                            "end": 2265
                        },
                        {
                            "start": 2266,
                            "end": 2436
                        },
                        {
                            "start": 2437,
                            "end": 2596
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74560546875
                }
            ],
            "relevance_judgement": 0.74560546875,
            "relevance_judgment_input_expanded": "# Title: From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review\n# Venue: arXiv.org\n# Authors: M. Ferrag, Norbert Tihanyi, M. Debbah\n## Abstract\nLarge language models and autonomous AI agents have evolved rapidly, resulting in a diverse array of evaluation benchmarks, frameworks, and collaboration protocols. However, the landscape remains fragmented and lacks a unified taxonomy or comprehensive survey. Therefore, we present a side-by-side comparison of benchmarks developed between 2019 and 2025 that evaluate these models and agents across multiple domains. In addition, we propose a taxonomy of approximately 60 benchmarks that cover general and academic knowledge reasoning, mathematical problem-solving, code generation and software engineering, factual grounding and retrieval, domain-specific evaluations, multimodal and embodied tasks, task orchestration, and interactive assessments. Furthermore, we review AI-agent frameworks introduced between 2023 and 2025 that integrate large language models with modular toolkits to enable autonomous decision-making and multi-step reasoning. Moreover, we present real-world applications of autonomous AI agents in materials science, biomedical research, academic ideation, software engineering, synthetic data generation, chemical reasoning, mathematical problem-solving, geographic information systems, multimedia, healthcare, and finance. We then survey key agent-to-agent collaboration protocols, namely the Agent Communication Protocol (ACP), the Model Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally, we discuss recommendations for future research, focusing on advanced reasoning strategies, failure modes in multi-agent LLM systems, automated scientific discovery, dynamic tool integration via reinforcement learning, integrated search capabilities, and security vulnerabilities in agent protocols.\n## B. Agent Architectures and Evaluation Frameworks\nSingh et al. [49] delves into Agentic Retrieval-Augmented Generation (Agentic RAG), a sophisticated evolution of traditional Retrieval-Augmented Generation systems that enhances the capabilities of large language models (LLMs). While LLMs have transformed AI through human-like text generation and language understanding, their dependence on static training data often results in outdated or imprecise responses. The paper addresses these limitations by embedding autonomous agents within the RAG framework, enabling dynamic, real-time data retrieval and adaptive workflows. It details how agentic design patterns such as reflection, planning, tool utilization, and multi-agent collaboration equip these systems to manage complex tasks and support multi-step reasoning. The survey offers a comprehensive taxonomy of Agentic RAG architectures, highlights key applications across various sectors, including healthcare, finance, and education, and outlines practical implementation strategies. \n\nComplementing this architectural perspective, Yehudai et al. [50] mark a significant milestone in artificial intelligence by surveying evaluation methodologies for agents powered by large language models (LLMs). It thoroughly reviews the capabilities of these agents, focusing on core functions such as planning, tool utilization, self-reflection, and memory, while assessing specialized applications ranging from web interactions to software engineering and conversational tasks. The authors uncover a clear trend toward developing more rigorous, dynamically updated evaluation frameworks by examining both targeted benchmarks for domain-specific applications and those designed for more generalist agents. \n\nMoreover, the paper critically highlights existing deficiencies in the field, notably the need for metrics that more effectively capture cost efficiency, safety, and robustness. In doing so, it maps the current landscape of agent evaluation and sets forth compelling directions for future inquiry, underscoring the importance of scalable and fine-grained evaluation techniques in the rapidly evolving AI domain. \n\nSimilarly, Chen et al. [51] focus on Role-Playing Agents (RPAs), a growing class of LLM-based agents that mimic human behavior across various tasks. Recognizing the inherent challenges in evaluating such diverse systems, the authors systematically reviewed 1,676 papers published between January 2021 and December 2024. Their extensive analysis identifies six key agent attributes, seven task attributes, and seven evaluation metrics that are prevalent in the current literature.",
            "reference_string": "[278165282 | Ferrag et al. | 2025 | Citations: 6]"
        },
        {
            "title": "Patchwork: A Unified Framework for RAG Serving",
            "venue": "",
            "year": 2025,
            "reference_count": 79,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.07833, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2264069110",
                    "name": "Bodun Hu"
                },
                {
                    "authorId": "2360698298",
                    "name": "Luis Pabon"
                },
                {
                    "authorId": "2362207523",
                    "name": "Saurabh Agarwal"
                },
                {
                    "authorId": "2262444276",
                    "name": "Aditya Akella"
                }
            ],
            "abstract": "Retrieval Augmented Generation (RAG) has emerged as a new paradigm for enhancing Large Language Model reliability through integration with external knowledge sources. However, efficient deployment of these systems presents significant technical challenges due to their inherently heterogeneous computational pipelines comprising LLMs, databases, and specialized processing components. We introduce Patchwork, a comprehensive end-to-end RAG serving framework designed to address these efficiency bottlenecks. Patchwork's architecture offers three key innovations: First, it provides a flexible specification interface enabling users to implement custom RAG pipelines. Secondly, it deploys these pipelines as distributed inference systems while optimizing for the unique scalability characteristics of individual RAG components. Third, Patchwork incorporates an online scheduling mechanism that continuously monitors request load and execution progress, dynamically minimizing SLO violations through strategic request prioritization and resource auto-scaling. Our experimental evaluation across four distinct RAG implementations demonstrates that Patchwork delivers substantial performance improvements over commercial alternatives, achieving throughput gains exceeding 48% while simultaneously reducing SLO violations by ~24%.",
            "corpus_id": 278535570,
            "sentences": [
                {
                    "corpus_id": "278535570",
                    "title": "Patchwork: A Unified Framework for RAG Serving",
                    "text": "With their rapid advancements, Large Language Models (LLMs) have become indispensable tools across a range of applications, including question answering [59,80], code generation [40,87], task automation [27], and search assistants. Despite major progress in underlying machine learning (ML) techniques, LLMs still face several fundamental limitations-such as hallucinations [20], outdated knowledge [37], and confinement to the snapshot of training data. \n\nTo address these issues, Retrieval-Augmented Generation (RAG) systems [37] have emerged as a widely adopted solution, particularly for applications where factual accuracy is critical. Figure 1 provides an overview of a typical RAG architecture. As illustrated, a RAG system supplements an LLM with a knowledge base-often an external, indexed database-whose contents are retrieved and incorporated into user queries before being passed to the LLM. Some RAG variants [6,32,68,69] go further by enabling recursive interactions between the LLM and the knowledge base, allowing * Both authors contributed equally to this research. iterative refinements of the response. This integration has been shown to significantly improve both the factual accuracy and generation quality of LLM outputs. Moreover, RAG systems offer a practical alternative to costly model retraining, providing a scalable and extensible mechanism for keeping LLMs up to date with new information. These advantages have spurred rapid research and real-world adoption of RAG techniques [17,18,25,46,58,84]. \n\nGiven the growing ubiquity of RAG systems, optimizing their performance is becoming increasingly critical. However, three key challenges make this far from straightforward: \n\n1. Rapid Evolution of the Stack: The RAG ecosystem is evolving at a breakneck pace, with new components, execution strategies, and architectural patterns emerging frequently. This constant change compounds the complexity faced by users in designing and deploying RAG pipelines. More importantly, the resulting fragmentation-a patchwork of bespoke solutions-makes it difficult to develop and apply general, reusable performance management techniques. \n\n2. Heterogeneity of Components: RAG consist of diverse components-including vector databases, query augmenters, and LLM serving engines-each with unique computational characteristics and scaling behaviors.",
                    "score": 0.3908534054751986,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 231
                        },
                        {
                            "start": 232,
                            "end": 454
                        },
                        {
                            "start": 457,
                            "end": 640
                        },
                        {
                            "start": 641,
                            "end": 701
                        },
                        {
                            "start": 702,
                            "end": 903
                        },
                        {
                            "start": 904,
                            "end": 1082
                        },
                        {
                            "start": 1083,
                            "end": 1121
                        },
                        {
                            "start": 1122,
                            "end": 1243
                        },
                        {
                            "start": 1244,
                            "end": 1419
                        },
                        {
                            "start": 1420,
                            "end": 1527
                        },
                        {
                            "start": 1530,
                            "end": 1636
                        },
                        {
                            "start": 1637,
                            "end": 1702
                        },
                        {
                            "start": 1705,
                            "end": 1879
                        },
                        {
                            "start": 1880,
                            "end": 1982
                        },
                        {
                            "start": 1983,
                            "end": 2154
                        },
                        {
                            "start": 2157,
                            "end": 2362
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 157,
                            "end": 160,
                            "matchedPaperCorpusId": "1373518"
                        },
                        {
                            "start": 922,
                            "end": 925,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 925,
                            "end": 928,
                            "matchedPaperCorpusId": "258615731"
                        },
                        {
                            "start": 1523,
                            "end": 1526,
                            "matchedPaperCorpusId": "268510197"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74072265625
                }
            ],
            "relevance_judgement": 0.74072265625,
            "relevance_judgment_input_expanded": "# Title: Patchwork: A Unified Framework for RAG Serving\n# Venue: \n# Authors: Bodun Hu, Luis Pabon, Saurabh Agarwal, Aditya Akella\n## Abstract\nRetrieval Augmented Generation (RAG) has emerged as a new paradigm for enhancing Large Language Model reliability through integration with external knowledge sources. However, efficient deployment of these systems presents significant technical challenges due to their inherently heterogeneous computational pipelines comprising LLMs, databases, and specialized processing components. We introduce Patchwork, a comprehensive end-to-end RAG serving framework designed to address these efficiency bottlenecks. Patchwork's architecture offers three key innovations: First, it provides a flexible specification interface enabling users to implement custom RAG pipelines. Secondly, it deploys these pipelines as distributed inference systems while optimizing for the unique scalability characteristics of individual RAG components. Third, Patchwork incorporates an online scheduling mechanism that continuously monitors request load and execution progress, dynamically minimizing SLO violations through strategic request prioritization and resource auto-scaling. Our experimental evaluation across four distinct RAG implementations demonstrates that Patchwork delivers substantial performance improvements over commercial alternatives, achieving throughput gains exceeding 48% while simultaneously reducing SLO violations by ~24%.\n## Introduction\nWith their rapid advancements, Large Language Models (LLMs) have become indispensable tools across a range of applications, including question answering [59,80], code generation [40,87], task automation [27], and search assistants. Despite major progress in underlying machine learning (ML) techniques, LLMs still face several fundamental limitations-such as hallucinations [20], outdated knowledge [37], and confinement to the snapshot of training data. \n\nTo address these issues, Retrieval-Augmented Generation (RAG) systems [37] have emerged as a widely adopted solution, particularly for applications where factual accuracy is critical. Figure 1 provides an overview of a typical RAG architecture. As illustrated, a RAG system supplements an LLM with a knowledge base-often an external, indexed database-whose contents are retrieved and incorporated into user queries before being passed to the LLM. Some RAG variants [6,32,68,69] go further by enabling recursive interactions between the LLM and the knowledge base, allowing * Both authors contributed equally to this research. iterative refinements of the response. This integration has been shown to significantly improve both the factual accuracy and generation quality of LLM outputs. Moreover, RAG systems offer a practical alternative to costly model retraining, providing a scalable and extensible mechanism for keeping LLMs up to date with new information. These advantages have spurred rapid research and real-world adoption of RAG techniques [17,18,25,46,58,84]. \n\nGiven the growing ubiquity of RAG systems, optimizing their performance is becoming increasingly critical. However, three key challenges make this far from straightforward: \n\n1. Rapid Evolution of the Stack: The RAG ecosystem is evolving at a breakneck pace, with new components, execution strategies, and architectural patterns emerging frequently. This constant change compounds the complexity faced by users in designing and deploying RAG pipelines. More importantly, the resulting fragmentation-a patchwork of bespoke solutions-makes it difficult to develop and apply general, reusable performance management techniques. \n\n2. Heterogeneity of Components: RAG consist of diverse components-including vector databases, query augmenters, and LLM serving engines-each with unique computational characteristics and scaling behaviors.",
            "reference_string": "[278535570 | Hu et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Ragnar\u00f6k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track",
            "venue": "European Conference on Information Retrieval",
            "year": 2024,
            "reference_count": 74,
            "citation_count": 16,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.16828, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1816753042",
                    "name": "Ronak Pradeep"
                },
                {
                    "authorId": "47583894",
                    "name": "Nandan Thakur"
                },
                {
                    "authorId": "71076877",
                    "name": "Sahel Sharifymoghaddam"
                },
                {
                    "authorId": "2308035714",
                    "name": "Eric Zhang"
                },
                {
                    "authorId": "2308034071",
                    "name": "Ryan Nguyen"
                },
                {
                    "authorId": "2308033528",
                    "name": "Daniel Campos"
                },
                {
                    "authorId": "2182067638",
                    "name": "Nick Craswell"
                },
                {
                    "authorId": "2300329796",
                    "name": "Jimmy Lin"
                }
            ],
            "abstract": "Did you try out the new Bing Search? Or maybe you fiddled around with Google AI~Overviews? These might sound familiar because the modern-day search stack has recently evolved to include retrieval-augmented generation (RAG) systems. They allow searching and incorporating real-time data into large language models (LLMs) to provide a well-informed, attributed, concise summary in contrast to the traditional search paradigm that relies on displaying a ranked list of documents. Therefore, given these recent advancements, it is crucial to have an arena to build, test, visualize, and systematically evaluate RAG-based search systems. With this in mind, we propose the TREC 2024 RAG Track to foster innovation in evaluating RAG systems. In our work, we lay out the steps we've made towards making this track a reality -- we describe the details of our reusable framework, Ragnar\\\"ok, explain the curation of the new MS MARCO V2.1 collection choice, release the development topics for the track, and standardize the I/O definitions which assist the end user. Next, using Ragnar\\\"ok, we identify and provide key industrial baselines such as OpenAI's GPT-4o or Cohere's Command R+. Further, we introduce a web-based user interface for an interactive arena allowing benchmarking pairwise RAG systems by crowdsourcing. We open-source our Ragnar\\\"ok framework and baselines to achieve a unified standard for future RAG systems.",
            "corpus_id": 270702738,
            "sentences": [
                {
                    "corpus_id": "270702738",
                    "title": "Ragnar\u00f6k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track",
                    "text": "Ragnar\u00f6k is an open-source, reproducible, and reusable framework implementing an end-to-end retrieval-augmented generation (RAG) pipeline, comprising two modules applied sequentially: (1) (R) retrieval and (2) (AG) augmented generation.Through the Ragnar\u00f6k framework, we will provide several baselines to all participants in the upcoming TREC 2024 RAG track.An overview of the framework is provided in Figure 1.We first describe both modules and expand on the I/O specifications in our framework.\n\nRetrieval Module.This module retrieves the relevant segments for a user topic as the input.It supports (i) first-stage lexical retrieval models such as BM25 [49] and (ii) reranking models such as RankZephyr [47].The retrieval system searches for relevant segments in the document collection and retrieves the top-100 segments further reranked by the reranker model to filter out the top-20 relevant segments for the next stage.\n\nAugmented Generation Module.This module takes in the user topic and the top-20 retrieved segments (from the retrieval module) as the input and a prompting strategy to the large language model (LLM) to generate the answer response with in-context citations for the topic.The answer response is divided into individual sentences, each sentence within the answer contains text and is grounded on retrieved documents provided as references.",
                    "score": 0.3832814172581209,
                    "section_title": "OUR FRAMEWORK",
                    "char_start_offset": 5759,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 236
                        },
                        {
                            "start": 236,
                            "end": 358
                        },
                        {
                            "start": 358,
                            "end": 411
                        },
                        {
                            "start": 411,
                            "end": 496
                        },
                        {
                            "start": 498,
                            "end": 515
                        },
                        {
                            "start": 515,
                            "end": 589
                        },
                        {
                            "start": 589,
                            "end": 710
                        },
                        {
                            "start": 710,
                            "end": 925
                        },
                        {
                            "start": 927,
                            "end": 955
                        },
                        {
                            "start": 955,
                            "end": 1197
                        },
                        {
                            "start": 1197,
                            "end": 1363
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 655,
                            "end": 659,
                            "matchedPaperCorpusId": "207178704"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7353515625
                }
            ],
            "relevance_judgement": 0.7353515625,
            "relevance_judgment_input_expanded": "# Title: Ragnar\u00f6k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track\n# Venue: European Conference on Information Retrieval\n# Authors: Ronak Pradeep, Nandan Thakur, Sahel Sharifymoghaddam, Eric Zhang, Ryan Nguyen, Daniel Campos, Nick Craswell, Jimmy Lin\n## Abstract\nDid you try out the new Bing Search? Or maybe you fiddled around with Google AI~Overviews? These might sound familiar because the modern-day search stack has recently evolved to include retrieval-augmented generation (RAG) systems. They allow searching and incorporating real-time data into large language models (LLMs) to provide a well-informed, attributed, concise summary in contrast to the traditional search paradigm that relies on displaying a ranked list of documents. Therefore, given these recent advancements, it is crucial to have an arena to build, test, visualize, and systematically evaluate RAG-based search systems. With this in mind, we propose the TREC 2024 RAG Track to foster innovation in evaluating RAG systems. In our work, we lay out the steps we've made towards making this track a reality -- we describe the details of our reusable framework, Ragnar\\\"ok, explain the curation of the new MS MARCO V2.1 collection choice, release the development topics for the track, and standardize the I/O definitions which assist the end user. Next, using Ragnar\\\"ok, we identify and provide key industrial baselines such as OpenAI's GPT-4o or Cohere's Command R+. Further, we introduce a web-based user interface for an interactive arena allowing benchmarking pairwise RAG systems by crowdsourcing. We open-source our Ragnar\\\"ok framework and baselines to achieve a unified standard for future RAG systems.\n## OUR FRAMEWORK\nRagnar\u00f6k is an open-source, reproducible, and reusable framework implementing an end-to-end retrieval-augmented generation (RAG) pipeline, comprising two modules applied sequentially: (1) (R) retrieval and (2) (AG) augmented generation.Through the Ragnar\u00f6k framework, we will provide several baselines to all participants in the upcoming TREC 2024 RAG track.An overview of the framework is provided in Figure 1.We first describe both modules and expand on the I/O specifications in our framework.\n\nRetrieval Module.This module retrieves the relevant segments for a user topic as the input.It supports (i) first-stage lexical retrieval models such as BM25 [49] and (ii) reranking models such as RankZephyr [47].The retrieval system searches for relevant segments in the document collection and retrieves the top-100 segments further reranked by the reranker model to filter out the top-20 relevant segments for the next stage.\n\nAugmented Generation Module.This module takes in the user topic and the top-20 retrieved segments (from the retrieval module) as the input and a prompting strategy to the large language model (LLM) to generate the answer response with in-context citations for the topic.The answer response is divided into individual sentences, each sentence within the answer contains text and is grounded on retrieved documents provided as references.",
            "reference_string": "[270702738 | Pradeep et al. | 2024 | Citations: 16]"
        },
        {
            "title": "Dynamic Q&A of Clinical Documents with Large Language Models",
            "venue": "",
            "year": 2024,
            "reference_count": 48,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.10733, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280063225",
                    "name": "Ran Elgedawy"
                },
                {
                    "authorId": "2274464131",
                    "name": "Ioana Danciu"
                },
                {
                    "authorId": "1387927897",
                    "name": "Maria Mahbub"
                },
                {
                    "authorId": "2149506151",
                    "name": "Sudarshan Srinivasan"
                }
            ],
            "abstract": "Electronic health records (EHRs) house crucial patient data in clinical notes. As these notes grow in volume and complexity, manual extraction becomes challenging. This work introduces a natural language interface using large language models (LLMs) for dynamic question-answering on clinical notes. Our chatbot, powered by Langchain and transformer-based LLMs, allows users to query in natural language, receiving relevant answers from clinical notes. Experiments, utilizing various embedding models and advanced LLMs, show Wizard Vicuna's superior accuracy, albeit with high compute demands. Model optimization, including weight quantization, improves latency by approximately 48 times. Promising results indicate potential, yet challenges such as model hallucinations and limited diverse medical case evaluations remain. Addressing these gaps is crucial for unlocking the value in clinical notes and advancing AI-driven clinical decision-making.",
            "corpus_id": 267061013,
            "sentences": [
                {
                    "corpus_id": "267061013",
                    "title": "Dynamic Q&A of Clinical Documents with Large Language Models",
                    "text": "Probabilistic models [5] rank results based on keyword occurrences, which can be effective for certain tasks, such as information retrieval in search engines [45], spam email detection [32], and sentiment analysis in text data [9], but they struggle to grasp the underlying meaning of the text. TF-IDF [41] is another widely used method that assigns weights to keywords in queries and documents based on their rarity. This approach helps identify important terms, but it still falls short in capturing the full semantic context and relationships between words. The shared limitations across these approaches are the lack of semantic understanding and the ability to interpret the context holistically. \n\nAs a result, these traditional techniques often miss relevant information when applied to complex natural language text. They fail to recognize synonyms, antonyms, word variations, context, and other language intricacies that are essential for accurate and comprehensive information retrieval. \n\nLarge language models (LLMs) based on the transformer architecture [49] can be used to overcome these limitations and unlock the deeper meaning. The rapidly advancing LLMs, like , have revolutionized the field of natural language processing, with their contextual understanding, generative capabilities, and fine-tuning adaptability [24,57]. \n\nIntegrating LLMs into information retrieval systems holds the potential to deliver more accurate, insightful, and context-aware results, transforming the way users access information in a more personalized and efficient manner. \n\nEmbedding models (EMs) serve as powerful tools in natural language processing by representing words or phrases in a continuous vector space where the geometric distances between vectors capture semantic relationships between words. EMs, such as word embeddings [4], play a crucial role in capturing intricate relationships and contextual nuances within the text. \n\nRetrieval augmented generation (RAG) refers to a novel technique where a model first retrieves relevant information from a large corpus or dataset and then generates responses or outputs based on the retrieved information. This approach combines the benefits of both retrieval-based and generation-based methods, allowing for more contextually relevant and informative responses in conversational systems. Recent research works are exploring the trade-offs between this method and traditional fine-tuning approaches for language models, with some favoring RAG for certain contexts [7,23].",
                    "score": 0.3721991141151649,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 2187,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 294
                        },
                        {
                            "start": 295,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 560
                        },
                        {
                            "start": 561,
                            "end": 701
                        },
                        {
                            "start": 704,
                            "end": 824
                        },
                        {
                            "start": 825,
                            "end": 997
                        },
                        {
                            "start": 1000,
                            "end": 1144
                        },
                        {
                            "start": 1145,
                            "end": 1341
                        },
                        {
                            "start": 1344,
                            "end": 1571
                        },
                        {
                            "start": 1574,
                            "end": 1805
                        },
                        {
                            "start": 1806,
                            "end": 1936
                        },
                        {
                            "start": 1939,
                            "end": 2161
                        },
                        {
                            "start": 2162,
                            "end": 2344
                        },
                        {
                            "start": 2345,
                            "end": 2527
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 21,
                            "end": 24,
                            "matchedPaperCorpusId": "7446821"
                        },
                        {
                            "start": 158,
                            "end": 162,
                            "matchedPaperCorpusId": "15993277"
                        },
                        {
                            "start": 185,
                            "end": 189,
                            "matchedPaperCorpusId": "207191509"
                        },
                        {
                            "start": 227,
                            "end": 230,
                            "matchedPaperCorpusId": "33573440"
                        },
                        {
                            "start": 1067,
                            "end": 1071,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.728515625
                }
            ],
            "relevance_judgement": 0.728515625,
            "relevance_judgment_input_expanded": "# Title: Dynamic Q&A of Clinical Documents with Large Language Models\n# Venue: \n# Authors: Ran Elgedawy, Ioana Danciu, Maria Mahbub, Sudarshan Srinivasan\n## Abstract\nElectronic health records (EHRs) house crucial patient data in clinical notes. As these notes grow in volume and complexity, manual extraction becomes challenging. This work introduces a natural language interface using large language models (LLMs) for dynamic question-answering on clinical notes. Our chatbot, powered by Langchain and transformer-based LLMs, allows users to query in natural language, receiving relevant answers from clinical notes. Experiments, utilizing various embedding models and advanced LLMs, show Wizard Vicuna's superior accuracy, albeit with high compute demands. Model optimization, including weight quantization, improves latency by approximately 48 times. Promising results indicate potential, yet challenges such as model hallucinations and limited diverse medical case evaluations remain. Addressing these gaps is crucial for unlocking the value in clinical notes and advancing AI-driven clinical decision-making.\n## INTRODUCTION\nProbabilistic models [5] rank results based on keyword occurrences, which can be effective for certain tasks, such as information retrieval in search engines [45], spam email detection [32], and sentiment analysis in text data [9], but they struggle to grasp the underlying meaning of the text. TF-IDF [41] is another widely used method that assigns weights to keywords in queries and documents based on their rarity. This approach helps identify important terms, but it still falls short in capturing the full semantic context and relationships between words. The shared limitations across these approaches are the lack of semantic understanding and the ability to interpret the context holistically. \n\nAs a result, these traditional techniques often miss relevant information when applied to complex natural language text. They fail to recognize synonyms, antonyms, word variations, context, and other language intricacies that are essential for accurate and comprehensive information retrieval. \n\nLarge language models (LLMs) based on the transformer architecture [49] can be used to overcome these limitations and unlock the deeper meaning. The rapidly advancing LLMs, like , have revolutionized the field of natural language processing, with their contextual understanding, generative capabilities, and fine-tuning adaptability [24,57]. \n\nIntegrating LLMs into information retrieval systems holds the potential to deliver more accurate, insightful, and context-aware results, transforming the way users access information in a more personalized and efficient manner. \n\nEmbedding models (EMs) serve as powerful tools in natural language processing by representing words or phrases in a continuous vector space where the geometric distances between vectors capture semantic relationships between words. EMs, such as word embeddings [4], play a crucial role in capturing intricate relationships and contextual nuances within the text. \n\nRetrieval augmented generation (RAG) refers to a novel technique where a model first retrieves relevant information from a large corpus or dataset and then generates responses or outputs based on the retrieved information. This approach combines the benefits of both retrieval-based and generation-based methods, allowing for more contextually relevant and informative responses in conversational systems. Recent research works are exploring the trade-offs between this method and traditional fine-tuning approaches for language models, with some favoring RAG for certain contexts [7,23].",
            "reference_string": "[267061013 | Elgedawy et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Semantic Embeddings for Arabic Retrieval Augmented Generation (ARAG)",
            "venue": "International Journal of Advanced Computer Science and Applications",
            "year": 2023,
            "reference_count": 26,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://thesai.org/Downloads/Volume14No11/Paper_135-Semantic_Embeddings_for_Arabic_Retrieval_Augmented_Generation.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.14569/ijacsa.2023.01411135?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14569/ijacsa.2023.01411135, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2269250335",
                    "name": "Hazem Abdelazim"
                },
                {
                    "authorId": "2148715555",
                    "name": "Mohamed Tharwat"
                },
                {
                    "authorId": "2269771285",
                    "name": "Ammar Mohamed"
                }
            ],
            "abstract": "\u2014In recent times, Retrieval Augmented Generation (RAG) models have garnered considerable attention, primarily due to the impressive capabilities exhibited by Large Language Models (LLMs). Nevertheless, the Arabic language, despite its significance and widespread use, has received relatively less research emphasis in this field. A critical element within RAG systems is the Information Retrieval component, and at its core lies the vector embedding process commonly referred to as \u201csemantic embedding\u201d. This study encompasses an array of multilingual semantic embedding models, intending to enhance the model\u2019s ability to comprehend and generate Arabic text effectively. We conducted an extensive evaluation of the performance of ten cutting-edge Multilingual Semantic embedding models, employing a publicly available ARCD dataset as a benchmark and assessing their performance using the average Recall@k metric. The results showed that the Microsoft E5 sentence embedding model outperformed all other models on the ARCD dataset, with Recall@10 exceeding 90%",
            "corpus_id": 265594594,
            "sentences": [
                {
                    "corpus_id": "265594594",
                    "title": "Semantic Embeddings for Arabic Retrieval Augmented Generation (ARAG)",
                    "text": "Retrieval Augmented Generation (RAG), introduced by Facebook Researchers in 2020 [1], is a pivotal AI framework facilitating information retrieval for Generative AI models, thereby enhancing their accuracy and capabilities. RAG empowers Large Language Models (LLMs) by granting them access to external knowledge sources, augmenting the content generation process. This dual functionality entails retrieval, wherein RAG meticulously selects pertinent information from provided sources and generation, whereby LLMs craft contextually relevant responses based on user input. \n\nThe advantages of RAG are multi-fold. Firstly, it bolsters the performance by grounding LLMs with factual, up-todate information from external knowledge repositories. Furthermore, RAG maintains contextual relevance in responses, contributing to a more engaging user experience in conversational AI applications. Its scalability is noteworthy, as RAG models seamlessly handle copious volumes of information, proving invaluable for data-intensive tasks. Additionally, the adaptability of RAG models allows fine-tuning for specific applications [2], rendering them versatile across diverse data and use cases. Customizability is another hallmark, permitting RAG models to specialize in particular domains or subjects through customization and fine-tuning on specific knowledge bases. Due to the importance of such a framework for enterprises, extensive research is currently being pursued to discover new algorithms and techniques to enhance the performance of such models bounded by the context-window limitations of LLMs. Although there is ongoing research to expand the window size for LLM to be able to ingest more data in the prompt, the use of techniques like RAG is still of great practical importance, not only on homogeneous unstructured data but also on heterogeneous data [3]. \n\nIn principle, at the heart of the information retrieval module is the semantic embedding module which converts a piece of text, whether a query or a context text chunk to a numeric feature vector that embodies all semantic features of the text. The development of word and sentence embeddings is a relatively recent area of research in natural language processing (NLP) and information retrieval. \n\nMost of the semantic models are English language-centred; however, in recent years, Multilingual embedding models were released [4].",
                    "score": 0.3832814172581209,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 18,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 223
                        },
                        {
                            "start": 224,
                            "end": 363
                        },
                        {
                            "start": 364,
                            "end": 571
                        },
                        {
                            "start": 574,
                            "end": 611
                        },
                        {
                            "start": 612,
                            "end": 740
                        },
                        {
                            "start": 741,
                            "end": 885
                        },
                        {
                            "start": 886,
                            "end": 1025
                        },
                        {
                            "start": 1026,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1354
                        },
                        {
                            "start": 1355,
                            "end": 1594
                        },
                        {
                            "start": 1595,
                            "end": 1858
                        },
                        {
                            "start": 1861,
                            "end": 2105
                        },
                        {
                            "start": 2106,
                            "end": 2257
                        },
                        {
                            "start": 2260,
                            "end": 2392
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.720703125
                }
            ],
            "relevance_judgement": 0.720703125,
            "relevance_judgment_input_expanded": "# Title: Semantic Embeddings for Arabic Retrieval Augmented Generation (ARAG)\n# Venue: International Journal of Advanced Computer Science and Applications\n# Authors: Hazem Abdelazim, Mohamed Tharwat, Ammar Mohamed\n## Abstract\n\u2014In recent times, Retrieval Augmented Generation (RAG) models have garnered considerable attention, primarily due to the impressive capabilities exhibited by Large Language Models (LLMs). Nevertheless, the Arabic language, despite its significance and widespread use, has received relatively less research emphasis in this field. A critical element within RAG systems is the Information Retrieval component, and at its core lies the vector embedding process commonly referred to as \u201csemantic embedding\u201d. This study encompasses an array of multilingual semantic embedding models, intending to enhance the model\u2019s ability to comprehend and generate Arabic text effectively. We conducted an extensive evaluation of the performance of ten cutting-edge Multilingual Semantic embedding models, employing a publicly available ARCD dataset as a benchmark and assessing their performance using the average Recall@k metric. The results showed that the Microsoft E5 sentence embedding model outperformed all other models on the ARCD dataset, with Recall@10 exceeding 90%\n## I. INTRODUCTION\nRetrieval Augmented Generation (RAG), introduced by Facebook Researchers in 2020 [1], is a pivotal AI framework facilitating information retrieval for Generative AI models, thereby enhancing their accuracy and capabilities. RAG empowers Large Language Models (LLMs) by granting them access to external knowledge sources, augmenting the content generation process. This dual functionality entails retrieval, wherein RAG meticulously selects pertinent information from provided sources and generation, whereby LLMs craft contextually relevant responses based on user input. \n\nThe advantages of RAG are multi-fold. Firstly, it bolsters the performance by grounding LLMs with factual, up-todate information from external knowledge repositories. Furthermore, RAG maintains contextual relevance in responses, contributing to a more engaging user experience in conversational AI applications. Its scalability is noteworthy, as RAG models seamlessly handle copious volumes of information, proving invaluable for data-intensive tasks. Additionally, the adaptability of RAG models allows fine-tuning for specific applications [2], rendering them versatile across diverse data and use cases. Customizability is another hallmark, permitting RAG models to specialize in particular domains or subjects through customization and fine-tuning on specific knowledge bases. Due to the importance of such a framework for enterprises, extensive research is currently being pursued to discover new algorithms and techniques to enhance the performance of such models bounded by the context-window limitations of LLMs. Although there is ongoing research to expand the window size for LLM to be able to ingest more data in the prompt, the use of techniques like RAG is still of great practical importance, not only on homogeneous unstructured data but also on heterogeneous data [3]. \n\nIn principle, at the heart of the information retrieval module is the semantic embedding module which converts a piece of text, whether a query or a context text chunk to a numeric feature vector that embodies all semantic features of the text. The development of word and sentence embeddings is a relatively recent area of research in natural language processing (NLP) and information retrieval. \n\nMost of the semantic models are English language-centred; however, in recent years, Multilingual embedding models were released [4].",
            "reference_string": "[265594594 | Abdelazim et al. | 2023 | Citations: 8]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "273850019",
            "title": "Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation RAG aims to enhance language models by retrieving and integrating relevant information from external knowledge sources, demonstrating significant improvements in handling knowledge-intensive tasks (Lewis et al. 2020;Guu et al. 2020). RAG models excel in various NLP applications, like question answering (Izacard and Grave 2021; Ram et al. 2023) and summarization (Lin et al. 2023). Several retrieval models have been developed to support RAG frameworks. BM25 (Robertson, Zaragoza et al. 2009), a sparse retrieval model, has been a foundational method for text retrieval tasks. DPR (Karpukhin et al. 2020a) indexes passages into a dense vector space for efficient retrieval and has been widely adopted in subsequent RAG models. Other notable dense retrieval models include Contriever (Izacard et al. 2021), SBERT (Reimers and Gurevych 2019a), and BGE (Xiao and Liu 2023), each contributing to the robustness and effectiveness of retrieval-augmented systems. \n\nRecent advancements in retrieval strategies have optimized the interaction between retrievers and language models. Models like Atlas (Izacard et al. 2023) and RETRO (Borgeaud et al. 2022) employ joint training and architectural modifications to better integrate retrieved information, though these methods are resource-intensive. \n\nLLM-Supervised Learning LLMs have demonstrated remarkable capabilities in natural language understanding and generation (Brown et al. 2020;Chowdhery et al. 2022). Leveraging the knowledge and capabilities of LLMs to guide the training of other models or themselves has recently emerged as a promising direction. Reinforcement Learning from AI Feedback (RLAIF) (Lee et al. 2023) proposes using an LLM to generate preference labels to train a reward model, which then guides the reinforcement learning process, achieving performance comparable to traditional human feedback-based approaches. \n\nIn addition, Wang et al. (2022) propose an LLMbootstrapping approach called self-instruct, where an LLM iteratively generates additional training data for itself. Wang, Yang, and Wei (2023) introduce the LLM-R framework which trains dense retrievers to identify high-quality incontext examples using feedback from LLMs.",
            "score": 0.6077284618077503,
            "section_title": "Related Work",
            "char_start_offset": 5310,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 988
                },
                {
                    "start": 991,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1912
                },
                {
                    "start": 1915,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 247,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 360,
                    "end": 376,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 491,
                    "end": 524,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 844,
                    "end": 872,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1124,
                    "end": 1145,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1462,
                    "end": 1484,
                    "matchedPaperCorpusId": "144546246"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68798828125
        },
        {
            "corpus_id": "269982120",
            "title": "xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token",
            "text": "Retrieval-augmented Generation Equipping a parametric language model with a non-parametric datastore has proven effective for various NLP tasks, such as language modeling [36,54,81], opendomain question answering [24,41,66,49], machine translation [35,11] and others.Given the vast  design space of this generation paradigm, numerous approaches with different focuses have been proposed.For instance, RETRO [8] and PlugLM [12] feature architectural innovations for better integration with the non-parametric datastore.REALM [21] introduces an end-to-end methodology for joint optimization of the language model and the retriever.REPLUG [66] and RA-DIT [48] enhance the retriever using LLM feedback to better align these two components.DSP [37] and InteR [17] explore sophisticated interactions between the retriever and the language model.Selfmem [13] employs a reward model to iteratively refine both retrieval and generation processes.Self-RAG [4] introduces a self-reflection mechanism that significantly enhances the quality and factuality of language models.For a more comprehensive review, refer to [18,5,3].Our work, xRAG, distinguishes itself from previous studies by adopting a modality fusion approach to the problem of retrieval augmentation, resulting in a both effective and efficient RAG system.",
            "score": 0.60495715958451,
            "section_title": "Related Work",
            "char_start_offset": 3884,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 267,
                    "end": 387
                },
                {
                    "start": 387,
                    "end": 518
                },
                {
                    "start": 518,
                    "end": 629
                },
                {
                    "start": 629,
                    "end": 735
                },
                {
                    "start": 735,
                    "end": 839
                },
                {
                    "start": 839,
                    "end": 937
                },
                {
                    "start": 937,
                    "end": 1063
                },
                {
                    "start": 1063,
                    "end": 1114
                },
                {
                    "start": 1114,
                    "end": 1309
                }
            ],
            "ref_mentions": [
                {
                    "start": 1111,
                    "end": 1113,
                    "matchedPaperCorpusId": "263866951"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6669921875
        },
        {
            "corpus_id": "273286031",
            "title": "Towards a Framework for AI Applications in Intralogistics",
            "text": "Large language models (LLMs) have exploded in popularity with the advent of Generative Pre-trained Transformer (GPT) coined by OpenAI and distributed by them through their chatbot named ChatGPT. Min et al. (2023) present the concepts of LLM architectures while their survey covers LLM techniques for NLP through training, fine-tuning, prompting, and text generation. Retrieval augmented generation (RAG) has been proposed for knowledge-intensive tasks (Lewis et al., 2020). While this is still an emerging area of research, we expect in an intralogistics context that domain knowledge capture and retrieval would use this approach.",
            "score": 0.5589831673117679,
            "section_title": "LLM & RAG",
            "char_start_offset": 40276,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 631
                }
            ],
            "ref_mentions": [
                {
                    "start": 195,
                    "end": 212,
                    "matchedPaperCorpusId": "240420063"
                },
                {
                    "start": 452,
                    "end": 472,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.460693359375
        },
        {
            "corpus_id": "268032392",
            "title": "Enhancing Retrieval Processes for Language Generation with Augmented Queries",
            "text": "Natural Language Processing (NLP) has witnessed significant advancements with the emergence of LLMs. Despite their prowess in downstream tasks, these models face challenges in accessing and manipulating knowledge, leading to suboptimal performance on knowledgeintensive tasks. A groundbreaking approach to address these limitations is Retrieval-Augmented Generation (RAG). In Lewis et al.'s seminal work (2020) [2], the authors introduce RAG models that combine pre-trained parametric and non-parametric memory for language generation. By leveraging a pre-trained seq2seq model as parametric memory and a dense vector index of Wikipedia as non-parametric memory, RAG achieves superior performance on a spectrum of knowledge-intensive NLP tasks. It outshines parametric seq2seq models and task-specific architectures, setting new benchmarks in open-domain question answering (QA) tasks. Furthermore, RAG models exhibit enhanced language generation capabilities, producing more specific, diverse, and factual language compared to state-of-the-art parametric-only seq2seq baselines. \n\nIn the realm of information retrieval (IR), recent research has made remarkable strides. Hambarde and Proenca (2023) [7] provide an extensive overview of IR models, discussing the state-of-the-art methods, including those based on terms, semantic retrieval, and neural approaches. Karpukhin et al. (2020) [8] revolutionize open-domain QA by introducing dense passage retrieval, demonstrating its practical implementation using only dense representations. This approach outperforms traditional sparse vector space models, leading to state-of-the-art performance on multiple open-domain QA benchmarks. Qu et al. (2020) [9] further optimize dense passage retrieval with RocketQA, addressing challenges such as training-inference discrepancy and limited training data. RocketQA significantly outperforms previous state-of-theart models on well-established datasets, showcasing its effectiveness in improving end-to-end QA systems. Advancements in similarity search, as demonstrated by Johnson et al.'s (2019) [10], their work, optimizing k-selection and addressing memory hierarchy challenges, sets new benchmarks in various similarity search scenarios.",
            "score": 0.556512978173041,
            "section_title": "Related works",
            "char_start_offset": 2269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1079
                },
                {
                    "start": 1082,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2231
                }
            ],
            "ref_mentions": [
                {
                    "start": 411,
                    "end": 414,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75830078125
        },
        {
            "corpus_id": "266741975",
            "title": "Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book Question Answering",
            "text": "Language models have demonstrated remarkable performances in a wide range of Natural Language Processing (NLP) tasks, including conversational agents, summarization, translation, and questionanswering (Brown et al., 2020;Bubeck et al., 2023;Wei et al., 2022). As scaling these models increases their ability to incorporate more and more knowledge (Brown et al., 2020;Chowdhery et al., 2022), instead of relying on traditional search engines, Metzler et al. (Metzler et al., 2021) suggest using LLM as a unified knowledge base able to perform question answering as well as document retrieval. However, even the larger models (Brown et al., 2020) are prone to producing inaccurate or false responses, commonly known as hallucinations (Ji et al., 2023). These have been extensively explored in various NLP tasks, including summarization and translation (Yuan et al., 2021;Guerreiro et al., 2023;Manakul et al., 2023;Ji et al., 2023;Lee et al., 2021). Numerous approaches have been suggested to tackle this problem, with all of them employing external techniques to detect and mitigate hallucinations. In question answering, retrieval augmented methods such as REALM (Guu et al., 2020), RAG (Lewis et al., 2020), or RETRO (Borgeaud et al., 2022;Lee et al., 2019a), were proposed to reduce LLM's hallucinations. These approaches consist in grounding LLM with a retriever model to add context from a large corpus of documents and to generate answers. These architectures are effective as they both improve factualness and reduce hallucinations for specific knowledge-intensive tasks such as Open-domain Question Answering (Lee et al., 2019a). However, retrieved documents are always considered without consideration of their helpfulness in solving the task.",
            "score": 0.5444276106733468,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1751
                }
            ],
            "ref_mentions": [
                {
                    "start": 732,
                    "end": 749,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 850,
                    "end": 869,
                    "matchedPaperCorpusId": "235593404"
                },
                {
                    "start": 869,
                    "end": 892,
                    "matchedPaperCorpusId": "251468136"
                },
                {
                    "start": 913,
                    "end": 929,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 929,
                    "end": 946,
                    "matchedPaperCorpusId": "232258000"
                },
                {
                    "start": 1163,
                    "end": 1181,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1187,
                    "end": 1207,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1218,
                    "end": 1241,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1241,
                    "end": 1259,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1616,
                    "end": 1635,
                    "matchedPaperCorpusId": "173990818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52001953125
        },
        {
            "corpus_id": "273962778",
            "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
            "text": "Information Retrieval: Advancements in deep learning have revolutionized information retrieval systems, enhancing their personalization and accuracy in retrieving relevant documents. Early information retrieval frameworks employed sparse retrievers [6] or dense retrievers [4,8] to represent large corpora but struggled to capture deep semantic relationships [13]. LLM-based retrievers (generative retrieval) have since emerged as notable methods, leveraging the rich prior knowledge of LLMs to significantly improve performance by converting documents into parametric knowledge and generating them instead of computing similarity scores [38]. However, the frequent encoding and decoding processes in LLMs severely hinder efficiency [38]. To address the trade-off between effectiveness and efficiency, we propose invar-retrieval in our architecture, enabling the model to efficiently retrieve the most relevant documents without introducing variance. \n\nRetrieval-augmented Language Model: Currently, retrieval-augmented language models have proven effective in answering questions by leveraging external information through the integration of novel retrievers and LLMs [38]. However, the architectural gap between retrieval and generation continues to hinder unified optimization across the entire retrieval-augmented generation system [2]. To address the isolation between retrieval and generation, a novel architecture called RA-DIT was introduced [18]. By aligning retriever scoring with LSR scoring [27], it has been shown to deliver state-of-the-art performance across various tasks. However, it still employs dense retrievers like DRAGON+ [17] in the retrieval stage, which fails to eliminate the problem at its source and introduces inefficiencies throughout the process. Correspondingly, we introduce a representation learning method and invariance loss in our Invar-RAG architecture, which partially addresses these issues and explores a novel approach to using a single LLM for multiple roles within the RAG system.",
            "score": 0.5333996140338054,
            "section_title": "Related Work",
            "char_start_offset": 21968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 950
                },
                {
                    "start": 953,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 2025
                }
            ],
            "ref_mentions": [
                {
                    "start": 638,
                    "end": 642,
                    "matchedPaperCorpusId": "230433817"
                },
                {
                    "start": 733,
                    "end": 737,
                    "matchedPaperCorpusId": "230433817"
                },
                {
                    "start": 1169,
                    "end": 1173,
                    "matchedPaperCorpusId": "230433817"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.765625
        },
        {
            "corpus_id": "276558144",
            "title": "Cross-Format Retrieval-Augmented Generation in XR with LLMs for Context-Aware Maintenance Assistance",
            "text": "LLMs [1] represent a significant leap forward in the field of artificial intelligence (AI) and natural language processing (NLP) [2], demonstrating unprecedented capabilities in understanding, generating, and manipulating human language. These models, built on deep learning architectures, typically leverage massive datasets and extensive computational resources to capture complex linguistic patterns, allowing them to perform a wide range of tasks, such as text generation, translation, summarization, and question-answering, with remarkable accuracy. LLMs like OpenAI's GPT series [3] and Meta's Llama models [4], among others, have set new benchmarks in AI research, pushing the boundaries of what machines can achieve in understanding and interacting with natural language. \n\nAs LLMs continue to advance, they are also reshaping the landscape of human-computer interaction. These models have showcased a variety of use-cases and increasingly being integrated into interactive systems where they engage in conversational AI, assistive technologies, and creative collaborations with humans. Their ability to process and generate natural language makes them pivotal in the development of new forms of interaction for intelligent virtual assistants. \n\nIn recent years, Retrieval-Augmented Generation (RAG) systems have emerged as a transformative approach within the AI landscape, particularly in enhancing the capabilities of LLMs [5]. By effectively combining retrieval mechanisms arXiv:2502.15604v1 [cs.IR] 21 Feb 2025 with generative models, RAG architectures enable the integration of factual information from various sources [6] to produce contextually relevant responses. This capability is particularly advantageous in domains requiring complex decision-making and information synthesis, such as maintenance tasks. In our proposed system, the RAG framework facilitates multi-format data processing, allowing for efficient retrieval from diverse file types, including text documents, CSV files, and databases. This dual approach improves response accuracy by grounding model outputs in realworld information while significantly accelerates the generation process. Thus, our research aims to evaluate and demonstrate the practical implications of RAG systems in optimizing maintenance workflows, and providing timely and comprehensive support to personnel operating in dynamic environments. \n\nThe evaluated architecture proposes the following contributions: \n\n\u2022 Cross-Format Functionality: The system can process and retrieve data from various file types, such as plain text, PDFs, CSVs, and databases.",
            "score": 0.5303345670641648,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 779
                },
                {
                    "start": 782,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2398
                },
                {
                    "start": 2401,
                    "end": 2465
                },
                {
                    "start": 2468,
                    "end": 2610
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6162109375
        },
        {
            "corpus_id": "271709366",
            "title": "SAKR: Enhancing Retrieval-Augmented Generation via Streaming Algorithm and K-Means Clustering",
            "text": "The evolution of RAG and its problems RAG was first proposed in the article \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" [2]. This article illustrated the limited ability of LLMs to access and precisely manipulate knowledge, so LLMs' performance on knowledge-intensive tasks lags task-specific architecture. Plus, pre-trained LLMs cannot easily expand or revise their memory, and they encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes [1]. So, this information retrieval mechanism, Retrieval-augmented generation, applies to LLMs to solve the problems that pre-trained LLMs face. This model, Naive RAG, builds an external knowledge database and retrieves relevant documents to help LLMs. However, it has some drawbacks, such as low query precision, missing important information, etc. \n\nThe emergence of Advanced RAG was aimed at improving its shortcomings [3]. Advanced RAG enhances its indexing techniques by employing a sliding window approach, fine-grained segmentation, and metadata integration. Furthermore, it integrates multiple optimization methods to streamline the retrieval process. Each community will generate a community answer and then incorporate it into a global answer. They improved the RAG's performance in terms of the comprehensiveness and diversity of answers. \n\nMicrosoft found that Naive RAG is not good at extracting global answers and fails on query-focused summarization tasks (QFS). So, they proposed an approach that builds the knowledge graph based on the extracted entities and relationships in the sentence and then divides the graph into communities using the Leiden algorithm [4]. Each community will generate and incorporate a community answer into a global answer. They improved the RAG's performance in terms of the comprehensiveness and diversity of answers. Comprehensive surveys have summarized these advancescovering retriever-generator coupling, pipeline segmentation, multimodal extensions [35]-and, from a pre-/post-retrieval perspective and outlined evaluation methods [36]. Inspired by previous research, we realized that the proposal of traditional RAG is significant, but there are still a lot of things to improve. What we want to do is to save the huge memory cost and the huge amount of time spent on each query of relevant documents.",
            "score": 0.5276823434413447,
            "section_title": "2.1",
            "char_start_offset": 3142,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 870
                },
                {
                    "start": 873,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1370
                },
                {
                    "start": 1373,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2107
                },
                {
                    "start": 2108,
                    "end": 2251
                },
                {
                    "start": 2252,
                    "end": 2373
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64453125
        },
        {
            "corpus_id": "273532207",
            "title": "Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination",
            "text": "Retrieval-Augmented Generation (Lewis et al. 2020c) models have emerged as a promising solution to address the knowledge-intensive nature of NLP tasks (Yang et al. 2024) (Hei et al. 2024). Unlike the two-stage pipeline of traditional ODQA systems, Retrieval-Augmented Generation models utilized the neural-retrieval-in-the-loop technique with Dense Passage Retrieval (Karpukhin et al. 2020) and BART (Lewis et al. 2020a) sequence-to-sequence language model (Lewis et al. 2020c) (Shuster et al. 2021). This method integrates a neural network architecture with transformers (Vaswani et al. 2017) as backbone consisting of an encoder-decoder framework. In this system, the encoder processes the input query and the retriever converts it into a dense vector representation followed by employing maximum inner product search to identify and retrieve the most pertinent passages from an extensive corpus. These retrieved passages are then concatenated with the input query and fed into the decoder to generates final response. The entire system is typically optimized through end-to-end training, enhancing its capability to produce accurate and contextually relevant responses (Lewis et al. 2020c). \n\nRetrieval Augmented Architectures have gained significant attention for their scalable and interpretable design (Siriwardhana et al. 2023). Recent studies have significantly enhanced the capabilities of RAG models within dialoguebased systems (Rackauckas 2024) (Kang et al. 2024). It uniquely integrates retrieval and generation and merged in a single architecture, leveraging both a pre-trained BART generator and dense vector representations of Wikipedia articles indexed with the FAISS (Johnson et al. 2019) library capable of scaling to knowledge sources containing millions of documents. However, existing research on RAG models has focused only on open-domain tasks with Wikipedia-based datasets and there is limited understanding of their effectiveness in domain-specific applications, particularly in customer service area. Therefore, in this paper we aim to investigate the performance of different RAG and RAG-like architectures by leveraging domain adaptation techniques, with a focus on evaluating their ability to generate accurate and contextually appropriate responses in specialized domains.",
            "score": 0.5261136886616791,
            "section_title": "Retrieval Augmented Architecture",
            "char_start_offset": 11263,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2303
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 169,
                    "matchedPaperCorpusId": "1373518"
                },
                {
                    "start": 476,
                    "end": 499,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 1308,
                    "end": 1334,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 1685,
                    "end": 1705,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66845703125
        },
        {
            "corpus_id": "267547568",
            "title": "Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs",
            "text": "The incorporation of large language models (LLMs) like GPT-4 into information retrieval represents a significant progress in the field. Retrieval Augmented Generation (RAG) is a crucial aspect of this integration, as it combines the capabilities of LLMs with dynamic information retrieval to improve the precision and pertinence of the generated responses. \n\nThe evolution from GPT to GPT-4 exemplifies major strides in natural language processing (NLP). These LLMs are built on sophisticated deep learning architectures, enabling them to comprehend and generate human-like text, significantly enriching user interaction with information retrieval systems [Brown et al., 2020]. \n\nRAG represents a novel approach in information retrieval, wherein the LLM not only generates responses but also retrieves and incorporates relevant external information in real-time. This mechanism allows the model to dynamically pull in data from a wide range of sources, ensuring that the answers provided are both current and contextually rich [Lewis et al., 2020]. RAG essentially combines the generative capabilities of models like GPT-4 with the information retrieval prowess of traditional search engines, leading to more accurate, informative, and up-to-date responses. \n\nIn practice, services such as Perplexity.ai [Per, 2023] and Bing AI Search [Bin, 2023] utilize LLMs enhanced with RAG to deliver a more sophisticated search experience. By leveraging RAG, these platforms are able to understand complex queries and generate responses that are not only contextually aware but also infused with the most relevant and recent information available across the web. This approach significantly surpasses traditional search methodologies, providing users with a comprehensive, concise, and highly informative answer [Fostikov, 2023]. \n\nCase studies focusing on the implementation of RAG in search technologies reveal a substantial improvement in user satisfaction. Users benefit from responses that are not only semantically aligned with their queries but also enriched with the latest information, offering a depth of understanding previously unattainable in standard search engines [Gao et al., 2023]. \n\nThe combination of LLMs, especially when combined with Retrieval Augmented Generation, signifies a significant transformation in the field of information retrieval.",
            "score": 0.5130762101937387,
            "section_title": "Large Language Models in Information Retrieval",
            "char_start_offset": 9096,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 356
                },
                {
                    "start": 359,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 677
                },
                {
                    "start": 680,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1257
                },
                {
                    "start": 1260,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1818
                },
                {
                    "start": 1821,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2188
                },
                {
                    "start": 2191,
                    "end": 2355
                }
            ],
            "ref_mentions": [
                {
                    "start": 656,
                    "end": 676,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71728515625
        },
        {
            "corpus_id": "268032903",
            "title": "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)",
            "text": "In our research, we concentrated primarily on the application of retrieval augmentation during the inference stage, without delving into its integration during pre-training or fine-tuning phases. Future work will aim to explore these compelling areas. Moreover, while our study has highlighted the privacy risks associated with commonly employed retrieval-augmented generation (RAG) systems, other retrieval-based language models (LMs) feature distinct components and architectures (Huang et al., 2023;Borgeaud et al., 2022) that warrant further investigation. In addition, developing effective strategies to protect retrieval data and leveraging RAG systems for the safeguarding of training data represent open research questions that we intend to pursue.",
            "score": 0.5130704803851458,
            "section_title": "Limitations",
            "char_start_offset": 29021,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 756
                }
            ],
            "ref_mentions": [
                {
                    "start": 502,
                    "end": 524,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5205078125
        },
        {
            "corpus_id": "252735056",
            "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering",
            "text": "Recently, Retrieval Augmented Architectures (Lewis et al., 2020b;Guu et al., 2020) have drawn a lot of attention due to their explainable, scalable, and adaptable nature. Unlike other open-domain QA architectures, RAG (Lewis et al., 2020b) combines the information retrieval stage and answer generation stage in a differentiable manner. It uses a combination of parametric and non-parametric memory, where the parametric memory consists of a pre-trained seq2seq BART (Lewis et al., 2019) generator, and the non-parametric memory consists of dense vector representations of Wikipedia articles indexed with the FAISS library (Johnson et al., 2017). RAG first encodes a question into a dense representation, retrieves the relevant passages from an indexed Wikipedia knowledge base, and then feeds them into the generator. The loss function can finetune both the generator and the question encoder at the same time. Lewis et al. (Lewis et al., 2020b) highlight RAG's ability to perform well in Wikipedia-based general question-answering datasets like Natural Questions (Kwiatkowski et al., 2019). Other recent work also highlights how the outputs generated from RAG models are much more factual due to RAG being conditioned on the retrieved documents, possibly providing an answer to the hallucination problem of generative language models. Shuster, Kurt, et al. (Shuster et al., 2021) also highlight how RAG reduces hallucinations in knowledge-grounded conversational tasks, where the task is to generate responses to dialogues based on a large Wikipedia knowledge base. Xu et al. (2021) illustrate the effectiveness of RAG in chat-bot frameworks and highlight how RAG models are able to recall and summarize conversations compared to standard seq2seq models with only parametric memory. This paper aims to understand how RAG could be extended to an end2end model and adapted to specific domains. To the best of our knowledge, this is the first time RAG is being investigated on domain adaptation for the task of ODQA systems. (Guu et al., 2020) is a similar Retrieval Augmented model to RAG. REALM introduced a novel masked language pre-training step that involves an end-to-end trainable retriever.",
            "score": 0.511998214682212,
            "section_title": "Retrieval Augmented Architecture",
            "char_start_offset": 6206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2197
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77880859375
        },
        {
            "corpus_id": "268510197",
            "title": "RAFT: Adapting Language Model to Domain Specific RAG",
            "text": "Retrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs) enhance LLMs by integrating a retrieval module that sources relevant information from external knowledge bases, significantly improving performance across various NLP tasks, including language modeling (Guu et al., 2020;Borgeaud et al., 2022;Khandelwal et al., 2019;Shi et al., 2023d;Lin et al., 2023b;Shi et al., 2023c;Asai et al., 2023;Xu et al., 2023;Wang et al., 2023) and open-domain question answering (Izacard et al., 2023;Lewis et al., 2020).For instance, Atlas (Izacard et al., 2023) fine-tunes T5 models with the retriever, treating documents as latent variables, while RETRO (Borgeaud et al., 2022) modifies the decoder-only architecture to include retrieved texts and conducts pre-training from scratch.kNN-LM (Khandelwal et al., 2019) interpolates between the LM's next token distribution and distributions computed from retrieved tokens at inference.(Shi et al., 2023d;Ram et al., 2023) assume black-box access to an LLM, combining it with either off-the-shelf or fine-tuned retriever.\n\nMemorization A key question around large neural language models is whether they truly \"understand\" text (Feldman, 2020;Power et al., 2022) or simply rely on surface pattern memorization (Carlini et al., 2019;T\u00e4nzer et al., 2022).(Feldman, 2020;Carlini et al., 2019;2022) develop methodologies to quantify the extent of memorization in neural models.(Brown et al., 2020;Power et al., 2022;Liu et al., 2022) further explored how memorization impacts the models' generalization capabilities.(Carlini et al., 2021;Shi et al., 2023b) demonstrated the ability of language models to memorize and regurgitate training data, raising significant privacy concerns (Kandpal et al., 2022;Pan et al., 2020).",
            "score": 0.5092968293390181,
            "section_title": "Related Works",
            "char_start_offset": 22734,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 530
                },
                {
                    "start": 530,
                    "end": 795
                },
                {
                    "start": 795,
                    "end": 944
                },
                {
                    "start": 944,
                    "end": 1079
                },
                {
                    "start": 1081,
                    "end": 1310
                },
                {
                    "start": 1310,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1569
                },
                {
                    "start": 1569,
                    "end": 1774
                }
            ],
            "ref_mentions": [
                {
                    "start": 282,
                    "end": 300,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 300,
                    "end": 322,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 434,
                    "end": 452,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 488,
                    "end": 510,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 510,
                    "end": 529,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 550,
                    "end": 572,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 666,
                    "end": 689,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1185,
                    "end": 1200,
                    "matchedPaperCorpusId": "186206616"
                },
                {
                    "start": 1267,
                    "end": 1289,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 1289,
                    "end": 1309,
                    "matchedPaperCorpusId": "247450508"
                },
                {
                    "start": 1310,
                    "end": 1325,
                    "matchedPaperCorpusId": "186206616"
                },
                {
                    "start": 1325,
                    "end": 1346,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 1430,
                    "end": 1450,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1469,
                    "end": 1486,
                    "matchedPaperCorpusId": "248965387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67431640625
        },
        {
            "corpus_id": "273532207",
            "title": "Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination",
            "text": "In this paper, we have explored potential of domain-adapted retrieval-augmented generation (RAG) models in enhancing the performance of QA task. This research presents a comprehensive analysis by investigating the effectiveness domain adaptation and it's impact reduction of hallucinations in the RAG-like Architectures. Our study introduces several novel contributions to the field by presentint Hotel-ConvQA, a detailed domain-specific dataset comprising a wide range of hotel-related conversations, which served as the foundation for evaluating the domain adaptation performance of various RAG and RAG-like architectures. Additionally, we conducted a detailed comparative analysis to understand their respective strengths and limitations in reducing hallucinations. While prior work has not explicitly determined the relationship between domain adaptation and hallucination reduction, our results emphasize that domain adaptation with adaptive retrieval mechanisms can significantly reduce hallucinations for all the RAG-based Architectures. \n\nBuilding on the insights gained from our paper, future research will aim to enrich the hotel domain dataset by incorporating a broader and more diverse array of scenarios, questions, and knowledge sources, facilitating more comprehensive evaluation of domain adaptation techniques. Subsequently, extending our analysis to include additional industry verticals such as healthcare, medicine etc would also provide valuable insights into the generalizability of the domain adaptation approaches explored in this work. Additionally, further research is needed by extending RAG as a component of conversational AI systems to evaluate their natural language understanding, dialogue management, and response generation capabilities. \n\nWe have made all our efforts to implement and test the effectiveness of the RETRO (Borgeaud et al. 2021) architecture on our hotel domain dataset using NVIDIA-Megatron and RETRO-pytorch frameworks; however, challenges related to system requirements and resource management impeded full integration. Therefore, future work could explore the comparison of RETRO with RAG-based models, enhancing our understanding of various retrieval-augmented strategies in handling domain-specific queries and reducing hallucinations. In the future, we aim to investigate the capabilities of RAG-based models in multi-hop question answering, extending their functionality beyond generating responses from a single document. \n\nWe believe that our study will serve as a practical framework for leveraging retrieval-augmented generation models in high-stakes customer service applications, paving the way for more accurate and data-driven virtual assistants.",
            "score": 0.5063863936848345,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 54339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 1044
                },
                {
                    "start": 1047,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1772
                },
                {
                    "start": 1775,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2292
                },
                {
                    "start": 2293,
                    "end": 2481
                },
                {
                    "start": 2484,
                    "end": 2713
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68701171875
        },
        {
            "corpus_id": "268248911",
            "title": "Reliable, Adaptable, and Attributable Language Models with Retrieval",
            "text": "Address These Issues? \n\nIn this section, we discuss how retrieval-augmented LMs can alleviate the aforementioned issues in parametric LMs. \n\nDefinition. A retrieval-augmented LM (Figure 1, bottom; detailed in Figure 2) typically consists of two key components: a retriever R and a parametric LM \u03b8. The retriever builds a search index I1 based on documents in the datastore D. During inference time, given an input sequence x, the retriever finds relevant text z 2 from the inference datastore, leveraging an index I: z = f R,I (x). Subsequently, the LM \u03b8 uses both the original prompt and the retrieved text to predict the output y: y = f \u03b8 (x, z). \n\nOrigins, progress, and recent shift. \n\nThe concept of retrieval augmentation has been extensively explored across various machine learning domains (Tian et al., 2019). In NLP, earlier efforts have been applied to specific tasks such as QA and machine translation. Chen et al. (2017) introduce DrQA, which combines a term-based information retrieval (IR) system with a neural QA model to answer knowledgeintensive questions. While IR and such task LMs were initially studied separately, several work explores more organic combinations of retrieval and LM by pre-training the two components jointly or sequentially, including REALM (Guu et al., 2020), RAG (Lewis et al., 2020a), RETRO (Borgeaud et al., 2022), etc. \n\nSuch earlier work designed special architectures and training objectives for the retrieval-augmented LM. Most recently, there has been a shift of view of retrieval-augmented LMsinstead of training retrieval-augmented LMs from scratch, some work supplementary integrate retrieval on top of existing powerful parametric LMs (e.g., GPT-3;Black et al. 2022) without any additional training. Such methods-often referred to simply as Retrieval-Augmented Generation or RAG-concatenate the original input sequence x with retrieved text z when prompting, yielding significant improvements over the base parametric LMs on certain knowledgeintensive tasks (Ram et al., 2023;Shi et al., 2023c).",
            "score": 0.5038035324767401,
            "section_title": "How Can Retrieval-Augmented LMs",
            "char_start_offset": 9288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 24,
                    "end": 138
                },
                {
                    "start": 141,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 648
                },
                {
                    "start": 651,
                    "end": 687
                },
                {
                    "start": 690,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 2048
                }
            ],
            "ref_mentions": [
                {
                    "start": 798,
                    "end": 817,
                    "matchedPaperCorpusId": "57759353"
                },
                {
                    "start": 915,
                    "end": 933,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 1281,
                    "end": 1299,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1334,
                    "end": 1357,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1701,
                    "end": 1719,
                    "matchedPaperCorpusId": "248177957"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8642578125
        },
        {
            "corpus_id": "277043297",
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) has gained significant attention in recent years for its potential to enhance natural language understanding and generation by combining large-scale retrieval systems with generative models. RAG leverages external knowledge sources, such as documents, databases, or structured data, to improve model performance and generate more accurate and contextually relevant outputs. This survey aims to provide a comprehensive overview of RAG by examining its fundamental components, including retrieval mechanisms, generation processes, and the integration between the two. We discuss the key characteristics of RAG, such as its ability to augment generative models with dynamic external knowledge, and the challenges associated with aligning retrieved information with generative objectives. We also present a taxonomy that categorizes RAG methods, ranging from basic retrieval-augmented approaches to more advanced models incorporating multi-modal data and reasoning capabilities. Additionally, we review the evaluation benchmarks and datasets commonly used to assess RAG systems, along with a detailed exploration of its applications in fields such as question answering, summarization, and information retrieval. Finally, we highlight emerging research directions and opportunities for improving RAG systems, such as enhanced retrieval efficiency, model interpretability, and domain-specific adaptations. This paper concludes by outlining the prospects for RAG in addressing real-world challenges and its potential to drive further advancements in natural language processing.",
            "score": 0.5034804059108214,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9296875
        },
        {
            "corpus_id": "277043297",
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) [127] has emerged as a key approach that integrates information retrieval with generative models to enhance natural language processing tasks. By leveraging external knowledge sources, RAG systems can generate more accurate and contextually relevant outputs, addressing complex challenges in areas like question answering [119], summarization [85], and open-domain dialogue. In recent years, a variety of RAG methods have been proposed, ranging from basic retrieval-augmented models to more advanced architectures incorporating multi-hop [190] reasoning and memory-augmented techniques [67]. These developments have highlighted the potential of RAG to improve the performance of NLP systems by dynamically combining retrieval and generation in a unified framework. 1 https://github.com/USTCAGI/Awesome-Papers-Retrieval-Augmented-Generation Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \n\nRAG models augment traditional language models by incorporating external knowledge sources, such as documents, databases, or structured data [93,103]., during the generation process. Unlike conventional models that rely solely on pre-trained parameters, RAG systems dynamically retrieve relevant information at generation time, allowing them to produce more informed and contextually accurate outputs. This approach addresses key limitations of traditional language models, such as their inability to access real-time or domain-specific knowledge, and mitigates the challenge of handling out-of-vocabulary or rare entities. For example, in question answering tasks [62,192], RAG models retrieve relevant passages from large corpora to generate more precise and informative answers, while in summarization [85,172], they leverage external documents to provide richer and more comprehensive summaries.",
            "score": 0.49681754670135403,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1462
                },
                {
                    "start": 1465,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2364
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 42,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 359,
                    "end": 364,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 380,
                    "end": 384,
                    "matchedPaperCorpusId": "226965071"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.818359375
        },
        {
            "corpus_id": "270688130",
            "title": "Integrating Knowledge Retrieval and Large Language Models for Clinical Report Correction",
            "text": "Retrieval-augmented generation (RAG) is a technique that enhances the performance of LLMs by integrating an external knowledge retrieval component (Gao et al., 2023). The RAG architecture consists of two main modules: a retriever and a generator. The retriever is responsible for searching and ranking relevant information from a large-scale knowledge base, given an input query. The generator, usually a pre-trained LLM, takes the input query and the retrieved knowledge as context to generate the final output. RAG has shown promising results in various natural language processing tasks, such as question answering, fact verification, and dialogue systems (Chen et al., 2024;Jin et al., 2024). \n\nTraditional RAG systems often rely on unstructured text retrieval from large-scale corpora, such as Wikipedia or domain-specific databases (Ke et al., 2024;Zhao et al., 2024). While this approach has shown success in various natural language processing tasks, it has limitations in retrieving valid information from massive free text compared to knowledge graph (KG) based retrieval (Wu et al., 2023). KGs often contain high-quality, curated knowledge that has been manually or semiautomatically extracted from reliable sources (Sanmartin, 2024). This curated knowledge tends to be more accurate and consistent compared to the potentially noisy and contradictory information found in unstructured text corpora (Jiang et al., 2024). The structured nature of KGs also enables easier integration of domain-specific knowledge, such as medical ontologies or scientific taxonomies, which can enhance the performance of RAG systems in specialized domains. \n\nHowever, constructing and maintaining largescale, high-quality KGs can be challenging and resource-intensive. To address this issue, this work presents a simple yet efficient method for constructing KGs directly from the reference data used in RAG systems. By leveraging the information already available in the reference corpus, this approach eliminates the need for extensive manual curation or external knowledge sources.",
            "score": 0.493313473937618,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 4166,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1647
                },
                {
                    "start": 1650,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2074
                }
            ],
            "ref_mentions": [
                {
                    "start": 659,
                    "end": 678,
                    "matchedPaperCorpusId": "261530434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65966796875
        },
        {
            "corpus_id": "275906690",
            "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
            "text": "Modern LLMs are primarily based on the transformer architecture, which can be easily adapted for specific tasks with robust performance, leading to large gains on downstream tasks like text classification, language understanding, reference resolution, common-sense inference, summarization, and machine translation [7]. Building upon the transformer, LLMs are typically designed using one of three main architectures: encoder, decoder and encoder-decoder [24,25]. The Table 1 summarized these three main architectural design, highlighting their examples, primary use cases, and knowledge integration benefits. \n\nEncoder LLMs: These models, such as BERT, RoBERTa, focus on text classification and retrieval tasks. They process input text bidirectionally, allowing them to generate high-quality embeddings that improve retrieval-augmented systems. By learning contextual relationships, encoder models enhance semantic search, information extraction, and retrieval-based NLP applications. \n\nDecoder LLMs: Examples include GPT-4, LLaMA-3, DeepSeek LLM, and OpenAI o1, which are mainly designed for tasks such as text generation, dialogue systems, code synthesis, and creative writing. These models generate text auto-regressively, predicting each token based on prior context. DeepSeek LLM exemplifies efficient open-source performance across both understanding and generation tasks, while OpenAI o1 introduces a novel reasoning-optimized architecture that differentiates it from conventional decoder models. Instead of allocating a fixed amount of computational resources uniformly across all inputs, o1 dynamically adjusts its internal computation in response to task complexity. Although highly capable in generative tasks, decoder models often benefit from retrieval-augmented generation (RAG), which grounds outputs in external knowledge to reduce hallucinations and improve factual accuracy in knowledge-intensive domains. \n\nEncoder-Decoder LLMs: Models like T5 and BART leverage both an encoder and a decoder to transform input text into output sequences, making them well-suited for machine translation, text summarization, and paraphrasing. Their dual-component structure allows them to integrate effectively with structured knowledge bases (KBs) and semantic reasoning systems, enhancing their ability to provide factually accurate and contextually relevant outputs.",
            "score": 0.4892592503714181,
            "section_title": "LLM architecture",
            "char_start_offset": 8250,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1924
                },
                {
                    "start": 1927,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2372
                }
            ],
            "ref_mentions": [
                {
                    "start": 315,
                    "end": 318,
                    "matchedPaperCorpusId": "267675587"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8525390625
        },
        {
            "corpus_id": "273963542",
            "title": "Leveraging Retrieval-Augmented Generation for Persian University Knowledge Retrieval",
            "text": "Recent advancements in RAG have focused on innovative techniques and methodologies to optimize retrieval and generation processes. Lewis et al. (2020) highlight the power of RAG in knowledge-intensive NLP tasks, demonstrating its potential to solve complex information retrieval challenges [1]. Shahul et al. (2023) introduced RAGAS, a framework for automated evaluation of RAG pipelines, emphasizing the importance of reference-free evaluation metrics to enhance the evaluation process of RAG systems. Siriwardhana et al. (2022) developed RAG-end2end, which optimizes RAG for domain-specific knowledge bases, significantly improving performance in specialized domains such as healthcare and news [4]. Yu (2022) explored the use of retrieval-augmented generation across heterogeneous knowledge, addressing the challenges of retrieving information from diverse sources [6]. Nakhod (2023) proposed applying RAG to elevate low-code developer skills by integrating domain-specific knowledge into large language models, thereby improving their practical utility [9]. Melz (2023) introduced ARM-RAG, a system that enhances large language models' intelligence through storing and retrieving reasoning chains, demonstrating significant improvements in problem-solving tasks [10]. Chen et al. (2023) provided a comprehensive evaluation of the impact of RAG on large language models, highlighting the potential bottlenecks and challenges in applying RAG across different tasks [7]. Heydari et al. (2024) proposed the Context Awareness Gate (CAG) architecture, a novel mechanism that dynamically adjusts the LLM's input prompt based on whether the user query necessitates external context retrieval, thereby enhancing the efficiency and accuracy of RAG systems [11].",
            "score": 0.4861634748561398,
            "section_title": "B. Recent Advances and Techniques in RAG",
            "char_start_offset": 3454,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1755
                }
            ],
            "ref_mentions": [
                {
                    "start": 702,
                    "end": 711,
                    "matchedPaperCorpusId": "250391000"
                },
                {
                    "start": 868,
                    "end": 871,
                    "matchedPaperCorpusId": "250391000"
                },
                {
                    "start": 873,
                    "end": 886,
                    "matchedPaperCorpusId": "266204857"
                },
                {
                    "start": 1057,
                    "end": 1060,
                    "matchedPaperCorpusId": "266204857"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52978515625
        },
        {
            "corpus_id": "260900354",
            "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models",
            "text": "Retrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2023;Lewis et al., 2020) and decoderonly (Khandelwal et al., 2020;Borgeaud et al., 2022;Shi et al., 2022) architectures. For decoder-only LMs, the computational cost typically increases quadratically with the input length, as well as with the number of retrieval passages. In contrast, for encoder-decoder LMs with a Fusion-in-Decoder architecture, the computation cost grows linearly with the number of retrieved passages, as they only perform self-attention over one passage at a time (Izacard & Grave, 2021). This concept is also investigated by Ye et al. (2023) for more efficient in-context learning. \n\nWhile there has been some research on in-context learning with retrieval-augmented decoder-only LMs, which can be straightforwardly implemented by concatenating retrieved passages with the query as the input of the LM (Mallen et al., 2022;Shi et al., 2023;Khattab et al., 2022), in-context learning with retrieval-augmented encoder-decoder LMs remains unexplored to the best of our knowledge. This is despite the fact that encoder-decoder LMs can be more efficient at incorporating multiple (e.g., 40) retrieved passages.",
            "score": 0.4848914463148757,
            "section_title": "Background and Related Work",
            "char_start_offset": 4627,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1004
                },
                {
                    "start": 1007,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1528
                }
            ],
            "ref_mentions": [
                {
                    "start": 382,
                    "end": 404,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 404,
                    "end": 423,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 440,
                    "end": 465,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 487,
                    "end": 504,
                    "matchedPaperCorpusId": "249152130"
                },
                {
                    "start": 886,
                    "end": 909,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 948,
                    "end": 964,
                    "matchedPaperCorpusId": "259370780"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "252735160",
            "title": "MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text",
            "text": "Pre-trained language models like GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), etc have been shown to capture a massive amount of world knowledge implicitly in their parameters. However, using such large models incurs an extremely high computation cost. As an alternative to a singular monolithic transformer, retrievalaugmented architectures like KNN-LM (Khandelwal et al., 2019), REALM (Guu et al., 2020), Figure 1: Visual information-seeking queries: These queries are unanswerable with text-only retrieval and require retrieving and reasoning over images.\n\nRAG (Lewis et al., 2020), FiD (Izacard and Grave, 2021), and RETRO (Borgeaud et al., 2021) have been proposed to decouple world knowledge from the model's parameters. More specifically, these models are trained to access an external memory to enhance the model's predictions. Such retrieval-augmented architectures have multiple beneficial properties including: decreased model size (Borgeaud et al., 2021), better attribution/explanation for model predictions (Lewis et al., 2020), and adaptability to new information without retraining (Verga et al., 2021). However, previous retrieval-augmented models are limited to memories that contain only text or structured data and hence cannot make use of the massive amount of multimodal knowledge available on the webmuch of which contains information only available in non-text modalities. Figure 1, shows several information-seeking queries that require retrieving and reasoning over visual knowledge. Here, a user first poses a question such as \"What can be found on the White House balconies at Christmas\". The system then retrieves relevant items from its memory, for exam-ple, the first image of Figure 1 with the caption \"White House during Christmas\", which it uses to produce the answer \"wreaths and garlands\". Existing text retrieval-augmented models would struggle with such queries because, in many cases, they would simply not have access to the answer as some knowledge does not exist in text form. That, coupled with the abundance of multimodal knowledge that exists, leads to the",
            "score": 0.4737692856002663,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 39,
                    "end": 59,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 401,
                    "end": 419,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 578,
                    "end": 598,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 604,
                    "end": 629,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 1035,
                    "end": 1055,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1112,
                    "end": 1132,
                    "matchedPaperCorpusId": "235097242"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67919921875
        },
        {
            "corpus_id": "268510706",
            "title": "A RAG Chatbot for Precision Medicine of Multiple Myeloma",
            "text": "Retrieval Augmented Generation (RAG) has emerged as a promising approach in natural lan-guage processing, combining the strengths of large language models with the ability to retrieve and incorporate relevant information from external knowledge sources. Several studies have explored the application of RAG in various domains, showcasing its potential for improved question answering and dialogue systems. \n\nOne of the seminal works in this area is the paper \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" by Lewis et al. (2020). The authors introduce the RAG architecture, which combines a pre-trained language model with a retrieval mechanism to generate responses based on relevant documents. They demonstrate the effectiveness of RAG on a range of knowledge-intensive tasks, including open-domain question answering and fact verification. \n\nBuilding upon this work, propose the RetrieverReader-Generator (RRG) architecture, which extends RAG by incorporating a reader component to comprehend and synthesize information from retrieved documents. RRG achieves state-of-the-art performance on several conversational question answering benchmarks, highlighting the importance of effective document retrieval and comprehension in RAG-based systems. \n\nIn the biomedical domain, RAG has shown promise in enhancing information retrieval and question answering. The work by applies RAG to the task of biomedical question answering using a large-scale corpus of scientific literature. By leveraging domain-specific pre-training and finetuning techniques, their model achieves impressive results on benchmark datasets, demonstrating the potential of RAG in specialized domains. \n\nThe application of RAG in the context of multiple myeloma research is a relatively unexplored area. While there have been studies focusing on information retrieval and text mining in the cancer domain , the specific use of RAG for multiple myeloma has not been extensively investigated. This presents an opportunity to leverage the advancements in RAG and adapt them to the unique challenges and requirements of multiple myeloma research. \n\nOur work builds upon the existing literature on RAG and aims to bridge the gap in its application to the multiple myeloma domain. By combining state-of-the-art NLP techniques, such as BioMed-RoBERTa-base for embedding generation and the . Mistral-7B language model for question answering, with a carefully curated knowledge base of multiple myeloma research articles, we propose a novel RAG-based chatbot system.",
            "score": 0.4735446544215099,
            "section_title": "Related Work",
            "char_start_offset": 3337,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 405
                },
                {
                    "start": 408,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1267
                },
                {
                    "start": 1270,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1690
                },
                {
                    "start": 1693,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2131
                },
                {
                    "start": 2134,
                    "end": 2263
                },
                {
                    "start": 2264,
                    "end": 2372
                },
                {
                    "start": 2373,
                    "end": 2546
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6552734375
        },
        {
            "corpus_id": "275820707",
            "title": "An Empirical Study of Retrieval-Augmented Code Generation: Challenges and Opportunities",
            "text": "Retrieval-augmented generation refers to improving the generation performance with the retrieved results provided by retrieval techniques. For language models, the knowledge learned from training data is all stored in the parameters of the neural network. The model might encounter challenges in generating the correct answer due to numerous parameters [9]. Furthermore, when confronted with knowledge that has never been learned during pre-training, the model might fail to provide the accurate response. The retrieved results can be regarded as a supplement to the implicit knowledge stored in the parameters of language models, encouraging the model to produce more accurate outputs [24]. In addition, database can be modified and constantly updated, enabling the trained models to adapt to a broader range of new data [78]. In other words, retrieval-augmented generation achieves scalability of the modification or replacement of retrieval sources without the need to alter the models. \n\nThe k-Nearest Neighbor Language Model (kNN-LM) [36] retrieve the k most similar training contexts for test context according to the distance in the embedding space of the pre-trained language model. In fact, the k training contexts correspond to k training targets. By normalizing and aggregating k training targets, kNN-LM can get a target distribution from k nearest neighbors, and the pre-trained language model can generate another target distribution directly according to current input. kNN-LM can merge the two distributions above by weighted sum to get the final target distribution. Different from kNN-LM, Retrieval-Augmented Language Model (REALM) [24], whose workflow can be summarized as the retriever-and-reader, has two components that are both trained. One is a neural knowledge retriever, which retrieves similar text with input using a dense inner product model. The other is the knowledge-augmented encoder, which predicts the final results based on input and the retrieved text in the last step. Actually, the prediction cannot generate texts using the encoder but extract a contiguous sequence from the retrieved text as the result. A similar workflow has been proposed and developed [8,49] before REALM occurs.",
            "score": 0.47108735754461806,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 15164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2223
                }
            ],
            "ref_mentions": [
                {
                    "start": 686,
                    "end": 690,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 822,
                    "end": 826,
                    "matchedPaperCorpusId": "252734952"
                },
                {
                    "start": 1039,
                    "end": 1043,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 1650,
                    "end": 1654,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7685546875
        },
        {
            "corpus_id": "273695367",
            "title": "RuleRAG: Rule-guided retrieval-augmented generation with language models for question answering",
            "text": "Large language models (LLMs) have achieved impressive language generation capability and excelled as knowledge learners for their well-known in-context learning (ICL) ability (Brown et al., 2020;Ouyang et al., 2022;Chowdhery et al., 2024). Despite the success, the full-parametric knowledge stored in LLMs requires substantial computational costs to keep their memory up-to-date and struggles to precisely manipulate fine-grained queries, especially in knowledge-intensive tasks (Jiang et al., 2023c;Shao et al., 2023). As complementary, RAG represents a novel framework that integrates LLMs with non-parametric information and injects the retrieved knowledge in a plug-and-play manner (Lewis et al., 2020;Dhingra et al., 2022). By explicitly decoupling the knowledge retrieval phase from the answer generation phase, RAG exhibits superior performance in many NLP tasks, such as open-domain QA (Trivedi et al., 2023) and natural language inference (Qin et al., 2023). \n\nFigure 1: (a) Without the help of rules, the current RAG can only retrieve relevant documents at the shallow semantic level, rather than the overall semantics of the query, and thus get confused in answering. (b) Guided by the rule r related to the query, our proposed RuleRAG first retrieves supportive documents that are logically related to the query and then attributes the correct answer, \"France\". \n\nHowever, two high-level issues exist in the current RAG frameworks. First, in the retrieval phase, the imperfect retrieval component can not guarantee that the recalled information will always be the most pertinent and helpful to the queries. The reason is that the retrievers in retrieval-augmented language models (RALMs) are mostly trained on unsupervised text (Izacard et al., 2024) or trained end-to-end (Guu et al., 2020;Borgeaud et al., 2022a), leading to their insufficiency in retrieving the necessary statements for reasoning (BehnamGhader et al., 2023).",
            "score": 0.4684883065216846,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 967
                },
                {
                    "start": 970,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1940
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 195,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 215,
                    "end": 238,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 479,
                    "end": 500,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 500,
                    "end": 518,
                    "matchedPaperCorpusId": "258866037"
                },
                {
                    "start": 686,
                    "end": 706,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 706,
                    "end": 727,
                    "matchedPaperCorpusId": "235669861"
                },
                {
                    "start": 894,
                    "end": 916,
                    "matchedPaperCorpusId": "254877499"
                },
                {
                    "start": 948,
                    "end": 966,
                    "matchedPaperCorpusId": "256827430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6767578125
        },
        {
            "corpus_id": "233324191",
            "title": "The NLP Cookbook: Modern Recipes for Transformer Based Deep Learning Architectures",
            "text": "In recent years, Natural Language Processing (NLP) models have achieved phenomenal success in linguistic and semantic tasks like text classification, machine translation, cognitive dialogue systems, information retrieval via Natural Language Understanding (NLU), and Natural Language Generation (NLG). This feat is primarily attributed due to the seminal Transformer architecture, leading to designs such as BERT, GPT (I, II, III), etc. Although these large-size models have achieved unprecedented performances, they come at high computational costs. Consequently, some of the recent NLP architectures have utilized concepts of transfer learning, pruning, quantization, and knowledge distillation to achieve moderate model sizes while keeping nearly similar performances as achieved by their predecessors. Additionally, to mitigate the data size challenge raised by language models from a knowledge extraction perspective, Knowledge Retrievers have been built to extricate explicit data documents from a large corpus of databases with greater efficiency and accuracy. Recent research has also focused on superior inference by providing efficient attention to longer input sequences. In this paper, we summarize and examine the current state-of-the-art (SOTA) NLP models that have been employed for numerous NLP tasks for optimal performance and efficiency. We provide a detailed understanding and functioning of the different architectures, a taxonomy of NLP designs, comparative evaluations, and future directions in NLP.",
            "score": 0.4678856077121992,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6181640625
        },
        {
            "corpus_id": "269125395",
            "title": "Knowledge Ply Chat",
            "text": "Despite their ability to store information and excel at many NLP tasks with fine-tuning, large language models tend to have issues about accurately accessing and altering knowledge, which leads to performance gaps in knowledge-intensive tasks compared to domain-specific architectures. Additionally, these models face problems when it comes to having transparent decision-making processes or updating their world knowledge. To mitigate these limitations, we propose a Retrieval Augmented Generation (RAG) system by improving the Mistral7B model specifically for RAG tasks. The novel training technique includes Parameter-Efficient Fine-Tuning (PEFT) which enables efficient adaptation of large pre- trained models on-the-fly according to task-specific requirements while reducing computational costs. In addition, this system combines pre-trained embedding models that use pre-trained cross-encoders for effective retrieval and reranking of information. This RAG system will thus leverage these state-of-the-art methodologies towards achieving top performances in a range of NLP tasks such as question answering and summarization.",
            "score": 0.46692197625482307,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63623046875
        },
        {
            "corpus_id": "270357334",
            "title": "RAG Does Not Work for Enterprises",
            "text": "Retrieval-Augmented Generation (RAG) is an emerging architecture that combines the strengths of pre-trained language models with external knowledge retrieval to enhance the accuracy, consistency, and contextual relevance of generated outputs [ Lewis et al., 2020 ].A typical RAG system consists of three main components: a retriever, a generator, and a knowledge base.\n\nThe retriever is responsible for finding the most relevant documents or passages from the knowledge base given an input query.It uses techniques from information retrieval and semantic search to efficiently search through large collections of text and rank the results based on their similarity to the query [ Karpukhin et al., 2020 ].Advanced retrieval methods may employ dense vector representations, sparse encodings, or a combination of both to capture semantic meaning beyond simple keyword matching [ Zhang et al., 2023 ].\n\nThe generator is a large pre-trained language model, such as GPT-4, Claude Opus or T5, that takes the input query and the retrieved documents as context to generate a final output.The generator is trained to condition its output on both the query and the retrieved knowledge, allowing it to incorporate relevant information and produce more accurate, consistent, and contextually appropriate responses [ Lewis et al., 2020 ].The generator may use techniques like attention, copying, or content selection to effectively fuse the retrieved knowledge with its own learned patterns.\n\nThe knowledge base is a structured or unstructured collection of documents that the RAG system can retrieve from.It can include a wide range of sources, such as web pages, books, articles, databases, or proprietary enterprise data [ Guu et al. 2020, Khandelwal et al., 2020 ].The knowledge base is typically pre-processed and indexed in a way that enables efficient retrieval based on semantic similarity.The quality, coverage, and freshness of the knowledge base are critical factors in the overall performance of the RAG system.\n\nDuring inference, a RAG system works as follows [ Lewis et al., 2020, Lewis et al., 2020, Karpukhin et al., 2020, Izacard and Grave, 2021 ]:\n\n1.The input query is passed to the retriever, which searches the knowledge base and returns a ranked list of relevant documents.",
            "score": 0.4634840142993173,
            "section_title": "Detailed explanation of RAG architecture and components",
            "char_start_offset": 5982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 265,
                    "end": 368
                },
                {
                    "start": 370,
                    "end": 496
                },
                {
                    "start": 496,
                    "end": 705
                },
                {
                    "start": 705,
                    "end": 898
                },
                {
                    "start": 900,
                    "end": 1080
                },
                {
                    "start": 1080,
                    "end": 1325
                },
                {
                    "start": 1325,
                    "end": 1478
                },
                {
                    "start": 1480,
                    "end": 1593
                },
                {
                    "start": 1593,
                    "end": 1756
                },
                {
                    "start": 1756,
                    "end": 1885
                },
                {
                    "start": 1885,
                    "end": 2010
                },
                {
                    "start": 2012,
                    "end": 2152
                },
                {
                    "start": 2154,
                    "end": 2156
                },
                {
                    "start": 2156,
                    "end": 2282
                }
            ],
            "ref_mentions": [
                {
                    "start": 1728,
                    "end": 1755,
                    "matchedPaperCorpusId": "253428554"
                },
                {
                    "start": 2124,
                    "end": 2151,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72021484375
        },
        {
            "corpus_id": "271088607",
            "title": "FACTS About Building Retrieval Augmented Generation-based Chatbots",
            "text": "Our work can be compared with RAG papers on various topics dealing with RAG quality along all the FACTS dimensions we presented (freshness, architecture, costs, testing and security).Due to lack of space, we contrast our work with selective works.Barnett et.al. [3] presented seven failure points when engineering RAG systems.In their work, they highlight the challenges of getting retrieval augmented generation right by presenting their findings from having built three chatbots.Wenqi Glantz [6] elaborated 12 RAG pain points and presented solutions.We experienced similar challenges first-hand when building our chatbots.However, none of these works discuss the challenges with complex queries, testing, dealing with document security, and the need for flexible architectures.In our work, we not only build on failure/pain points of RAGs as mentioned above, but also present our 15 control points in RAG pipelines and offer specific solutions for each stage.Also, we extend our insights and present practical techniques for handling complex queries, testing, and security.We present a reference architecture for one of the implementations of agentic architectures for complex query handling, strategies for testing and evaluating subjective query responses, and raised awareness for dealing with document ACLs and security.Furthermore, we present a reference architecture for a flexible generative-AI based Chatbot platform.ChipNemo [10] presents evidence for using a domain adapted language model for improving RAG's performance on domain specific questions.They finetuned the e5-small-unsupervised model with 3,000 domain specific auto-generated samples.We tried finetuning e5-large embeddings model in Scout Bot.Our results did not demonstrate significant improvements.We are presently collecting high quality human-annotated data to repeat the experiments.This could be an important direction to explore in the future for our work.Another interesting technique was presented by Setty et.al. [15], in improving RAG performance using Hypothetical Document Embeddings (HYDE) technique.HyDE uses an LLM to generate a theoretical document when responding to a query and then does the similarity search with both the original question and hypothetical answer.This is a promising approach but might make the architecture complex.\n\nActive Retrieval augmented generation (FLARE) [7] iteratively synthesizes a hypothetical next sentence.",
            "score": 0.46214551591028885,
            "section_title": "RELATED WORK",
            "char_start_offset": 19390,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 183,
                    "end": 247
                },
                {
                    "start": 247,
                    "end": 258
                },
                {
                    "start": 258,
                    "end": 326
                },
                {
                    "start": 326,
                    "end": 481
                },
                {
                    "start": 481,
                    "end": 552
                },
                {
                    "start": 552,
                    "end": 624
                },
                {
                    "start": 624,
                    "end": 779
                },
                {
                    "start": 779,
                    "end": 961
                },
                {
                    "start": 961,
                    "end": 1075
                },
                {
                    "start": 1075,
                    "end": 1326
                },
                {
                    "start": 1326,
                    "end": 1427
                },
                {
                    "start": 1427,
                    "end": 1562
                },
                {
                    "start": 1562,
                    "end": 1659
                },
                {
                    "start": 1659,
                    "end": 1718
                },
                {
                    "start": 1718,
                    "end": 1775
                },
                {
                    "start": 1775,
                    "end": 1863
                },
                {
                    "start": 1863,
                    "end": 1938
                },
                {
                    "start": 1938,
                    "end": 1994
                },
                {
                    "start": 1994,
                    "end": 2089
                },
                {
                    "start": 2089,
                    "end": 2260
                },
                {
                    "start": 2260,
                    "end": 2329
                },
                {
                    "start": 2331,
                    "end": 2434
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.269287109375
        },
        {
            "corpus_id": "273850363",
            "title": "Advanced RAG Models with Graph Structures: Optimizing Complex Knowledge Reasoning and Text Generation",
            "text": "This study enhances the Retrieval-Augmented Generation (RAG) model by incorporating graph structures through Graph Neural Networks (GNNs) to support complex reasoning and improve generation quality. Prior research has informed various aspects of this work, from foundational NLP improvements to methodologies for structuring and processing complex, interconnected data. \n\nA central component of the RAG model is the transformer architecture, which has been instrumental in handling semantic complexity. Du et al. [14] discuss transformers' applications in managing intricate semantic information in natural language processing (NLP), highlighting the mechanisms that allow models to capture nuanced relationships within text. Such advancements are directly relevant to this study's aim to process complex graph structures within RAG models, enhancing retrieval capabilities and accuracy in text generation. \n\nSeveral studies have contributed to understanding structured knowledge processing through GNNs and other embedding techniques. Wei et al. [15] propose a selfsupervised GNN model that improves feature extraction across heterogeneous information networks, a methodology closely aligned with this paper's approach. Their techniques provide a foundation for integrating GNNs in RAG models, improving the model's ability to understand complex associations within structured knowledge, such as in knowledge graphs. Similarly, Yang et al. [16] demonstrated the potential of dynamic hypergraphs in managing sequences and associations within data, supporting the notion that graph-enhanced models can effectively capture interconnected information within knowledge-rich domains. The model's retrieval module benefits from embedding strategies and attention mechanisms for feature enhancement, as highlighted by Liu et al. [17]. Their work on using separation embedding and self-attention strengthens understanding of how embeddings can improve interpretability and contextual sensitivity, both crucial for this paper's RAG model in ensuring knowledge consistency and coherence. Cang et al. [18] also explore ensemble methods with transformerbased models to handle specialized datasets, providing insight into techniques for augmenting model adaptability when working with structured, domain-specific information. \n\nAnother relevant contribution involves approaches for model optimization and improved robustness in handling structured data.",
            "score": 0.46171691627089256,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 4110,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 906
                },
                {
                    "start": 909,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2312
                },
                {
                    "start": 2315,
                    "end": 2440
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.693359375
        },
        {
            "corpus_id": "276580132",
            "title": "PICASO: Permutation-Invariant Context Composition with State Space Models",
            "text": "More recently, many of these ideas have been rediscovered and implemented on modern parallel hardware as basic building blocks for Foundation Models. Gu & Dao (2023) proposed Mamba, an input-dependent linear SSM (termed 'selective') based on LIV systems, that achieves comparable performance to Transformers (Vaswani, 2017) on language modeling while enjoying faster inference. Mamba-2 (Dao & Gu, 2024) further improved computational time by implementing SSM layers with structured matrix multiplications to better leverage modern Tensor Cores. Although pure SSM models can compete with Transformer blocks on many NLP tasks, they lag behind on tasks that require strong recall capabilities. To balance inference efficiency and model capabilities, Hybrid models combining Attention and SSM blocks have been proposed. Lieber et al. (2024) combined SSM blocks along with global-attention blocks to create a hybrid architecture with Mixture-of-Expert layers for training larger models. To further improve long context ability and efficiency, Ren et al. (2024) leveraged sliding window attention, while Zancato et al. (2024) developed a general family of architecture that include Transformers, SSMs and their hybrid combinations, leveraging both verbatim and fading memory, in both long-and short-term. \n\nRetrieval-Augmented Generation and In-Context Learning. Our work falls within the scope of In-Context Retrieval-Augmented Language Models (Ram et al., 2023), where language models are conditioned on retrieved contexts via concatenation. Retrieval Augmented Generation (RAG) allows language models to leverage knowledge stored in external databases, which greatly improves performance on knowledge-intensive and domain-specific tasks (Lewis et al., 2020). In our work, we simply use a pre-trained sentence embedding model for retrieval, and we refer to Gao et al. (2023) for a detailed survey on other mechanisms. Apart from retrieval, processing (multiple) retrieved contexts can also greatly increase generation latency. Izacard et al. (2023) mitigates this by independently processes each retrieved context with a LLM encoder, using cross attention over the concatenated encoder outputs.",
            "score": 0.4613659828593992,
            "section_title": "INTRODUCTION",
            "char_start_offset": 6353,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1298
                },
                {
                    "start": 1301,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2190
                }
            ],
            "ref_mentions": [
                {
                    "start": 1439,
                    "end": 1457,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 1734,
                    "end": 1754,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.671875
        },
        {
            "corpus_id": "275570331",
            "title": "Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG",
            "text": "Large Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human like text generation and natural language understanding. However, their reliance on static training data limits their ability to respond to dynamic, real time queries, resulting in outdated or inaccurate outputs. Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by integrating real time data retrieval to provide contextually relevant and up-to-date responses. Despite its promise, traditional RAG systems are constrained by static workflows and lack the adaptability required for multistep reasoning and complex task management. Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embedding autonomous AI agents into the RAG pipeline. These agents leverage agentic design patterns reflection, planning, tool use, and multiagent collaboration to dynamically manage retrieval strategies, iteratively refine contextual understanding, and adapt workflows to meet complex task requirements. This integration enables Agentic RAG systems to deliver unparalleled flexibility, scalability, and context awareness across diverse applications. This survey provides a comprehensive exploration of Agentic RAG, beginning with its foundational principles and the evolution of RAG paradigms. It presents a detailed taxonomy of Agentic RAG architectures, highlights key applications in industries such as healthcare, finance, and education, and examines practical implementation strategies. Additionally, it addresses challenges in scaling these systems, ensuring ethical decision making, and optimizing performance for real-world applications, while providing detailed insights into frameworks and tools for implementing Agentic RAG.",
            "score": 0.46077352306505437,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9189453125
        },
        {
            "corpus_id": "218869575",
            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "text": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
            "score": 0.457190172091137,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82568359375
        },
        {
            "corpus_id": "275471724",
            "title": "Enhancing Retrieval-Augmented Generation: A Study of Best Practices",
            "text": "Language Models (LMs) such as GPT, BERT, and T5 have demonstrated remarkable versatility, excelling in a wide range of NLP tasks, including summarization (Bahrainian et al., 2022), extracting relevant information from lengthy documents, question-answering, and storytelling (Brown et al., 2020b;Devlin et al., 2019;Raffel et al., 2020). However, their static knowledge and opaque reasoning raise concerns about maintaining factual accuracy and reliability as language and knowledge evolve (Huang et al., 2024;Jin et al., 2024). As new events emerge, and scientific advancements are made, it becomes crucial to keep models aligned with current information (Shi et al., 2024a). However, continuously updating models is both costly and inefficient. To address this, RAG models have been proposed as a more efficient alternative, integrating external knowledge sources during inference to provide up-to-date and accurate information (Lewis et al., 2020;Borgeaud et al., 2022;Lee et al., 2024). RAG models augment language models by incorporating verifiable information, improving factual accuracy in their responses (Gao et al., 2023;Kim et al., 2023). This approach not only mitigates some conceptual limitations of traditional LMs but also unlocks practical, real-world applications. By integrating a domain-specific knowledge base, RAG models transform LMs into specialized experts, enabling the development of highly targeted applications and shifting them from generalists to informed specialists (Siriwardhana et al., 2023). In recent years, this advancement has led to many proposed architectures and settings for an optimal RAG model (Li et al., 2024;Dong et al., 2024). However, the best practices for designing RAG models are still not well understood. \n\nIn this paper, we comprehensively examine the efficacy of RAG in enhancing Large LM (LLM) responses, addressing nine key research questions: \n\n(1) How does the size of the LLM affect the response quality in an RAG system? (2) Can subtle differences in prompt significantly affect the alignment of retrieval and generation? (3) How does the retrieved document chunk size impact the response quality?",
            "score": 0.4546449163075953,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1758
                },
                {
                    "start": 1761,
                    "end": 1901
                },
                {
                    "start": 1904,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2159
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 179,
                    "matchedPaperCorpusId": "248780473"
                },
                {
                    "start": 274,
                    "end": 295,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 295,
                    "end": 315,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 315,
                    "end": 335,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 489,
                    "end": 509,
                    "matchedPaperCorpusId": "265067168"
                },
                {
                    "start": 509,
                    "end": 526,
                    "matchedPaperCorpusId": "267782658"
                },
                {
                    "start": 929,
                    "end": 949,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 949,
                    "end": 971,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 971,
                    "end": 988,
                    "matchedPaperCorpusId": "269804192"
                },
                {
                    "start": 1130,
                    "end": 1147,
                    "matchedPaperCorpusId": "264426402"
                },
                {
                    "start": 1498,
                    "end": 1525,
                    "matchedPaperCorpusId": "252735056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6103515625
        },
        {
            "corpus_id": "269543427",
            "title": "Bridging Human and AI Decision-Making with LLMs: The RAGADA Approach",
            "text": "The recent enhancements of Large Language Models (LLMs) like ChatGPT has marked a significant shift in artificial intelligence (AI), establishing a new paradigm in human-machine interaction.These LLMs have transitioned from academic innovations to a 'killer application in AI' with wide variety of use cases across various industries, offering intuitive and adaptable interfaces for diverse applications.Their advancement in natural language processing has revolutionized machine comprehension, enabling complex dialogue and task execution.\n\nLLMs are evolving beyond sophisticated chatbots to offer a platform that transforms interaction with machines, democratizing AI access for users with varied technical expertise.A pressing research question is aligning these algorithms with human values and objectives (Christian, 2020), highlighting the need for a multidisciplinary approach in AI (Wilson, 1999).AI now merges data science and mathematics with ethics and leadership, ensuring a holistic development and deployment of technologies like LLMs.\n\nThis paper presents RAGADA, an innovative software architecture integrating Retrieval Augmented Generation with dynamic, user-friendly interfaces for both customers and executives.RAGADA aims to revolutionize AI systems' user interaction, focusing on natural language processing to enhance customer experience and allow executives to transparently adjust algorithmic decisions, thereby improving user satisfaction and strategic agility in corporate environments.\n\nThe integration of LLMs with Retrieval Augmented Generation (RAG) represents a key development in enhancing NLP and AI.RAG effectively combines pre-trained language models with external knowledge retrieval, significantly improving LLMs' adaptability and accuracy (Karpukhin et al., 2020).It mitigates the constraints of LLMs' static knowledge bases by dynamically incorporating external data, thus boosting the models' contextual relevance (Lewis et al., 2020).\n\nResearch underscores the utility of LLMs in complex decision-making (Petroni et al., 2019), and RAG's ability to update these models with current information is especially valuable in dynamic corporate scenarios (Guu et al., 2020), (Borgeaud et al., 2022).The LLM-RAG synergy augments automated decision-making in businesses by addressing challenges in managing real-time, context-specific data.",
            "score": 0.4534483209259762,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 190,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 540
                },
                {
                    "start": 542,
                    "end": 719
                },
                {
                    "start": 719,
                    "end": 905
                },
                {
                    "start": 905,
                    "end": 1049
                },
                {
                    "start": 1051,
                    "end": 1231
                },
                {
                    "start": 1231,
                    "end": 1513
                },
                {
                    "start": 1515,
                    "end": 1634
                },
                {
                    "start": 1634,
                    "end": 1803
                },
                {
                    "start": 1803,
                    "end": 1976
                },
                {
                    "start": 1978,
                    "end": 2234
                },
                {
                    "start": 2234,
                    "end": 2373
                }
            ],
            "ref_mentions": [
                {
                    "start": 1955,
                    "end": 1975,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 2210,
                    "end": 2233,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51904296875
        },
        {
            "corpus_id": "267412060",
            "title": "When Large Language Models Meet Vector Databases: A Survey",
            "text": "To better harness the knowledge from various types of data, kNN-LMs [46] explore how incorporating nearest neighbor search into language models can enhance their ability to generalize by effectively leveraging memorized examples. EASE [47] is distinctive in its use of entities as a strong indicator of text semantics, providing rich training signals for sentence embedding. In-context RALM [48] proves that with the language model architecture unchanged, simply appending grounding documents to the input will improve the performance. SURGE [49] enhances dialogue generation by incorporating contextrelevant sub-graphs from a knowledge graph. Another work that combines knowledge graphs and LLMs is RET-LLM [50], which is designed to equip LLMs with a general writeread memory unit. These studies have focused on retrieval granularity and data structuring levels, with coarse granularity providing more, but less precise, information. Conversely, structured text retrieval offers detailed information at the cost of efficiency. \n\nTo utilize both internal knowledge and external resources, SKR [51] improves LLMs' ability by enabling them to assess what they know and identify when to seek external information to answer questions more effectively. Selfmem [52] enhances retrieval-augmented text generation by creating an unbounded memory pool using the model's output and selecting the best output as memory for subsequent rounds. FLARE [53] uses predictions of upcoming sentences to anticipate future content and retrieve relevant documents for regenerating sentences, especially when they contain low-confidence tokens. Atlas [54] demonstrates impressive performance in tasks like question-answering and fact-checking, outperforming much larger models despite having fewer parameters. Many other works like these also aim to make the RAG system more efficient and competent. \n\nV. DISCUSSION: CHALLENGES AND FUTURE WORK 1) Are vector searches all you need?: Although vector data is an efficient representation of unstructured data, vector-based search methodologies are still not well suited for operations such as structured queries (SQL), post-query filtering, comprehensive full-text searches, relation-based graph, and keyword search mechanisms that are fundamental to conventional database systems and search engines.",
            "score": 0.4530278571117612,
            "section_title": "B. Retrieval Optimizations of RAG",
            "char_start_offset": 23533,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1877
                },
                {
                    "start": 1880,
                    "end": 2324
                }
            ],
            "ref_mentions": [
                {
                    "start": 235,
                    "end": 239,
                    "matchedPaperCorpusId": "248572170"
                },
                {
                    "start": 391,
                    "end": 395,
                    "matchedPaperCorpusId": "256459451"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.580078125
        },
        {
            "corpus_id": "265212816",
            "title": "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models",
            "text": "Retrieval-Augmented Language Models (RALMs) represent a significant advancement in natural language processing, combining the power of large language models with the specificity and detail provided by external knowledge sources (Guu et al., 2020;Lewis et al., 2020;Izacard et al., 2022). These models first leverage a retriever to scan a vast evidence corpus, such as Wikipedia, to identify a set of documents pertinent to the user's query. Following this, a reader component is employed to meticulously analyze these documents and formulate a response. This two-pronged approach ensures both relevance and depth in the generated answers. Recent follow-up work has mainly focused on improving the retriever (Karpukhin et al., 2020;Qu et al., 2021;Sachan et al., 2022;Ma et al., 2023) or the reader (Izacard and Grave, 2021;Cheng et al., 2021;Yu et al., 2022), training the system end-to-end (Lewis et al., 2020;Singh et al., 2021), and integrating the retrieval systems with large-scale black-box language models (Yu et al., 2023a;Shi et al., 2023c;Yu et al., 2023b;Trivedi et al., 2023). Another line of RALMs such as kNN-LM (Khandelwal et al., 2020;Zhong et al., 2022) retrieves a set of tokens and interpolates between the next token distribution and kNN distributions computed from the retrieved tokens at inference. The evolution has also led to the emergence and popularity of retrieval-augmented products, such as ChatGPT plugin, Langchain, and New Bing.",
            "score": 0.4483924175418692,
            "section_title": "A.1.1 Retrieval-Augmented Language Models",
            "char_start_offset": 27634,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1461
                }
            ],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 265,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 707,
                    "end": 731,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 731,
                    "end": 747,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 767,
                    "end": 783,
                    "matchedPaperCorpusId": "258546861"
                },
                {
                    "start": 798,
                    "end": 823,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 823,
                    "end": 842,
                    "matchedPaperCorpusId": "230437698"
                },
                {
                    "start": 842,
                    "end": 858,
                    "matchedPaperCorpusId": "238583601"
                },
                {
                    "start": 891,
                    "end": 911,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 911,
                    "end": 930,
                    "matchedPaperCorpusId": "235390519"
                },
                {
                    "start": 1066,
                    "end": 1087,
                    "matchedPaperCorpusId": "254877499"
                },
                {
                    "start": 1126,
                    "end": 1151,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 1151,
                    "end": 1170,
                    "matchedPaperCorpusId": "249062699"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6298828125
        },
        {
            "corpus_id": "277780258",
            "title": "ExpertRAG: Efficient RAG with Mixture of Experts - Optimizing Context Retrieval for Adaptive LLM Responses",
            "text": "Large language models (LLMs) have achieved remarkable success in many NLP tasks, yet they face persistent challenges in knowledge-intensive applications. A key limitation is the reliance on storing factual knowledge purely in model parameters. As models grow, their ability to recall or update specific facts becomes problematic [1]. Retrieval-Augmented Generation (RAG) was introduced to address this by equipping models with access to external nonparametric memory (e.g. a text corpus or database), allowing them to fetch relevant information on-the-fly [2]. RAG combines a parametric neural generator with a retrieval module, producing outputs that are more specific and factual than those of parametric-only models [1]. However, standard RAG pipelines retrieve documents for every query, which can be inefficient when the model's internal knowledge is already sufficient. Unnecessary retrieval incurs extra latency and may introduce distractors, highlighting a need for dynamic retrieval strategies that invoke external lookup only when needed. \n\nIn parallel, Mixture-of-Experts (MoE) architectures have emerged as a solution for scaling model capacity without proportional increases in computation [3]. In an MoE model, multiple expert subnetworks (e.g. feed-forward layers) are trained, and a learned gating function routes each input token or example to a subset of these experts. This sparse activation means only a few experts process each token, allowing the total parameter count to increase (potentially to trillions) while keeping the compute per token comparable to a much smaller model [4]. Notable MoE instances include 2 Literature Review",
            "score": 0.44760771816579314,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1048
                },
                {
                    "start": 1051,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1655
                }
            ],
            "ref_mentions": [
                {
                    "start": 329,
                    "end": 332,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 719,
                    "end": 722,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1203,
                    "end": 1206,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83447265625
        },
        {
            "corpus_id": "233296655",
            "title": "Case-based Reasoning for Natural Language Queries over Knowledge Bases",
            "text": "Retrieval augmented LM architectures: A growing class of models (e.g., Guu et al., 2020;Lewis et al., 2020b) augments language models with a non-parametric memory, instead of solely relying on information stored in model parameters. In contrast, our CBR approach retrieves similar queries, instead of relevant supporting context, w.r.t the input query and use their solution (logical forms) to derive a new solution for the query. Recently, (Lewis et al., 2020c) also noted that the train set often contain similar questions w.r.t. the evaluation set, and concurrent work uses this insight to derive the answer to natural language questions using similar, retrieved questions (Lewis et al., 2021). Our work develops a case-based reasoning approach for KBQA and is further capable of answering compositional questions from multiple simple questions.\n\nRetrieve and edit: Our model shares similarities with the RETRIEVE-AND-EDIT framework (Hashimoto et al., 2018) which utilizes the case of the nearest-neighbor w.r.t input.\n\nTheir \"edit\" step is similar to our \"reuse\" step, however, they simply rely on the sequence-to-sequence model to generate answer from the retrieved case without \"revise\" and \"retain\" steps. Furthermore, our \"reuse\" step brings in new challenges as parametric model have to compose one SPARQL query from multiple cases in contrast to RETRIEVE-AND-EDIT that only considers a single nearest case.\n\nK-NN based approach in other NLP applications: Nearest neighbor models have been used in a number of NLP applications such as parts-of-speech tagging (Daelemans et al., 1996) and morphological analysis (Bosch et al., 2007).\n\nWiseman & Stratos (2019) achieved accurate sequence labeling by explicitly and only copying labels from retrieved neighbors. Another recent line of work use training examples at test time to improve language generation (Weston et al., 2018;Pandey et al., 2018;Cao et al., 2018;Peng et al., 2019). Khandelwal et al. (2020) also observed improvements in language modeling by utilizing explicit examples from past training data obtained via nearest neighbor search in a continuous vector space.",
            "score": 0.44729063985885503,
            "section_title": "Related Work",
            "char_start_offset": 26908,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 108,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1568,
                    "end": 1592,
                    "matchedPaperCorpusId": "505"
                },
                {
                    "start": 1862,
                    "end": 1883,
                    "matchedPaperCorpusId": "52006529"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.458251953125
        },
        {
            "corpus_id": "267750726",
            "title": "Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs",
            "text": "To facilitate this, SlimPLM utilizes the heuristic answer again to generate multiple queries, each reflecting a specific aspect of the initial response. These queries are then individually assessed for their need for retrieval, filtering out queries that do not require retrieval. By this means, the remaining queries can retrieve more relevant knowledge that is lacking in LLMs. The integration of SlimPLM into existing RAG frameworks offers a flexible and effective enhancement without notably increasing computational costs or response latency. Experimental results across five commonly used questionanswering datasets validate SlimPLM's effectiveness in determining the necessity for retrieval and improving retrieval results. \n\nOur contributions are threefold: (1) We pro-pose a novel approach that leverages a small proxy model to generate heuristic answers, helping determine when and how to perform retrieval for LLMs. \n\n(2) We devise a retrieval necessity judgment model based on the heuristic answer. It is capable of accurately identifying which queries necessitate further information retrieval. \n\n(3) We formulate a query rewriting strategy that decomposes the heuristic answer into distinct claims. This is complemented by a claim-based filtering mechanism to enhance the relevance of the retrieval results for LLMs' text generation. \n\n2 Related Work 2.1 Retrieval-Augmented Generation (RAG) \n\nRAG has been studied for a long time. In the era of pre-trained language models, RAG has been applied to provide models with relevant knowledge, significantly enhancing the generation quality in applications such as dialogue systems (Tahami et al., 2020;Tao et al., 2019) and question-answering systems (Izacard and Grave, 2021;Tahami et al., 2020). With the development of LLMs, RAG has emerged as a crucial strategy to tackle the problem of hallucination and outdated information (Shuster et al., 2021;White, 2023). \n\nThe mainstream RAG methods follow a \"retrieve-then-read\" architecture. In this setup, a retrieval module first gathers external knowledge, providing additional context that is subsequently processed by LLMs to generate the final output (Ram et al., 2023;Yu et al., 2023b).",
            "score": 0.44538265834148005,
            "section_title": "Introduction",
            "char_start_offset": 4116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 926
                },
                {
                    "start": 929,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1347
                },
                {
                    "start": 1350,
                    "end": 1405
                },
                {
                    "start": 1408,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1925
                },
                {
                    "start": 1928,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2200
                }
            ],
            "ref_mentions": [
                {
                    "start": 1641,
                    "end": 1662,
                    "matchedPaperCorpusId": "216080802"
                },
                {
                    "start": 1662,
                    "end": 1679,
                    "matchedPaperCorpusId": "59317725"
                },
                {
                    "start": 1711,
                    "end": 1736,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 1736,
                    "end": 1756,
                    "matchedPaperCorpusId": "216080802"
                },
                {
                    "start": 1890,
                    "end": 1912,
                    "matchedPaperCorpusId": "233240939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5947265625
        },
        {
            "corpus_id": "271956497",
            "title": "DSTI at LLMs4OL 2024 Task A: Intrinsic versus extrinsic knowledge for type classification",
            "text": "Large language models (LLMs) have seen widespread applications across different tasks in the fields of Natural Language Processing and Knowledge Representation. Particularly, LLM-based systems are used to tackle ontology-related tasks such as ontology learning [1], knowledge graph construction [2], ontology matching [3][4] and ontology generation [5]. Retrieval-Augmented-Generation (RAG) systems, which build on the capabilities of LLMs by enhancing retrieval using external knowledge sources, have also shown promising results in tasks involving the use of ontologies [6]. On the other hand, symbolic methods like semantic representation using primes and universals [7] form another research frontier in the area of knowledge representation which is at the heart of ontologies [8]. \n\nIn this work, we evaluate and compare the performance of fine-tuned models on Task A of the LLMs4OL [9][10] [11] 2024 challenge1 using intrinsic LLM knowledge and external knowledge sources we define as semantic towers. The rest of the work is organized as follows. In section 2, we present our methodology. Section 3 describes our experimental framework. In section 4, we report our results and discuss our findings. Finally, we conclude in section 5.",
            "score": 0.4439982908604859,
            "section_title": "Introduction and related work",
            "char_start_offset": 32,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1240
                }
            ],
            "ref_mentions": [
                {
                    "start": 572,
                    "end": 575,
                    "matchedPaperCorpusId": "267155089"
                },
                {
                    "start": 888,
                    "end": 891,
                    "matchedPaperCorpusId": "260334867"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53564453125
        },
        {
            "corpus_id": "274131235",
            "title": "Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation",
            "text": "We implement a RAG LLM low resource language translator by combining retrieval-based techniques with LLMs to ensure accurate and context-aware translations. The overall architecture is shown in Figure 1. The system utilizes dictionary entries, which are indexed through two complementary approaches: keyword-to-document mappings and vector embeddings. Key-to-document mappings in systems refer to a process where keywords are linked directly to the documents or data entries that contain or are relevant to those keywords. The keyword retriever will retrieve corresponding documents according to the key-to-document mapping, if the keyword is inside our storage. The vector embedding indexing process organizes raw linguistic data into retrievable units by associating words with dictionary definitions and using text-embedding-ada-002 model to encode the text into high-dimensional vectors that capture semantic relationships beyond mere surface forms. \n\nFigure 1: Illustrating a retrieval-augmented generation (RAG) architecture: Documents are indexed using both keyword and embedding vector methods, stored in separate databases. A retrieval agent accesses these indexes to provide relevant information, which is then processed by a GPT-4 model to deliver responses to users. \n\nOur model operates with this dual retrieval mechanism. First, a keyword-based index allows for fast and efficient lookup by identifying exact matches between query terms and dictionary entries. This method ensures that direct translations of words or phrases are retrieved whenever possible. Second, in cases where no exact keyword matches are found, the system employs a vector-based retrieval method using cosine similarity. This approach encodes the query into a semantic vector and calculates similarity scores between the query vector and all indexed vectors. The top K most similar entries are then retrieved, ensuring that semantically related content such as synonyms or conceptually similar expressions are identified, even in the absence of exact matches. \n\nAfter retrieval, the system uses a GPT-based model to synthesize the retrieved content into a coherent and fluent translation. This generative step integrates the context from retrieved entries to resolve ambiguities and produce a natural, human-readable translation. By balancing lightweight keyword-based retrieval with deeper semantic understanding through vector-based retrieval, this hybrid approach enhances both the accuracy and relevance of the translation output.",
            "score": 0.44256305877079244,
            "section_title": "Methodology",
            "char_start_offset": 40068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 953
                },
                {
                    "start": 956,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1278
                },
                {
                    "start": 1281,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 2046
                },
                {
                    "start": 2049,
                    "end": 2175
                },
                {
                    "start": 2176,
                    "end": 2316
                },
                {
                    "start": 2317,
                    "end": 2521
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7470703125
        },
        {
            "corpus_id": "273901202",
            "title": "RAC: Retrieval-augmented Conversation Dataset for Open-domain Question Answering in Conversational Settings",
            "text": "A query rewriting model (ConQRR) is optimized for passage retrieval performance rather than just human readability. Their experiments demonstrates that human-rewritten queries are precisely clear, but may omit context useful for retrieval, affecting performance. The proposed model significantly enhances retrieval effectiveness by aligning the query rewriting process with the retrieval task's requirements. Mo et al. (2023) explores generative query reformulation to improve conversational search. A dual approach combining query rewriting and query expansion to address ambiguous queries and supplement them with additional context were proposed. The ConvGQR model integrates both rewriting and expansion techniques to produce more effective search queries. Emipirical results show that the combined approach outperforms traditional methods in generating queries that lead to better retrieval performance. Mao et al. (2023) introduces LLM4CS, a framework leveraging large language models (LLMs) to interpret users' contextual search intent in conversational search scenarios. By generating multiple query rewrites and hypothetical responses, the framework creates an integrated representation of the user's search intent. Evaluations on conversational search benchmarks demonstrate the framework's effectiveness and robustness, outperforming existing methods and even human rewrites in some cases. The study underscores the potential of LLMs in enhancing conversational search systems. \n\nA.3 Retrieval-augmented Generation Lewis et al. (2020b) first introduced the word retrieval-augmented generation for knowledgeintensive NLP tasks. The paper introduces a RAG approach that combines retrieval mechanisms with generative models to handle knowledge-intensive NLP tasks. By incorporating retrieved information from knowledge bases, the model can generate more accurate and informed responses for tasks like question answering. Izacard and Grave (2021) explores enhancing generative models for open-domain QA by incorporating passage retrieval, proposing Fusionin-Decoder (FiD) architecture. Generative models have shown promise without external knowledge but require large parameters, making them costly. The authors investigate how these models can benefit from retrieving relevant passages. The approach achieves state-of-the-art results on benchmarks like Natural Questions (NQ) and Triv-iaQA, showing significant performance improvement with more retrieved passages.",
            "score": 0.4411240558220877,
            "section_title": "Limitations",
            "char_start_offset": 26385,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1488
                },
                {
                    "start": 1491,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2206
                },
                {
                    "start": 2207,
                    "end": 2294
                },
                {
                    "start": 2295,
                    "end": 2472
                }
            ],
            "ref_mentions": [
                {
                    "start": 409,
                    "end": 425,
                    "matchedPaperCorpusId": "258887946"
                },
                {
                    "start": 909,
                    "end": 926,
                    "matchedPaperCorpusId": "257495903"
                },
                {
                    "start": 1526,
                    "end": 1546,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1929,
                    "end": 1953,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68115234375
        },
        {
            "corpus_id": "275954545",
            "title": "Uncertainty Quantification and Decomposition for LLM-based Recommendation",
            "text": "Large Language Models for Recommendation. Large language models (LLMs) have recently gained widespread adoption in recommendation systems due to their powerful ability to comprehend complex contexts and to utilize external knowledge for generation [11,73]. Early work [9,19,63] adapts the architectures of language models for the recommendation task and outperforms conventional matrix factorization architectures [22,38,58]. As cutting-edge LLMs [29,64,66], pre-trained on extensive corpora and distributed publicly, show remarkable performance in open-domain tasks [4,6,72], subsequent research highlights their effectiveness for the zero-shot [18,23,25,70] and few-shot [47,60] ranking task. Moreover, recent methods [2,21,33,45,81] involve fine-tuning LLMs with instruction on recommendation datasets, to mitigate the disparity between natural language understanding tasks used to train LLMs and the recommendation task [2,35]. Lately, the state-of-the-art methods [10,25,70,79,80] adopt a list-wise ranking paradigm with the retrieval-augmented generation [3,27]. In this approach, the generation of ranking lists is conditioned on candidates retrieved by candidate generators as illustrated in Figure 1, to better facilitate updates and reduce hallucination [11]. We refer readers to [11,73] for a detailed survey. \n\nUncertainty of Large Language Models. In the era of LLMs, where human behaviors are influenced by the outputs of these models, recent research underscores the imperative of evaluating the reliability of LLM-generated responses [1,[74][75][76]. The predictive uncertainty [39,46], quantified as the entropy of the predictive distribution, is widely adopted to measure the reliability of LLMs [12,16,34]. Despite significant progress in uncertainty estimation, with a primary focus on classification [46,65,74,76] and questionanswering [39,40], there is a scarcity of research on the uncertainty of recommendations generated by LLMs.",
            "score": 0.4410701858902774,
            "section_title": "Related Work",
            "char_start_offset": 5476,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1954
                }
            ],
            "ref_mentions": [
                {
                    "start": 274,
                    "end": 277,
                    "matchedPaperCorpusId": "119181611"
                },
                {
                    "start": 414,
                    "end": 418,
                    "matchedPaperCorpusId": "211043589"
                },
                {
                    "start": 418,
                    "end": 421,
                    "matchedPaperCorpusId": "58370896"
                },
                {
                    "start": 421,
                    "end": 424,
                    "matchedPaperCorpusId": "10795036"
                },
                {
                    "start": 567,
                    "end": 570,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 570,
                    "end": 572,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 650,
                    "end": 653,
                    "matchedPaperCorpusId": "261049680"
                },
                {
                    "start": 653,
                    "end": 656,
                    "matchedPaperCorpusId": "258686540"
                },
                {
                    "start": 723,
                    "end": 726,
                    "matchedPaperCorpusId": "261823711"
                },
                {
                    "start": 973,
                    "end": 976,
                    "matchedPaperCorpusId": "258686540"
                },
                {
                    "start": 1061,
                    "end": 1064,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1064,
                    "end": 1067,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 1553,
                    "end": 1557,
                    "matchedPaperCorpusId": "247613322"
                },
                {
                    "start": 1557,
                    "end": 1561,
                    "matchedPaperCorpusId": "53715584"
                },
                {
                    "start": 1561,
                    "end": 1565,
                    "matchedPaperCorpusId": "232404053"
                },
                {
                    "start": 1594,
                    "end": 1598,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 1598,
                    "end": 1601,
                    "matchedPaperCorpusId": "268890296"
                },
                {
                    "start": 1714,
                    "end": 1718,
                    "matchedPaperCorpusId": "3461501"
                },
                {
                    "start": 1718,
                    "end": 1721,
                    "matchedPaperCorpusId": "160705"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59619140625
        },
        {
            "corpus_id": "274982275",
            "title": "Formal Language Knowledge Corpus for Retrieval Augmented Generation",
            "text": "RAG is a sophisticated framework designed to enhance language models by coupling them with external retrieval systems, addressing limitations inherent in static, solely parameter-based language models. RAG integrates a dual-component architecture where a retriever dynamically searches a structured external corpus for relevant information based on the input query, and a generator LLM uses the retrieved content as context to generate accurate and contextually enriched responses [Gao et al., 2023, Mialon et al., 2023]. This setup mitigates common issues such as hallucinations and factual inaccuracies in language models by grounding generated text in real-world, verified information. In practice, RAG systems employ dense vector embeddings to ensure retrieval relevance, capturing semantic relationships within documents beyond mere keyword matching. The retrieved information is subsequently fed into the generator, allowing it to synthesize data with pre-existing knowledge for enhanced coherence and contextual accuracy. \n\nRAGs can employ two main types of retrieval mechanisms: dense and sparse [Mialon et al., 2023]. Sparse retrievers rely on bag-of-words representations, excelling at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities, enhancing the model's comprehension of related concepts. By appending retrieved documents directly to the model's context, these retrievers allow the language model to ground its responses in a broader context, thereby increasing accuracy and factual consistency across complex tasks. \n\nThe success of retrieval-augmented models in various domains has catalyzed interest in their application to more demanding reasoning tasks. A great representative of such tasks is the construction and verification of mathematical proofs, which requires solving problems step-by-step, and generating precise mathematical statements. Recent approaches, such as chain-of-thought (CoT) prompting [Lewkowycz et al., 2022] combined with retrieval, highlight the potential for retrieval-augmented models to provide sequential reasoning support. These models can generate reasoning paths interspersed with retrieval steps to guide complex problem-solving processes, such as multi-step question answering, enabling models to leverage external information dynamically at each reasoning stage.",
            "score": 0.43899103315585686,
            "section_title": "A. Retrieval-Augmented Language Models",
            "char_start_offset": 3315,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1626
                },
                {
                    "start": 1629,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2166
                },
                {
                    "start": 2167,
                    "end": 2411
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "218869575",
            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "text": "Single-Task Retrieval Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [5,25], fact checking [50], fact completion [42], long-form question answering [12], Wikipedia article generation [32], dialogue [36,59,9,13], translation [16], and language modeling [17,23]. Our work unifies previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks. \n\nGeneral-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classification tasks in the GLUE benchmarks [54,55] after fine-tuning [43,8]. GPT-2 [44] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [28] and T5 [45,46] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, unified architecture, by learning a retrieval module to augment pre-trained, generative language models. \n\nLearned Retrieval There is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [39,22] similar to ours. Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search [40], reinforcement learning [6,57,56], or a latent variable approach [27,18] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be fine-tuned for strong performance on a variety of tasks.",
            "score": 0.4376638787058579,
            "section_title": "Related Work",
            "char_start_offset": 28170,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1454
                },
                {
                    "start": 1457,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2116
                }
            ],
            "ref_mentions": [
                {
                    "start": 189,
                    "end": 192,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 192,
                    "end": 195,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 211,
                    "end": 215,
                    "matchedPaperCorpusId": "4711425"
                },
                {
                    "start": 233,
                    "end": 237,
                    "matchedPaperCorpusId": "212411919"
                },
                {
                    "start": 268,
                    "end": 272,
                    "matchedPaperCorpusId": "196170479"
                },
                {
                    "start": 303,
                    "end": 307,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "52333947"
                },
                {
                    "start": 322,
                    "end": 325,
                    "matchedPaperCorpusId": "52006529"
                },
                {
                    "start": 325,
                    "end": 327,
                    "matchedPaperCorpusId": "53218829"
                },
                {
                    "start": 344,
                    "end": 348,
                    "matchedPaperCorpusId": "19206366"
                },
                {
                    "start": 372,
                    "end": 376,
                    "matchedPaperCorpusId": "2318481"
                },
                {
                    "start": 376,
                    "end": 379,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 872,
                    "end": 876,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 876,
                    "end": 879,
                    "matchedPaperCorpusId": "143424870"
                },
                {
                    "start": 902,
                    "end": 904,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1761,
                    "end": 1765,
                    "matchedPaperCorpusId": "202572744"
                },
                {
                    "start": 1790,
                    "end": 1793,
                    "matchedPaperCorpusId": "17476563"
                },
                {
                    "start": 1793,
                    "end": 1796,
                    "matchedPaperCorpusId": "13764176"
                },
                {
                    "start": 1831,
                    "end": 1835,
                    "matchedPaperCorpusId": "173990818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75830078125
        },
        {
            "corpus_id": "256389797",
            "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
            "text": "Retrieval-augmented Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language modeling (Min et al., 2022;Borgeaud et al., 2022;Khandelwal et al., 2020) and open-domain question answering (Lewis et al., 2020;Izacard et al., 2022b;Hu et al., 2022). Specifically, using the input as query, (1) a retriever first retrieves a set of documents (i.e., sequences of tokens) from a corpus and then (2) a language model incorporates the retrieved documents as additional information to make a final prediction. This style of retrieval can be added to both encoderdecoder (Yu, 2022;Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2020;Borgeaud et al., 2022;Shi et al., 2022;Rubin et al., 2022). For example, Atlas (Izacard et al., 2022b) finetunes an encoder-decoder model jointly with the retriever by modeling documents as latent variables, while RETRO (Borgeaud et al., 2022) changes the decoderonly architecture to incorporate retrieved texts and pretrains the language model from scratch. Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs. Another line of retrieval-augmented LMs such as kNN-LM (Khandelwal et al., 2020;Zhong et al., 2022) retrieves a set of tokens and interpolates between the LM's next token distribution and kNN distributions computed from the retrieved tokens at inference. Although kNN-LM does not require additional training, it requires access to internal LM representations to compute the kNN distribution, which are not always available for large LMs such as GPT-3. In this work, we investigate ways to improve large black-box language models with retrieval.",
            "score": 0.4353487993929209,
            "section_title": "Background and Related Work",
            "char_start_offset": 5661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1799
                }
            ],
            "ref_mentions": [
                {
                    "start": 239,
                    "end": 261,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 261,
                    "end": 285,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 695,
                    "end": 705,
                    "matchedPaperCorpusId": "250391000"
                },
                {
                    "start": 752,
                    "end": 777,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 777,
                    "end": 799,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 816,
                    "end": 835,
                    "matchedPaperCorpusId": "245218561"
                },
                {
                    "start": 997,
                    "end": 1020,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1310,
                    "end": 1335,
                    "matchedPaperCorpusId": "207870430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7978515625
        },
        {
            "corpus_id": "252735160",
            "title": "MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text",
            "text": "Retrieval Augmented Models Retrieval augmented models are hybrid models containing both parameterized sequence models and a nonparametric memory, infusing world knowledge into existing language models. Among them, KNN-LM (Khandelwal et al., 2019) was first proposed to retrieve instances from a text training corpus to help language modeling. Later, RETRO (Borgeaud et al., 2021) was proposed to scale up the text corpus to trillions of tokens, enabling the model to achieve similar perplexity to GPT-3 (Brown et al., 2020) with 25x fewer model parameters. Another family of models, such as REALM (Guu et al., 2020), RAG (Lewis et al., 2020), and FiD (Izacard and Grave, 2021), integrate Wikipedia passages as a datastore to benefit downstream knowledge intensive tasks (e.g. Question Answering). REALM is an encoder-only model trained with masked lan-guage modeling, while RAG and FiD adopt an encoder-decoder model with a generative language modeling objective. Compared to them, MuRAG is the first retrieval-augmented model that is capable of using knowledge presented in multiple modalities (i.e. visual and textual knowledge data), whereas all prior methods are restricted to using text-only knowledge.",
            "score": 0.4353487993929209,
            "section_title": "Related Work",
            "char_start_offset": 4933,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 246,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 503,
                    "end": 522,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 597,
                    "end": 615,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 621,
                    "end": 641,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 651,
                    "end": 676,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76220703125
        },
        {
            "corpus_id": "274326044",
            "title": "Advancements and Applications of Large Language Models in Natural Language Processing: A Comprehensive Review",
            "text": "Abstract. Large language models (LLMs) have revolutionized the field of natural language processing (NLP), demonstrating remarkable capabilities in understanding, generating, and manipulating human language. This comprehensive review explores the development, applications, optimizations, and challenges of LLMs. This paper begin by tracing the evolution of these models and their foundational architectures, such as the Transformer, GPT, and BERT. We then delve into the applications of LLMs in natural language understanding tasks, including sentiment analysis, named entity recognition, question answering, and text summarization, highlighting real-world use cases. Next, we examine the role of LLMs in natural language generation, covering areas such as content creation, language translation, personalized recommendations, and automated responses. We further discuss LLM applications in other NLP tasks like text style transfer, text correction, and language model pre-training. Subsequently, we explore techniques for optimizing and improving LLMs, including model compression, explainability, robustness, and security. Finally, we address the challenges posed by the significant computational requirements, sample inefficiency, and ethical considerations surrounding LLMs. We conclude by discussing potential future research directions, such as efficient architectures, few-shot learning, bias mitigation, and privacy-preserving techniques, which will shape the ongoing development and responsible deployment of LLMs in NLP.",
            "score": 0.43523911040946517,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6328125
        },
        {
            "corpus_id": "275820489",
            "title": "The machine learning platform for developers of large systems",
            "text": "Developing new computing systems is a complex endeavor due to changing technology, constantly evolving requirements during development and system maintenance during their lifetime. The high volume of maintenance work after deployment includes troubleshooting, patching, updating, and modifying components to accommodate new features or security requirements. Investigating unusual events might include scanning system descriptions, the archives of administrator records, administrative orders, official recommendations, system logs, etc. The main aim is to keep the investigation time within reasonable limits. \n\nThe progress in artificial neural networks (ANNs) and large language models looked promising approaches to address the above challenges. The appropriate architecture for achieving this is Retrieval-Augmented Generation (RAG) [1,2]. RAG combines the strengths of large language models (LLMs) with specific knowledge about the local system. Such a service is available on the Internet; however, not every development authority might permit to send all technical details to a remote Internet portal. Local RAG is also handy in security contexts, where real-time access to local system logs, administrator experience records, and detailed component descriptions is essential for accurate analysis and decision-making. At the same time, those data must not be desired with any system outside the local organization. RAG includes several components: External Knowledge Source (local documents), Embedding Model which converts the query and local documents into vectors; Retriever, which searches the data in documents most relevant to a user query; Language Model (generator), which generates the final answer taking into account user query, the most relevant part of local documents, and Prompt Template, which instructs the language model (LM) what to do. \n\nIn general, it is possible to use a simple schema: \n\n\u2022 The administrator can enter a question (or statement) by typing one in natural language. \n\n\u2022 The LM generates the answer (inference) in natural language, using data from local databases and archives. \n\nIn [3], some experience with RAG architecture in computing networks was observed, and [4] has even more advanced ideas. It seems important to determine the main conditions under which simple (na\u00efve) RAG architecture would be helpful.",
            "score": 0.4326668557461624,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 610
                },
                {
                    "start": 613,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1864
                },
                {
                    "start": 1867,
                    "end": 1917
                },
                {
                    "start": 1920,
                    "end": 2010
                },
                {
                    "start": 2013,
                    "end": 2121
                },
                {
                    "start": 2124,
                    "end": 2243
                },
                {
                    "start": 2244,
                    "end": 2357
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.348388671875
        },
        {
            "corpus_id": "273119297",
            "title": "A graph-based approach to closed-domain natural language generation",
            "text": "Graph-based Natural Language Processing (NLP) methods have seen significant advancements in recent years with the development of Large Language Models (LLMs) and Retrieval Augmented Generation (RAG). LLMs are sophisticated models that recognize numerous NLP tasks by analyzing the users' natural language instructions called prompts. However, their industrial use is questionable due to such ethical concerns as false information generation called hallucinations, high risks of data breaches, and plagiarism. The paper introduces a novel NLP architecture, the Graph-Based Block-to-Block Generation (G3BG), which leverages state-of-the-art deep learning techniques, the power of attention mechanisms, distributional semantics, graph-based information retrieval, and decentralized networks. The model encodes user prompts to mitigate data breach risk, retrieves relevant information from a graph knowledge base, and forms a block for a conditional language model using LLMs to perform a new secure type of RAG. The model is closed-domain and small-scale oriented. It exhibits superior performance across low-resource NLP tasks, which makes it prominent for industrial use. The research presents a novel graph-based dataset. The dataset comprises private data features to encode and closed-domain textual information for information retrieval. The dataset is used to train and evaluate the G3BG model. The model allows cutting 100x training dataset volume achieving Perplexity ~6.51 on the Language Generation task and F1-Score ~90.3 on the Information Retrieval task comparable to most state-of-the-art language models. The experimental results prove the effectiveness of the proposed method and contribute to the algorithmic approaches toward LLM risk mitigation.",
            "score": 0.43144239419214214,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.419921875
        },
        {
            "corpus_id": "268031947",
            "title": "Retrieval is Accurate Generation",
            "text": "We compare the proposed method with standard LM in the zero-shot setting, also drawing the following state-of-the-art retrieval-augmented methods as baselines: \n\nBase LM is the standard token-level language model using the Transformer (Vaswani et al., 2017) architecture. We fine-tune the pre-trained GPT-25 (Radford et al., 2019). \n\nkNN-LM (Khandelwal et al., 2020) is a retrieval-augmented LM that interpolates the next-token distribution of the base LM with a k-nearest neighbors (kNN) model. \n\nRETRO (Borgeaud et al., 2022) 6 is a retrieval-augmented LM incorporated with a pre-trained document retriever, a document encoder and a cross-attention mechanism. \n\nCoG (Lan et al., 2023) 7 is another retrieval-augmented LM that adopts a two-stage search pipeline. It first retrieves semantically-relevant documents, and then considers all n-grams within them as candidate phrases.",
            "score": 0.4310919816026011,
            "section_title": "BASELINES",
            "char_start_offset": 17622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 162,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 331
                },
                {
                    "start": 334,
                    "end": 495
                },
                {
                    "start": 498,
                    "end": 661
                },
                {
                    "start": 664,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 880
                }
            ],
            "ref_mentions": [
                {
                    "start": 235,
                    "end": 257,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 308,
                    "end": 330,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 341,
                    "end": 365,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 504,
                    "end": 527,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 668,
                    "end": 686,
                    "matchedPaperCorpusId": "259298789"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.763671875
        },
        {
            "corpus_id": "231717977",
            "title": "Adaptive Semiparametric Language Models",
            "text": "This class of models temporarily stores M past hidden states (typically, in the order of thousands), so it is a working-memory model as opposed to long-term memory. In addi- tion, they also rely on interpolating probabilities of a backbone language model and a cache component (similar to kNN-LN when the cache size is unbounded). \n\nOther retrieval augmented methods. An early version of a neural language model that includes a retrieval component is presented in Guu et al. (2018). They follow a retrieve-then-edit approach to generate a sentence, which requires approximating an expectation over an edit prior. \n\nOutside language modeling, there are several recent retrieval-augmented methods that have been used for question answering (de Masson d' Autume et al., 2019;Guu et al., 2020;Xiong et al., 2021;Kassner and Schutze, 2020), controllable generation (Xu et al., 2020), machine translation (Bapna and Firat, 2019;Khandelwal et al., 2021), and oneshot learning (Kaiser et al., 2017). These methods share some similarities with our proposed model since it involves a retrieval component. However, the difference in the downstream tasks (language modeling vs. question answering vs. machine translation), results in different items that are stored in and retrieved from the key-value database. 2021) use source and target sentences. Our gating mechanism resembles the gate that is used to incorporate information from a non-parametric memory component to a machine translation model in Bapna and Firat (2019), although the memory entries, the decoder architecture, and the downstream task are different. \n\nIn addition, these models are only models of long-term memory. Their evaluation tasks often do not need working memory because the entire input sequence is short enough that it can be fed as an input to a transformer as a whole.",
            "score": 0.4304305441238103,
            "section_title": "Training details",
            "char_start_offset": 15112,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 330
                },
                {
                    "start": 333,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 612
                },
                {
                    "start": 615,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1609
                },
                {
                    "start": 1612,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1840
                }
            ],
            "ref_mentions": [
                {
                    "start": 464,
                    "end": 481,
                    "matchedPaperCorpusId": "2318481"
                },
                {
                    "start": 772,
                    "end": 789,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 789,
                    "end": 808,
                    "matchedPaperCorpusId": "221970302"
                },
                {
                    "start": 808,
                    "end": 834,
                    "matchedPaperCorpusId": "218487766"
                },
                {
                    "start": 860,
                    "end": 877,
                    "matchedPaperCorpusId": "222125036"
                },
                {
                    "start": 922,
                    "end": 946,
                    "matchedPaperCorpusId": "222125236"
                },
                {
                    "start": 969,
                    "end": 990,
                    "matchedPaperCorpusId": "12122362"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5537109375
        },
        {
            "corpus_id": "277313225",
            "title": "CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation",
            "text": "Large language models (LLMs) have transformed natural language processing (NLP), enabling a wide range of applications (Anthropic, 2024;Google, 2024;OpenAI, 2024). However, their reliance on static, pre-trained knowledge limits their ability to incorporate and reason over dynamically updated external information, particularly in knowledge-intensive domains. Retrieval-Augmented Generation (RAG) addresses this limitation by combining external retrieval with generative modeling to improve contextual understanding and response quality (Lewis et al., 2021). \n\nRecent efforts to improve RAG focus on two fronts: 1) enhancing retrieval efficiency through adaptive and modular frameworks (Gan et al., 2024;Ravuru et al., 2024;Zhang et al., 2024a); and 2) better structuring external knowledge, with graphbased RAGs emerging as a dominant approach (Edge et al., 2024;Guo et al., 2024;Potts, 2024). Despite these advancements, existing RAG architectures still face critical limitations that impact retrieval quality and response accuracy, primarily due to three key issues: 1) disruption of contextual integrity caused by the text chunking design; 2) reliance on semantic similarity rather than causal relevance for retrieval; and 3) a lack of accuracy in selecting truly relevant documents. \n\nThrough theoretical and empirical analysis, we rethink the limitations of current RAG systems by introducing a novel perspective grounded in context recall and precision metrics. Using this lens, we find that both regular and graph-based RAGs often fail to retrieve causally grounded content or accurately align it with the user query. We identify this fundamental issue as a primary reason why LLMs in RAG frameworks frequently produce seemingly relevant yet shallow responses that lack essential details. \n\nTo address these gaps, we propose CausalRAG, a novel RAG framework that integrates causal graphs to guide retrieval. By identifying cause-effect relationships within external knowledge, CausalRAG preserves contextual coherence and improves reasoning fidelity. This leads to more accurate and causally grounded responses, while reducing hallucinations and enhancing answer faithfulness. \n\nWe evaluate CausalRAG across datasets and varying context lengths, comparing it with regular and other competitive graph-based RAGs over multiple metrics. Results demonstrate that Causal-RAG achieves superior performance across different contexts.",
            "score": 0.4292505159464041,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 558
                },
                {
                    "start": 561,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1287
                },
                {
                    "start": 1290,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1796
                },
                {
                    "start": 1799,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2184
                },
                {
                    "start": 2187,
                    "end": 2341
                },
                {
                    "start": 2342,
                    "end": 2434
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.775390625
        },
        {
            "corpus_id": "273532207",
            "title": "Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination",
            "text": "While ongoing advancements in Large Language Models have demonstrated remarkable success across various NLP tasks, Retrieval Augmented Generation Model stands out to be highly effective on downstream applications like Question Answering. Recently, RAG-end2end model further optimized the architecture and achieved notable performance improvements on domain adaptation. However, the effectiveness of these RAG-based architectures remains relatively unexplored when fine-tuned on specialized domains such as customer service for building a reliable conversational AI system. Furthermore, a critical challenge persists in reducing the occurrence of hallucinations while maintaining high domain-specific accuracy. In this paper, we investigated the performance of diverse RAG and RAG-like architectures through domain adaptation and evaluated their ability to generate accurate and relevant response grounded in the contextual knowledge base. To facilitate the evaluation of the models, we constructed a novel dataset HotelConvQA, sourced from wide range of hotel-related conversations and fine-tuned all the models on our domain specific dataset. We also addressed a critical research gap on determining the impact of domain adaptation on reducing hallucinations across different RAG architectures, an aspect that was not properly measured in prior work. Our evaluation shows positive results in all metrics by employing domain adaptation, demonstrating strong performance on QA tasks and providing insights into their efficacy in reducing hallucinations. Our findings clearly indicate that domain adaptation not only enhances the models' performance on QA tasks but also significantly reduces hallucination across all evaluated RAG architectures.",
            "score": 0.42907892905864997,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "273929348",
            "title": "An Innovative Solution to Design Problems: Applying the Chain-of-Thought Technique to Integrate LLM-Based Agents With Concept Generation Methods",
            "text": "The RAG refers to a hybrid AI model architecture for natural language processing that combines retrieval and generation mechanisms [20]. The key advantage offered by RAG models lies in their ability to integrate retrieved external knowledge. When the agent is presented with a question, it first uses a retrieval system (such as dense passage retrieval (DPR)) to retrieve the most relevant texts from an augmented design knowledge base. These retrieved documents are then input as contextual information into a sequence-to-sequence generation model. This mechanism enables the RAG to generate more accurate and relevant outputs in response to complex scenarios, to expand the knowledge boundaries of LLMs, and to provide up-to-date information [23]. Previous researchers have used this technology primarily to create question-answering systems for the purpose of solving complex problems in specific domains. For example, Xi et al. [20] used RAG technology to construct an enterprise knowledge management system solution, and Balaguer et al [24] reported that combining an RAG with fine-tuning in LLMs can significantly improve the model's accuracy when answering domain-specific questions. \n\nTherefore, this study focuses on RAG technology, relies on the expansion of excellent design cases as a dynamic retrieval case library, and uses famous design cases based on certain principles and methods as design knowledge. This approach allows the agent to generate responses or complete specified tasks by integrating an enhanced context.",
            "score": 0.4280117044024063,
            "section_title": "1) EXPANDING DESIGN KNOWLEDGE RETRIEVAL",
            "char_start_offset": 8668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1190
                },
                {
                    "start": 1193,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1535
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.625
        },
        {
            "corpus_id": "256358593",
            "title": "Semi-Parametric Video-Grounded Text Generation",
            "text": "Semi-parametric language models show impressive success on many knowledge-intensive NLP tasks such as opendomain question answering and fact varification (Guu et al., 2020;Lewis et al., 2020;Izacard & Grave, 2021;Izacard et al., 2022), or language modeling (Khandelwal et al., 2019;Borgeaud et al., 2021). The semi-parametric model often consists of a non-parametric module, i.e., a retriever, and a parametric generator. This approach assumes large external knowledge such as Wikipedia or other large text corpora. The non-parametric retriever returns top-k relevant knowledge for a given input from the large data store. The retrieval is often based on a maximum inner product search (MIPS) within pre-computed vectors of the data store (Johnson et al., 2019). Then, the parametric generator effectively aggregates the knowledge with the given input. The nonparametric module has several useful properties like controllability, explainability, and debuggability by providing the origin of model decisions. Meanwhile, the parametric model provides better empirical performances than the nonparametric model. Combining the best of two worlds, a semi-parametric architecture is limitedly adopted for protein structure prediction (Jumper et al., 2021), image generation (Blattmann et al., 2022), and image-text QA (Chen et al., 2022;Lin & Byrne, 2022). Inspired by these studies, we adapt the retrieval-augmented generation framework to the video-language domain for the first time.",
            "score": 0.4278590721361082,
            "section_title": "Semi-Parametric Language Models",
            "char_start_offset": 7251,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1480
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 172,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 172,
                    "end": 191,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 191,
                    "end": 213,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 739,
                    "end": 761,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1228,
                    "end": 1249,
                    "matchedPaperCorpusId": "235959867"
                },
                {
                    "start": 1268,
                    "end": 1292,
                    "matchedPaperCorpusId": "253098838"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5009765625
        },
        {
            "corpus_id": "273532207",
            "title": "Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination",
            "text": "Large Language Models (LLMs) generally store a vast amount of data encoded as factual knowledge in their parameters through fine-tuning on large corpora (Lewis et al. 2020c). In recent years, LLMs have demonstrated significant advancements in natural language processing tasks including question answering, summarization, and dialogue systems. Besides, LLMs play a pivotal role in developing powerful agents, serving as essential components for reasoning and are fundamental for adaptation to new observations such as GPT in the context of Conversational AI (Wu et al. 2023) (Li, Yuan, and Zhang 2024a). Recently, Retrieval Augmented Generation (RAG) models have demonstrated significant improvements compared to LLMs by leveraging external knowledge sources because of its capability to combine pretrained parametric memory with explicit non-parametric memory (Lewis et al. 2020c) (Xu et al. 2024) (Johnson et al. 2023). Unlike the inherent builtin knowledge architectures of Large Language Models with limited access to external knowledge, RAG models have proven to be a promising approach for generating informative and context-aware responses. Recent advancements in the field of retrieval-augmented generation models have shown great potential for improving the factual accuracy and coherence of language models' responses rather than fine-tuning the entire LLM repeatedly (Rackauckas 2024). \n\nMoreover, similar retrieval augmented architectures like REALM (Guu et al. 2020) utilizes a fixed retriever component during the fine-tuning process and RETRO (Borgeaud et al. 2021) suggested that frozen BART embedding will be adequate for general-purpose retrieval from Wikipedia-like datasets. On the other hand, recent advancement on the original architecture of RAG, RAG-end2end (Siriwardhana et al. 2023) observed that updating both the passage encoder and query encoder improves overall performance substantially in QA tasks across various domains like Covid-19, News etc. However, their works have been primarily focused on the role of domain-adapted retrievers without exploring any further impact of context awareness while achieving domain adaptation beyond general-purpose datasets.",
            "score": 0.4268795056642635,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2192
                }
            ],
            "ref_mentions": [
                {
                    "start": 882,
                    "end": 898,
                    "matchedPaperCorpusId": "267938726"
                },
                {
                    "start": 1462,
                    "end": 1478,
                    "matchedPaperCorpusId": "252992904"
                },
                {
                    "start": 1782,
                    "end": 1807,
                    "matchedPaperCorpusId": "252735056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67919921875
        },
        {
            "corpus_id": "268248911",
            "title": "Reliable, Adaptable, and Attributable Language Models with Retrieval",
            "text": "This section briefly reviews the applications of retrieval-augmented LMs and used datastores. \n\nApplications. Retrieval-augmented LMs are shown to be effective across a range of NLP tasks, including discriminative and generative tasks. The majority of prior work is often evaluated on knowledge-intensive tasks, such as open-domain QA (Kwiatkowski et al., 2019), fact verification (Thorne et al., 2018) and knowledge-grounding dialogue (Shuster et al., 2021). For such tasks, Wikipedia is often used as the sole knowledge source, while some recent work directly combines LMs with commercial search engine APIs. For non-knowledge-intensive tasks, the usage of training instances (labeled data) as the datastore has been widely explored, demonstrating effectiveness on tasks like machine translation (Khandelwal et al., 2021;Zhong et al., 2022). Some recent works such as kNN-Prompt (Shi et al., 2022) or NPM (Min et al., 2023b) leverage larger pre-training corpora (e.g., the Pile; Gao et al. 2020) for more general language understanding tasks (e.g., sentiment analysis) or entity translations. Yu et al. (2022) build a new large-scale corpus consisting of 20 million commonsense documents collection from both open-domain knowledge sources. Several works on code generations use similar codes (Hayati et al., 2018) or documentation (Zhou et al., 2023) of APIs. Designing and building a reliable datastore is a key challenge in retrieval-augmented LMs. Across those papers, retrieval-augmented LMs have shown significant improvements over parametric LMs. \n\nFurthermore, retrieval-augmented LMs have been applied beyond general-domain, English text data. Several works explore retrieving from multilingual data (Asai et al., 2021;Nie et al., 2023) or multiple modalities (Yasunaga et al., 2022;Chen et al., 2022)-which includes underexplored modalities such as robot controls (Zha et al., 2023).",
            "score": 0.4268795056642635,
            "section_title": "B.2. Applications and Datastore",
            "char_start_offset": 42413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 96,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1554
                },
                {
                    "start": 1557,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1894
                }
            ],
            "ref_mentions": [
                {
                    "start": 335,
                    "end": 361,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 381,
                    "end": 402,
                    "matchedPaperCorpusId": "4711425"
                },
                {
                    "start": 436,
                    "end": 458,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 798,
                    "end": 823,
                    "matchedPaperCorpusId": "222125236"
                },
                {
                    "start": 823,
                    "end": 842,
                    "matchedPaperCorpusId": "249062699"
                },
                {
                    "start": 881,
                    "end": 899,
                    "matchedPaperCorpusId": "249152130"
                },
                {
                    "start": 907,
                    "end": 926,
                    "matchedPaperCorpusId": "254220735"
                },
                {
                    "start": 1095,
                    "end": 1111,
                    "matchedPaperCorpusId": "253098034"
                },
                {
                    "start": 1294,
                    "end": 1315,
                    "matchedPaperCorpusId": "52136345"
                },
                {
                    "start": 1333,
                    "end": 1352,
                    "matchedPaperCorpusId": "252734952"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6025390625
        },
        {
            "corpus_id": "235593085",
            "title": "Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering",
            "text": "In September 2020, Facebook open-sourced a new NLP model called Retrieval Augmented Generation (RAG) on the Hugging Face Transformer library. RAG is capable to use a set of support documents from an external knowledge base as a latent variable to generate the final output. The RAG model consists of an Input Encoder, a Neural Retriever, and an Output Generator. All three components are initialized with pre-trained transformers. However, the original Hugging Face implementation only allowed fine-tuning the Input Encoder and the Output Generator in an end-toend manner, while the Neural Retriever needs to be trained seperately. To the best of our knowledge, an end-to-end RAG implementation that trains all three components does not exist. In this paper, we introduce a novel approach to extending the RAG implementation which fulfills the task of training all three components end-to-end. Although this appears straightforward, there are many engineering challenges that need to be addressed. \n\nAs the name suggests, RAG adds an intermediate information retrieval step before the final generation process. It combines the ideas of neural information retrieval with seq2seq generation in an end-to-end manner. During the training phase, RAG takes an input and encodes it to a high-dimensional vector. This vector then gets used as a query to select the most relevant passages from an external database using Maximum Inner Product Search (MIPS) [Johnson et al., 2017, Guo et al., 2020]. Afterward, The input and the selected set of documents get fed into a generator that produces the final answer. It is important to emphasize that components of RAG are being initialized with pre-trained BERT and BART transformers. Finally, The probability of selecting documents given an input context (p(z|x)) is used to propagate gradients back to the question encoder. During the process1 . of making the RAG end-to-end trainable, we mainly work on the Retriever part, which uses a neural retriever model called Dense Passage Retrieval (DPR) [Karpukhin et al., 2020]. \n\nOur main contributions can be summarized as follows: \n\n\u2022 Highlighting the importance of making entire RAG architecture end-to-end trainable. \n\n\u2022 Exploration of engineering challenges regarding the extension of original RAG.",
            "score": 0.4263709163282725,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2060
                },
                {
                    "start": 2063,
                    "end": 2115
                },
                {
                    "start": 2118,
                    "end": 2203
                },
                {
                    "start": 2206,
                    "end": 2286
                }
            ],
            "ref_mentions": [
                {
                    "start": 1469,
                    "end": 1488,
                    "matchedPaperCorpusId": "218614141"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60791015625
        },
        {
            "corpus_id": "271162228",
            "title": "Mitigating Entity-Level Hallucination in Large Language Models",
            "text": "In recent years, large language models (LLMs) have achieved remarkable success in a variety of natural language processing (NLP) tasks and have already become an indispensable component of many AI applications [2,5,32,43,50].Due to the outstanding capabilities and wide applications of LLMs, they have revolutionized how users access information, shifting from traditional search engines to direct question-and-answer interactions with LLMs.However, despite their impressive performance, it is well recognized that existing LLMs may generate text that, while appearing coherent and plausible on the surface, is fundamentally inaccurate or lacks grounding in reality.This phenomenon is commonly referred to as LLM hallucination [13,23,28,41,51].\n\nTo address hallucination, the most popular method adopted in existing studies is Retrieval-Augmented Generation (RAG).In this method, relevant knowledge is retrieved from an external corpus and utilized as input for LLMs, which has been proven to be effective in many NLP tasks [1,14,16,18,38,40].Traditional RAG methods often adopt single-round retrieval that uses the original input of LLM as the query to retrieve relevant external information.Although this method is effective for simple tasks, it frequently fails in complex tasks like long-form generation and multi-hop questionanswering.This is primarily because the LLM needs continuously changing information in these complex text generation tasks [15].In contrast, multi-round RAG [1,15,29,44] performs multiple retrievals during the generation process of LLMs.Depending on the timing of retrieval augmentation, various methods have been proposed in this area.For example, RETRO [1] and IC-RALM [29] trigger the retrieval module based on a pre-defined number of generated tokens.FLARE [15] triggers the retrieval module whenever a token's predictive probability is below a certain threshold.DRA-GIN [40] defines an empirical method based on uncertainty and self-attention to determine when to trigger retrieval.\n\nDespite these advancements, a significant oversight remains: none of the existing studies explicitly verify whether the retrieval is triggered at an optimal timing.",
            "score": 0.42529340418569267,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 225,
                    "end": 441
                },
                {
                    "start": 441,
                    "end": 666
                },
                {
                    "start": 666,
                    "end": 744
                },
                {
                    "start": 746,
                    "end": 864
                },
                {
                    "start": 864,
                    "end": 1043
                },
                {
                    "start": 1043,
                    "end": 1193
                },
                {
                    "start": 1193,
                    "end": 1340
                },
                {
                    "start": 1340,
                    "end": 1458
                },
                {
                    "start": 1458,
                    "end": 1567
                },
                {
                    "start": 1567,
                    "end": 1666
                },
                {
                    "start": 1666,
                    "end": 1785
                },
                {
                    "start": 1785,
                    "end": 1897
                },
                {
                    "start": 1897,
                    "end": 2017
                },
                {
                    "start": 2019,
                    "end": 2183
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 213,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 727,
                    "end": 731,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 1024,
                    "end": 1027,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1033,
                    "end": 1036,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1487,
                    "end": 1490,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1685,
                    "end": 1688,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7080078125
        },
        {
            "corpus_id": "269137180",
            "title": "Reducing hallucination in structured outputs via Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation is a common approach to limit generation of false or outdated information in classical NLP tasks such as question answering and summarization (Lewis et al., 2020;Izacard and Grave, 2021;Shuster et al., 2021).In the GenAI era, it refers to a process where relevant information from specific data sources is retrieved prior to generating text; the generation is then based on this retrieved information (Gao et al., 2024).Our work differs from standard RAG as we apply it to a structured output task.Instead of retrieving facts, we retrieve JSON objects that could be part of the JSON output document.Providing plausible JSON objects to the LLM before generation increases the likelihood that the output JSON properties exist and that the generated JSON can be executed.\n\nA crucial ingredient of RAG is the retriever since its output will be part of the LLM input.Compared to classical methods such as TF-IDF or BM25 that use lexical information, Dense Retrieval has been shown to be more effective as it maps the semantics to a multidimensional space where both queries and documents are represented (Reimers and Gurevych, 2019;Gao et al., 2021;Karpukhin et al., 2020;Xiong et al., 2020).These retrievers are often used in open-domain question answering systems (Guu et al., 2020;Lee et al., 2019), where both queries and documents are unstructured data and thus share the same semantic space.In our case, the queries are unstructured (natural language) and the documents (JSON objects) are structured.Our retrieval training is similar to Structure Aware DeNse ReTrievAl (SANTA), which proposes a training method to align the semantics between code and text (Li et al., 2023b).",
            "score": 0.425211672666966,
            "section_title": "Related Work",
            "char_start_offset": 3108,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 451
                },
                {
                    "start": 451,
                    "end": 529
                },
                {
                    "start": 529,
                    "end": 630
                },
                {
                    "start": 630,
                    "end": 799
                },
                {
                    "start": 801,
                    "end": 893
                },
                {
                    "start": 893,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1423
                },
                {
                    "start": 1423,
                    "end": 1532
                },
                {
                    "start": 1532,
                    "end": 1707
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 193,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 193,
                    "end": 217,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 217,
                    "end": 238,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 1130,
                    "end": 1158,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1158,
                    "end": 1175,
                    "matchedPaperCorpusId": "233296292"
                },
                {
                    "start": 1175,
                    "end": 1198,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1198,
                    "end": 1217,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1292,
                    "end": 1310,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1310,
                    "end": 1327,
                    "matchedPaperCorpusId": "173990818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.401611328125
        },
        {
            "corpus_id": "265158225",
            "title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey",
            "text": "Retrieval-augmented generation models like RAG (Lewis et al., 2020) and RALM (Ram et al., 2023) enhance LLMs' contextual awareness for knowledge-intensive tasks by providing relevant documents during generation, reducing hallucination without altering the LLM architecture. These methods, which are helpful for tasks needing external knowledge, augment top-k relevant documents to inputs. However, as shown in Figure 3, using well-organized, curated knowledge from structured sources or knowledge graphs, aligns more closely with factual accuracy. Baek et al. (Baek et al., 2023) introduced KAPING, which matches entities in questions to retrieve related triples from knowledge graphs for zero-shot question answering. Wu et al. (Wu et al., 2023) found that converting these triples into textualized statements enhances LLM performance. Sen et al. (Sen et al., 2023) developed a retriever module trained on a KGQA model, addressing the inadequacy of similarity-based retrieval for complex questions. StructGPT (Jiang et al., 2023) augments LLMs with data from knowledge graphs, tables, and databases, utilizing structured queries for information extraction. Other notable works include IAG (Zhang et al., 2023b), KICGPT (Wei et al., 2023), and SAFARI (Wang et al., 2023b). \n\nLLMs serve as natural language interfaces, ex-Figure 3: Knowledge-aware inference by incorporating KG-augmented retrieval (Baek et al., 2023). \n\ntracting and generating information without relying on their internal knowledge. Tools like the ChatGPT plugin use Langchain (Chase, 2022) and LlamaIndex (Liu, 2022) to integrate external data, prompting LLMs for context-retrieved, knowledgeaugmented outputs. However, relying solely on internal databases can limit performance due to restricted knowledge bases. Mallen et al. (Mallen et al., 2023) investigated LLMs' factual knowledge retention, finding that augmenting with retrieved data improves performance. However, these models perform well with popular entities and relations but face challenges with less popular subjects, and increasing model size doesn't improve their performance in such cases.",
            "score": 0.42493415497390674,
            "section_title": "KG-Augmented Retrieval",
            "char_start_offset": 6997,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1272
                },
                {
                    "start": 1275,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2126
                }
            ],
            "ref_mentions": [
                {
                    "start": 47,
                    "end": 67,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 77,
                    "end": 95,
                    "matchedPaperCorpusId": "256868474"
                },
                {
                    "start": 1190,
                    "end": 1211,
                    "matchedPaperCorpusId": "265506019"
                },
                {
                    "start": 1220,
                    "end": 1238,
                    "matchedPaperCorpusId": "266176753"
                },
                {
                    "start": 1797,
                    "end": 1818,
                    "matchedPaperCorpusId": "254877603"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5478515625
        },
        {
            "corpus_id": "250391000",
            "title": "Retrieval-augmented Generation across Heterogeneous Knowledge",
            "text": "The way of augmenting the input of PLMs with external information is often referred to as open-book setting (Mihaylov et al., 2018). A prominent method in the open-book setting is retrieval-augmented generation (RAG) (Lewis et al., 2020b;Yu et al., 2022c), a new learning paradigm that fuses PLMs and traditional IR techniques, which has achieved state-of-the-art performance in many knowledgeintensive NLP tasks (Petroni et al., 2021). Compared with large-scale PLMs counterparts, e.g., GPT-3, the RAG model has some remarkable ad-vantages: (i) The knowledge is not implicitly stored in model parameters, but is explicitly acquired in a plug-and-play manner, leading to great scalability; (ii) Instead of generating from scratch, the model generates outputs based on some retrieved references, which eases the difficulty of text generation. \n\nAlthough the RAG models have been widely used in the existing literature, most of the work has focused on retrieving unstructured text from general domain corpus, e.g., Wikipedia. However, the performance is often limited by the coverage of only one certain knowledge. For example, only a finite portion of questions could be answered from Wikipedia passages in many open-domain QA datasets, while the remaining could only rely on the input question because no supportive documents could be retrieved (Oguz et al., 2022). In this paper, I first elaborate on the current obstacles to retrieving knowledge from a single-source homogeneous corpus. Then, I demonstrate several pieces of evidence from both existing literature and my own experiments, and provide multiple potential solutions on retrieval-augmented generation methods across heterogeneous knowledge. \n\n2 Background I will first provide a formal definition of the RAG framework and list necessary notations. RAG aims to predict the output y based on the source input x (x, y are from a corpus D), while a document reference set Z is accessible (e.g., Wikipedia).",
            "score": 0.42467255172780766,
            "section_title": "Introduction",
            "char_start_offset": 1600,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 841
                },
                {
                    "start": 844,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1704
                },
                {
                    "start": 1707,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 131,
                    "matchedPaperCorpusId": "52183757"
                },
                {
                    "start": 238,
                    "end": 255,
                    "matchedPaperCorpusId": "222272210"
                },
                {
                    "start": 413,
                    "end": 435,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 1345,
                    "end": 1364,
                    "matchedPaperCorpusId": "235399987"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7041015625
        },
        {
            "corpus_id": "274023710",
            "title": "Adopting RAG for LLM-Aided Future Vehicle Design",
            "text": "This is because the Bi-encoder and Cross-encoder models used in the are optimized for fast retrieval, regardless of document size. These findings highlight the scalability of the search process and emphasize the advantage of using RAG methods to maintain fast response times, even in large-scale Question-Answering systems. In this study, we explored the integration of Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) to enhance design and software development in the automotive industry. Our research demonstrates that LLMs, when augmented with RAG, can effectively address the challenge of handling non-disclosable data in automotive domains. We implemented two case studies: a standardization compliance chatbot and a design copilot, leveraging RAG to provide context-aware and accurate responses. \n\nThis study demonstrates that current locally deployable open-source Large Language Models (LLMs) are still behind the commercial ChatGPT's underlying GPT-4 when it comes to providing the necessary accuracy for question-and-answer tasks in the automotive industry using Retrieval Augmented Generation (RAG) technology. While the outcomes of models like LLAMA3 and Mistral seem more promising, they do not yet meet the stringent requirements for reliable performance in this specialized field. However, GPT-4 stands out by providing almost completely correct answers, indicating that proprietary models are currently more capable than opensource alternatives. \n\nFuture work should focus on improving the accuracy and reliability of open-source models to better support automotive applications by performing model fine tuning. Additionally, we will aim to extend the evaluation by testing additional open-source and commercial LLMs in order to identify the most effective models for various automotive applications. The effect of different bi-encoder and cross-encoder models on the performance of the RAG pipeline will also be evaluated, exploring models with varying architectures and training datasets to optimize retrieval and generation processes. Finally, the current manual assessment of LLM outputs against ground truth answers will be automated using semantic comparison models, such as cross-encoder models, to enhance accuracy and efficiency.",
            "score": 0.42458390557098075,
            "section_title": "V. EXPERIMENTS AND EVALUATION",
            "char_start_offset": 18715,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1483
                },
                {
                    "start": 1486,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2276
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5009765625
        },
        {
            "corpus_id": "269188036",
            "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
            "text": "To provide clarity and structure, this paper is organized as follows: Section 2 outlines the overall RAG workflow, dividing the methodologies into pre-retrieval, retrieval, post-retrieval, and generation phases. Sections 3 through 6 explore the core techniques within each phase. Section 7 focuses on the evaluation methodologies for RAG. Section 8 summarizes the reviewed studies, detailing the retrievers and generators used, while Section 9 discusses challenges and future research directions, extending beyond text-based studies to include multimodal data applications. The paper concludes with Section 10. \n\nOther related surveys provide valuable insights into the evolving RAG landscape from different angles. Gao et al. [38] identified three key stages in RAG development: pre-training enhancement, inference, and fine-tuning. Zhao et al. [162] focused on the diverse applications of RAG, including text, code, image, and video generation, emphasizing augmented intelligence in generative tasks. Meanwhile, Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how interactions between retrievers, language models, and augmentations influence model architectures and applications. \n\nIn this paper, we aim to offer a comprehensive and unified framework for understanding RAG from an information retrieval (IR) perspective, identifying key challenges and areas for improvement. We delve into the core technologies that drive RAG, assessing their effectiveness in addressing retrieval and generation tasks. Additionally, this survey introduces the evaluation methods employed in RAG research, highlights current limitations, and proposes promising avenues for future exploration.",
            "score": 0.42433399003964706,
            "section_title": "Introduction",
            "char_start_offset": 2330,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 610
                },
                {
                    "start": 613,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1212
                },
                {
                    "start": 1215,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1708
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77880859375
        },
        {
            "corpus_id": "274965455",
            "title": "Accelerating Retrieval-Augmented Generation",
            "text": "An evolving solution to address hallucination and enhance accuracy in large language models (LLMs) is Retrieval-Augmented Generation (RAG), which involves augmenting LLMs with information retrieved from an external knowledge source, such as the web. This paper profiles several RAG execution pipelines and demystifies the complex interplay between their retrieval and generation phases. We demonstrate that while exact retrieval schemes are expensive, they can reduce inference time compared to approximate retrieval variants because an exact retrieval model can send a smaller but more accurate list of documents to the generative model while maintaining the same end-to-end accuracy. This observation motivates the acceleration of the exact nearest neighbor search for RAG. In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL device that implements a scale-out near-memory acceleration architecture with a novel cache-coherent interface between the host CPU and near-memory accelerators. IKS offers 13.4--27.9\u00d7 faster exact nearest neighbor search over a 512GB vector database compared with executing the search on Intel Sapphire Rapids CPUs. This higher search performance translates to 1.7--26.3\u00d7 lower end-to-end inference time for representative RAG applications. IKS is inherently a memory expander; its internal DRAM can be disaggregated and used for other applications running on the server to prevent DRAM -- which is the most expensive component in today's servers -- from being stranded.",
            "score": 0.42392820410598686,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75146484375
        },
        {
            "corpus_id": "275358357",
            "title": "Knowledge Retrieval Based on Generative AI",
            "text": "Retrieval-Augmented Generation (RAG) [4] overcomes the limitations of Large Language Models (LLMs), which rely on static, pre-trained datasets that can become outdated and lack domain-specific information. This restricts LLMs' ability to generate accurate and up-to-date responses. RAG integrates Information Retrieval (IR) systems with LLMs, enabling them to query external knowledge sources and access real-time, domain-relevant data. In a typical RAG framework, a retriever processes user queries and retrieves relevant documents based on semantic similarity. These documents are then combined with the original query and passed to the LLM to generate a more accurate and comprehensive response. RAG also addresses the issue of model obsolescence by dynamically accessing updated information without retraining. However, balancing precision and recall during retrieval can be challenging, and re-ranking is required to prioritize relevant information, ensuring that LLM responses are accurate and contextually appropriate in complex queries.",
            "score": 0.4227106823263234,
            "section_title": "C. Retrieval-Augmented Generation",
            "char_start_offset": 3961,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1044
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 40,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77734375
        },
        {
            "corpus_id": "270620444",
            "title": "Improving Zero-shot LLM Re-Ranker with Risk Minimization",
            "text": "Large Language Models (LLMs) exhibit remarkable capabilities but face several challenges including hallucination and outdated knowledge (Zhao et al., 2023;Ji et al., 2023).Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating external knowledge (Ram et al., 2023;Gao et al., 2023).In the RAG system, a re-ranking model can serve as a second-pass document optimizer and refiner for the knowledge retrieval.This is particularly critical in open-domain Question Answering (QA) tasks, where it leads to large gains in performance (Karpukhin et al., 2020;Zhu et al., 2023).The re-ranker assesses the relevance of the documents retrieved by the initial retriever (e.g., BM25 (Robertson and Zaragoza, 2009)) and effectively prioritizes the most relevant items at the top.This not only enhances retrieval efficiency and responsiveness but also resolves the challenge of context window expansion by limiting the total number of documents (Gao et al., 2023).\n\nMost previous approaches trained the re-ranker on manual supervision signals with complex architectures (Karpukhin et al., 2020;Nogueira et al., 2020;Formal et al., 2021), which require significant human efforts and demonstrate weak generalizability (Nguyen et al., 2016;Izacard et al., 2021).As the size of models scales up (e.g., exceeding 10 billion parameters), it becomes increasingly difficult to fine-tune the dedicated re-ranking models.To address this challenge, recent efforts have attempted to leverage the zero-shot language understanding and generation capabilities of LLMs to directly enhance document re-ranking in an unsu-pervised way.\n\nRecent studies have explored LLMs for permutation generation (Ma et al., 2023;Sun et al., 2023) as re-rankers, which yield significant performance by generating a ranked list of a group of documents.However, these models face high time complexity with long lists and the performance is highly sensitive to the document order in the prompt.(Zhu et al., 2023).",
            "score": 0.4227106823263234,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 172,
                    "end": 317
                },
                {
                    "start": 317,
                    "end": 441
                },
                {
                    "start": 441,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 800
                },
                {
                    "start": 800,
                    "end": 984
                },
                {
                    "start": 986,
                    "end": 1279
                },
                {
                    "start": 1279,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1637
                },
                {
                    "start": 1639,
                    "end": 1838
                },
                {
                    "start": 1838,
                    "end": 1978
                },
                {
                    "start": 1978,
                    "end": 1997
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 171,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 281,
                    "end": 299,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 562,
                    "end": 586,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 705,
                    "end": 735,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1090,
                    "end": 1114,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1114,
                    "end": 1136,
                    "matchedPaperCorpusId": "212725651"
                },
                {
                    "start": 1136,
                    "end": 1156,
                    "matchedPaperCorpusId": "235792467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.491455078125
        },
        {
            "corpus_id": "269457256",
            "title": "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing",
            "text": "Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks. This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications. The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment. It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: https://github.com/2471023025/RALM_Survey.",
            "score": 0.4227106823263234,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9248046875
        },
        {
            "corpus_id": "248505738",
            "title": "Meta Learning for Natural Language Processing: A Survey",
            "text": "Neural network architecture search (NAS) is another common meta-learning technique applied to NLP including language modeling (WikiText-103 (Merity et al., 2017), PTB (Mikolov et al., 2010)), NER (CoNLL-2003 (Sang andDe Meulder, 2003)), TC (GLUE (Wang et al., 2019a)), and MT (WMT'14 (Bojar et al., 2014)). As discussed in Section 3.5, these techniques are often trained/evaluated with a single, matched dataset, which is different from other meta-learning approaches.\n\nMoreover, in contrast to conventional NAS methods that focus on learning the topology in an individual recurrent or convolutional cell, NAS methods have to be redesigned in order to make the search space suitable for NLP problems, where contextual information often plays an important role. Jiang et al. (2019) pioneers the application of NAS to NLP tasks beyond language modeling (NER in this case), and improves differentiable NAS by redesigning its search space for natural language processing. Li et al. (2020b) extends the search space of NAS to cover more RNN architectures and allow the exploring of intra-and inter-token connection to increase the expressibility of searched networks. As the popularity of pre-trained language models (PLM) grows in NLP area, researchers also apply NAS to discover better topology for PLM such as BERT.  introduces Hardware-Aware Transformers (HAT) to search Transformer architecture optimized for inference speed and memory footprint in different hardware platforms. NAS-BERT (Xu et al., 2021b) and AdaBERT (Chen et al., 2020a) explores task-agnostic and task-dependent network compression techniques with NAS respectively. Efficient- BERT (Dong et al., 2021) applies NAS to search for more efficient architecture of feed-forward network that is suitable for edge device deployment.\n\nTo show the efficacy of NAS, we summarize the performance of several state-of-the-art NAS approaches on GLUE benchmarks (Wang et al., 2019a) in Table 2. These approaches are applied to BERT to discover architectures with smaller sizes, faster inference speed, and better model accuracy. For comparison, performance",
            "score": 0.42238053726491487,
            "section_title": "Neural Network Architecture Search",
            "char_start_offset": 26271,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 234,
                    "matchedPaperCorpusId": "2470716"
                },
                {
                    "start": 246,
                    "end": 266,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 761,
                    "end": 780,
                    "matchedPaperCorpusId": "202785477"
                },
                {
                    "start": 968,
                    "end": 985,
                    "matchedPaperCorpusId": "218516935"
                },
                {
                    "start": 1488,
                    "end": 1506,
                    "matchedPaperCorpusId": "235253768"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28466796875
        },
        {
            "corpus_id": "265659387",
            "title": "RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!",
            "text": "The emergence of instruction fine-tuned large language models (LLMs) has expanded the horizon of applications in areas ranging from natural language processing to information retrieval to software engineering. This development has been particularly influential in the realm of text ranking. Notably, there has been a surge in initiatives focused on zero-shot listwise reranking using LLMs, referred to as \"prompt-decoders\", as evidenced by works such as RankGPT (Sun et al., 2023) and LRL (Ma et al., 2023b). However, a common limitation in these endeavors is their reliance on proprietary models. While these models facilitate rapid prototyping and are conveniently accessible via API endpoints, they pose challenges in terms of scientific reproducibility. This issue is critical both from the standpoint of adhering to the principles of robust scientific methodology and practically in the context of achieving consistent, reliable measurements in experimental evaluations. \n\nRecently, RankVicuna (Pradeep et al., 2023b) helped address this pressing need within the academic community for an open-source LLM that can proficiently execute reranking tasks, improving over the much larger proprietary model Rank-GPT 3.5 . However, RankVicuna still lags behind the state-of-the-art RankGPT 4 in effectiveness. Bridging this gap and striving beyond with an opensource model would be of great value to the NLP and IR communities working towards RAG architectures that require high-precision results. \n\nThis paper introduces RankZephyr, an opensource LLM for reranking that bridges the gap and, in a few cases, goes beyond RankGPT 4 , the stateof-the-art reranker. Our investigation is steered by several research questions, informed by comprehensive experiments and analyses: \n\n\u2022 Can an open-source LLM, specifically Rank-Zephyr with only 7B parameters, improve on the listwise reranking effectiveness of much larger proprietary models such as RankGPT 4 in a zeroshot setting? \n\n\u2022 What is the impact on effectiveness of employing multiple reranking passes over the candidate list?",
            "score": 0.4219492135071555,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1495
                },
                {
                    "start": 1498,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1771
                },
                {
                    "start": 1774,
                    "end": 1972
                },
                {
                    "start": 1975,
                    "end": 2076
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.329345703125
        },
        {
            "corpus_id": "276422200",
            "title": "Efficient OpAmp Adaptation for Zoom Attention to Golden Contexts",
            "text": "Recent advancements in large language models (LLMs) (OpenAI, 2023;Dubey et al., 2024;Yang et al., 2024;Liu et al., 2024a) have demonstrated remarkable capabilities in understanding, generating, and reasoning across diverse domains, significantly advancing their application in various fields. Among these applications, question answering (QA) based on provided contexts has emerged as one of the most prominent use cases for LLMs. \n\nAs LLMs' capabilities continue to evolve and user expectations grow, users increasingly sup- ply multiple documents retrieved in Retrieval-Augmented Generation (RAG) scenarios or longcontext reference documents to guide LLMs in generating contextually relevant responses. However, in practice, such retrieved documents or longcontext references often contain substantial noise, including information irrelevant to the user's query. Recent studies (Ye et al., 2025;Liu et al., 2024b) highlight a critical challenge that LLMs frequently struggle to accurately identify and extract key information from these noisy contexts, limiting their effectiveness in real-world applications. \n\nAs illustrated in Figure 1, we visualize the normalized attention scores assigned to retrieved documents in the RAG scenario, which includes various noisy documents and a single golden document. The task involves identifying the correct answer within noisy contexts. Our analysis evaluates several LLMs, including Llama3.1-8Bbase (Meta, 2024), Llama3.1-8B-inst (Meta, 2024), and Llama3-ChatQA2-8B (Xu et al., 2024), the latter of which has been fine-tuned specifically for long-context and RAG applications. The visualization demonstrates that the Transformer architecture tends to allocate only a small proportion of attention scores to the golden document, while disproportionately focusing on irrelevant or laterpositioned documents. These findings highlight a persistent challenge for Transformer-based architectures including effectively identifying and prioritizing relevant documents in the presence of noise. The issue (Ye et al., 2025) arises from the non-negligible allocation of attention scores to irrelevant content, which ultimately obscures the correct answer and undermines model performance.",
            "score": 0.42132913338708766,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 430
                },
                {
                    "start": 433,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1111
                },
                {
                    "start": 1114,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2222
                }
            ],
            "ref_mentions": [
                {
                    "start": 880,
                    "end": 897,
                    "matchedPaperCorpusId": "110224670"
                },
                {
                    "start": 897,
                    "end": 915,
                    "matchedPaperCorpusId": "259360665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56591796875
        },
        {
            "corpus_id": "246240957",
            "title": "Artefact Retrieval: Overview of NLP Models with Knowledge Base Access",
            "text": "Many NLP models gain performance by having access to a knowledge base. A lot of research has been devoted to devising and improving the way the knowledge base is accessed and incorporated into the model, resulting in a number of mechanisms and pipelines. Despite the diversity of proposed mechanisms, there are patterns in the designs of such systems. In this paper, we systematically describe the typology of artefacts (items retrieved from a knowledge base), retrieval mechanisms and the way these artefacts are fused into the model. This further allows us to uncover combinations of design decisions that had not yet been tried. Most of the focus is given to language models, though we also show how question answering, fact-checking and knowledgable dialogue models fit into this system as well. Having an abstract model which can describe the architecture of specific models also helps with transferring these architectures between multiple NLP tasks.",
            "score": 0.420401177688313,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75244140625
        },
        {
            "corpus_id": "273234268",
            "title": "No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in LLMs, Even for Vigilant Users",
            "text": "A.1 Retrieval Before retrieval, external documents must first be processed from raw data into a list of small, noticeable chunks that can be efficiently handled by language models. Since external data sources may vary significantly in format, it is necessary to align these sources into uniform, context-rich chunks. Following this, an embedding model is employed to encode the chunks, creating embeddings that facilitate the indexing [56]. From the perspective of encoding mechanisms, retrieval methods can be broadly categorized into two types: sparse and dense, depending on how the information is encoded [5]. Sparse methods rely on explicit term matching, while dense methods leverage learned embeddings to capture deeper semantic relationships within the data. Sparse retrieval is primarily word-based and widely employed in text retrieval tasks. Classical approaches such as TF-IDF and BM25 [57] rely on inverted index matching to identify relevant documents. BM25, in particular, is often applied from a macro perspective, where entire passages are treated as singular retrieval units [58,59,60], [61]. However, a key limitation of sparse retrieval in the context of RAG is its untrained nature, leading to retrieval performance highly dependent on both the quality of the data source and the specificity of the query. In contrast, dense retrieval encodes user queries and external knowledge into vector representations, enabling application across a wide range of data formats [62]. Simple dense retrieval methods [63] compute similarity scores between the query vector and the vectors of indexed chunks, retrieving the top K similar chunks to the query. These retrieved chunks are then incorporated as an extended context within the prompt, facilitating more accurate and contextually relevant responses. \n\nEmbedding models are a crucial component of dense retrieval systems. A straightforward approach involves utilizing off-the-shelf NLP models. BERT-based architectures [64] are commonly employed in retrieval models. A prevalent design within RAG frameworks involves constructing bi-encoders with the BERT structure-one encoder dedicated to processing queries and the other for documents [65,66]. Further advancements in RAG models are achieved through large-scale specialized pre-training, which enhances their performance on knowledge-intensive tasks.",
            "score": 0.41999374679511403,
            "section_title": "A More details of Retrieval and Generation",
            "char_start_offset": 32867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1814
                },
                {
                    "start": 1817,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2210
                },
                {
                    "start": 2211,
                    "end": 2367
                }
            ],
            "ref_mentions": [
                {
                    "start": 609,
                    "end": 612,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 898,
                    "end": 902,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1486,
                    "end": 1490,
                    "matchedPaperCorpusId": "254044526"
                },
                {
                    "start": 1523,
                    "end": 1527,
                    "matchedPaperCorpusId": "244714119"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5615234375
        },
        {
            "corpus_id": "273022770",
            "title": "Optimizing Token Usage on Large Language Model Conversations Using the Design Structure Matrix",
            "text": "Large Language Models (LLMs) have been around half a decade already, but it was not until the end of 2022 that gained widespread public interest when OpenAI released the ChatGPT interface, making their GPT-3 LLM (Brown et al., 2020) easily accessible to the masses. The technology behind LLMs, the transformer architecture, was introduced by Google in 2017, bringing together tokenization, word embeddings, and the novel attention mechanism (Vaswani et al., 2017) assigning different importance to different chunks of the processed text. \n\nThe results of the novel transformer architecture, with initial models based on it such as BERT (Devlin et al., 2019), quickly showed a significant performance increase with regards to the previous models used in Natural Language Processing (NLP) tasks and started a race to train ever-bigger models of billions of parameters, taking huge general corpora of data. GPT-4 from OpenAI (OpenAI et al., 2024), Gemini 1.5 Pro from Alphabet (Gemini Team et al., 2024), Llama 3 from Meta, Claude 3 Opus from Anthropic, or Mistral 8x7B by Mistral AI (Jiang et al., 2024), are some of the most prominent and advanced LLMs for the time being. As models become better at a variety of benchmarks, the focus is turning to developing their applications across industries, with fine-tuning, Retrieval Augmented Generation (RAG) (Lewis et al., 2021), and integration into broader tools being explored.",
            "score": 0.41975830126556035,
            "section_title": "Large Language Models",
            "char_start_offset": 3486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 537
                },
                {
                    "start": 540,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1424
                }
            ],
            "ref_mentions": [
                {
                    "start": 212,
                    "end": 232,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 636,
                    "end": 657,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.424072265625
        },
        {
            "corpus_id": "267412619",
            "title": "LitLLM: A Toolkit for Scientific Literature Review",
            "text": "LLMs have demonstrated significant capabilities in storing factual knowledge and achieving stateof-the-art results when fine-tuned on downstream Natural Language Processing (NLP) tasks (Lewis et al., 2020). However, they also face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes (Huang et al., 2023;Gao et al., 2023;Li et al., 2024). These limitations have motivated the development of RAG (Retrieval Augmented Generation), which incorporates knowledge from external databases to enhance the accuracy and credibility of the models, particularly for knowledge-intensive tasks (Gao et al., 2023). RAG has emerged as a promising solution to the challenges faced by LLMs. It synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases (Gao et al., 2023). This approach allows for continuous knowledge updates and integration of domainspecific information in an attempt to limit the effect of outdated knowledge. The proposed work builds upon the advancements around RAG to provide a more efficient solution for academic writing. \n\nOn the other hand, there has been a notable emphasis on utilizing Large Language Models (LLMs) for tasks related to information retrieval and ranking (Zhu et al., 2023). The work by Sun et al. (2023) leverages generative LLMs such as Chat-GPT and GPT-4 for relevance ranking in information retrieval, demonstrating that these models can deliver competitive results to state-of-the-art supervised methods. Pradeep et al. (2023b,a) introduce different open-source LLM for listwise zeroshot reranking, further motivating the proposed approach of using LLMs for reranking in our work. \n\nThe exploration of large language models (LLMs) and their zero-shot abilities has been a significant focus in recent research. For instance, one study investigated using LLMs in recommender systems, demonstrating their promising zero-shot ranking abilities, although they struggled with the order of historical interactions and position bias (Hou et al., 2023).",
            "score": 0.4185848365485682,
            "section_title": "Related Work",
            "char_start_offset": 4064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1713
                },
                {
                    "start": 1716,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2077
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 205,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.371826171875
        },
        {
            "corpus_id": "272423592",
            "title": "RAG based Question-Answering for Contextual Response Prediction System",
            "text": "With the advent of ChatGPT and similar tools in mainstream media, Large Language Models (LLMs) have emerged as the standard solution for addressing a wide range of language understanding tasks. However, they can generate incorrect or biased information [34], as their responses are based on patterns learned from data that may not always contain necessary knowledge in a close domain. To address this issue, Retrieval Augmented Generation (RAG) [20] is commonly used to ground LLMs in factual information. The RAG architecture processes user input by first retrieving a set of documents similar to the query, which the language model then uses to generate a final prediction. While RAG-based architectures have been successful in various open-domain question answering (Q/A) tasks [15,30,49], limited research has explored their scaling dynamics in real conversational scenarios. Therefore, our research is one of the pioneering efforts in exploring the feasibility of an RAGbased approach for developing a knowledge-grounded response prediction system specifically tailored for the contact center of a major retail company. LLMs have recently been widely adopted across various industries, particularly in contact centers, to enhance chatbot development and agent-facing automation [8,12,37]. A prime example is the Response Prediction System (RPS), an agent-assist solution that generates contextually relevant responses, enabling agents to efficiently address customer queries with a single click. This boosts productivity, improves customer experience, and streamlines communication processes. In industry settings, the focus is on generating accurate, contextually appropriate responses with minimal latency. Therefore, RAG-based responses, grounded in company policies, deliver swift and accurate resolutions to customer issues. Figure 1 demonstrates a possible example of RPS in real settings, where the agent can directly utilize the generated response with a single click. \n\nHowever, implementing RAG for industry-specific use cases to assist human agents in generating valid responses involves several architectural decisions that can affect performance and viability. The retrieval style can be integrated into both encoder-decoder [16,44]) and decoder-only models [5,19,26,27], with various embedding and prompting techniques influencing the final LLM output.",
            "score": 0.41780257446585495,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 1981
                },
                {
                    "start": 1984,
                    "end": 2178
                },
                {
                    "start": 2179,
                    "end": 2371
                }
            ],
            "ref_mentions": [
                {
                    "start": 785,
                    "end": 788,
                    "matchedPaperCorpusId": "252735056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6826171875
        },
        {
            "corpus_id": "271570928",
            "title": "Knowledge Pyramid Construction for Multi-Level Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) enhances the generative capabilities of language models by incorporating retrieved knowledge for in-context learning [22], [26]. While general language models excel in producing responses for Fig. 2. The framework of PolyRAG includes: (1) Knowledge Pyramid Construction, starting from extracting knowledge from corpora data to form the initial layers, then performing the layer interactions, which are knowledge completion and condensation. (2) Given the refined Knowledge Pyramid, the LLM inference obtains the contexts through a multi-level query, which retrieves the knowledge from the higher layer down to the lower layer in a waterfall pattern. \n\ngeneral queries, they are prone to generating hallucinations when tasked with domain-specific knowledge usage. RAG addresses this issue by retrieving established knowledge corpora and providing this information as context to the language model [22]. NaiveRAG represents the most basic architecture within this framework, in which the system retrieves the top-k documents that are most relevant to the query and integrate them into the prompt, thereby grounding the responses in more relevant information [25]. \n\nExpanding on NaiveRAG, advanced RAG incorporates additional modules or structures to improve retrieval precision. Reranking is a notable example, where a reranker is employed to refine the initial ranked list (e.g., Re2G [37] and bgereranker [38], both are based on BERT [39]). Furthermore, studies have indicated that excessive noise and lengthy context can have a negative impact on inference performance. To address this, prompt compression methods such as Selective Context [40] and LLMLingua [41] have been developed. These methods emphasize key information while reducing noise and context length, as discussed in [22]. While current knowledge retrieval methods that rely on single queries can offer some contextual assistance for domain-specific knowledge, they heavily rely on the expressive abilities of the original collections of information and consistently require a trade-off between more detailed knowledge and less noisy information [42], [43]. These techniques do not effectively tackle the issues of retrieving responses that require integrating information from numerous sources, nor do they fulfill the need for dense knowledge in question-and-answer interactions. To address these constraints, we suggest employing varied knowledge representations.",
            "score": 0.41650522710216176,
            "section_title": "B. Retrieval-Augmented Generation",
            "char_start_offset": 7859,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 686
                },
                {
                    "start": 689,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 2161
                },
                {
                    "start": 2162,
                    "end": 2385
                },
                {
                    "start": 2386,
                    "end": 2470
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 164,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1193,
                    "end": 1197,
                    "matchedPaperCorpusId": "258841283"
                },
                {
                    "start": 1679,
                    "end": 1683,
                    "matchedPaperCorpusId": "214641123"
                },
                {
                    "start": 1698,
                    "end": 1702,
                    "matchedPaperCorpusId": "252186384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77001953125
        },
        {
            "corpus_id": "273821590",
            "title": "PersianRAG: A Retrieval-Augmented Generation System for Persian Language",
            "text": "In recent years, the field of NLP has witnessed significant advancements, especially in the development of LLMs capable of generating coherent and contextually relevant text. Despite their impressive capabilities, generative language models often tend to provide outdated information or fabricate facts a phenomenon commonly referred to as \"Hallucination\". This limitation persists even when models are aligned with human preferences through reinforcement learning [1] or style alignment techniques [1][2][3][4]. RAG systems have emerged as a promising solution to these challenges. By integrating the strengths of pre-trained models and retrieval mechanisms, RAG provides a powerful framework that enhances model performance and reduces errors in generated content [5]. Additionally, RAG enables the rapid deployment of applications tailored to specific organizations and domains without needing to update the underlying model's parameters, provided that relevant documents are available for retrieval. Several methods have been proposed to enhance LLMs through query-dependent retrieval [5][6][7]. A typical RAG process includes critical components such as embedding (semantic representation of documents and queries), retrieval (efficient access to relevant documents), and generation (producing responses based on retrieved information). Implementing RAG requires key decisions regarding document segmentation, selecting embedding models for semantic representation, choosing vector databases for efficient storage and retrieval, and optimizing large language models (refer to Figure 1). The inherent complexity at each stage of this process creates significant challenges in implementing RAG systems which are more highlighted in a low resource language like Persian. One approach involves using embedding models directly to calculate semantic similarities between queries and documents. These embedding models are often trained adversarially using positive and negative query-response pairs [8,9]. The choice and combination of techniques at each stage profoundly affect the effectiveness and efficiency of RAG systems. Furthermore, according to the latest reviews, there no research on optimizing RAG implementations for Persian languages, with a focus on core components such as retrieval, embedding, and generative models. Historically, most NLP research and development have focused on English, resulting in a shortage of resources, tools, and datasets for other languages. Consequently, the development of advanced NLP systems for languages like Persian lags behind the growing need for such technologies.",
            "score": 0.4160689493855593,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2125
                },
                {
                    "start": 2126,
                    "end": 2331
                },
                {
                    "start": 2332,
                    "end": 2483
                },
                {
                    "start": 2484,
                    "end": 2616
                }
            ],
            "ref_mentions": [
                {
                    "start": 1092,
                    "end": 1095,
                    "matchedPaperCorpusId": "265594594"
                },
                {
                    "start": 1095,
                    "end": 1098,
                    "matchedPaperCorpusId": "189762189"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5478515625
        },
        {
            "corpus_id": "265329916",
            "title": "Large Language Models and Information Retrieval",
            "text": "The research investigates fundamental architectural designs integral to Large Language Models (LLMs): the encoder-decoder architecture and the decoder-only architecture. These structural frameworks have played a pivotal role in advancing language understanding and generation, laying the foundation for innovative applications across diverse domains. \n\nThe encoder-decoder architecture, a widely embraced structure in Natural Language Processing (NLP) tasks, comprises two essential components: an encoder and a decoder. The encoder processes the input sequence, generating a fixed-length context vector. Conversely, the decoder receives this context vector and produces the output sequence. Typically, recurrent layers such as LSTM or GRUs construct the encoder, processing the input sequentially and capturing contextual information. Attention mechanisms further enhance this architecture by enabling the decoder to selectively focus on different parts of the input during decoding. \n\nIn contrast, the decoder-only architecture revolves around a solitary transformer-based decoder, devoid of an explicit encoder. This architectural model generates the output sequence based on a learned positional representation of the input sequence. Initially introduced in encoder-decoder models, transformer-based architectures have been adapted for decoder-only models. They incorporate selfattention mechanisms, allowing the model to weigh the significance of each input token when generating the output token. The transformer decoder processes tokens in parallel, enhancing computational efficiency and significantly accelerating training and inference processes. These architectural paradigms have significantly propelled the field of language modeling. Encoder-decoder architectures have notably augmented language understanding by efficiently capturing contextual relationships in both input and output sequences, crucial for translation and summarization tasks. On the other hand, the decoder-only architecture, particularly with transformer models, has immensely elevated generation capabilities. The self-attention mechanism's ability to consider all input tokens simultaneously results in coherent and contextually appropriate text generation. Understanding these architectural designs is fundamental in the development of sophisticated LLMs adept at both understanding and generating human-like text. Their flexibility and effectiveness continue to drive innovation, paving the way for a multitude of applications in the realm of natural language processing.",
            "score": 0.4124818366967099,
            "section_title": "LLM Architecture",
            "char_start_offset": 13306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 984
                },
                {
                    "start": 987,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2243
                },
                {
                    "start": 2244,
                    "end": 2401
                },
                {
                    "start": 2402,
                    "end": 2559
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8017578125
        },
        {
            "corpus_id": "271781965",
            "title": "Natural language processing with transformers: a review",
            "text": "Natural language processing (NLP) tasks can be addressed with several deep learning architectures, and many different approaches have proven to be efficient. This study aims to briefly summarize the use cases for NLP tasks along with the main architectures. This research presents transformer-based solutions for NLP tasks such as Bidirectional Encoder Representations from Transformers (BERT), and Generative Pre-Training (GPT) architectures. To achieve that, we conducted a step-by-step process in the review strategy: identify the recent studies that include Transformers, apply filters to extract the most consistent studies, identify and define inclusion and exclusion criteria, assess the strategy proposed in each study, and finally discuss the methods and architectures presented in the resulting articles. These steps facilitated the systematic summarization and comparative analysis of NLP applications based on Transformer architectures. The primary focus is the current state of the NLP domain, particularly regarding its applications, language models, and data set types. The results provide insights into the challenges encountered in this research domain.",
            "score": 0.4119867446800125,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5068359375
        },
        {
            "corpus_id": "261243995",
            "title": "Building Trust in Conversational AI: A Comprehensive Review and Solution Architecture for Explainable, Privacy-Aware Systems using LLMs and Knowledge Graph",
            "text": "The LLM can interpret and respond to journalism-related queries based on previous context without extracting explicit facts, leveraging its inherent understanding of relational data. VII. DISCUSSIONS Conversational AI, while advancing rapidly, faces hurdles in balancing linguistic depth with accurate information representation. As highlighted in our comprehensive review through the Large Language Model Explorer (LLMXplorer), which provides a systemic overview of numerous LLMs, linguistic proficiency often comes at the expense of transparency. Knowledge Graphs, while offering factual precision, might fall short in mimicking human conversational fluidity. \n\nOur exhaustive applied analysis of the practical use cases, challenges, and limitations of LLMs across industries underscores their vast potential. Still, it also highlights the need for architectures that can bridge the aforementioned gaps. In response, our work introduces a functional solution architecture that uniquely integrates KGs and LLMs. This system not only stands out in its linguistic capabilities but also ensures factual consistency. With the incorporation of RBAC, we further the cause of data security, restricting users to role-specific information, and thereby fostering trust for real-world and industry-wide use cases. \n\nThe application domain of media and journalism, exemplified using rich data from the real-world product (AI NewsHub) platform, serves as both a case study and a validation point. It is a testament to the architecture's robustness and efficiency. Notably, the adaptability of the proposed architecture means it can seamlessly cater to a myriad of use cases and applications, emphasizing its crossindustry-wide relevance. Some pivotal aspects and design choices to underline the significance of the proposed architecture include: \n\n\u2022 Usage of Specialized LLM (Llama-2-Chat): Our choice of the LLM variant, specifically fine-tuned for dialogue, caters to Fig. 13: Functional architecture illustrating the integration of Large Language Models (LLMs), Knowledge Graphs (KG), and Role-Based Access Control (RBAC). \n\nenhanced conversational accuracy and quality. \u2022 Neo4j for Knowledge Graph Navigation: The architecture uses Neo4j based on its open-sourceness, industry-wide adoption, proficiency in navigating knowledge structures and efficient data extraction via a query language, Cypher.",
            "score": 0.4100227057535505,
            "section_title": "Contextual Interpretations:",
            "char_start_offset": 22268,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 661
                },
                {
                    "start": 664,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1834
                },
                {
                    "start": 1837,
                    "end": 2114
                },
                {
                    "start": 2117,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2391
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.299072265625
        },
        {
            "corpus_id": "269740933",
            "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
            "text": "This, in turn, significantly hinders the widespread adoption of LLMs in various real-world applications.\n\nTo address these limitations, recent efforts have been made to take advantage of RAG to enhance the capabilities of LLMs in various tasks [6,53,62,135], especially those demanding high for the latest and reliable knowledge such as Question Answer (QA), AI4Science, and software engineering.For example, Lozano et al. [92] introduces a scientific QA system based on retrieving scientific literature dynamically.MolReGPT leverages RAG to enhance the in-context learning ability of ChatGPT for molecular discovery [77].It is also been demonstrated that RAG can effectively reduce hallucinations in conversational tasks [137,171].As illustrated in Figure 1, an LLM-based dialog system will not be able to answer well for out-of-scope queries.With the help of RAG to retrieve relevant knowledge from external database and integrate it into the process of generation, the dialog system succeeds in giving correct answers.Given the remarkable progress in advancing LLMs with RAG, there is an imperative need for a systematic review of recent advances in Retrieval-Augmented Large Language Models (RA-LLMs).Decoder-only models generate text in a left-to-right fashion.As a representative Decoder-only model, GPT (Generative Pre-trained Transformer) [114] predicts the next token in a sequence based on the context provided by the previous tokens.Their architecture makes them particularly effective for tasks like language generation, code generation, and creative writing.Encoder-Decoder models, such as T5 (Text-To-Text Transfer Transformer) [116], uniquely transform a variety of NLP tasks into text generation problems.To be more specific, the encoder in T5 processes the input sequence to capture its meaning, while the decoder generates the output sequence based on the encoded information.This T5 architecture is well-suited for tasks that involve converting one sequence into another, such as machine translation, summarization, and conversational response generation.",
            "score": 0.41000898998385293,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3918,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 106,
                    "end": 396
                },
                {
                    "start": 396,
                    "end": 516
                },
                {
                    "start": 516,
                    "end": 622
                },
                {
                    "start": 622,
                    "end": 732
                },
                {
                    "start": 732,
                    "end": 844
                },
                {
                    "start": 844,
                    "end": 1021
                },
                {
                    "start": 1021,
                    "end": 1205
                },
                {
                    "start": 1205,
                    "end": 1266
                },
                {
                    "start": 1266,
                    "end": 1444
                },
                {
                    "start": 1444,
                    "end": 1571
                },
                {
                    "start": 1571,
                    "end": 1721
                },
                {
                    "start": 1721,
                    "end": 1894
                },
                {
                    "start": 1894,
                    "end": 2074
                }
            ],
            "ref_mentions": [
                {
                    "start": 244,
                    "end": 247,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "227746078"
                },
                {
                    "start": 250,
                    "end": 253,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 423,
                    "end": 427,
                    "matchedPaperCorpusId": "264487188"
                },
                {
                    "start": 722,
                    "end": 727,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 727,
                    "end": 731,
                    "matchedPaperCorpusId": "236034497"
                },
                {
                    "start": 1642,
                    "end": 1647,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63427734375
        },
        {
            "corpus_id": "268063656",
            "title": "Grounding Language Models for Visual Entity Recognition",
            "text": "FROMAGe [34], GILL [33], GenLLaVa [26] and LISA [37] achieve satisfactory results over imagetext-to-text retrieval, image generation and reasoning segmentation by inserting special tokens to augment the MLLMs original functionality. Our approach, built upon one of the latest MLLM model architecture, presents a compelling alternative to the bi-encoder shallow dot-product interaction or encoder-decoder sparse surface form matching employed in previous works on visual entity recognition. Our method also integratives contrastive learning on the outputs of entities and question pairs, analogous to the image-text contrastive learning used in CLIP-like models [54,56,66] and among visual features used for self-supervised contrastive learning [3,11,16,25]. \n\nRetrieval-Augmented Language Models (RALMs) represent a class of NLP solutions focusing on knowledge-intensive tasks. RALMs typically condition a language model on relevant documents from a grounding corpus during generation, thereby enhancing the performance in knowledge-intensive language understanding tasks. Lewis et al . [39] jointly fine-tune a retriever with an encoderdecoder model, enabling the community to explore the RALM paradigm in language understanding. Guu et al . [23] train a bi-directional variant and also demonstrate superior performance. Apart from retrieved items in context, F\u00e9vry et al . [17] are the first to integrate retrieved entity supervision by injecting intermediate representations. While augmented context helps in mitigating the well-known issue of hallucination, it does not ensure a faithful prediction with respect to an external knowledge base. Our retrieve-generate framework is distinct from existing retrieval-augmented methods. AutoVER reduces the candidate entity set from millions to hundreds for generative language models through retrieval [30] and dynamically constrains language model generation using a prefix tree. This framework coincides with the design philosophy of agents [22] in Interactive NLP [74], where an agent interacts with the dynamic environment by performing beam-search on all available options.",
            "score": 0.40971046051989024,
            "section_title": "Related Work",
            "char_start_offset": 7888,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2126
                }
            ],
            "ref_mentions": [
                {
                    "start": 8,
                    "end": 12,
                    "matchedPaperCorpusId": "258947258"
                },
                {
                    "start": 661,
                    "end": 665,
                    "matchedPaperCorpusId": "252917745"
                },
                {
                    "start": 665,
                    "end": 668,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 668,
                    "end": 671,
                    "matchedPaperCorpusId": "258615395"
                },
                {
                    "start": 747,
                    "end": 750,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 750,
                    "end": 753,
                    "matchedPaperCorpusId": "260738239"
                },
                {
                    "start": 1087,
                    "end": 1091,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1375,
                    "end": 1379,
                    "matchedPaperCorpusId": "215768768"
                },
                {
                    "start": 1850,
                    "end": 1854,
                    "matchedPaperCorpusId": "259137758"
                },
                {
                    "start": 1991,
                    "end": 1995,
                    "matchedPaperCorpusId": "254853929"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.450927734375
        },
        {
            "corpus_id": "273404260",
            "title": "The Large Language Model GreekLegalRoBERTa",
            "text": "If one wants to solve an NLP task for a low resource language like Greek, one solution is to use a multilingual LLM. For Greek, both M-BERT and XLM-RoBERTa [11] will do for the task, since their training corpora includes Greek documents. The first monolingual language model to be proposed for the Greek language is GreekBERT [20]. It has been applied to the tasks of part-of-speech (POS) tagging, NER and natural language inference (NLI), and it was shown to outperform M-BERT and XLM-RoBERTa on these tasks. \n\nNowadays, there has been a notable emphasis on the development and advancement of generative multilingual models. Some outstanding examples of these models are OPT [34], BLOOM [28] and GPT3 [5], while the most recent and prominent are PaLM [4], Chinchilla [16] and Llama [31]. These models demonstrate significant performance in both natural and programming languages. While OPT, BLOOM, and Llama are open-source models trained on publicly available data, GPT-3, Chinchilla, and PaLM are closed source and trained on private datasets. Very recently, the generative LLM Meltemi has been developed for the Greek language 3 . Nevertheless, considering that each model comprises hundreds of billions of parameters, the resources required to utilize these models are substantial. \n\nMoreover, in the generative question answering task, the responses of these models suffer from extensive hallucination. The term hallucination refers to the phenomenon where a model generates information that is incorrect, misleading, or entirely fabricated. Retrieval Augmented Generation (RAG) [22] emerged to address this issue. RAG architectures consist of a retriever and a generator . The generator is an encoder decoder architecture such as BART [21], Llama [31], and BLOOM [28]. The retriever is an encoder architecture such as DPR [18], E5 [33] and Nomic [25], and it is responsible for retrieving the top-k passages ( 1 , 2 , ..., ) from a database given a query . Then, the query along with the k passages are passed to the generator.",
            "score": 0.40849922168585373,
            "section_title": "RELATED WORK",
            "char_start_offset": 3032,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2034
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 160,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 688,
                    "end": 692,
                    "matchedPaperCorpusId": "253420279"
                },
                {
                    "start": 752,
                    "end": 755,
                    "matchedPaperCorpusId": "215768750"
                },
                {
                    "start": 1585,
                    "end": 1589,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1742,
                    "end": 1746,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1770,
                    "end": 1774,
                    "matchedPaperCorpusId": "253420279"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31591796875
        },
        {
            "corpus_id": "242043855",
            "title": "Automatic Taxonomy Classification by Pretrained Language Model",
            "text": "The second problem is updating the text-processing architecture. Context information means additional information generated by word order and combination. The previous method [6] uses RNN to handle context information; however, transformer architecture [9,10] becomes standard in NLP. Many researchers have published papers using transformer architecture and have shown significant progress. Empirically, we propose that using transformer architecture also yields improvement in our ontology-generation area. In conclusion, our subjects handle OOV words and architecture updates. \n\nOur contribution can be summarized as follows. To improve the limitations of our previous approach, we propose applying a pretrained language model (PLM), such as BERT [9], to taxonomically structure generation tasks. Recently, with the advent of BERT, transfer learning has become a popular trend for language models, enabling extremely high performance by simply finetuning the target downstream tasks. Although various language processing techniques are used in this method, we focus on byte pair encoding (BPE) and transformer architecture. BPE is proposed as a simple data compression technique and is also effective for NLP's subword tokenization [11]. Because BERT posted state-of-the-art scores in various tasks using transformers [12] as their core structure, many language models after BERT have adopted the transformer architecture at their base, also posting state-of-the-art results. Additionally, it is expected that the processing units have high processing power for context information. From these features, we expect significant improvements both experimentally and functionally. We also illustrate a real example to apply our ontology classifier by pretrained language model to a QA dataset.",
            "score": 0.40837925029608524,
            "section_title": "1.",
            "char_start_offset": 7119,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 579
                },
                {
                    "start": 582,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1792
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 178,
                    "matchedPaperCorpusId": "201814161"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.325927734375
        },
        {
            "corpus_id": "273654037",
            "title": "A Survey of Large Language Models for Arabic Language and its Dialects",
            "text": "This section explores the various architectures employed by Arabic language models, focusing on their designs and functionalities. The architectures of these models typically fall into three main categories: encoder, decoder, and encoderdecoder models. Each of these architectures plays a unique role in NLP tasks, affecting how models understand and generate Arabic text. Table 1 summarizes encoder-only models, while Table 2 summarizes decoder-only and encoderdecoder models. This overview highlights the diversity in model design, reflecting specific use cases and strengths across different NLP applications.",
            "score": 0.40770068191129194,
            "section_title": "ARABIC LARGE LANGUAGE MODELS ARCHITECTURES",
            "char_start_offset": 4607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 612
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.455322265625
        },
        {
            "corpus_id": "246063428",
            "title": "LaMDA: Language Models for Dialog Applications",
            "text": "Most of the existing literature focuses on the problem of open-domain question-answering rather than dialog generation, and the models themselves are used to index and rank knowledge sources, rather than trained to use an intermediate tool. Given these differences, we note that the range of existing approaches to this problem include the RNNLM [34], RAG [35], REALM [36], and FiD [37] architectures. Zhu et al. [38] provide a survey of further recent work. See Karpukhin et al. [39] for details on the 'dense passage retriever' used in RAG. Recent work in this direction has expanded and elaborated on neural models' ability to retrieve and rank passages [40]. The RETRO architecture demonstrates that language models can be primed with results retrieved from a database as large as two trillion tokens [41]. At a broad level, our approach is also comparable to that of Byrne et al. [42], which fine-tunes the model to use external APIs for movie ticketing dialog. \n\nParts of our findings are similar to recent studies on dialog groundedness. Granting access to external knowledge bases has been shown to reduce the rate at which models hallucinate unsourced statements in dialog across a variety of retrieval systems and model architectures [31]. Another study finds that a question-answering system's accuracy is improved by separating it into a reasoning unit and a response generator, analogous to our separation of 'Base' and 'Research' models in our study [43]. Meanwhile, the WebGPT framework includes a language system that can interact with the open web via a text-only interface, and learns to imitate humans in answering questions by citing external sources [44]. Komeili et al. [45] compare different types of pre-trained models and retrieval methods, and reach a similar conclusion that augmenting language models with a search engine provides more factually grounded responses. They encode the input context with grounded information from search to generate the next response, while we augment the generated responses with information from known sources in our method. This allows us to fine-tune the model for groundedness without sacrificing gains in safety or quality from other fine-tuning treatments.",
            "score": 0.407496477420189,
            "section_title": "Related work",
            "char_start_offset": 6311,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2221
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.464599609375
        },
        {
            "corpus_id": "271270159",
            "title": "Retrieval-Enhanced Machine Learning: Synthesis and Opportunities",
            "text": "Background. In recent years, the research landscape surrounding large language models (LLMs) has witnessed substantial growth, underscored by the profound potential these models hold for various natural language processing (NLP) tasks. One of the significant advancements that has propelled this field forward is the scaling of the number of parameters of LLMs, which has enabled the training of models with unprecedented size and complexity [267]. We witness a similar trend in other fields adjacent to machine learning, for example, large vision foundation models for representing images and videos [5,40]. Concurrently, the notion of in-context learning (ICL) [39] has emerged as a transformative capability, allowing LLMs to dynamically adapt and incorporate new information during its inference. In parallel, the information retrieval (IR) community has been actively exploring techniques aimed at improving the efficiency, effectiveness, and robustness of accessing information from large-scale collections. \n\nThe convergence of these two domains has given rise to a new trend in research, where models are equipped with retrieval capabilities to access external knowledge during both training and inference stages [148,257]. This integration of retrieval mechanisms into the prediction pipeline started to gain significant traction, as it allows models to ground their predictions in external knowledge without necessitating an increase in model capacity. Methods presented by Hashemi et al. [68] and Lewis et al. [123] are among the earliest work in this space; the former focuses on retrieval-augmented representation learning by extending the transformer network, while the latter studies the paradigm of retrieval-augmented generation (RAG) for knowledge-intensive language tasks. That said, using retrieval results to improve a machine learning systems is not new. Pseudo-relevance feedback methods-methods for representing search queries using the top retrieved documents-are perhaps the first set of methods in this category [10,30]. The ICL ability inherent in the LLMs has played a pivotal role in facilitating the dissemination and adoption of these retrieval-augmented approaches. By integrating retrieved documents into the prompt of the LLMs, researchers have been able to harness the external knowledge sources without fundamentally altering the underlying model architecture. \n\nMotivation.",
            "score": 0.4068739431190821,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 1013
                },
                {
                    "start": 1016,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2047
                },
                {
                    "start": 2048,
                    "end": 2198
                },
                {
                    "start": 2199,
                    "end": 2397
                },
                {
                    "start": 2400,
                    "end": 2411
                }
            ],
            "ref_mentions": [
                {
                    "start": 601,
                    "end": 604,
                    "matchedPaperCorpusId": "232417054"
                },
                {
                    "start": 604,
                    "end": 607,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 1221,
                    "end": 1226,
                    "matchedPaperCorpusId": "256868474"
                },
                {
                    "start": 1226,
                    "end": 1230,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 1499,
                    "end": 1503,
                    "matchedPaperCorpusId": "219687568"
                },
                {
                    "start": 1521,
                    "end": 1526,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 2039,
                    "end": 2043,
                    "matchedPaperCorpusId": "553561"
                },
                {
                    "start": 2043,
                    "end": 2046,
                    "matchedPaperCorpusId": "33071665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83056640625
        },
        {
            "corpus_id": "270764517",
            "title": "Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models",
            "text": "Large Language Models (LLMs) such as GPT-4 (Achiam et al., 2023) and Mixtral (Jiang et al., 2024) have been revolutionizing natural language processing tasks with their diverse zero-shot capabilities.Through extensive pretraining on various large-scale textual sources-such as web pages, research articles, books, examples, and code-these systems have shown remarkable natural language capabilities.As a result, their responses are increasingly human-like and closely aligned with human intentions (Zhu et al., 2023).Recently, as LLMs have become more capable, extensive research has been conducted on their use and effectiveness in information retrieval (IR) systems (Zhu et al., 2023).\n\nA typical information retrieval system comprises multiple components organized into a processing pipeline.This pipeline features two primary stages: the retriever and the reranker (Lin et al., 2022).While the retriever selects the most relevant passages from a large-scale corpus, the re-ranker focuses on re-ordering (i.e., re-ranking) the candidate passages, using their relevance.Each component can thus be optimized for its given task.\n\nThe advent of large language models (LLMs) has impacted the information retrieval (IR) pipeline in many ways.While research in this field has mainly focused on the use of LLMs in the first stage of the pipeline (Zhu et al., 2023) (i.e., the retriever), the investigation of LLMs for the re-ranking stage remains a relatively new challenge.Recently, some research has focused on using LLMs for zeroshot re-ranking, leading to significant improvements (Sun et al., 2023).\n\nDespite recent advances in retrieval methods, some retrieved passages are likely to be unrelated to the query.Generally, all of the retrieved passages from the retriever (typically BM25 (Lin et al., 2021)) are passed to the LLM-based re-ranker, to generate the ranked list of passages based on their relevance to the query.",
            "score": 0.4064586197702927,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 200,
                    "end": 399
                },
                {
                    "start": 399,
                    "end": 517
                },
                {
                    "start": 517,
                    "end": 687
                },
                {
                    "start": 689,
                    "end": 795
                },
                {
                    "start": 795,
                    "end": 888
                },
                {
                    "start": 888,
                    "end": 1072
                },
                {
                    "start": 1072,
                    "end": 1128
                },
                {
                    "start": 1130,
                    "end": 1239
                },
                {
                    "start": 1239,
                    "end": 1469
                },
                {
                    "start": 1469,
                    "end": 1599
                },
                {
                    "start": 1601,
                    "end": 1711
                },
                {
                    "start": 1711,
                    "end": 1924
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.485595703125
        },
        {
            "corpus_id": "276860338",
            "title": "LLM-Based Text Style Transfer: Have We Taken a Step Forward?",
            "text": "Large Language Models (LLMs) demonstrated remarkable abilities in various NLP tasks by leveraging the Transformer architecture [53]. Recently proposed methods employ techniques such as zero-shot, few-shot, and Chainof-Thought (CoT) prompting [54], [55], [56] to obtain taskspecific outputs. Fine-tuning techniques and their parameter efficient variants, such as Parameter Efficient Fine-tuning (PEFT) [57] and Prefix Tuning [58] focus on adapting the models for particular tasks. By incorporating external knowledge in the generation process, Retrieval Augmented Generation (RAG) [59] techniques aim to enhance the performance. Knowledge Augmentation (KA) techniques enhance the performance of knowledge-intensive tasks [60], [61]. We encourage the reader to refer to the Appendix for a more detailed description of these techniques.",
            "score": 0.40561094091890754,
            "section_title": "III. LLM-BASED APPROACHES FOR TEXT GENERATION",
            "char_start_offset": 12263,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 833
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 246,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 248,
                    "end": 252,
                    "matchedPaperCorpusId": "238260199"
                },
                {
                    "start": 254,
                    "end": 258,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 401,
                    "end": 405,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 424,
                    "end": 428,
                    "matchedPaperCorpusId": "230433941"
                },
                {
                    "start": 580,
                    "end": 584,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 720,
                    "end": 724,
                    "matchedPaperCorpusId": "158046772"
                },
                {
                    "start": 726,
                    "end": 730,
                    "matchedPaperCorpusId": "252968266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56787109375
        },
        {
            "corpus_id": "278715464",
            "title": "Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate",
            "text": "The Generation Augmented Retrieval (GAR) (Mao et al., 2021) is a common approach that leverages the capabilities of language models to perform query decomposition (Chen et al., 2024b), query rewriting (Ma et al., 2023), and query expansion (Wang et al., 2023), helping to supplement missing background knowledge in queries to achieve higher retrieval quality. In addition to passage retrieval, GAR can also play a role in code retrieval (Li et al., 2024). Our SIAR can be seen as a type of GAR that utilizes the self-inductive abilities of large language models (LLMs).",
            "score": 0.40553552308713625,
            "section_title": "Generation Augmented Retrieval",
            "char_start_offset": 25538,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 569
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 59,
                    "matchedPaperCorpusId": "221802772"
                },
                {
                    "start": 163,
                    "end": 183,
                    "matchedPaperCorpusId": "271884400"
                },
                {
                    "start": 240,
                    "end": 259,
                    "matchedPaperCorpusId": "257505063"
                },
                {
                    "start": 437,
                    "end": 454,
                    "matchedPaperCorpusId": "266899699"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57421875
        },
        {
            "corpus_id": "258615731",
            "title": "Active Retrieval Augmented Generation",
            "text": "Generative language models (LMs) (Brown et al., 2020;Ouyang et al., 2022;OpenAI, 2023;Chowdhery et al., 2022;Zhang et al., 2022;Touvron et al., 2023;Zhao et al., 2023) have become a foundational component in natural language processing (NLP) systems with their remarkable abilities. Although LMs have memorized some world knowledge during training (Petroni et al., 2019;Roberts et al., 2020;Jiang et al., 2020), they still tend to hallucinate and create imaginary content (Maynez et al., 2020;Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant information from external knowledge resources is a promising direction to address hallucination (Khandelwal et al., 2020;Izacard et al., 2022). \n\nRetrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve documents based on the user's input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017;Guu et al., 2020;Lewis et al., 2020;Izacard and Grave, 2021;Sachan et al., 2021;Lee et al., 2021;Jiang et al., 2022;Izacard et al., 2022;Nakano et al., 2021;Qian et al., 2023;Lazaridou et al., 2022;Shi et al., 2023). These single-time retrieval augmented LMs outperform purely parametric LMs, particularly for shortform knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019;Joshi et al., 2017), where the information needs are clear in the user's input, and it is sufficient to retrieve relevant knowledge once solely based on the input.",
            "score": 0.40448808704071193,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1527
                }
            ],
            "ref_mentions": [
                {
                    "start": 86,
                    "end": 109,
                    "matchedPaperCorpusId": "236460328"
                },
                {
                    "start": 348,
                    "end": 370,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 370,
                    "end": 391,
                    "matchedPaperCorpusId": "211205183"
                },
                {
                    "start": 391,
                    "end": 410,
                    "matchedPaperCorpusId": "208513249"
                },
                {
                    "start": 472,
                    "end": 493,
                    "matchedPaperCorpusId": "218487034"
                },
                {
                    "start": 493,
                    "end": 511,
                    "matchedPaperCorpusId": "226254579"
                },
                {
                    "start": 672,
                    "end": 697,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 697,
                    "end": 718,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 922,
                    "end": 941,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 1057,
                    "end": 1078,
                    "matchedPaperCorpusId": "235078802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.607421875
        },
        {
            "corpus_id": "236573493",
            "title": "A comparative study of deep learning based language representation learning models",
            "text": "Deep learning (DL) approaches use various processing layers to learn hierarchical representations of data. Recently, many methods and designs of natural language processing (NLP) models have shown significant development, especially in text mining and analysis. For learning vector-space representations of text, there are famous models like Word2vec, GloVe, and fastText. In fact, NLP took a big step forward when BERT and recently GTP-3 came out. In this paper, we highlight the most important language representation learning models in NLP and provide an insight of their evolution. We also summarize, compare and contrast these different models on sentiment analysis, and thus discuss their main strengths and limitations. Our obtained results show that BERT is the best language representation learning model.",
            "score": 0.403997658119428,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50390625
        },
        {
            "corpus_id": "276317428",
            "title": "Towards Automated Fact-Checking of Real-World Claims: Exploring Task Formulation and Assessment with LLMs",
            "text": "NLP provides the foundation for efficiently processing and interpreting text, making it critical for addressing misinformation detection. Advances in transformer-based architectures have significantly improved language modeling and generation. Vaswani et al. [10] introduced the transformer architecture, which leverages multi-head self-attention to capture contextual dependencies between tokens. Variants such as BERT [11] and GPT [12] illustrate encoder-only and decoder-only applications, respectively. BERT is well-suited for sequence classification tasks, while GPT generates text autoregressively for sequence-to-sequence tasks. In the context of this study, we refer to these lightweight architectures as SLMs [13]. Kaplan et al. [14] showed that the loss of a language scales as a power law with model parameter size, pre-training dataset size, and compute budget, guiding the development of current large language models (LLMs). Most LLMs follow the GPT architecture with modifications, aligning their b ,ehavior to specific tasks through instruction fine-tuning, reinforcement learning from human feedback (RLHF) [15], direct preference optimization (DPO) [16], or supervised fine-tuning. Prompts serve as inputs to these models, acting as task instructions. Effective prompt engineering, including template design and task-specific examples in a few-shot scenario, enhances their utility [17,18]. Retrieval-Augmented Generation (RAG) [19] is a technique that combines retrieval mechanisms with NLG, allowing LLMs to ground their responses in external evidence. This approach can enhance factual accuracy of LLM generated responses by enforcing consistency with the evidence, and thus may reduce hallucinations in generative tasks. Automated fact-checking frameworks typically consist of claim detection, evidence retrieval, and claim verification [4]. Claim detection identifies check-worthy claims [3], often guided by factors like relevance or harm. Evidence retrieval involves collecting and selecting relevant information to justify verdicts [20]. Claim verification can be broken down into two main tasks: (a) verdict prediction and (b) justification production [4]. Verdict prediction describes the task of classifying the veracity of a claim. Specific labeling schemes may be required to fit particular use cases or domains.",
            "score": 0.4037225177607061,
            "section_title": "Related Work",
            "char_start_offset": 4201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2261
                },
                {
                    "start": 2262,
                    "end": 2343
                }
            ],
            "ref_mentions": [
                {
                    "start": 1859,
                    "end": 1862,
                    "matchedPaperCorpusId": "237304047"
                },
                {
                    "start": 1911,
                    "end": 1914,
                    "matchedPaperCorpusId": "4651333"
                },
                {
                    "start": 2058,
                    "end": 2062,
                    "matchedPaperCorpusId": "1434196"
                },
                {
                    "start": 2179,
                    "end": 2182,
                    "matchedPaperCorpusId": "237304047"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60595703125
        },
        {
            "corpus_id": "270521566",
            "title": "HIRO: Hierarchical Information Retrieval Optimization",
            "text": "Final generated response from the augmented prompt Augmented prompt returned to the LM Figure 1: Architectural Overview of a Retrieval-Augmented Language Model (RALM). A query is inputted, processed through an embedding model to generate a vector representation, which is then matched in a vector database to retrieve relevant contexts. These contexts are fed, along with the original query, into the LLM, resulting in an informed response. \n\nThe adoption of hierarchical structures in RAG models, exemplified by RAPTOR [15], represents a pivotal shift towards more organized and efficient information storage and retrieval. RAPTOR's approach, through recursive summarisation to create a hierarchical tree, facilitates nuanced access to information at various levels of abstraction. This innovation not only improves coherence and information density tailored to specific tasks but also encourages exploration into other hierarchical frameworks like graphs for enhanced document interrelation preservation. These advancements signal a new phase in computational linguistics, focusing on sophisticated, hierarchical data use for better knowledge representation and retrieval. However, existing hierarchical models often face challenges in balancing information density and retrieval efficiency, leading to potential information overload or loss. \n\nDynamic Querying Mechanisms in RAG Models address the evolving nature of queries where traditional static retrieval methods, such as TF-IDF and BM25, have been complemented by more adaptive techniques. The Tree Traversal method progressively selects the top-k most relevant nodes from each layer of the hierarchical data based on similarity, allowing for adjustments in depth and number of nodes to control the information's specificity and breadth. Conversely, the Collapsed Tree approach flattens the hierarchy to allow for simultaneous comparison of all nodes. Figure 5 and Figure 6 illustrate the workings of these algorithms, providing visual representations. \n\nDespite these innovations, a limitation arises from the static quantity of data retrieved, which may not always align with the query's needs, leading to potential information overload for LLMs. This static k value, irrespective of query complexity, can hamper LLMs' performance by either providing insufficient data for complex queries or overwhelming them with excessive context for simpler ones. Additionally, returning both parent and child nodes due to similar matching levels often results in inefficient, redundant information. Optimizing the retrieval process to avoid such duplication is crucial for improving LLM performance in hierarchical document structures.",
            "score": 0.4024982055363106,
            "section_title": "Language Model",
            "char_start_offset": 4240,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 440
                },
                {
                    "start": 443,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1344
                },
                {
                    "start": 1347,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2011
                },
                {
                    "start": 2014,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2411
                },
                {
                    "start": 2412,
                    "end": 2547
                },
                {
                    "start": 2548,
                    "end": 2684
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7529296875
        },
        {
            "corpus_id": "270702738",
            "title": "Ragnar\u00f6k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track",
            "text": "Retrieval Augmented Generation (RAG) [10,21,22,29] has emerged as a popular technique to augment large language model (LLM) generation for knowledge-intensive tasks such as open-domain question answering or fact verification [44].Using the top- retrieved segments from a suitable retrieval system, RAG systems output an answer summary grounded on the relevant context.RAG systems mitigate factual inconsistencies in LLM outputs [19,26,29,34], and enhance interpretability [21] and generalization [20], thus facilitating a wider adoption across several domains like Medicine [55] and Finance [23].\n\nSeveral companies provide end-to-end RAG frameworks such as Bing Search [39], or Google Gemini [5].Most of these systems are either proprietary or offer limited user customization.Likewise, the absence of a standardized RAG framework makes implementing RAG at a large scale challenging.Implementing atop existing frameworks requires custom code for multiple steps including retrieval, reranking, and generation.To promote wider adoption of RAG in academia, we develop Ragnar\u00f6k, a user-friendly, reusable, endto-end RAG framework offering code for customizable retrievers, rerankers, and generation models.\n\nRagnar\u00f6k comprises two key modules: (R) Retrieval and (AG) Augmented Generation.The retrieval module incorporates both the retrieval and re-ranking stages to yield the top- retrieved segments for an input user topic.Next, the augmented generation module uses the user-provided topic and retrieved segments to produce an RAG answer, formatted into individual sentences, citing the relevant information from the top- retrieved segments.Ragnar\u00f6k is deeply integrated with existing Python frameworks, such as Pyserini [31] and rank_llm [46,47] and can be easily installed via PyPI using \"pip install pyragnarok\".The framework offers easy-to-use REST APIs and an integrated WebUI to enhance user-friendliness and improve the human evaluation experience.\n\nThe Ragnar\u00f6k framework will be used for providing baselines in the upcoming TREC 2024 Retrieval Augmented Generation (RAG) Track. 1 An ideal framework should include a sufficiently large document collection covering diverse information and non-factoid, decompositional topics requiring long-form answers.",
            "score": 0.4024982055363106,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 596
                },
                {
                    "start": 598,
                    "end": 697
                },
                {
                    "start": 697,
                    "end": 778
                },
                {
                    "start": 778,
                    "end": 884
                },
                {
                    "start": 884,
                    "end": 1009
                },
                {
                    "start": 1009,
                    "end": 1203
                },
                {
                    "start": 1205,
                    "end": 1285
                },
                {
                    "start": 1285,
                    "end": 1421
                },
                {
                    "start": 1421,
                    "end": 1639
                },
                {
                    "start": 1639,
                    "end": 1813
                },
                {
                    "start": 1813,
                    "end": 1953
                },
                {
                    "start": 1955,
                    "end": 2259
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 41,
                    "end": 44,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 44,
                    "end": 47,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 47,
                    "end": 50,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 428,
                    "end": 432,
                    "matchedPaperCorpusId": "258865710"
                },
                {
                    "start": 432,
                    "end": 435,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 435,
                    "end": 438,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 438,
                    "end": 441,
                    "matchedPaperCorpusId": "259360665"
                },
                {
                    "start": 472,
                    "end": 476,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1719,
                    "end": 1723,
                    "matchedPaperCorpusId": "235366815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59814453125
        },
        {
            "corpus_id": "277113376",
            "title": "Graph-Based Re-ranking: Emerging Techniques, Limitations, and Opportunities",
            "text": "Retrieval Augmented Generation (RAG) is an established research area that combines pretrained parametric and nonparametric memory for downstream language generation [Lewis et al., 2020]. Recently, there has been an emergence of using Knowledge Graphs as the external non-parametric datastore, in which structural information is queried to capture relational knowledge [Dong et al., 2024b]. Graph-RAG approaches typically follow a two-phased retrieval procedure, also known as re-ranking [Peng et al., 2024]. Twophased retrieval approaches consist of a primary retrieval method in which given a search query, an initial set of probable responses is fetched using techniques such as approximate nearest neighbor (ANN) indexing, traditional keywordbased search, or embedding-based retrieval. The initial retrieval process generally often prioritizes compute efficiency over perfect accuracy, leading to prompting with irrelevant, noisy context and increased hallucination to the final output [Glass et al., 2022]. In a two-phase setup, re-ranking methods distill the initial set of retrieved documents by re-scoring the initial list according to a refined relevance score. * Equal Contribution. \n\nQuerying the complex structure of the knowledge graph presents challenges for popular Large Language Model (LLM)-based re-ranker models. This challenge has inspired recent research in exploring the potential of Graph Neural Networks (GNNs) for exploiting structural information across entities and capturing relational knowledge for prompting a language model for generation. \n\nFor effective graph-based re-ranking, researchers have developed expansive and specialized methods focused primarily on developing (1) unique GNN model architectures and (2) constructing unique graph representation structures specifically optimized for retrieval tasks. However, many of these methods have only recently emerged, leading to gaps in community-wide evaluation of model architectures and best practices. Additionally, these methods have not yet been collated into a systematic review. To fill this gap, we provide a comprehensive overview of emerging GNN-based ranking model architectures and their corresponding graph representation construction methodologies. We conclude by providing recommendations on future research based on our findings and analysis of limitations.",
            "score": 0.4024982055363106,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1569
                },
                {
                    "start": 1572,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2246
                },
                {
                    "start": 2247,
                    "end": 2357
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 185,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 368,
                    "end": 388,
                    "matchedPaperCorpusId": "258558037"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68994140625
        },
        {
            "corpus_id": "8563794",
            "title": "Supporting Annotation Layers for Natural Language Processing",
            "text": "Today most natural language processing (NLP) algorithms make use of the results of previous processing steps. For example, a word sense disambiguation algorithm may combine the output of a tokenizer, a part-of-speech tagger, a phrase boundary recognizer, and a module that classifies noun phrases into semantic categories. Currently there is no standard way to represent and store the results of such processing for efficient retrieval. \n\nWe propose a framework for annotating text with the results of NLP processing and then querying against those annotations in flexible ways. The framework includes a query language and an indexing architecture for efficient retrieval, built on top of a relational database management system (RDBMS). The model allows for both hierarchical and overlapping layers of annotation as well as for querying at multiple levels of description. \n\nIn the remainder of the paper we describe related work, illustrate the annotation model and the query language and describe the indexing architecture and the experimental results, thus showing the feasibility of the approach for a variety of NLP tasks.",
            "score": 0.40144289621052925,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 436
                },
                {
                    "start": 439,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 872
                },
                {
                    "start": 875,
                    "end": 1127
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49169921875
        },
        {
            "corpus_id": "267682657",
            "title": "A Review of Greek NLP Technologies for Chatbot Development",
            "text": "Classical NLP approaches focus on lexical, grammatical and syntactical methodologies.However, these methodologies do not encapsulate textual semantic context.This context is captured by word embedding models, such as Word2Vec [9] and FastText [10].Classical NLP approaches also use simple neural ML architectures, which are less accurate than their DL counterparts.Considering the above, notable modern NLP research efforts for the Greek language are presented below.Specifically, this section is organized as follows: Section 2.1 presents research works pertaining to Greek embeddings; Section 2.2 showcases prominent Greek DL language models; Section 2.3 highlights important Greek DL applications.",
            "score": 0.4009741064911052,
            "section_title": "GREEK NLP RESOURCES",
            "char_start_offset": 3545,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 85,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 248
                },
                {
                    "start": 248,
                    "end": 365
                },
                {
                    "start": 365,
                    "end": 467
                },
                {
                    "start": 467,
                    "end": 700
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11676025390625
        },
        {
            "corpus_id": "258947730",
            "title": "RFiD: Towards Rational Fusion-in-Decoder for Open-Domain Question Answering",
            "text": "Open Domain Question Answering (ODQA). The prevailing approach to ODQA involves using a retriever to pinpoint relevant passages for a given question from a vast database, such as Wikipedia, and then employing a reader to generate the final answer. This is achieved by integrating the retrieved passages and question with a large pretrained language model. Retrievers commonly use methods ranging from string-matching algorithms like BM25, to dense retrievers such as DPR , and semi-parametric retrievers like SEAL (Bevilacqua et al., 2022). The reader models fall into two primary categories: Extractive readers, such as DPR-reader , identify the answer spans from the retrieved passages, while Generative readers, including the Fusion-in-Decoder model (FiD) (Izacard and Grave, 2020b) and the Retrieval-Augmented Generation model (RAG) , generate the answer in a sequence-to-sequence manner.\n\nOur work seeks to enhance the reasoning abil-ity of the FiD reader without modifying the retriever. To this end, KG-FiD  uses knowledge graphs to rerank and concatenate related passages for improved performance and efficiency. GRAPE (Ju et al., 2022) incorporates knowledge graphs into FiD by integrating the Relation-aware graph neural network into the encoder. R2-D2 (Fajcik et al., 2021) combines a passage reranker, an extractive reader, and two generative readers into a single comprehensive ensemble model. Unlike these approaches, our work does not involve the use of external knowledge or alternate model architectures, instead focusing on spurious patterns and reader rationale capabilities.\n\nRationale. Recently, spurious patterns have come into the spotlight in NLP (Slack et al., 2020;Jo and Bengio, 2017), demonstrating a significant impact on model generalization (Kaushik et al., 2020(Kaushik et al., , 2021. Various strategies have been implemented to curb spurious features in tasks like sentiment analysis (Lu et al., 2022;Yang et al., 2021), NER (Zeng et al., 2020;, NLI (Wu et al., 2022) and more (Wang and Culotta, 2020). Our work",
            "score": 0.4004112879531297,
            "section_title": "Related Work",
            "char_start_offset": 2998,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1127,
                    "end": 1144,
                    "matchedPaperCorpusId": "252734715"
                },
                {
                    "start": 1671,
                    "end": 1691,
                    "matchedPaperCorpusId": "211041098"
                },
                {
                    "start": 1691,
                    "end": 1711,
                    "matchedPaperCorpusId": "9934597"
                },
                {
                    "start": 1772,
                    "end": 1793,
                    "matchedPaperCorpusId": "203591519"
                },
                {
                    "start": 1793,
                    "end": 1816,
                    "matchedPaperCorpusId": "222133021"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50634765625
        },
        {
            "corpus_id": "259064264",
            "title": "Reimagining Retrieval Augmented Language Models for Answering Queries",
            "text": "As language models have grown larger (Kaplan et al., 2020;Hoffmann et al., 2022), they have fared better and better on question answering tasks (Hendrycks et al., 2021) and have become the foundation of impressive demos like Chat-GPT (Ouyang et al., 2022;ChatGPT3-OpenAI). Models like GPT-3 (Brown et al., 2020) and Chat-GPT generate fluent, human-like text, which comes the potential for misuse as in high-stakes healthcare settings (Dinan et al., 2021). Large language models (LLMs) also come with several significant issues (Hoffmann et al., 2022;Bender et al., 2021). \n\nLLMs are costly to train, deploy, and maintain, both financially and in terms of environmental impact (Bender et al., 2021). These models are also almost always the exclusive game of industrial companies with large budgets. Perhaps most importantly, the ability of LLMs to make predictions is not commensurate with their ability to obtain insights about their predictions. Such models can be prompted to generate false statements (Wallace et al., 2019a), often do so unprompted (Asai et al., 2022) and when combined with its ability to easily fool humans, can lead to misuse (Macaulay, 2020). \n\nIn recent years, we have seen the promise of retrieval-augmented language models partially addressing the aforementioned shortcomings (Guu et al., 2020;Lewis et al., 2020;Borgeaud et al., 2021;Izacard et al., 2022;Yasunaga et al., 2022a). The architecture of such models is semi-parametric, where the model integrates model parameters and knowledge from external data sources to make its predictions. The first step of performing a task in these architectures is to retrieve relevant knowledge from the external sources, and then perform finer-grained reasoning.",
            "score": 0.39932777916961776,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1166
                },
                {
                    "start": 1169,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1731
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 168,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 234,
                    "end": 255,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 291,
                    "end": 311,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 550,
                    "end": 570,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 676,
                    "end": 697,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1004,
                    "end": 1027,
                    "matchedPaperCorpusId": "201698258"
                },
                {
                    "start": 1052,
                    "end": 1071,
                    "matchedPaperCorpusId": "245219136"
                },
                {
                    "start": 1303,
                    "end": 1321,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.451416015625
        },
        {
            "corpus_id": "253080421",
            "title": "Decoding a Neural Retriever\u2019s Latent Space for Query Suggestion",
            "text": "Neural retrieval models have been at the core of recent improvements among a range of different NLP tasks. Lewis et al. (2020b) augment a language generation model, BART (Lewis et al., 2020a), with a DPR neural retriever and evaluate on multiple knowledge-intensive NLP tasks; most notably, they improve over previous models on multiple open-domain QA benchmarks using an abstractive method. Izacard and Grave (2021) propose the Fusion-in-Decoder method to aggregate a large set of documents from the neural retriever and provide them to the model during answer generation. Their focus is on open-domain QA where they significantly outperform previous models when considering a large set of documents during decoding. Shuster et al. (2021) use neural retrieval models to improve conversational agents in knowledgegrounded dialogue. They show that the issue of hallucination -i.e., generating factual incorrect knowledge statements -can be significantly re-duced when using a neural-retriever-in-the-loop architecture. Separating the retrieval-augmented knowledge generation and the conversational response generation can further improve the issue of hallucination in knowledge-grounded dialogue and helps fuse modular QA and dialogue models (Adolphs et al., 2021). Recently, retrieval query generation approaches have been proposed to improve open-domain dialogue (Komeili et al., 2021) and language modeling (Shuster et al., 2022).\n\nQuery Generation Query optimization is a longstanding problem in IR (Lau and Horvitz, 1999;Teevan et al., 2004). Recent work has investigated query refinement with reinforcement learning for Open Domain and Conversational Question Answering (Nogueira and Cho, 2017;Buck et al., 2018;. The methods presented in this paper are a natural complement to the work of Adolphs et al. (2022), who propose a heuristic approach to generate multistep query refinements, used to train sequential query generation models for the task of learning to search. Their method is also inspired by relevance feedback, but they seek to reach the gold document purely in language space, by brute force exploration. For this purpose, they use specialized search operators to condition the retrieval results as desired. Huebscher et al. (",
            "score": 0.3993217032102638,
            "section_title": "Applications of Neural Retrievers",
            "char_start_offset": 5109,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 127,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 170,
                    "end": 191,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 718,
                    "end": 739,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 1525,
                    "end": 1545,
                    "matchedPaperCorpusId": "1180143"
                },
                {
                    "start": 1675,
                    "end": 1699,
                    "matchedPaperCorpusId": "125545"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.474853515625
        },
        {
            "corpus_id": "261030382",
            "title": "Domain Adaptive Code Completion via Language Models and Decoupled Domain Databases",
            "text": "Retrieval-augmented language models (R-LMs) utilize retrieval-based techniques to improve the performance of LMs and can be divided into two main categories: block R-LMs and token R-LMs. Block R-LMs [12], [13], [48] are similar to one-shot or few-shot learning [49], where one or a few examples are retrieved from a database instead of being randomly selected. Token R-LMs [15], [24], [50] retrieve tokens from database and then combine the retrieved results into the LM. Compared with block R-LMs, token R-LMs can update retrieval results at the same time of generating new tokens, hence our approach uses the architecture of token R-LM. However, token R-LMs suffer from high storage costs and require hyper-parameters selection to combine the inference results from the database and language model. \n\nVarious approaches have been proposed to address the limitations of token R-LMs. For example, kNN-Adapter [51] uses a trained network to determine the combination weights. To reduce the search cost, RetoMaton [52] uses the automaton states to save search time, while AdaptRet [53] uses a trained network to decide whether to use the retrieval module. GNN-LM [50] selects similar texts and builds a contextual graph to incorporate into the language model. In contrast, our proposed approach does not require training or the addition of an additional module. By decoupling the datastore and language model, we can save the storage cost and utilize Bayesian inference to select suitable hyper-parameters at the same time.",
            "score": 0.39857759446045876,
            "section_title": "B. Retrieval-augment Language Model",
            "char_start_offset": 37070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 800
                },
                {
                    "start": 803,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1521
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 203,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "247450969"
                },
                {
                    "start": 373,
                    "end": 377,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 379,
                    "end": 383,
                    "matchedPaperCorpusId": "222125236"
                },
                {
                    "start": 385,
                    "end": 389,
                    "matchedPaperCorpusId": "239016943"
                },
                {
                    "start": 1012,
                    "end": 1016,
                    "matchedPaperCorpusId": "246431219"
                },
                {
                    "start": 1079,
                    "end": 1083,
                    "matchedPaperCorpusId": "237452184"
                },
                {
                    "start": 1161,
                    "end": 1165,
                    "matchedPaperCorpusId": "239016943"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7783203125
        },
        {
            "corpus_id": "269605025",
            "title": "R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models",
            "text": "Recently, large language models (LLMs) have attracted extensive attention, which are typically pre-trained on large datasets and implicitly store substantial amounts of world or domain knowledge [33,45]. However, LLMs are also prone to the hallucination problem, and thus, they may generate erroneous responses [49]. In contrast, retrieval-augmented LLMs [17,10,47,35] retrieve knowledge from an external datastore when needed, thereby reducing hallucinations and increasing the knowledge coverage in responses. \n\nIn the literature, there are two major research aspects in this field: \n\n(1) Datastore Indexing [17,10,44,48] and (2) Document Retrieval [35,27]. For Datastore Indexing, these approaches utilize pre-trained models to generate static embeddings for documents, which are viewed as mounted external memory, and they leverage various semantic similarities to enhance indexing. For Document Retrieval, the system initially retrieves a collection of relevant documents based on the semantic relevance between the user query and the documents. Then, the LLMs concatenate these highly related documents in an unordered manner to the prompt input [4], which makes LLMs better at answering factual questions. These methods essentially organize the information related to the user query from the perspective of coarse-grained memory , ignoring the fine-grained relationships between retrieved documents and the knowledge mastery characteristics of LLMs [14,22]. For instance, the ordering of the top-K retrieved documents can be further adjusted to enhance the performance of retrieval-augmented LLMs in answering questions more accurately, as illustrated in Figure 1. \n\nIn this paper, we propose the Reinforced Retriever-Reorder-Responder framework (R 4 ) to formalize a new retrieval-augmented generation (RAG) pipeline. To reorder the retrieved top-K documents and enhance the response effectiveness of the LLMs, we divide the reorder learning process into the following two steps: Document Order Adjustment: Prior research indicates that LLMs have a better recall of information at the beginning and the ending positions of retrieved documents in prompts [14,22].",
            "score": 0.39857759446045876,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1671
                },
                {
                    "start": 1674,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 2170
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 202,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 355,
                    "end": 359,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 362,
                    "end": 365,
                    "matchedPaperCorpusId": "253802096"
                },
                {
                    "start": 610,
                    "end": 614,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 617,
                    "end": 620,
                    "matchedPaperCorpusId": "249674500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7939453125
        },
        {
            "corpus_id": "274423377",
            "title": "Zero-Indexing Internet Search Augmented Generation for Large Language Models",
            "text": "Large language models (LLM) have demonstrated remarkable capabilities, fueling a new wave of innovative AI applications [1]. To incorporate information not included during their training phase, retrieval augmented generation (RAG) has been developed as an efficient method to enhance LLM performance by integrating externally retrieved information [2,3]. Usually, RAG systems use an internal retrieval module that employs various indexing mechanisms to manage a static corpus. In this paper, we study an alternative approach, exploring the design and implementation of a RAG paradigm that leverages standard search engine APIs (such as Google and Bing), which allows for the flexible and dynamic integration of the most update-to-date online information, thereby improving the quality of content generated by LLMs. \n\nWhile RAG systems with a static retrieval component, which leverages a fixed corpus of data, are effective for tasks within well-defined knowledge domains, Internet search augmented generation [4,5,6,7,8,9] offers distinct advantages. By accessing the most update-to-date information available online, internet search augmented generation is particularly beneficial for tasks that require up-to-date data, such as news updates, market trends, or recent scientific discoveries, enabling the model to produce more relevant and timely responses [9]. Moreover, Internet search augmented systems can retrieve information from a vast array of sources across the web, encompassing a broader range of topics and perspectives, thereby enhancing the comprehensiveness and diversity of the generated content [6]. In fact, OpenAI has also released its toolkit CHATGPT-SEARCH to integrate information from the Internet to improve the user experience very recently [10].",
            "score": 0.39857759446045876,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1773
                }
            ],
            "ref_mentions": [
                {
                    "start": 1013,
                    "end": 1015,
                    "matchedPaperCorpusId": "236034557"
                },
                {
                    "start": 1019,
                    "end": 1021,
                    "matchedPaperCorpusId": "258190866"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "261531340",
            "title": "Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction",
            "text": "Taxonomy relations exist in many knowledge engineering and modeling applications. The most common application in knowledge engineering is the hypernym relations in lexicon databases such as WordNet [6], while the taxonomy relations form a tree of related words. Inheritance (or generalization) is also a type of taxonomy relation and typically exists in most object-oriented languages, domain modeling, relational databases, and OWL ontologies. Depending on whether multiparent is allowed, the relations can either form a tree of DAG. Finally, aggregation and composition relations in UML class diagrams can also be seen as tree-structured taxonomies. \n\nC. Language Models a) Large language models (LLMs): Language modeling is a classic task in natural language processing (NLP) that involves predicting the probability of a sequence of tokens. Large language models (LLMs) leverage deep neural networks, commonly using transformer blocks [7]. \n\nFor a sequence of input tokens (referred to as a prompt) s = {s 1 , s 2 , ..., s k\u22121 }, LLMs estimate the probability of the next token P (s k |s 1 , ..., s k\u22121 ). LLMs can also be used for text generation through auto-regression, where the newly predicted token is appended to the token sequence to predict subsequent tokens. In this scenario, a search method such as beam search is typically utilized to generate the final output sequence [8]. However, such search methods also introduce variability to the output of LLMs across different runs for the same input. \n\nAfter pre-training an LLM for language modeling, it can be customized for other tasks by fine-tuning, which involves updating a subset of its parameters [9]. Recently, LLMs such as GPT-3 [8] and GPT-4 [10] show remarkable generalizability, as they can be adapted to other tasks by providing a few examples within the prompt, without parameter updates [8]. \n\nThis paper explores the application of LLM fine-tuning and prompting techniques in the context of taxonomy construction. In particular, we compare fine-tuning approaches for widely-used open-source and cost-effective LLMs, while also investigating the potential of prompting techniques for more powerful, black-box LLMs accessible through an API.",
            "score": 0.39834373526943356,
            "section_title": "B. Taxonomy Relations Applications",
            "char_start_offset": 6760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1511
                },
                {
                    "start": 1514,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1869
                },
                {
                    "start": 1872,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2218
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 201,
                    "matchedPaperCorpusId": "1671874"
                },
                {
                    "start": 939,
                    "end": 942,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1387,
                    "end": 1390,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1701,
                    "end": 1704,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1865,
                    "end": 1868,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.355224609375
        },
        {
            "corpus_id": "271161731",
            "title": "PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents",
            "text": "Large Language Models (LLMs) such as GPT-4 [24] and LLaMA 3 [32] have significantly advanced the field of natural language processing (NLP) by demonstrating impressive performance across various tasks and exhibiting emergent abilities that push the boundaries of artificial intelligence [7].However, these models face challenges such as generating unreliable outputs due to issues like hallucination and outdated parametric memories [5].\n\nRetrieval-Augmented Generation (RAG) models have shown promise in addressing these issues by integrating externally retrieved information to support more effective performance on complex, knowledge-intensive tasks [20].Despite these advancements, the deployment of RAG systems within broader AI frameworks continues to face significant challenges, particularly in handling noise and irrelevance in retrieved data [8].\n\nA key limitation of existing RAG systems is their inability to adapt outputs to users' specific informational and contextual needs.Personalized techniques in information retrieval, such as adaptive retrieval based on user interaction data and context-aware strategies, are increasingly recognized as essential for enhancing user interaction and satisfaction [30,31].These methods aim to refine the retrieval process dynamically, tailoring it more closely to individual user profiles and situational contexts [1].The integration of agent-based systems with personalized RAG architectures presents a compelling avenue for research.Such systems utilize a multi-agent framework to simulate complex, adaptive interactions tailored to user-specific requirements [35].By embedding intelligent, user-oriented agents within the RAG framework, these systems can evolve into more sophisticated tools that not only retrieve relevant information but also align it closely with the user's specific preferences and contexts in real-time.Importantly, the personalization strategy employed in these systems is fully transparent to the user, ensuring that the user is aware of how their information is being used to tailor the results.\n\nIn this study, we present PersonaRAG, an innovative methodology that extends traditional RAG frameworks by incorporating user-centric agents into the retrieval process.This approach addresses the previously mentioned limitations by promoting active engagement with retrieved content and utilizing dynamic, realtime user data to continuously refine and personalize interactions.PersonaRAG aims to enhance the precision and relevance of LLM outputs, adapting dynamically to user-specific needs while maintaining full transparency regarding the personalization process.",
            "score": 0.39781218704753973,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 291
                },
                {
                    "start": 291,
                    "end": 437
                },
                {
                    "start": 439,
                    "end": 658
                },
                {
                    "start": 658,
                    "end": 856
                },
                {
                    "start": 858,
                    "end": 989
                },
                {
                    "start": 989,
                    "end": 1224
                },
                {
                    "start": 1224,
                    "end": 1370
                },
                {
                    "start": 1370,
                    "end": 1487
                },
                {
                    "start": 1487,
                    "end": 1619
                },
                {
                    "start": 1619,
                    "end": 1880
                },
                {
                    "start": 1880,
                    "end": 2075
                },
                {
                    "start": 2077,
                    "end": 2245
                },
                {
                    "start": 2245,
                    "end": 2454
                },
                {
                    "start": 2454,
                    "end": 2643
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 290,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 433,
                    "end": 436,
                    "matchedPaperCorpusId": "256662612"
                },
                {
                    "start": 653,
                    "end": 657,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 852,
                    "end": 855,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 1216,
                    "end": 1220,
                    "matchedPaperCorpusId": "207744803"
                },
                {
                    "start": 1220,
                    "end": 1223,
                    "matchedPaperCorpusId": "316030"
                },
                {
                    "start": 1366,
                    "end": 1369,
                    "matchedPaperCorpusId": "700267"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.609375
        },
        {
            "corpus_id": "273482672",
            "title": "Optimizing Retrieval-Augmented Generation with Elasticsearch for Enhanced Question-Answering Systems",
            "text": "Recent advancements in question-answering systems and information retrieval have largely been driven by the integration of sophisticated retrieval mechanisms and deep learning models. This section highlights key developments in pre-trained models, sentiment analysis, text classification, and neural networks, all of which are closely related to optimizing the Retrieval-Augmented Generation (RAG) framework using Elasticsearch. \n\nPre-trained models have demonstrated significant improvements in NLP tasks such as text classification and named entity recognition (NER) [16]. An ensemble learning approach driven by the ALBERT model has been shown to enhance text classification, providing insights into how domain-specific models can improve task accuracy when applied to large-scale data [17]. Comparative studies of pretrained models for NER also offer valuable benchmarks, underscoring how various pre-trained architectures improve entity extraction in RAG systems [18]. These approaches highlight the potential for further enhancing Elasticsearchbased retrieval within RAG. In sentiment analysis, the use of graph neural networks (GNNs) combined with syntactic features provides an advanced framework for understanding and classifying sentiment in text [19]. This method demonstrates the importance of syntactic and semantic feature integration, a concept that aligns with enhancing the semantic retrieval capabilities in Elasticsearch to support more complex question-answering systems. \n\nMethods for transforming multidimensional data into interpretable formats also play a crucial role in advanced data mining tasks [20]. A novel approach for converting time-series data into event sequences has been proposed to improve the interpretability of machine learning outcomes [21]. This focus on transforming complex data into structured, retrievable information supports the idea that Elasticsearch's advanced indexing can facilitate more effective retrieval in RAG systems. Finally, multimodal transformers, which combine word embeddings such as ELMo with deep learning algorithms, have been applied in image description tasks, further illustrating the potential for integrating various data types into NLP models [22]. This multimodal approach supports the broader goal of enhancing RAG systems, particularly in scenarios where both textual and non-textual data must be retrieved and processed efficiently. The integration of deep learning models, graphbased techniques, and advanced pre-trained architectures has significantly improved NLP tasks such as text classification, NER, and sentiment analysis.",
            "score": 0.39768610721765124,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 4664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 428
                },
                {
                    "start": 431,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1491
                },
                {
                    "start": 1494,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2223
                },
                {
                    "start": 2224,
                    "end": 2411
                },
                {
                    "start": 2412,
                    "end": 2609
                }
            ],
            "ref_mentions": [
                {
                    "start": 569,
                    "end": 573,
                    "matchedPaperCorpusId": "267337563"
                },
                {
                    "start": 1623,
                    "end": 1627,
                    "matchedPaperCorpusId": "272719041"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62744140625
        },
        {
            "corpus_id": "260164994",
            "title": "FinTree: Financial Dataset Pretrain Transformer Encoder for Relation Extraction",
            "text": "In designing our approach for relation extraction in financial documents, we first conducted an exploratory analysis to identify an appropriate pretrained language model to serve as our foundational backbone. The models evaluated in this study included well-known architectures in the NLP field, known for their exemplary performance in various tasks. Specifically, we considered BERT [4], RoBERTa [5], ALBERT [6], and DeBERTa [7] in their Base and Large configurations. All chosen language models were trained on the same REFinD train dataset with the same hyperparameters to get fairness. We measure the performance using the development dataset. We adopted F1-score metrics in macro, micro, and weighted configurations to evaluate the models comprehensively. \n\nOur exploratory analysis revealed that DeBERTa consistently outperformed the other models. A detailed exposition of the performance metrics for each model is in Table 1. Accordingly, we used DeBERTa as our language model backbone for all subsequent experiments.",
            "score": 0.3972021862757163,
            "section_title": "METHODS 2.1 Model Selection",
            "char_start_offset": 2287,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 761
                },
                {
                    "start": 764,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1025
                }
            ],
            "ref_mentions": [
                {
                    "start": 385,
                    "end": 388,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 410,
                    "end": 413,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 427,
                    "end": 430,
                    "matchedPaperCorpusId": "244346093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.336181640625
        },
        {
            "corpus_id": "272368251",
            "title": "The Design of an LLM-powered Unstructured Analytics System",
            "text": "Large language models have inspired the imagination of industry, and enterprises are using LLMs for product search, customer support chatbots, code co-pilots, and application assistants, to name a few. In enterprise settings, accuracy is paramount, so to limit hallucinations, most of these applications are backed by semantic search architectures. They answer queries based on data retrieved from a few documents or chunks in their knowledge base, a technique known as retrieval-augmented generation (RAG) [9]. \n\nWe increasingly see that enterprises want to go beyond RAG, and run semantic analyses that require complex reasoning across their repositories of unstructured documents. For example, they want to analyze investment opportunities from research reports and presentations in financial services. From interview transcripts, they want to understand the sentiment toward a particular brand in comparison to others for marketing purposes. From legal summaries, investigators want to understand rules that were violated and the actions taken across a broad set of companies and cases. \n\nIn addition to simple \"hunt and peck\" style patterns seen in RAG, these analyses often require \"sweep and harvest\" style patterns, where one needs to sweep through large collections, perform semantic operations described by natural language (for example, to filter, extract, or summarize information), and then synthesize an answer. For example, \"What percent of environmentally caused incidents were due to wind?\" or \"What is yearly revenue growth and outlook of companies whose CEO recently changed?\". We also see \"data integration\" style patterns where users want to combine information from multiple document collections. For example, \"list the fastest growing companies in the BNPL market and their competitors\", where the competitive information may involve a lookup in a database, in addition to unstructured document search. We also expect compositions of these patterns will become prevalent. \n\nTo answer these types of queries, we're building a system for unstructured analytics that is powered by LLMs, eponymously called Aryn. We take inspiration from relational databases, where we borrow the principles of declarative query processing. Users specify what they want to ask in natural language, and the system automatically constructs a plan (the how) and executes it to compute the answer. We use LLMs liberally for multiple purposes throughout the Aryn stack.",
            "score": 0.3971496472820241,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1090
                },
                {
                    "start": 1093,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 1994
                },
                {
                    "start": 1997,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2242
                },
                {
                    "start": 2243,
                    "end": 2395
                },
                {
                    "start": 2396,
                    "end": 2466
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4404296875
        },
        {
            "corpus_id": "265221515",
            "title": "Sequencing Matters: A Generate-Retrieve-Generate Model for Building Conversational Agents",
            "text": "In this paper, we propose a hybrid method, that combines information retrieval and machine learning methods with state-of-the-art advancements in Language Models across several different architectures. Our solution augmented passage retrieval methods such as BM25 with LLM-enabled response generation and grounding. \n\nWe used multiple off-the-shelf models and algorithms, fine-tuning and enhancing some, and creating some of our own models using existing approaches. Llama is the main LLM that we use in this work. We relied on Llama for all conversational aspects of our solution, and for the initial step of answering the question which we could then use for grounding. Because No Llama response was ever used as a final output, we used the less advanced 13B-chat model for performance reasons. \n\nRetrieval is used two times in our architecture. For the ranking of the PTKBs, common approaches include using keyword similarity [IB19], where keywords of the statement are compared against the keywords of the question. This has many drawbacks, including the fact that it cannot handle follow-up questions (for instance, if the user asks \"What are the best diets\", and the response is a list of 3 diets, if the user asks \"What's so great about the 3rd one?\", there will be no PTKB matches for statements regarding dietary restrictions). Finally, for the task of retrieving the passages from the corpus, many common solutions place too much reliance on the quality and authenticity of the passages in the corpus. Since most corpora today are scrapped from the internet, they contain data from many malicious or untrustworthy sources. Some solutions will just retrieve passages and use the top N passages in generating their response. This can lead to incorrect or bad-quality responses. \n\nTo improve our results, we also trained a Logistic Regression text classification model using TF-IDF Vectorization, using several datasets representing reliable passages (Wikipedia Articles) and unreliable vs. malicious passages (Retrieved from ClueWeb-22B via Keywords often associated with spam and BM25). \n\nFigure 1 illustrates the architecture of our system. It contains a few components and works in the following steps.",
            "score": 0.39709981230435576,
            "section_title": "PROPOSED METHOD 2.1 Overall Architecture",
            "char_start_offset": 5151,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 315
                },
                {
                    "start": 318,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1785
                },
                {
                    "start": 1788,
                    "end": 2095
                },
                {
                    "start": 2098,
                    "end": 2150
                },
                {
                    "start": 2151,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 929,
                    "end": 935,
                    "matchedPaperCorpusId": "85500146"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2432861328125
        },
        {
            "corpus_id": "271924141",
            "title": "Improving embedding with contrastive fine-tuning on small datasets with expert-augmented scores",
            "text": "Text embedding models are fundamental in natural language processing (NLP), serving as lowdimensional vector representations that capture semantic similarity between texts (Aggarwal & Zhai, 2012; Angelov, 2020). They are critical for tasks such as text classification, retrieval, question answering, and dialogue systems. Recent advancements in large language models (LLMs) have spurred interest in retrieval-augmented systems that integrate LLM reasoning with the efficiency of text embeddings. \n\nTwo main research directions exist: enhancing performance in semantic textual similarity (STS) through supervised fine-tuning, normalization, and unsupervised contrastive learning; and addressing text retrieval through dual-encoder architectures and self-supervised pre-training (Li et al., 2023;Wang et al., 2022;Izacard & Grave, 2020;Ren et al., 2021;;Devlin, 2018;Vaswani, 2017). Recent studies have aimed to create unified text representation models through large-scale contrastive learning and prompt-based learning (Muennighoff, 2022), evaluated on benchmarks like the massive text embedding benchmark (MTEB) (Muennighoff et al., 2022), which assesses models across 56 datasets and seven categories. \n\nInnovative models, such as GTE and E5 (Li et al., 2023;Wang et al., 2022), have been developed to address the challenges of creating general-purpose text embeddings. GTE uses multi-stage contrastive learning over diverse datasets, while E5 introduces a progressive learning mechanism to refine embeddings by focusing on zero-shot learning. \n\nEmbeddings play a crucial role in retrieval augmented generation (RAG) (Lewis et al., 2020;Es et al., 2023;Salemi & Zamani, 2024). They represent input text and retrieved knowledge sources in a vector space for efficient similarity comparisons. High-quality embeddings improve retrieval accuracy by capturing the semantic and syntactic information of the text. Embeddings can be combined with metadata or context information to enhance retrieval and generation.",
            "score": 0.39694898178123295,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 495
                },
                {
                    "start": 498,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2009
                }
            ],
            "ref_mentions": [
                {
                    "start": 1619,
                    "end": 1639,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1655,
                    "end": 1677,
                    "matchedPaperCorpusId": "269293655"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.658203125
        },
        {
            "corpus_id": "278636195",
            "title": "AutoPentest: Enhancing Vulnerability Management With Autonomous LLM Agents",
            "text": "Minaee et al. [17] explain that LLMs represent a significant advance in the field of Natural Language Processing (NLP). They are powerful tools designed to understand and generate human language using large datasets and complex neural network architectures. These models are characterised by their massive scale, often containing billions of parameters trained on large corpora of text data from diverse sources. \n\nAt their core, LLMs are often based on the transformer architecture, which uses self-attention mechanisms to process and generate text. This architecture allows the models to capture complex patterns and dependencies in language, enabling them to perform a wide range of tasks such as translation, summarisation and question answering. The training process consists of two main stages: pretraining and fine-tuning. During pre-training, the model learns general language features from large unlabelled text datasets. Fine-tuning then adapts the model to specific tasks using smaller, labelled datasets. \n\nAmong the most notable families of LLMs are the Generative Pre-trained Transformers (GPT) and the Large Language Model Meta AI (LLaMA): \n\n\u2022 GPT family: Developed by OpenAI, the GPT series8 has set benchmarks in the field. Starting with GPT-1, each successive version has increased in size and capability. GPT-3, for example, has 175 billion parameters and is able to perform tasks with minimal task-specific training data, demonstrating the concept of incontext learning. GPT-4 extends these capabilities with improvements in language understanding and generation. \u2022 LLaMA family: The LLaMA models9 developed by Meta are designed to be highly efficient and open source. They range from smaller models with a few billion parameters to larger models that match or exceed the performance of proprietary models such as GPT-3. The LLaMA models are particularly notable for their ability to perform well on a variety of benchmarks with relatively few resources. The latest version of the LLaMA model family is Llama 4, which was released in April 2025. Minaee et al. [17] also describe prompt engineering, the temperature value, LLM agents and Retrieval-Augmented Generation (RAG). These concepts will now be explained. \n\na) Prompt engineering: This technique is used to maximise the utility of LLMs.",
            "score": 0.39644263852267736,
            "section_title": "C. Large Language Models",
            "char_start_offset": 7506,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1154
                },
                {
                    "start": 1157,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2194
                },
                {
                    "start": 2195,
                    "end": 2232
                },
                {
                    "start": 2235,
                    "end": 2313
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55126953125
        },
        {
            "corpus_id": "267947932",
            "title": "Exploratory analysis on the natural language processing models for task specific purposes",
            "text": "Natural language processing (NLP) is a technology that has become widespread in the area of human language understanding and analysis. A range of text processing tasks such as summarisation, semantic analysis, classification, question-answering, and natural language inference are commonly performed using it. The dilemma of picking a model to help us in our task is still there. It\u2019s becoming an impediment. This is where we are trying to determine which modern NLP models are better suited for the tasks set out above in order to compare them with datasets like SQuAD and GLUE. For comparison, BERT, RoBERTa, distilBERT, BART, ALBERT, and text-to-text transfer transformer (T5) models have been used in this study. The aim is to understand the underlying architecture, its effects on the use case and also to understand where it falls short. Thus, we were able to observe that RoBERTa was more effective against the models ALBERT, distilBERT, and BERT in terms of tasks related to semantic analysis, natural language inference, and question-answering. The reason is due to the dynamic masking present in RoBERTa. For summarisation, even though BART and T5 models have very similar architecture the BART model has performed slightly better than the T5 model.",
            "score": 0.3959137955798028,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.345947265625
        },
        {
            "corpus_id": "254823295",
            "title": "FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference",
            "text": "Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the architecture used for FiD was chosen by making minimal modifications to a standard T5 model, which our analysis shows to be highly suboptimal for a retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to the encoder, while the majority of inference time results from memory bandwidth constraints in the decoder. We propose two simple changes to the FiD architecture to alleviate memory bandwidth constraints, and speed up inference by 7x. This allows us to use a much larger decoder at modest cost. We denote FiD with the above modifications as FiDO, and show that it strongly improves performance over existing FiD models for a wide range of inference budgets. For example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves better performance than FiD-Large.",
            "score": 0.39590017828460367,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71728515625
        },
        {
            "corpus_id": "258331833",
            "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
            "text": "This article presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream Natural Language Processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. First, we offer an introduction and brief summary of current language models. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, generation tasks, emergent abilities, and considerations for specific tasks. We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at https://github.com/Mooler0410/LLMsPracticalGuide. An LLMs evolutionary tree, editable yet regularly updated, can be found at llmtree.ai.",
            "score": 0.39585238241547704,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41259765625
        },
        {
            "corpus_id": "271781965",
            "title": "Natural language processing with transformers: a review",
            "text": "Regarding related work, Lin et al. (2022) conducted a comprehensive review of Transformers. The authors explored different architectures of Transformers, highlighting their strengths and weaknesses. They also discussed modifications made to the original Transformer architecture, such as the introduction of self-attention mechanisms and positional encodings. Furthermore, Lin et al. (2022) proposed a taxonomy to categorize different types of Transformers based on their architectural characteristics. This taxonomy serves as a useful framework for researchers and practitioners to better understand the design choices and trade-offs involved in implementing Transformers. Although the paper by Lin et al. (2022) covered a wide range of applications of Transformers, it did not specifically focus on their application in NLP tasks. \n\nIn comparison with the existing studies, in this review we present a structured overview of NLP by addressing the domain of applicability, the problems that NLP solves, the existent architectures, and the data sources that can be used. To achieve the abovementioned goals, we want to address the following research questions: i) What is the current status of the NLP Transformers concerning its applications, language models, and data sets? ii) What are the limitations and challenges of NLP Transformers, and how have researchers attempted to address them? \n\nTo summarize, this review contributes with a structured assessment of the challenges and solutions that NLP transformer-based approaches encompass, along with the problems that can occur: language concepts limitations, specific language characteristics, and expressions. Furthermore, we look at some of the methods that have been implemented to improve the performance of NLP tasks (Xie et al., 2021;Rothe, Narayan & Severyn, 2020;Ham et al., 2021). Taking into account the wide topics that NLP addresses and the solutions that continue to evolve, this study aims to provide an overview of the domains that can be addressed with NLP and the new approaches that occur in this field of study.",
            "score": 0.3950132693096503,
            "section_title": "Rationale",
            "char_start_offset": 2241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1392
                },
                {
                    "start": 1395,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2085
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 41,
                    "matchedPaperCorpusId": "235368340"
                },
                {
                    "start": 373,
                    "end": 390,
                    "matchedPaperCorpusId": "235368340"
                },
                {
                    "start": 696,
                    "end": 713,
                    "matchedPaperCorpusId": "235368340"
                },
                {
                    "start": 1777,
                    "end": 1795,
                    "matchedPaperCorpusId": "219792180"
                },
                {
                    "start": 1795,
                    "end": 1826,
                    "matchedPaperCorpusId": "198967997"
                },
                {
                    "start": 1826,
                    "end": 1843,
                    "matchedPaperCorpusId": "235414966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55078125
        },
        {
            "corpus_id": "271709396",
            "title": "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
            "text": "There are three common LLM architectures, the Encoder-Decoder architecture, exemplified by the traditional transformer model. This architecture comprises six encoders and six decoders, data input into the system will first passes through the encoder, where it undergoes sequential feature extraction via the model's self-attention mechanism. Subsequently, the decoders utilize the word vectors produced by the encoders to generate outputs, this technique is common to see in machine translation tasks, where the encoder processes word vectors from one language through several attention layers and feedforward networks, thereby creating representations of the context. The decoder then uses this information to incrementally construct the correct translated text. A recent example of this architecture is the CodeT5+ model, launched by Salesforce AI Research in 2023 [30]. This model is an enhancement of the original T5 architecture, which designed to improve performance in code understanding and generation tasks. It incorporates a flexible architecture and diversified pre-training objectives to optimize its effectiveness in these specialized areas. This development highlights the competency of Encoder-Decoder architectures in tackling increasingly complex NLP challenges. \n\nThe Encoder-only architecture, as the name suggests it eliminates the decoder from the entire structure making the data more compact. Unlike RNNs, this architecture is stateless and uses a masking mechanism that allows input processing without relying on hidden states, and also accelerating parallel processing speeds and providing excellent contextual awareness. BERT (Bidirectional Encoder Representations from Transformers) is a representative model of this architecture, this model is a large language model built solely on the encoder architecture. BERT leverages the encoder's powerful feature extraction capabilities and pre-training techniques to learn bidirectional representations of text, achieving outstanding results in sentiment analysis and contextual analysis [31]. \n\nThe Decoder-only archiecture, in the transformer framework primarily involves the decoder receiving processed word vectors and generating output. Utilizing the decoder to directly generate text accelerates tasks such as text generation and sequence prediction. This characteristic with high scalability is known as auto-regressiveness, which is why popular models like GPT use this architecture.",
            "score": 0.39493512229695155,
            "section_title": "B. Model Architecture",
            "char_start_offset": 11840,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1279
                },
                {
                    "start": 1282,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 2064
                },
                {
                    "start": 2067,
                    "end": 2212
                },
                {
                    "start": 2213,
                    "end": 2327
                },
                {
                    "start": 2328,
                    "end": 2462
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60400390625
        },
        {
            "corpus_id": "270878612",
            "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs",
            "text": "Retrieval-augmented generation (RAG) (Lewis et al., 2020;Izacard & Grave, 2021;Lin et al., 2024;Wang et al., 2024) is a widely used technique for customizing large language models (LLMs) to handle long-tail knowledge (Mallen et al., 2023;Asai et al., 2024b), provide up-to-date information (Kasai et al., 2023), and adapt to specific domains and tasks (Xiong et al., 2024) without modifying the model weights.In general, a dense embedding based retriever (Karpukhin et al., 2020;Lin et al., 2023;Wang et al., 2022) first retrieves top-k chunked contexts from a collection documents or external database for a given question.Then, LLM reads the top-k contexts to generate the answer.\n\nHowever, the current RAG pipeline has the following limitations: i) LLMs are not good at reading too many chunked contexts (e.g., top-100) even with the long-context window, not only due to efficiency reasons, but also because a shorter list of top-k (e.g., 5, 10) contexts usually leads to higher accuracy of generation (e.g., see Table 5 in Xu et al., 2024b).ii) Given a small k, one needs a mechanism to ensure the high recall of relevant contents.Relying solely on a retrieval model may be inadequate due to challenges in learning effective local alignments across the entire embedding space to support accurate matching (Luan et al., 2021).In practice, a separate ranking model (Nogueira et al., 2020;Glass et al., 2022;Ma et al., 2023) that cross-encodes question and candidate context can work better than a dense embedding-based retriever for obtaining the most relevant top-k contexts from top-N candidates (N \u226b k).iii) However, the zero-shot generalization capability of the expert ranking model can be relatively limited compared to the versatile LLM itself.",
            "score": 0.3946961894953656,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 409
                },
                {
                    "start": 409,
                    "end": 624
                },
                {
                    "start": 624,
                    "end": 682
                },
                {
                    "start": 684,
                    "end": 1045
                },
                {
                    "start": 1045,
                    "end": 1135
                },
                {
                    "start": 1135,
                    "end": 1329
                },
                {
                    "start": 1329,
                    "end": 1608
                },
                {
                    "start": 1608,
                    "end": 1753
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 79,
                    "end": 96,
                    "matchedPaperCorpusId": "263605962"
                },
                {
                    "start": 96,
                    "end": 113,
                    "matchedPaperCorpusId": "263835270"
                },
                {
                    "start": 217,
                    "end": 238,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 290,
                    "end": 310,
                    "matchedPaperCorpusId": "251105205"
                },
                {
                    "start": 455,
                    "end": 479,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 479,
                    "end": 496,
                    "matchedPaperCorpusId": "256868909"
                },
                {
                    "start": 496,
                    "end": 514,
                    "matchedPaperCorpusId": "254366618"
                },
                {
                    "start": 1027,
                    "end": 1044,
                    "matchedPaperCorpusId": "263620134"
                },
                {
                    "start": 1367,
                    "end": 1390,
                    "matchedPaperCorpusId": "212725651"
                },
                {
                    "start": 1390,
                    "end": 1409,
                    "matchedPaperCorpusId": "250391085"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65234375
        },
        {
            "corpus_id": "278165282",
            "title": "From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review",
            "text": "Singh et al. [49] delves into Agentic Retrieval-Augmented Generation (Agentic RAG), a sophisticated evolution of traditional Retrieval-Augmented Generation systems that enhances the capabilities of large language models (LLMs). While LLMs have transformed AI through human-like text generation and language understanding, their dependence on static training data often results in outdated or imprecise responses. The paper addresses these limitations by embedding autonomous agents within the RAG framework, enabling dynamic, real-time data retrieval and adaptive workflows. It details how agentic design patterns such as reflection, planning, tool utilization, and multi-agent collaboration equip these systems to manage complex tasks and support multi-step reasoning. The survey offers a comprehensive taxonomy of Agentic RAG architectures, highlights key applications across various sectors, including healthcare, finance, and education, and outlines practical implementation strategies. \n\nComplementing this architectural perspective, Yehudai et al. [50] mark a significant milestone in artificial intelligence by surveying evaluation methodologies for agents powered by large language models (LLMs). It thoroughly reviews the capabilities of these agents, focusing on core functions such as planning, tool utilization, self-reflection, and memory, while assessing specialized applications ranging from web interactions to software engineering and conversational tasks. The authors uncover a clear trend toward developing more rigorous, dynamically updated evaluation frameworks by examining both targeted benchmarks for domain-specific applications and those designed for more generalist agents. \n\nMoreover, the paper critically highlights existing deficiencies in the field, notably the need for metrics that more effectively capture cost efficiency, safety, and robustness. In doing so, it maps the current landscape of agent evaluation and sets forth compelling directions for future inquiry, underscoring the importance of scalable and fine-grained evaluation techniques in the rapidly evolving AI domain. \n\nSimilarly, Chen et al. [51] focus on Role-Playing Agents (RPAs), a growing class of LLM-based agents that mimic human behavior across various tasks. Recognizing the inherent challenges in evaluating such diverse systems, the authors systematically reviewed 1,676 papers published between January 2021 and December 2024. Their extensive analysis identifies six key agent attributes, seven task attributes, and seven evaluation metrics that are prevalent in the current literature.",
            "score": 0.39380885161687496,
            "section_title": "B. Agent Architectures and Evaluation Frameworks",
            "char_start_offset": 8695,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 990
                },
                {
                    "start": 993,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1700
                },
                {
                    "start": 1703,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2114
                },
                {
                    "start": 2117,
                    "end": 2265
                },
                {
                    "start": 2266,
                    "end": 2436
                },
                {
                    "start": 2437,
                    "end": 2596
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74560546875
        },
        {
            "corpus_id": "271386000",
            "title": "Advancements in Deep Learning Architectures for Natural Language Processing Tasks",
            "text": "Natural Language Processing (NLP) is evolving along with computer and human language communication. As more industries seek to create smart systems that can comprehend and control human language, deep learning is the best technology for most NLP jobs. The depth of learning approach has solved NLP problems like text categorization, sentiment evaluation, machine translation, and speech recognition. \n\nIn contrast, NLP is dynamic. Continuous development presents new obstacles for more complex deep learning architecture. The rise of social media and exposure to personal content, whether written or spoken, has highlighted the need for models to analyses sarcasm, irony, and informal expressions. The growing demand for customized experiences drove conversation engagement and conversational AI systems development. \n\nThus, researchers are developing new neural networks using deep learning architecture for NLP tasks. Recent advances in deep learning architecture improve model capacities to show long-distance relations, process sequential data, and learn from large data sets. These new developments are suitable and effective for a wide range of NLP jobs; thus, they must be evaluated and analyzed. \n\nIts goal is to summarise recent deep learning architectures used for NLP assignments. A deep learning paper will examine RNNs, CNNs, and Transformer-based models. A brief discussion of transfer learning and attention mechanisms in NLP will follow. Additionally, the study provides detailed information about these advancements and their potential uses in natural language processing.",
            "score": 0.3929229168702132,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1589
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.450927734375
        },
        {
            "corpus_id": "277954732",
            "title": "Multi-Stage Retrieval for Operational Technology Cybersecurity Compliance Using Large Language Models: A Railway Casestudy",
            "text": "LLMs are being used in multiple sectors for tasks that involve the analysis of textual artefacts in different ways. In many cases, researchers employ an ensemble of LLM techniques in many domains to enhance performance and accuracy [20], [21]. However, these techniques are not yet fully exploited in the OTCS domain. \n\nOne new technique in such research is retrieval augmented generation (RAG) [22] which combines the strengths of traditional information retrieval systems (such as databases) with the capabilities of generative large language models. By combining this additional knowledge with its own language skills, AI can write text that is more accurate, up-to-date, and relevant to the individual's specific needs [23]. This is powered by a retrieval engine that works by efficiently retrieving informational nodes from an external database, using a retriever, and then incorporating this context into the context window of the LLM. \n\nResearch has already addressed compliance in the architecture, engineering and construction industry (AEC) [24]. One such approach focused on evaluating different prompt engineering techniques [25]. Their compliance goal was much simpler than that of this project. More specifically, the authors used pairs of fire safety regulations and building design specifications. Their conclusions determined that the design of prompt engineering largely determined the performance of the LLM. In this research, detailed prompts are constructed to instruct the LLM. Despite their small data set and less complex domain, LLMs showed promise in classifying compliance and non-compliance. The disadvantage of using long context is that it may not perform well with larger specifications. Regulations are likely to target key areas of the design specifications and, as a result, the context may become diluted. This dilution can make it challenging for the system to focus on the most relevant information, potentially reducing the accuracy and effectiveness of compliance verification. \n\nAn example, in the medical field, involves an investigation of how compliance can be assessed against reporting guidelines in clinical trials [26]. They manually extracted textquestion pairs as their method. The findings showed that the LLM demonstrated an acceptable classification accuracy of greater than 95% in its compliance evaluation. They attribute this success to the fine-tuning of their models as increased performance is observed when this occurs.",
            "score": 0.39289587099041484,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 6820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 941
                },
                {
                    "start": 944,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2016
                },
                {
                    "start": 2019,
                    "end": 2166
                },
                {
                    "start": 2167,
                    "end": 2226
                },
                {
                    "start": 2227,
                    "end": 2360
                },
                {
                    "start": 2361,
                    "end": 2478
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 236,
                    "matchedPaperCorpusId": "270766156"
                },
                {
                    "start": 238,
                    "end": 242,
                    "matchedPaperCorpusId": "270491236"
                },
                {
                    "start": 395,
                    "end": 399,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1051,
                    "end": 1055,
                    "matchedPaperCorpusId": "59792326"
                },
                {
                    "start": 2161,
                    "end": 2165,
                    "matchedPaperCorpusId": "266221753"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.360595703125
        },
        {
            "corpus_id": "266550947",
            "title": "Towards Better Monolingual Japanese Retrievers with Multi-Vector Models",
            "text": "Document retrieval has become a very important component of many applications, especially in the context of the rapid development of Retrieval-Augmented Generation (RAG) applications, leveraging document retrieval to improve the capabilities of Large Language Models [7] (LLMs). Since the advent of the transformers architecture, deep-learning based approaches to document retrieval have become widely used [26]. \n\nHowever, the focus has largely been on a handful of high-resources languages, such as English [11] and Mandarin [24], with other languages receiving comparatively little attention. Large-scale corpora are largely lacking in most languages [28], if they exist at all. As a result, the best-performing models on most lower-resources languages are multilingual models [2], which come with considerable downsides. Firstly, they are inefficient at the language-level, as highlighted by multilingual models requiring parameters count that are 3-to-5 times larger than their monolingual counterparts models to reach similar performance in high-resources languages such as English [2,23]. Moreover, they also showcase noticeable proficiency gaps between higher and lower resource languages [10] and recent work on LLMs has shown that even within high-resources languages, cultural specificities are often lost by multilingual models [20]. \n\nFor the Japanese language, there has historically been a lack of high-quality, large-scale training datasets [6,19], albeit there has recently been a growing local [3] and international efforts [29] to alleviate this issue, partially by relying on machine-translated English data [1]. However, the use of machine translation can result in varying performances depending on the target language, which is especially apparent in languages that differ from English as much as Japanese [1,21]. As a result, mono-lingual Japanese retrievers, while more computationally efficient, have strongly lagged in performance in comparison to multilingual approaches [2]. \n\nRecently, multi-vector retrieval methods such as ColBERT [5] have demonstrated impressive out-of-domain generalisation performance on many English-language retrieval tasks, while training on orders of magnitude fewer documents than similarly-performing single-vector models [15].",
            "score": 0.39289060749048715,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1345
                },
                {
                    "start": 1348,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 2003
                },
                {
                    "start": 2006,
                    "end": 2285
                }
            ],
            "ref_mentions": [
                {
                    "start": 267,
                    "end": 270,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 407,
                    "end": 411,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 654,
                    "end": 658,
                    "matchedPaperCorpusId": "237213465"
                },
                {
                    "start": 1197,
                    "end": 1201,
                    "matchedPaperCorpusId": "209515542"
                },
                {
                    "start": 1340,
                    "end": 1344,
                    "matchedPaperCorpusId": "261682140"
                },
                {
                    "start": 1457,
                    "end": 1460,
                    "matchedPaperCorpusId": "249687727"
                },
                {
                    "start": 1542,
                    "end": 1546,
                    "matchedPaperCorpusId": "261557946"
                },
                {
                    "start": 1832,
                    "end": 1835,
                    "matchedPaperCorpusId": "5733815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.488037109375
        },
        {
            "corpus_id": "5563288",
            "title": "Solving the Problem of Cascading Errors: Approximate Bayesian Inference for Linguistic Annotation Pipelines",
            "text": "Almost any system for natural language understanding must recover hidden linguistic structure at many different levels: parts of speech, syntactic dependencies, named entities, etc. For example, modern semantic role labeling (SRL) systems use the parse of the sentence, and question answering requires question type classification, parsing, named entity tagging, semantic role labeling, and often other tasks, many of which are dependent on one another and must be pipelined together. Pipelined systems are ubiquitous in NLP: in addition to the above examples, commonly parsers and named entity recognizers use part of speech tags and chunking information, and also word seg-mentation for languages such as Chinese. Almost no NLP task is truly standalone. \n\nMost current systems for higher-level, aggregate NLP tasks employ a simple 1-best feed forward architecture: they greedily take the best output at each stage in the pipeline and pass it on to the next stage. This is the simplest architecture to build (particularly if reusing existing component systems), but errors are frequently made during this pipeline of annotations, and when a system is given incorrectly labeled input it is much harder for that system to do its task correctly. For example, when doing semantic role labeling, if no syntactic constituent of the parse actually corresponds to a given semantic role, then that semantic role will almost certainly be misidentified. It is therefore disappointing, but not surprising, that F-measures on SRL drop more than 10% when switching from gold parses to automatic parses (for instance, from 91.2 to 80.0 for the joint model of Toutanova (2005)). \n\nA common improvement on this architecture is to pass k-best lists between processing stages, for example (Sutton and McCallum, 2005;Wellner et al., 2004). Passing on a k-best list gives useful improvements (e.g., in Koomen et al. (2005)), but efficiently enumerating k-best lists often requires very substantial cognitive and engineering effort, e.g., in (Huang and Chiang, 2005;Toutanova et al., 2005).",
            "score": 0.3928767665059793,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 755
                },
                {
                    "start": 758,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1663
                },
                {
                    "start": 1666,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 2069
                }
            ],
            "ref_mentions": [
                {
                    "start": 1771,
                    "end": 1798,
                    "matchedPaperCorpusId": "1544330"
                },
                {
                    "start": 1798,
                    "end": 1819,
                    "matchedPaperCorpusId": "79054"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40283203125
        },
        {
            "corpus_id": "244954723",
            "title": "Improving language models by retrieving from trillions of tokens",
            "text": "We design our retrieval-enhanced architecture to be capable of retrieving from a database with trillions of tokens. For this purpose, we retrieve at the level of contiguous token chunks instead of individual tokens which reduces storage and computation requirements by a large linear factor. Our method first constructs a key-value database, where values store raw chunks of text tokens and keys are frozen B embedddings (Devlin et al., 2019). We use a frozen model to avoid having to periodically re-compute embeddings over the entire database during training. Each training sequence is then split into chunks, which are augmented with their -nearest neighbour retrieved from the database. An encoder-decoder architecture integrates retrieval chunks into the model's predictions. We summarize the R architecture in Fig. 2, and detail it in this section. We end the section by introducing Left: simplified version where a sequence of length  = 12 is split into  = 3 chunks of size  = 4. For each chunk, we retrieve  = 2 neighbours of  = 5 tokens each. The retrieval pathway is shown on top. Right: Details of the interactions in the C operator. Causality is maintained as neighbours of the first chunk only affect the last token of the first chunk and tokens from the second chunk. a new methodology to evaluate language models when an evaluation set is partially present in the training set.",
            "score": 0.39257392718311546,
            "section_title": "Method",
            "char_start_offset": 4278,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1392
                }
            ],
            "ref_mentions": [
                {
                    "start": 421,
                    "end": 442,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.445068359375
        },
        {
            "corpus_id": "269983737",
            "title": "DuetRAG: Collaborative Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Language Models Augmenting language models with relevant information obtained from various external knowledge bases has been shown to significantly improve the performance of various NLP tasks, including language modeling (Guu et al., 2020;Borgeaud et al., 2022;Shi et al., 2023;Lin et al., 2023) and open domain question answering (Izacard et al., 2022;Zhang et al., 2024).RAG mainly adopts the \"retrieve then read\" paradigm.Specifically, the input question is first used as the query, then the retrieval module retrieves relevant documents from the external knowledge base, and finally the retrieved documents and questions are merged into a complete input to generate final output.For example, RETRO (Borgeaud et al., 2022) modifies the autoregressive LM to focus on relevant documents through chunked cross-attention, thereby introducing new parameters to the model.REPLUG (Shi et al., 2023) assumes black-box access to LM and optimizes it by fine-tuning the retriever.RAFT (Zhang et al., 2024) proposes a fine-tuned data that additionally contains relevant documents and answers with reasoning chains to train language models for domain-specific open-book settings.Finetuning for RAG Recently, related work has studied how to improve the overall performance by fine-tuning the LLM or retriever in the RAG framework.For example, RADIT (Lin et al., 2023) proposes a dual-instruction fine-tuning framework to fine-tune both the LLM and the retriever simultaneously.InstructRetro (Wang et al., 2023) pre-trains a larger autoregressive large-scale language model with retrieval function and performs instruction fine-tuning based on it.ChatQA (Liu et al., 2024) additionally proposes a context-enhanced instruction fine-tuning stage, specifically to enhance the model's ability to perform context awareness in conversational QA.RAFT (Zhang et al., 2024) proposes a kind of fine-tuning data that additionally contains related documents and answers with reasoning chains to fine-tune LLM and improve LLM's ability to understand the retrieved documents under the RAG framework.",
            "score": 0.3908534054751986,
            "section_title": "Related Work",
            "char_start_offset": 2461,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 704
                },
                {
                    "start": 704,
                    "end": 890
                },
                {
                    "start": 890,
                    "end": 993
                },
                {
                    "start": 993,
                    "end": 1190
                },
                {
                    "start": 1190,
                    "end": 1340
                },
                {
                    "start": 1340,
                    "end": 1487
                },
                {
                    "start": 1487,
                    "end": 1656
                },
                {
                    "start": 1656,
                    "end": 1848
                },
                {
                    "start": 1848,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 260,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 260,
                    "end": 282,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 723,
                    "end": 746,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77490234375
        },
        {
            "corpus_id": "278535570",
            "title": "Patchwork: A Unified Framework for RAG Serving",
            "text": "With their rapid advancements, Large Language Models (LLMs) have become indispensable tools across a range of applications, including question answering [59,80], code generation [40,87], task automation [27], and search assistants. Despite major progress in underlying machine learning (ML) techniques, LLMs still face several fundamental limitations-such as hallucinations [20], outdated knowledge [37], and confinement to the snapshot of training data. \n\nTo address these issues, Retrieval-Augmented Generation (RAG) systems [37] have emerged as a widely adopted solution, particularly for applications where factual accuracy is critical. Figure 1 provides an overview of a typical RAG architecture. As illustrated, a RAG system supplements an LLM with a knowledge base-often an external, indexed database-whose contents are retrieved and incorporated into user queries before being passed to the LLM. Some RAG variants [6,32,68,69] go further by enabling recursive interactions between the LLM and the knowledge base, allowing * Both authors contributed equally to this research. iterative refinements of the response. This integration has been shown to significantly improve both the factual accuracy and generation quality of LLM outputs. Moreover, RAG systems offer a practical alternative to costly model retraining, providing a scalable and extensible mechanism for keeping LLMs up to date with new information. These advantages have spurred rapid research and real-world adoption of RAG techniques [17,18,25,46,58,84]. \n\nGiven the growing ubiquity of RAG systems, optimizing their performance is becoming increasingly critical. However, three key challenges make this far from straightforward: \n\n1. Rapid Evolution of the Stack: The RAG ecosystem is evolving at a breakneck pace, with new components, execution strategies, and architectural patterns emerging frequently. This constant change compounds the complexity faced by users in designing and deploying RAG pipelines. More importantly, the resulting fragmentation-a patchwork of bespoke solutions-makes it difficult to develop and apply general, reusable performance management techniques. \n\n2. Heterogeneity of Components: RAG consist of diverse components-including vector databases, query augmenters, and LLM serving engines-each with unique computational characteristics and scaling behaviors.",
            "score": 0.3908534054751986,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 454
                },
                {
                    "start": 457,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1527
                },
                {
                    "start": 1530,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2154
                },
                {
                    "start": 2157,
                    "end": 2362
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 160,
                    "matchedPaperCorpusId": "1373518"
                },
                {
                    "start": 922,
                    "end": 925,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 925,
                    "end": 928,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 1523,
                    "end": 1526,
                    "matchedPaperCorpusId": "268510197"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "corpus_id": "266999263",
            "title": "JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims",
            "text": "Equipping language models (LM) with external memory has shown to enhance their performance in knowledge intensive NLP tasks (Chen et al., 2017;Thorne et al., 2018;Guu et al., 2020;Lewis et al., 2020b;Sachan et al., 2021;Izacard and Grave, 2021b;Borgeaud et al., 2022;Izacard et al., 2023). Typically, a retriever is used to retrieve relevant documents from a large corpus, which enriches the in-put of a language model and contributes to the final output. However, due to the high cost of acquiring query-document annotations and training retrievers, many implementations rely on off-the-shelf retrievers, such as TF-IDF and BM25 (Jones, 2004;Robertson et al., 1994), which use term-matching techniques. In this setup, only the parameters of LMs are fine-tuned. \n\nRecent research has demonstrated the advantages of jointly training the retriever and the LM in an end-to-end manner, which leverages the supervision signals from the LM to train the retriever (Guu et al., 2020;Lewis et al., 2020b;Sachan et al., 2021;Izacard and Grave, 2021b;Izacard et al., 2023). Moreover, considering the remarkable performance of large language models (LLMs) in various few-shot NLP tasks, some studies suggest enhancing LLMs with the retrievers or web search engines (Mallen et al., 2023;Si et al., 2023;Yu et al., 2023;Shi et al., 2023;Zhang and Gao, 2023). For example, REPLUG (Shi et al., 2023) optimizes the retriever by minimizing the KL divergence between the retrieval likelihood and the black-box LLM likelihood over retrieved documents. However, there exists inherent limitations in the interaction between retriever and black-box LLMs, such as their restricted ability to provide or access specific information. We refer readers to a comprehensive survey of retrieval-augmented LMs (Mialon et al., 2023).",
            "score": 0.3908534054751986,
            "section_title": "Retrieval-augmented language models",
            "char_start_offset": 10545,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 761
                },
                {
                    "start": 764,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1800
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 143,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 143,
                    "end": 163,
                    "matchedPaperCorpusId": "4711425"
                },
                {
                    "start": 163,
                    "end": 180,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 180,
                    "end": 200,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 200,
                    "end": 220,
                    "matchedPaperCorpusId": "235390519"
                },
                {
                    "start": 220,
                    "end": 245,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 245,
                    "end": 267,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 630,
                    "end": 643,
                    "matchedPaperCorpusId": "2996187"
                },
                {
                    "start": 643,
                    "end": 666,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 957,
                    "end": 975,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 975,
                    "end": 995,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 995,
                    "end": 1015,
                    "matchedPaperCorpusId": "235390519"
                },
                {
                    "start": 1015,
                    "end": 1040,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 1253,
                    "end": 1274,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 1274,
                    "end": 1290,
                    "matchedPaperCorpusId": "252917981"
                },
                {
                    "start": 1290,
                    "end": 1306,
                    "matchedPaperCorpusId": "252408513"
                },
                {
                    "start": 1323,
                    "end": 1343,
                    "matchedPaperCorpusId": "263334529"
                },
                {
                    "start": 1778,
                    "end": 1799,
                    "matchedPaperCorpusId": "256868474"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5703125
        },
        {
            "corpus_id": "237563234",
            "title": "Slot Filling for Biomedical Information Extraction",
            "text": "Recent years have witnessed a series of significant advances in the field of QA, primarily owing to the Transformer architecture [24] and the BERT language model paradigm [2]. These advances, both in terms of methods [1,18,5,16] and datasets [11,27], motivated researchers to formulate a series of different NLP tasks as open domain QA, such as entity linking or relation extraction [14,21]. In this work we follow this paradigm by formulating biomedical IE as a slot-filling task.\n\nIn open domain QA, given a query, a retrieval module retrieves relevant documents from the knowledge source (such as Wikipedia) and a reading comprehension module is then used to extract a span from the relevant documents, the answer. The retrieval step was, up to very recently, dominated by statistical-based approaches, namely BM25 or tf-idf [1]. ORQA [13] and REALM [5] have been the first neural based methods to clearly outperform statistical based retrieval, although they required expensive language model pre-training. Dense Passage Retrieval (DPR) [9] has improved upon these methods by considering a dual BERT-based encoder, one for the queries and one for passages and training it to recognize relevant vs irrelevant passages. This approach has proved superior to other neural based approaches and has quickly become the preferred method for open domain QA in subsequent work [16,7,20].\n\nAmong the subsequent works, Retrieval Augmented Generation [16] employs an architecture based on DPR and BART [15] that is optimized end to end during finetuning, to retrieve relevant documents and generate answers to queries. Also, Fusion-in-decoder [7] employs DPR or BM25 as retrievers coupled with a T5 language model, to generate answers by attending at multiple passages simultaneously. For simplicity, we are not considering these approaches in this work, leaving their implementation for the biomedical domain for future work.\n\nIn an effort to fuel further research on this field, Petroni et al. [21] introduced KILT, a new benchmark of knowledge intensive tasks, which contains among others two slot filling datasets, zero-shot RE which was first presented in [14] and T-REx introduced in [3]",
            "score": 0.39041770117994146,
            "section_title": "Related Work",
            "char_start_offset": 6764,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 133,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 217,
                    "end": 220,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 220,
                    "end": 223,
                    "matchedPaperCorpusId": "195739828"
                },
                {
                    "start": 242,
                    "end": 246,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 246,
                    "end": 249,
                    "matchedPaperCorpusId": "52822214"
                },
                {
                    "start": 383,
                    "end": 387,
                    "matchedPaperCorpusId": "793385"
                },
                {
                    "start": 387,
                    "end": 390,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 828,
                    "end": 831,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 838,
                    "end": 842,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1041,
                    "end": 1044,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1375,
                    "end": 1377,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 1493,
                    "end": 1497,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1634,
                    "end": 1637,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51611328125
        },
        {
            "corpus_id": "265213270",
            "title": "Scalable and Effective Generative Information Retrieval",
            "text": "Fine-tune the Pre-trained language models (LMs) [11,24,34,39] on information retrieval (IR) tasks have proven to be more effective compared to traditional models [19,30,46], such as BM 25 in various scenarios. This might because LMs, pre-traiend on vast amounts of text data, can have more deep understanding of language semantics. The contextualized representations by LMs also provide flexibility to make it adapt to different IR model designs. The integration of these LMs with neural IR models can be broadly categorized into four main streams: (1) neural sparse retrieval models, (2) neural re-ranking models, (3) dense retrieval models, and (4) generative retrieval models. \n\nNeural sparse retrieval models, inspired by conventional bag-ofwords approaches like TF-IDF [36] and BM25 [37], adapt BERT to reweight subwords, thereby enhancing IR performance. To maintain the sparsity of high-dimensional vectors, they utilize L1 [48] or Flop [13] regularizers. This characteristic sparsity allows them to be incorporated into fast search frameworks based on the inverted index [38]. \n\nRe-ranking with LMs is another approach where LMs serve as re-rankers [30,52]. By feeding a concatenated query and document, these models produce a relevance score. Despite their often superior performance, they are only suited for document re-ranking due to efficiency constraints. \n\nDense retrieval models are based on bi-encoder architectures [16,17,19,20,33,46,49,50]. These models, typically leveraging BERT, encode each document and query into dense representations. For efficient retrieval, they employ approximated nearest neighbor (ANN) search [26,46]. Lastly, the generative retrieval paradigm [3,41] is an innovative approach drawing inspiration from successful generative LMs [8,31,34]. In this paradigm, models like T5 are treated as retrievers. Each document is mapped to a distinct sequence, often denoted as a DocID. At inference, given a specific query, a constrained beam search [41,51] retrieves a list of the most probable DocIDs.",
            "score": 0.39010745102222905,
            "section_title": "RELATED WORK",
            "char_start_offset": 32782,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 679
                },
                {
                    "start": 682,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1369
                },
                {
                    "start": 1372,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2037
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 52,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 774,
                    "end": 778,
                    "matchedPaperCorpusId": "16829071"
                },
                {
                    "start": 1440,
                    "end": 1443,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1446,
                    "end": 1449,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1640,
                    "end": 1644,
                    "matchedPaperCorpusId": "8915893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62109375
        },
        {
            "corpus_id": "233346995",
            "title": "Finding Fuzziness in Neural Network Models of Language Processing",
            "text": "Recent advances in the field of NLP have lead to the rise of a class of highly parameterized neural network-based models called pre-trained language models (PLMs). These models are trained on vast amounts of text using the language modeling objective -estimating probabilities of missing words in context. 1 A common architecture that governs the computation in these models is the transformer architecture [16]. The attention module in the transformer architecture allows PLMs to 'remember' all words in a given sentence context, while predicting the missing word, thereby making its internal representations more finely attuned to the entire context. Though originally proposed for ranking sentences (by assigning them probabilities), the language modeling objective guides PLMs into learning general-purpose language representations which can be fine-tuned to any supervised learning task endto-end, i.e., all representations get updated in order to optimize for a given task(s). \n\nTheir expressiveness and adaptability has enabled PLMs to achieve state of the art results on several high-level NLP tasks, such as Question Answering, Natural Language Inference, Reading Comprehension, etc. PLMs come in two main variants: \n\n(1) Incremental PLMs, which are trained to predict words when conditioned on a left context, e.g., \"I went to the library to \"; and (2) Masked PLMs, which have bidirectional context while predicting tokens, e.g., \"I went to the to read.\" Examples of Incremental PLMs include GPT2 [13] while those of Masked PLMs include BERT [3] and RoBERTa [8].",
            "score": 0.3896812524448575,
            "section_title": "Pre-trained Language Models",
            "char_start_offset": 8286,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 982
                },
                {
                    "start": 985,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1572
                }
            ],
            "ref_mentions": [
                {
                    "start": 1507,
                    "end": 1511,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1552,
                    "end": 1555,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51611328125
        },
        {
            "corpus_id": "270737760",
            "title": "PharmaGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry",
            "text": "This section provides an in-depth overview of the Phar-maGPT initiative, detailing the project's scope, objectives, and the collaborative efforts behind the development of this advanced domain-specific language model.This comprehensive overview sets the stage for subsequent sections that explore the architecture, training methodologies, and unique contributions of PharmaGPT to bio-pharmaceutical and chemical research.By establishing this context, we aim to enrich the reader's understanding and highlight the strategic importance of this model in integrating cutting-edge natural language processing techniques with domain-specific research needs.This connection is crucial for advancing research and development in these critical scientific areas.\n\nOrganization of the Large Model Research Team The Large Model Research Team (LMRT) as shown in Fig 1 is at the forefront of advancing natural language processing (NLP) technologies.By employing a comprehensive and structured approach that encompasses data handling, model development, evaluation, cross-disciplinary collaboration, and domain-specific applications, LMRT is pushing the boundaries of domain-specific LLM research, innovation and application.This section provides an overview of LMRT's organizational structure, highlighting the team's focus on data integrity, advanced modeling techniques, rigorous evaluation methodologies, and the application of NLP in biomedical and chemical domains.Data At the core of LMRT's methodology is a robust data infrastructure, characterized by meticulous data preparation, ethical sourcing, stringent governance policies, advanced tooling, and in-depth analysis.The team recognizes the critical importance of high-quality, diverse, and relevant data sets in the development of powerful and responsible NLP models.By focusing on data integrity and employing state-of-the-art data handling techniques, LMRT ensures that the models are trained on reliable and representative data, laying the foundation for accurate and meaningful results.\n\nModeling LMRT's modeling framework is incorporating advanced tokenization techniques, metadata integration, support for multilingual capabilities, innovative architecture designs, efficient information retrieval methods, and versatile prompting strategies.These elements combine to enhance the models' understanding of complex language patterns and improve their applicability across various NLP tasks.The team's expertise in developing cutting-edge modeling techniques enables LMRT to create NLP models that are not only powerful but also adaptable to a wide range of real-world scenarios.",
            "score": 0.3896502998926169,
            "section_title": "PharmaGPT Workshop",
            "char_start_offset": 14044,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 217,
                    "end": 421
                },
                {
                    "start": 421,
                    "end": 651
                },
                {
                    "start": 651,
                    "end": 752
                },
                {
                    "start": 754,
                    "end": 935
                },
                {
                    "start": 935,
                    "end": 1210
                },
                {
                    "start": 1210,
                    "end": 1456
                },
                {
                    "start": 1456,
                    "end": 1663
                },
                {
                    "start": 1663,
                    "end": 1814
                },
                {
                    "start": 1814,
                    "end": 2037
                },
                {
                    "start": 2039,
                    "end": 2295
                },
                {
                    "start": 2295,
                    "end": 2441
                },
                {
                    "start": 2441,
                    "end": 2629
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.281005859375
        },
        {
            "corpus_id": "271946777",
            "title": "LLM-PBE: Assessing Data Privacy in Large Language Models",
            "text": "LLMs [86,90,104,107] are a class of advanced models designed to understand, interpret, and generate human-like text, representing a significant milestone in the field of NLP. Fundamentally, these models are built on sophisticated neural network architectures, primarily transformer-based [115] designs, known for their deep learning capabilities in handling sequential data. The architecture of LLMs typically involves multiple layers of self-attention mechanisms, which enable the models to process and generate text by effectively capturing the context and nuances of language over large spans of text. The applications of LLMs are remarkably diverse, extending far beyond basic text generation. In the realm of data management, LLMs have revolutionized information retrieval, making it possible to extract and synthesize information from unstructured data sources with unprecedented efficiency. The emergence of LLMs has thus not only pushed the boundaries of \n\nmachine understanding of language but also opened up new possibilities for data analysis and interaction, marking a transformative phase in the intersection of AI, linguistics, and data science.",
            "score": 0.3895764436537954,
            "section_title": "PRELIMINARIES AND RELATED WORK 2.1 Large Language Models",
            "char_start_offset": 7292,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1159
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.441162109375
        },
        {
            "corpus_id": "267406766",
            "title": "CorpusLM: Towards a Unified Language Model on Corpus for Knowledge-Intensive Tasks",
            "text": "CorpusLM is a multi-task learning architecture designed to handle various types of knowledge-intensive tasks. It is a unified language model capable of performing generative retrieval, closed-book generation, and retrieval-augmented generation through the same autoregressive greedy generation. The model identifies different tasks using specific prefixes. \n\nThe overview of CorpusLM is illustrated in Figure 1. The training of CorpusLM involves the following three basic tasks: \n\n\u2022 Generative Retrieval: retrieving relevant documents to a given query by generating a ranked DocID list, facilitating the model's ranking ability, and can be achieved through greedy decoding. \n\n\u2022 Closed-book Generation: generating answers solely based on the query input, without relying on external information, similar to classic auto-regressive language models. \u2022 Retrieval-Augmented Generation: generating answers by first retrieving relevant content using DocID list generation, and then generate references and final response through continuous greedy decoding, enhancing effective and efficient RAG. \n\nMoreover, in order to effectively integrate generative retrieval and RAG, it is necessary to improve the model's understanding of Do-cIDs and the relationship between DocIDs and their corresponding knowledge. Therefore, we incorporate a group of unsupervised Do-cID understanding tasks into the multi-task learning framework to enhance the model's understanding of the meaning behind DocIDs: \n\n\u2022 DocID Understanding: The model is equipped with auxiliary tasks that deepen its understanding of DocIDs' structure and semantic meaning. \n\nBy training on the above four types of tasks that share common patterns and connections, CorpusLM develops a more comprehensive understanding of the relationships between retrieval and downstream tasks, as well as the meaning behind the DocIDs, thereby gaining a more robust grasp of each individual task. Formally, the training of CorpusLM aims to optimize objectives with a combined loss function as below: \n\nwhere Lrank, L gen , and Lrag are corresponding loss functions for generative retrieval, closed-book generation, and retrieval-augmented generation represented by Equations ( 1)-( 3). Laux is the loss function for the DocID Understanding task. The specific forms of these loss functions will be explained in subsequent sections.  1 ,  2 ,  3 , and  4 are weighting coefficients for each task's loss.",
            "score": 0.38888023795084137,
            "section_title": "CorpusLM: the Unified Language Model",
            "char_start_offset": 10571,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 356
                },
                {
                    "start": 359,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 478
                },
                {
                    "start": 481,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1482
                },
                {
                    "start": 1485,
                    "end": 1623
                },
                {
                    "start": 1626,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2034
                },
                {
                    "start": 2037,
                    "end": 2220
                },
                {
                    "start": 2221,
                    "end": 2280
                },
                {
                    "start": 2281,
                    "end": 2365
                },
                {
                    "start": 2366,
                    "end": 2436
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76416015625
        },
        {
            "corpus_id": "277468228",
            "title": "Semantic Mastery: Enhancing LLMs with Advanced Natural Language Understanding",
            "text": "Large language models (LLMs) have greatly improved their capability in performing NLP tasks. However, deeper semantic understanding, contextual coherence, and more subtle reasoning are still difficult to obtain. The paper discusses state-of-the-art methodologies that advance LLMs with more advanced NLU techniques, such as semantic parsing, knowledge integration, and contextual reinforcement learning. We analyze the use of structured knowledge graphs, retrieval-augmented generation (RAG), and fine-tuning strategies that match models with human-level understanding. Furthermore, we address the incorporation of transformer-based architectures, contrastive learning, and hybrid symbolic-neural methods that address problems like hallucinations, ambiguity, and inconsistency in the factual perspectives involved in performing complex NLP tasks, such as question-answering text summarization and dialogue generation. Our findings show the importance of semantic precision for enhancing AI-driven language systems and suggest future research directions to bridge the gap between statistical language models and true natural language understanding.",
            "score": 0.38830881262426287,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.703125
        },
        {
            "corpus_id": "276724720",
            "title": "A Pilot Empirical Study on When and How to Use Knowledge Graphs as Retrieval Augmented Generation",
            "text": "Recently, Large Language Models (LLMs) have demonstrated remarkable capabilities in Natural Language Processing (NLP) tasks [47,2]. However, LLMs face critical challenges including hallucination [29], limited incorporation with real-time knowledge [26], and opaque reasoning processes [61]. Thus, Retrieval-Augmented Generation (RAG) [12] frameworks have emerged as a promising solution by searching most relevant contents from external knowledge base using similarity methods [7]. However, RAG typically treats document contents as independent units, struggling to capture complex relational information and hierarchical interconnections within the data [24,22]. \n\nTo address these limitations, graph-based RAG [6], particularly those incorporating Knowledge Graphs (KGs) known as KG-RAG, has emerged as a promising paradigm [56,11,16,31]. KG-RAG leverages semantic relationships between entities [18] to enable more sophisticated reasoning capabilities [35,43] and enhance performance in domain-specific applications [49]. However, due to the rapid proliferation of related techniques, these KG-RAG works have emerged in a disjointed manner, much like mushrooms after rain, with significant variations in their use of scenarios, datasets, KG-RAG configurations, and LLMs. They tend to focus on isolated technical innovations across different pipeline stages, without systematic comparison across varied tasks. Moreover, recent reviews [27,57,28,59] primarily focuses on qualitative analyses, with a lack of quantitative assessments regarding the impact of key configurations across different tasks. \n\nTo address this research gap, we aim to explore the key factors that answer the questions of when and how to use KG-RAG, thereby laying the foundation for a quantitative empirical study. Specifically, we identify two critical gaps in current KG-RAG research: its applicability across diverse scenarios and the effectiveness of different pipeline configurations. First, the applicability of KG-RAG remains insufficiently explored across several dimensions: task domains (ranging from open-domain to domain-specific tasks), task scenarios (including QA, Diagnosis, and Exams) [59], LLM capabilities (from open-source to commercial models), and KG quality (from specialized to general KGs).",
            "score": 0.38758200516020525,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1600
                },
                {
                    "start": 1603,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2290
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 128,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 128,
                    "end": 130,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 195,
                    "end": 199,
                    "matchedPaperCorpusId": "269790923"
                },
                {
                    "start": 248,
                    "end": 252,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 285,
                    "end": 289,
                    "matchedPaperCorpusId": "272882110"
                },
                {
                    "start": 334,
                    "end": 338,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 477,
                    "end": 480,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 655,
                    "end": 659,
                    "matchedPaperCorpusId": "259360665"
                },
                {
                    "start": 659,
                    "end": 662,
                    "matchedPaperCorpusId": "273323692"
                },
                {
                    "start": 826,
                    "end": 830,
                    "matchedPaperCorpusId": "256460921"
                },
                {
                    "start": 830,
                    "end": 833,
                    "matchedPaperCorpusId": "265351547"
                },
                {
                    "start": 836,
                    "end": 839,
                    "matchedPaperCorpusId": "273819701"
                },
                {
                    "start": 898,
                    "end": 902,
                    "matchedPaperCorpusId": "273654355"
                },
                {
                    "start": 955,
                    "end": 959,
                    "matchedPaperCorpusId": "259936842"
                },
                {
                    "start": 959,
                    "end": 962,
                    "matchedPaperCorpusId": "277435671"
                },
                {
                    "start": 1437,
                    "end": 1441,
                    "matchedPaperCorpusId": "259165563"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71240234375
        },
        {
            "corpus_id": "277633852",
            "title": "Generative AI Enhanced Financial Risk Management Information Retrieval",
            "text": "Natural Language Processing (NLP) has become a powerful tool for analyzing domain-specific data at scale. The financial domain uses NLP to translate varied data for rapid decision-making. For example, sentiment analysis in finance has proven to be effective in predicting stock market behavior and credit risks (Du et al., 2024;Hiew et al., 2019;Puh & Babac, 2023;Todd et al., 2024). This encourages additional investigation into utilizing NLP techniques such as Question Answering (QA) within the field of risk management (as the central theme of this paper). These systems can assist experts in quickly retrieving the specific information they need, improving efficiency and decision-making and yielding considerable profits. \n\nLarge Language Models (LLMs) are advanced artificial intelligence systems based on the Transformer architecture, which enables them to process and generate text by leveraging self-attention mechanisms and deep neural networks (Vaswani et al., 2017). While LLMs are not inherently QA systems, they can be used as one. At their core, LLMs are sequence-to-sequence models trained on text data to predict the most likely next token in a sequence. This allows them to generate text, complete sentences, summarize content, and even engage in conversational dialogue. They have revolutionized NLP by leveraging massive datasets and billions of parameters to generate human-like text, answer queries, and perform complex language-related tasks. However, despite their impressive capabilities, LLMs face critical challenges. One major issue is hallucination, where the model generates plausible-sounding but incorrect or misleading information (Huang et al., 2025). Additionally, LLMs struggle with domain-specific knowledge, as they are trained on broad datasets that may not include up-to-date or specialized content required for particular industries like finance, healthcare, or law (Song et al., 2025). Another key limitation is their static nature-once trained, they cannot dynamically incorporate new knowledge without expensive retraining (Y. Wang et al., 2024). \n\nTo address these issues, Retrieval-Augmented Generation (RAG) has emerged as an effective and efficient solution for domain-specific applications (Lewis et al., 2021).",
            "score": 0.3875430700152044,
            "section_title": "Introduction",
            "char_start_offset": 2656,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2091
                },
                {
                    "start": 2094,
                    "end": 2261
                }
            ],
            "ref_mentions": [
                {
                    "start": 311,
                    "end": 328,
                    "matchedPaperCorpusId": "266337417"
                },
                {
                    "start": 346,
                    "end": 364,
                    "matchedPaperCorpusId": "258003448"
                },
                {
                    "start": 364,
                    "end": 382,
                    "matchedPaperCorpusId": "268147491"
                },
                {
                    "start": 956,
                    "end": 978,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1665,
                    "end": 1685,
                    "matchedPaperCorpusId": "265067168"
                },
                {
                    "start": 2072,
                    "end": 2090,
                    "matchedPaperCorpusId": "267523037"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.384765625
        },
        {
            "corpus_id": "264146201",
            "title": "Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models",
            "text": "The performance of different LLMs on two distinct datasets, utilizing the specified temperature value, was displayed. Metrics were computed for each LLM, offering a comprehensive perspective on their summarization capabilities, as available on the GitHub repository cited in this paper [13]. \n\nThese tables, as referenced in Table I and Table II, present a comprehensive evaluation of various Large Language Models (LLMs) for text summarization across two distinct datasets: CNN/Daily Mail 3.0.0 and XSum. The performance of each LLM is assessed using several key metrics, including BLEU, ROUGE, and BERT. \n\nThe table highlights varying performance across LLMs and datasets. Notably, the OpenAI model, text-davinci-003, consistently exhibits strong performance, achieving high BLEU, ROUGE, and BERT Scores. This exceptional performance can be attributed to davinci being the largest and most powerful model, with 175 billion parameters and 45TB of text data. \n\nWhen comparing the two 7b parameter fine-tuned models, MPT-7b-instruct performed slightly better than Falcon-7binstruct. However, their overall performance was somewhat similar. These findings underscore the significance of model architecture and size in text summarization tasks, as well as the potential of OpenAI's model for achieving state-of-the-art results in diverse NLP applications.",
            "score": 0.38727003457100523,
            "section_title": "B. Results",
            "char_start_offset": 10413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 291
                },
                {
                    "start": 294,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 605
                },
                {
                    "start": 608,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1352
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.280029296875
        },
        {
            "corpus_id": "269457256",
            "title": "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing",
            "text": "This article summarizes some of the recent RAG work, but it introduces the retrievers and generators independently, which is not conducive to the upgrading and interactions with the components of subsequent work.Li et al. (2022b) focus on text generation only.The article has fewer figures and tables, and the content is more abstract, which is not conducive to the reader's understanding.\n\nAlso, surveys on RAG only tells half of the story in retrieval-augmented methods in NLP.Not only do tasks associated with NLG require retrieval enhancement techniques, but NLU tasks also necessitate external information.To date, there is a scarcity of comprehensive surveys that thoroughly review the application of augmented retrieval techniques across the spectrum of NLP.In order to improve the current situation, this paper presents the following contributions:\n\n(1) The article does not merely focus on the work related to RAG; it also places significant emphasis on RALM and aligns with the concept of NLP.The work related to generation aligns with NLG, while the rest of the work aligns with NLU.\n\n(2) The two components of RALM, the Retriever and the Language Model, are described in detail, and the different interaction modes of these two components are precisely defined for the first time.\n\n(3) A comprehensive overview of the RALM work schedule is provided, along with a summary of the common and novel applications of current RALM, accompanied by an analysis of the associated limitations.Potential solutions to these limitations are proposed, along with recommendations for future research directions.\n\nFigure 1 provides a general overview of the framework of RALM methods.The following is a summary of the paper: Section 2 defines RALM.Section 3 provides a detailed classification and summary of the work of retrievers in RALM.Section 4 provides a detailed classification and summary of the work of LMs in RALM.Section 5 provides a classification and summary of specific enhancements to RALM.Section 6 of RALM is a classification and summary of the sources of retrieved data.Section 7 is a summary of RALM applications.Section 8 is a summary of RALM evaluations and benchmarks.Finally, Section 9 is a discussion of the limitations of existing RALM and directions for future work.",
            "score": 0.38704866882156796,
            "section_title": "Introduction",
            "char_start_offset": 3744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 212,
                    "end": 260
                },
                {
                    "start": 260,
                    "end": 389
                },
                {
                    "start": 391,
                    "end": 479
                },
                {
                    "start": 479,
                    "end": 611
                },
                {
                    "start": 611,
                    "end": 765
                },
                {
                    "start": 765,
                    "end": 856
                },
                {
                    "start": 858,
                    "end": 1003
                },
                {
                    "start": 1003,
                    "end": 1094
                },
                {
                    "start": 1096,
                    "end": 1292
                },
                {
                    "start": 1294,
                    "end": 1494
                },
                {
                    "start": 1494,
                    "end": 1607
                },
                {
                    "start": 1609,
                    "end": 1679
                },
                {
                    "start": 1679,
                    "end": 1743
                },
                {
                    "start": 1743,
                    "end": 1834
                },
                {
                    "start": 1834,
                    "end": 1918
                },
                {
                    "start": 1918,
                    "end": 1999
                },
                {
                    "start": 1999,
                    "end": 2082
                },
                {
                    "start": 2082,
                    "end": 2126
                },
                {
                    "start": 2126,
                    "end": 2184
                },
                {
                    "start": 2184,
                    "end": 2286
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6875
        },
        {
            "corpus_id": "245131171",
            "title": "You Only Need One Model for Open-domain Question Answering",
            "text": "Neural Retriever Augmented Language Modeling (NRALM): Augmenting language models with neural retrieval has been shown to be very effective, such as by retrieving nearest neighbor words for LM tasks (Khandelwal et al., 2020;Yogatama et al., 2021) or Machine Translation (Khandelwal et al., 2021). Dinan et al. (2019) proposed a decomposed transformer for conversation tasks, which enabled pre-computation of the external knowledge embeddings. ORQA (Lee et al., 2019) proposed the ICT task to pre-train a decomposed retriever, and DPR (Karpukhin et al., 2020) enhanced this approach with in-batch negatives and hard negatives to eliminate the pre-training. Synthetic Data Augmentation is also commonly used, such as in DPR-PAQ (Oguz et al., 2021), PAIR (Ren et al., 2021), Hu et al. (2021). Per-token embeddings or multiple embeddings were used in ColBERT (Khattab et al., 2020), ME-BERT (Luan et al., 2021), Lin et al. (2021), Lee et al. (2021). \n\nSimilar to our approach of re-ranker on top of a shared retriever, PreTTR (MacAvaney et al., 2020) pre-computed term representations for all documents, and used these to run only the upper layers of a transformer reranker model. Decoupled Transformer (Elfdaeel and Peshterliev, 2021) also shares the lower layers of a transformer encoder to serve as a reranker, using the upper layers as a reader and focuses on computationally efficient reranking. Our approach extends these approaches by also incorporating a retriever and a decoder in the model.",
            "score": 0.38704866882156796,
            "section_title": "Related Works",
            "char_start_offset": 21486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 944
                },
                {
                    "start": 947,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1495
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 223,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 223,
                    "end": 245,
                    "matchedPaperCorpusId": "231717977"
                },
                {
                    "start": 269,
                    "end": 294,
                    "matchedPaperCorpusId": "222125236"
                },
                {
                    "start": 296,
                    "end": 315,
                    "matchedPaperCorpusId": "53218829"
                },
                {
                    "start": 447,
                    "end": 465,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 533,
                    "end": 557,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 751,
                    "end": 768,
                    "matchedPaperCorpusId": "236477844"
                },
                {
                    "start": 854,
                    "end": 876,
                    "matchedPaperCorpusId": "220302658"
                },
                {
                    "start": 886,
                    "end": 904,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 926,
                    "end": 943,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1021,
                    "end": 1045,
                    "matchedPaperCorpusId": "216641996"
                },
                {
                    "start": 1198,
                    "end": 1229,
                    "matchedPaperCorpusId": "236924539"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.496826171875
        },
        {
            "corpus_id": "266163023",
            "title": "Survey on Memory-Augmented Neural Networks: Cognitive Insights to AI Applications",
            "text": "Retrieval augmentation proves to be highly beneficial in various knowledge-intensive NLP applications, especially those where factuality is a crucial requirement. This approach enriches the applications by providing access to a broader range of factual information, enhancing their accuracy and reliability. Retrieval Augmented Language Model (REALM) [27] is the first method to jointly train a knowledge retriever and a knowledge-augmented language encoder in an unsupervised manner. Retrieval augmented generation (RAG) [6] fine-tunes a pre-trained retriever (e.g., DPR [28]) and a pre-trained sequence-to-sequence model (e.g., BART [29]). RAG achieves superior performance on various knowledge-intensive tasks, including question answering, question generation and fact verification. Retrieval Augmented Translation (RAT) [30] improves neural machine translation by treating the external knowledge base as a dictionary. More recently, Chain-of-Noting (CoN) [31] proposes to enhance the robustness of retrieval augmented models by first generating sequential reading notes based on the retrieved documents, then formulating the final answer. This approach enables a comprehensive evaluation of the relevance and factuality of the retrieval results. \n\nBeyond the benefit of providing truthful information for knowledge-intensive tasks, retrieval augmentation also enhances the capability of language models without increasing trainable parameters. Retrieval Enhanced Transformers (RETRO) [5] augments a language model with an external knowledge base consisting of 2 trillion tokens, and achieves performance comparable to GPT-3 [32] and Jurassic-1 [33], which has 25x more parameters. Similarly, Atlas [34] achieves state-of-the-art performance the few-shot learning capabilities in language models while still maintains a relatively small parameter size. In-Context Retrieval Augmented Language Modeling (RALM) [35] further reduces the training cost by directly prepending retrieved documents to the input and freezing the weights of language models.",
            "score": 0.38704866882156796,
            "section_title": "Natural Language Processing",
            "char_start_offset": 22318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 825,
                    "end": 829,
                    "matchedPaperCorpusId": "252815975"
                },
                {
                    "start": 1629,
                    "end": 1633,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81494140625
        },
        {
            "corpus_id": "269740933",
            "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
            "text": "Retrieval-augmented generation (RAG), a cutting-edge AI technique, has achieved remarkable success across various applications, including recommendation, molecule generation, protein representation, and software engineering, owing to the potent capabilities of retrieval in providing supplementary information to enhance generation performance.Recently, increasing efforts have been made to alleviate the limitations of large language models (LLMs), such as hallucination and out-of-date internal knowledge, by leveraging retrieval to provide the latest auxiliary information and teaching LLMs to harness the retrieved external knowledge.With the rapid advancements in retrieval-augmented large language models (RA-LLMs), there is a pressing need for a comprehensive and systematic overview.To bridge this gap, in this paper, we comprehensively review the RA-LLMs from the perspectives of morel architecture, training strategy, and application area, providing researchers with an in-depth understanding.Moreover, since the studies of RA-LLMs are still in the early stage, we also discuss the current limitations and several potential research directions for future research.",
            "score": 0.38704866882156796,
            "section_title": "CONCLUSION",
            "char_start_offset": 60360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 344
                },
                {
                    "start": 344,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 791
                },
                {
                    "start": 791,
                    "end": 1003
                },
                {
                    "start": 1003,
                    "end": 1174
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84130859375
        },
        {
            "corpus_id": "271386000",
            "title": "Advancements in Deep Learning Architectures for Natural Language Processing Tasks",
            "text": "- Recent years have been an active testing ground for artificial neural networks for language understanding, a very important aspect of NLP. In this respect, emerging NLP technologies are largely motivated by the rising requirements to cope with the issues raised by different NLP tasks, allowing the processing and analysis of large text data samples, uncovering complex language behaviors, as well as extracting valuable information from disorganized text. NLP (Natural Language Processing) has proven to be the most successful field of machine learning thanks to its capability to teach itself and detect all kinds of features on its own based on enormous amounts of data. In NLP tasks like language modelling, text classification, emotion analysis, and machine translation, RNNs, CNNs, and transformer-based models have been used in new ways. While NLP is generally agreed upon the difficulties it faces, the progress of technology also gives birth to unexpected challenges. Thus, two factors, namely the expanding collections of large text datasets and the pressing need for more accurate and time-saving NLP models that emerge as a consequence are giving rise to new kinds of deep learning models and techniques. Here, this paper analyzes as a whole the most recent achievement of neural architectures for natural language processing applications. From introducing current models and approaches in NLP, highlighting their strengths and weaknesses, and identifying the areas to be researched in the future, this paper will conduct this discussion.",
            "score": 0.38693224899436296,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5
        },
        {
            "corpus_id": "69392244",
            "title": "Building Soil Taxonomy Ontology by the Way of Connective Based Ontology Learning",
            "text": "The main component of the architecture of the system is the involvement of OpenNLP to the basic java compiler (Fig. 1). This helps in doing the NLP task as per the requirement of the system. Another highlight of the system is the algorithm library which implicitly contains the NLP unit and this library is plugged in to the system. Other components are the API\"s like JENA, OWL Prot\u00e9g\u00e9 and OWL Syntax are use to deal with the prot\u00e9g\u00e9 and ontology. \n\nWe have developed a system that is enabled of extracting taxonomic class and property.",
            "score": 0.3863748857393973,
            "section_title": "Architecture of the developed software",
            "char_start_offset": 5314,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 448
                },
                {
                    "start": 451,
                    "end": 537
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.048858642578125
        },
        {
            "corpus_id": "269603222",
            "title": "RT: a Retrieving and Chain-of-Thought framework for few-shot medical named entity recognition",
            "text": "To the best of our knowledge, this is the first investigation to combine a retrieval-based LLM and Chain-of-Thought methodology to enhance the performance in biomedical few-shot NER. The retrieval-based LLM aids in retrieving the most relevant examples of the input sentence, offering crucial knowledge to predict the entity in the sentence. More specifically, the integration of a retrieval-based language model (LLM) represents a significant advancement in NLP tasks, such as question answering and NER task. The retrieval-based LLM excels in the retrieval of highly relevant examples associated with the input sentence. This capability plays a pivotal role in refining the model's understanding of contextual nuances and distinguishing between different types of entities. It furnishes the model with crucial knowledge, thereby improving its accuracy in predicting entities. By leveraging the wealth of information retrieved, the model becomes adept at discerning and predicting entities within the given sentence. This nuanced approach represents a significant leap in the advancement of language models, bolstering their effectiveness in comprehending and interpreting complex textual data, particularly in the context of the biomedical NER task. \n\nIn tandem with its retrieval-based counterpart, the Chainof-Thought methodology adds an additional layer of sophistication to the entity recognition process. Simultaneously employed, this methodology serves as an invaluable tool by offering comprehensive explanations pertaining to entity recognition. This concurrent utilization enhances the large language model's ability to predict entities within a given context. The Chain-of-Thought methodology acts as a guiding thread, providing insights into the logical connections and reasoning processes involved in identifying entities. Through this synergy, the integration of Chain-of-Thought methodology and the retrieval-based language model creates a robust framework for advancing the accuracy and depth of biomedical entity prediction. \n\nThe results from Tables 1 and 2 clearly demonstrate that the decoder-only model surpasses both the encoder-only and encoder-decoder models. Unlike the encoder-only model, exemplified by BERT, the distinctive architecture of the decoder-only model enables smooth navigation through the name recognition process and supports incremental progression (Chain-of-Thought). Additionally, it streamlines the incorporation of retrieved knowledge, thereby augmenting its efficacy.",
            "score": 0.38596076587709865,
            "section_title": "Discussion",
            "char_start_offset": 22690,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 2042
                },
                {
                    "start": 2045,
                    "end": 2184
                },
                {
                    "start": 2185,
                    "end": 2411
                },
                {
                    "start": 2412,
                    "end": 2515
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55859375
        },
        {
            "corpus_id": "265308533",
            "title": "ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science",
            "text": "The continuous advancement in natural language processing (NLP) has led to the development of various novel model architectures that overcome existing limitations and demonstrate state-of-the-art performances. The retrieval augmented language models (RALM) primarily address the grounding and scalability challenges in standard language models (LM). RALM aims to address these limitations by combining a LM with an external knowledge base. In this framework, the LM generates text conditioned not only on the input query but also on relevant knowledge retrieved from the knowledge base. The retrieved knowledge is usually the text chunks or passages from documents that provide factual grounding to contextualize the model's pre-Preprint version dictions. In other words, this approach decentralizes model knowledge into parameters and external knowledge sources, thereby addressing the challenges of scalability and adaptability. \n\nTypically in RALM, text data from an external knowledge base is segmented and encoded into vectors (also known as vector databases). The retriever component of RALM retrieves relevant documents based on the similarity between the query and vectors corresponding to documents in the database. Many existing RALMs rely solely on semantic/lexical information of the documents for retrieval. However, in certain scenarios, the structural relationship between documents can further support the retriever in retrieving contextually relevant documents. For instance, a scientific paper in materials science might reference papers that describe relevant advances in nuclear physics, and viceversa. Having such relational information explicitly present in the scientific documents would allow the model to draw on the interdisciplinary scientific knowledge in a similar way to how scientists do. Thus, it would be beneficial to learn about the relationships between documents (e.g., citations, co-authorship, etc.) in a corpus of scientific publications and connect different scientific concepts. \n\nTo address the challenges of adequate structural component in RALM and retrieval faithfulness in science-focused tasks, we propose a novel model architecture (ATLANTIC) in this work that systematically incorporates structural and textual information into the RALM. We develop AT-LANTIC on top of the standard RALM, ATLAS (Izacard et al. 2022) architecture.",
            "score": 0.38591610178735913,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 930
                },
                {
                    "start": 933,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 2020
                },
                {
                    "start": 2023,
                    "end": 2287
                },
                {
                    "start": 2288,
                    "end": 2379
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82080078125
        },
        {
            "corpus_id": "260154895",
            "title": "GPT-3 Models are Few-Shot Financial Reasoners",
            "text": "In general question answering (open QA) settings, ColBERT by [5] and ColBERT-QA by [6], tackle freezing the document encoder during training and having limited interactionwith query. Using document matrix pre-computation and late stage interaction introduced with ColBERT, ColBERT-QA finds useful passages for more questions. Improved passage relevance helps the reader component answer more accurately with greater attribution. \n\nBaleen by [6] introduce a pipeline for multi-hop retrieval on top of ColBERT for thetask of Multi-hop QA. Multi-hop QA involves synthesizing an answer only present in twoor more documents. Baleen extends the ColBERT late interaction for this task by summa-rizing the pertinent information from retrieved passages to inform the next retrieval, and also by allowing the document matrix representations of different documents to \"focus\" on distinct parts of the same query. Many Multi-hop questions are multi-part complex queries. So, different documents could attend to different aspects of the query. As a re-sult of its more deliberate architecture and its stronger retrieval modeling, Baleen raises answer-recall@20 from 89% by MDR to 96% for HotPotQA benchmark finds all required passages in 92% of the examples in HoVer-up from 45% in the baseline. \n\nRAG, by [7], investigate a general-purpose fine-tuning method for retrieval-augmented generation (RAG) -models that integrate pretrained parametric and non-parametric memory for language generation. Furthermore, RAG demonstrates state-of-the-art per-formance without separate re-ranking or reader component present in many other neural retrieval systems. They compare two RAG formulations: one that uses the same retrieved texts throughout the whole produced sequence, and the other that can utilize a different passage per token. On a variety of knowledge-intensive NLP tasks, they fine-tune and assess models, and establish the state-of-the-art on three open domain QA tasks, outper-forming parametric seq2seq models and task-specific retrieve-and-extract architectures. \n\nFinally, there are finance domain-specific language models.",
            "score": 0.385783559114656,
            "section_title": "PRIOR LITERATURE",
            "char_start_offset": 3858,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 428
                },
                {
                    "start": 431,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 2057
                },
                {
                    "start": 2060,
                    "end": 2119
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 86,
                    "matchedPaperCorpusId": "220302658"
                },
                {
                    "start": 441,
                    "end": 444,
                    "matchedPaperCorpusId": "220302658"
                },
                {
                    "start": 1293,
                    "end": 1296,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51904296875
        },
        {
            "corpus_id": "277548245",
            "title": "Comparing large Language models and human annotators in latent content analysis of sentiment, political leaning, emotional intensity and sarcasm",
            "text": "The introduction of the Transformer architecture 22 and pre-trained language models such as BERT 23 and RoBERTa 24 significantly advanced NLP capabilities. These models utilized attention mechanisms to capture long-range dependencies in text, leading to state-of-the-art results in various tasks. \n\nBeyond Transformer architectures, emerging methods provide complementary solutions. For example, knowledge-graph-based architectures can improve stance detection tasks 17 , and boundary-aware LLM designs are increasingly valuable for few-shot named entity recognition 18 . Integrating both constituency and dependency parse information has proven beneficial for relation extraction 25 . \n\nLarge Language Models (LLMs) like GPT-2 26 and GPT-3 27 expanded these capabilities by increasing model size and training data. GPT-3, with 175 billion parameters, demonstrated remarkable proficiency in zero-shot and few-shot learning scenarios, performing well on tasks it was not explicitly trained for 27 . \n\nRecent studies have explored LLMs in sentiment analysis and related tasks. Chang & Bergen 28 investigated the use of GPT-3 for sentiment classification and found that it performed competitively with fine-tuned models on specific datasets. Similarly, Floridi and Chiriatti 29 discussed the potential of GPT-3 in understanding and generating human-like text, highlighting its applicability in content analysis. \n\nThe incorporation of context-aware mechanisms 20 , consideration of intended versus perceived meanings 21 , and the use of multi-modal data [14][15][16] represent critical steps toward improving model performance in complex NLP tasks. The development of domain-specific models like PoliBERTweet 30 highlights the potential benefits of customizing language models to better capture specific content areas, such as political discourse. The integration of symbolic reasoning with deep learning in SenticNet 6 further highlights the importance of combining different AI approaches to enhance understanding and interpretation of subtle linguistic features 31 . \n\nHowever, challenges remain regarding the ethical and practical implications of relying on LLMs. Concerns include model bias, the interpretability of results, and the tendency of LLMs to produce plausible but incorrect or biased outputs 32,33 . Additionally, studies have shown that while LLMs excel in language tasks, their performance in detecting sarcasm and nuanced emotions is inconsistent 34 . \n\nThe consistency of LLMs over time is another area of interest.",
            "score": 0.3857045696106935,
            "section_title": "The rise of transformer models and LLMs",
            "char_start_offset": 42,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 296
                },
                {
                    "start": 299,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1408
                },
                {
                    "start": 1411,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2066
                },
                {
                    "start": 2069,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2312
                },
                {
                    "start": 2313,
                    "end": 2467
                },
                {
                    "start": 2470,
                    "end": 2532
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 51,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 97,
                    "end": 99,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 467,
                    "end": 469,
                    "matchedPaperCorpusId": "272571492"
                },
                {
                    "start": 567,
                    "end": 569,
                    "matchedPaperCorpusId": "274446290"
                },
                {
                    "start": 681,
                    "end": 683,
                    "matchedPaperCorpusId": "268247791"
                },
                {
                    "start": 728,
                    "end": 730,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 741,
                    "end": 743,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 993,
                    "end": 995,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1457,
                    "end": 1459,
                    "matchedPaperCorpusId": "220330867"
                },
                {
                    "start": 1514,
                    "end": 1516,
                    "matchedPaperCorpusId": "207847660"
                },
                {
                    "start": 1551,
                    "end": 1555,
                    "matchedPaperCorpusId": "266743010"
                },
                {
                    "start": 1559,
                    "end": 1563,
                    "matchedPaperCorpusId": "218517490"
                },
                {
                    "start": 1706,
                    "end": 1708,
                    "matchedPaperCorpusId": "250164279"
                },
                {
                    "start": 2062,
                    "end": 2064,
                    "matchedPaperCorpusId": "220635321"
                },
                {
                    "start": 2305,
                    "end": 2308,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 2308,
                    "end": 2310,
                    "matchedPaperCorpusId": "266335947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31982421875
        },
        {
            "corpus_id": "272770453",
            "title": "AQA: Adaptive Question Answering in a Society of LLMs via Contextual Multi-Armed Bandit",
            "text": "Large language models (LLMs) have facilitated the development of diverse question answering (QA) systems and pipelines, each with distinct performance across domains (Cuconasu et al. 2024;Jeong et al. 2024;Li et al. 2024;Ram et al. 2023;Xu et al. 2024). The increasing complexity of these QA pipelines stems from the integration of various steps, each designed to either mitigate particular errors introduced by a module (or interactions between them) or to address questions of different types or with varying requirements (Jeong et al. 2024;Trivedi et al. 2023). Although these complex modular designs aim to enhance overall system robustness and accuracy in generating answers, it substantially increases inference costs. Additionally, a single sophisticated answering strategy may not be the most suitable solution for all types of questions (Jeong et al. 2024). \n\nComplex LLM-based QA systems. The large variety of recent retrieval augmented generation (RAG) approaches (Ram et al. 2023) provides a good example of the increasing complexity of QA systems. RAG enables the use of external knowledge during inference without re-training or modifications to the LLM-architecture. To improve its effectiveness, many additional parameters and modules have been proposed for RAG, e.g., retrieving subgraphs from a structured knowledge base alongside or instead of passages (BehnamGhader, Miret, and Reddy 2023;Dai et al. 2024;Pan et al. 2023Pan et al. , 2024)), employing summarization techniques (Edge et al. 2024), introducing noise to retrieval results (Cuconasu et al. 2024), and natural language inference modules that preprocess the retrieved content for the LLM (Yoran et al. 2024a). Another example of complex QA systems are those designed to cater to more complex questions, such as multi-hop questions (Yang et al. 2018), which require the integration of several pieces of knowledge.",
            "score": 0.38569470161795527,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 866
                },
                {
                    "start": 869,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1892
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 188,
                    "matchedPaperCorpusId": "267301416"
                },
                {
                    "start": 206,
                    "end": 221,
                    "matchedPaperCorpusId": "263610099"
                },
                {
                    "start": 237,
                    "end": 252,
                    "matchedPaperCorpusId": "267938726"
                },
                {
                    "start": 543,
                    "end": 563,
                    "matchedPaperCorpusId": "254877499"
                },
                {
                    "start": 1372,
                    "end": 1409,
                    "matchedPaperCorpusId": "254854344"
                },
                {
                    "start": 1440,
                    "end": 1459,
                    "matchedPaperCorpusId": "259165563"
                },
                {
                    "start": 1555,
                    "end": 1577,
                    "matchedPaperCorpusId": "267301416"
                },
                {
                    "start": 1668,
                    "end": 1688,
                    "matchedPaperCorpusId": "263608822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5283203125
        },
        {
            "corpus_id": "273022981",
            "title": "Exploring Social Media Image Categorization Using Large Models with Different Adaptation Methods: A Case Study on Cultural Nature's Contributions to People",
            "text": "Natural Language Processing (NLP) has undergone several key breakthroughs that have enabled the development of powerful LLMs. Notable among these advancements are the introduction of the Transformer architecture-particularly the self-attention mechanism-in 2017 (Vaswani 2017), and the emergence of Self-Supervised Learning (SSL) techniques (Oord et al. 2018), which allow models to learn rich linguistic representations from vast amounts of unlabeled text data. These innovations have collectively led to LLMs with substantially improved generalization capabilities across a wide range of NLP tasks. Some of the most impactful LLMs include: 1) BERT (Bidirectional Encoder Representations from Transformers), which was among the first models to achieve strong performance across various NLP benchmarks; 2) RoBERTa (A Robustly Optimized BERT Pretraining Approach) (Liu et al. 2019b), which builds on BERT with enhanced training strategies and larger datasets; and GPT-3 (Generative Pretrained Transformer 3) (Brown 2020), one of the largest LLMs, with 175 billion parameters, demonstrating remarkable performance across diverse tasks and domains. \n\nLLMs can be adapted to new tasks through various techniques: 1) Full fine-tuning or lightweight finetuning (i.e., linear probing), which involves training the model on labeled datasets for specific applications. \n\nFor example, ChatGPT is based on a fine-tuned version of GPT-4. 2) Instruction tuning, where the model is trained on a diverse set of NLP tasks framed as natural language instructions, as implemented in models like FLAN-T5 and FLAN-PaLM (Chung et al. 2024). 3) Prompting, which leverages few-shot learning or prompt engineering to guide the model's responses without modifying its parameters. 4) Retrieval-Augmented Generation (RAG) (Lewis et al. 2020), which enhances model outputs by incorporating relevant information retrieved from an external knowledge base. 5) Compression-Augmented Generation (CAG), a lightweight alternative to RAG, which uses compressed representations of external knowledge to guide the generation process.",
            "score": 0.38529521506916453,
            "section_title": "Large Language Models:",
            "char_start_offset": 11374,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 1145
                },
                {
                    "start": 1148,
                    "end": 1359
                },
                {
                    "start": 1362,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2095
                }
            ],
            "ref_mentions": [
                {
                    "start": 262,
                    "end": 276,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1599,
                    "end": 1618,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 1795,
                    "end": 1813,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54443359375
        },
        {
            "corpus_id": "264439519",
            "title": "TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction",
            "text": "With the increase in computing power and accumulation of enormous text data, large language models (LLMs) such as ChatGPT (OpenAI, 2023b) and GPT-4 (OpenAI, 2023a)  in casual conversations with users, LLMs exhibit astonishing performance by leveraging strong incontext learning ability. However LLMs may produce vague responses or incorrect answers in certain specialized domains, owing to the absence of relevant knowledge or a restricted scope of information acquired during the training stage, which might potentially result in untruthful answers and even cause physical damages to users (Xiang et al., 2023). For QA in such domains, retrievalaugmented generation (RAG) (Lewis et al., 2020), where the system retrieves external knowledge beforehand and then utilizes LLMs to generate answers leveraging retrieved knowledge, can greatly reduce the hallucinations generated (Shi et al., 2023;Shuster et al., 2021). \n\nMany current commercial LLMs are black-box models, where the model architectures and the weight information are not disclosed. These LLMs own superior text comprehension abilities, yet in many cases they can only output desired answers through complicated prompt engineering. On the other hand, deploying open-source LLMs to local servers is resource-intensive, in contrast to deploying smaller models such as T5 (Raffel et al., 2020). Some commercial LLMs like GPT-3.5-turbo (Ope-nAI, 2023c) and GPT-4 offer access through API calls; however, these models charge users based on the size of input and output1 . For individuals or companies looking to create their own services using LLMs through API calls, utilizing commercial ones can be resource-consuming if requests are made frequently. Therefore, it is necessary to minimize the number of input tokens while maintaining optimal performance during the API calls. \n\nIn this work, we propose a token compres- Figure 1: Illustration of different ways to utilize LLMs for QA. Top: directly using LLM. Middle: using a retrieval-augmented LLM. Bottom: using retrieval-augmented LLM with our proposed token compression methods.",
            "score": 0.38432234985166686,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 915
                },
                {
                    "start": 918,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1835
                },
                {
                    "start": 1838,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2093
                }
            ],
            "ref_mentions": [
                {
                    "start": 893,
                    "end": 914,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 1331,
                    "end": 1352,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.356689453125
        },
        {
            "corpus_id": "254853791",
            "title": "Multi hash embeddings in spaCy",
            "text": "The Python package spaCy 1 is a popular suite of Natural Language Processing software, designed for production use-cases. It provides a selection of well-tuned algorithms and models for common NLP tasks, along with well optimized data structures. The library also pays careful attention to stability, usability and documentation. \n\nEarly versions of spaCy assumed that users would mostly use the default models and architectures, and did not offer a fine-grained API for to customize and control training. This changed with the release of v3, which introduced a new configuration system and also came with a newly designed and documented machine learning system, Thinc. 2 Together these give users full control of the modelling details. \n\nOver the years spaCy has developed its own model architectures and default hyperparameters, informed by practical considerations that are not only motivated by scoring well on a single standard benchmark. In particular, spaCy prioritizes run-time efficiency on CPU, the ability to run efficiently on long documents, robustness to domain-shift, and the ability to fine-tune the model after training, without access to the original training data. \n\nIn this technical report, we focus on one of the more unusual features of spaCy's default model architectures: its strategy for embedding lexical items. We explain how this layer operates, provide experiments that show its sensitivity to key hyperparameters, and compare it to a more standard embedding architecture on a few datasets. Our experiments focus on how the embedding layer functions in the context of spaCy's default Named Entity Recognition architecture. \n\nWe these experiments aim to provide useful background information to users who wish to customize, change or extend spaCy's default architectures in their own experiments. Miikkulainen and Dyer (1991)   2 BACKGROUND Natural language processing (NLP) systems take text as input, broken down into a list of words or subwords. Word embeddings have become a de facto standard for NLP, enabling us to represent compact tokens that can generalize well. These embeddings associate words with continuous vectors and are trained such that functionally similar words have similar representations. The trained embeddings encode useful syntactic and semantic information.",
            "score": 0.3841797946723532,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 329
                },
                {
                    "start": 332,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 736
                },
                {
                    "start": 739,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1183
                },
                {
                    "start": 1186,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1652
                },
                {
                    "start": 1655,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2240
                },
                {
                    "start": 2241,
                    "end": 2313
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.333984375
        },
        {
            "corpus_id": "271270817",
            "title": "A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks",
            "text": "Artificial Intelligence has advanced significantly with the introduction of LLMs.LLMs are trained on huge corpora of text documents with millions and billions of tokens.It has been shown that as the number of model parameters increase, the performance of machine learning models improve and such has been the case with these LLMs.They have attained unprecedented performance on a wide array of NLP tasks Chang et al. (2023) because of which they have attracted a lot of interest from academia and different industries including medicine, law, finance and more.The present phase of research on LLMs focuses on their reasoning capacity via prompts rather than just next token prediction which has opened a new field of research around prompt engineering.\n\nPrompt engineering is the process of creating natural language instructions, or prompts, to extract knowledge from LLMs in an organized manner.Prompt engineering, in contrast to earlier conventional models, relies only on the embedded knowledge of LLMs and does not require extensive parameter re-training or fine-tuning based on the underlying NLP task.Understanding model parameters in terms of real world knowledge embedded in them is beyond human capabilities and hence this new field of prompt engineering has caught everyone's attention as it allows natural language exchange between researchers and LLMs to achieve the goals of the underlying NLP task.\n\nIn this work, we enumerate several prompting strategies and group them according to different NLP tasks that they have been used for.We provide a taxonomy diagram, tabulate the prompting techniques tried on various datasets for different NLP tasks, discuss the LLMs employed, and list potential SoTA methods for each dataset.As a part of this survey, we have reviewed and analyzed 44 research papers in total, the majority of which have been published in the previous two years and cover 39 prompting techniques applied on 29 different NLP tasks.There have not been a lot of prior systematic surveys on prompt engineering.Sahoo et al. (2024) surveys 29 prompting technique papers based on their applications.This is a very broad categorization as a single application can encapsulate numerous NLP tasks.For example, one of the applications which they discuss is reasoning and logic which can have plethora of NLP tasks like commonsense reasoning, mathemathical problem solving, multi-hop reasoning etc.",
            "score": 0.38398850442035154,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 81,
                    "end": 169
                },
                {
                    "start": 169,
                    "end": 330
                },
                {
                    "start": 330,
                    "end": 560
                },
                {
                    "start": 560,
                    "end": 752
                },
                {
                    "start": 754,
                    "end": 897
                },
                {
                    "start": 897,
                    "end": 1108
                },
                {
                    "start": 1108,
                    "end": 1413
                },
                {
                    "start": 1415,
                    "end": 1548
                },
                {
                    "start": 1548,
                    "end": 1740
                },
                {
                    "start": 1740,
                    "end": 1961
                },
                {
                    "start": 1961,
                    "end": 2037
                },
                {
                    "start": 2037,
                    "end": 2123
                },
                {
                    "start": 2123,
                    "end": 2218
                },
                {
                    "start": 2218,
                    "end": 2417
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2060546875
        },
        {
            "corpus_id": "275336280",
            "title": "GeAR: Generation Augmented Retrieval",
            "text": "Document retrieval serve as the foundational technology behind large-scale information systems, playing a crucial role in applications such as web search, open-domain question answering (QA) (Chen et al., 2017;Karpukhin et al., 2020), and retrieval-augmented generation (RAG) (Lewis et al., 2020;Liu et al., 2024a;Gao et al., 2024). The predominant approach in passage retrieval is to construct a bi-encoder model. In this architecture, queries and documents are encoded separately, transforming each into vector representations that enable computation of their semantic similarity in a high-dimensional space. \n\nHowever, this similarity calculation process faces several challenges. First, the complex semantic relationship between query and document is mapped to a scalar similarity, which cannot reflect enough information and is difficult to understand (Brito and Iser, 2023). Second, when dealing with long documents, such as those with 256, 512, or even more tokens, identifying the section most relevant to the query and contributing most to the similarity is highly desirable but challenging to achieve (Luo et al., 2024;G\u00fcnther et al., 2024). Moreover, many NLP tasks, such as sentence selection, search result highlighting, needle in a haystack (Liu et al., 2024b;An et al., 2024;Wang et al., 2024), and fine-grained citations (Gao et al., 2023;Zhang et al., 2024), require a deep and fine-grained understanding of the text. \n\nGiven this need for fine-grained understanding, the bi-encoder that simply aligns the entire document to the query seems insufficient, as its conventional contrastive loss mainly emphasizes global semantics (Khattab and Zaharia, 2020). To complement this core localization capability of the retriever, we propose a novel and challenging fundamental question: Can we enhance and integrate the information localization capability of existing retrievers without sacrificing their inherent retrieval capabilities? \n\nTo address these challenges, we proposed a novel approach GeAR (Generation-Augmented Retrieval).",
            "score": 0.3835835184747096,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 610
                },
                {
                    "start": 613,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1434
                },
                {
                    "start": 1437,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1946
                },
                {
                    "start": 1949,
                    "end": 2045
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 210,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 210,
                    "end": 233,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 276,
                    "end": 296,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 296,
                    "end": 314,
                    "matchedPaperCorpusId": "267770122"
                },
                {
                    "start": 857,
                    "end": 879,
                    "matchedPaperCorpusId": "259949750"
                },
                {
                    "start": 1255,
                    "end": 1274,
                    "matchedPaperCorpusId": "259360665"
                },
                {
                    "start": 1290,
                    "end": 1308,
                    "matchedPaperCorpusId": "270710703"
                },
                {
                    "start": 1337,
                    "end": 1355,
                    "matchedPaperCorpusId": "258865710"
                },
                {
                    "start": 1644,
                    "end": 1671,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.492431640625
        },
        {
            "corpus_id": "271720097",
            "title": "Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations",
            "text": "In recent years, Large Language Models (LLMs) have emerged as the cornerstone of Natural Language Processing (NLP), revolutionizing various domains with unprecedented capabilities. These versatile models have demonstrated remarkable abilities in diverse applications, ranging from assisting in code generation [1] [2], to facilitating news summarization [3] [4], and even augmenting information retrieval systems for improved search accuracy and efficiency [5] [6]. Furthermore, these models' sheer scale and complexity present unique challenges and opportunities, prompting researchers and practitioners to explore novel model training, optimization, and deployment methods. Optimizing large models for speed, reducing resource consumption, and making them more accessible is a significant part of LLM research. \n\nThe primary objective of this research paper is to explore various techniques for reducing resource requirements and compressing large language models, including analyzing each method in-depth and highlighting its unique challenges and practical implications. The discussed methods include quantization, pruning, knowledge distillation, and architectural optimizations. To better understand the relationship between these techniques, they are categorized into a taxonomy that presents an overview of the optimization landscape and helps navigate it for a better understanding of the research trajectory. Refer to figure 1 for a visual representation of the categorization and to the respective section for a more detailed look at the discussed literature in each category.",
            "score": 0.38352694374064566,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1587
                }
            ],
            "ref_mentions": [
                {
                    "start": 314,
                    "end": 317,
                    "matchedPaperCorpusId": "235755472"
                },
                {
                    "start": 354,
                    "end": 357,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 358,
                    "end": 361,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 457,
                    "end": 460,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 461,
                    "end": 464,
                    "matchedPaperCorpusId": "266359151"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6953125
        },
        {
            "corpus_id": "260900354",
            "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models",
            "text": "Chung et al. (2022); Longpre et al. (2023) finetune T5 (Raffel et al., 2020) with a large mixture of tasks using instruction tuning (Mishra et al., 2022;Wei et al., 2022;Sanh et al., 2022) to improve model performance and generalization to unseen tasks in both zero-shot and few-shot settings. \n\nOn the other hand, LLMs still face challenges such as hallucination and limitations in representing the long-tail and most recent knowledge (Mallen et al., 2022;Huang et al., 2022;Luu et al., 2022;Jang et al., 2022;Zheng et al., 2023). Retrieval-augmented language models (Izacard et al., 2023;Borgeaud et al., 2022;Wang et al., 2023;Shi et al., 2023) have emerged as a powerful approach to address these issues by retrieving relevant knowledge from an external corpus. Among these, the encoder-decoder models, such as ATLAS (Izacard et al., 2023), stand out. They benefit from the strong representation ability of a bidirectional encoder, coupled with of the efficacy of a Fusion-in-Decoder architecture (Izacard & Grave, 2021), enabling the effective integration of multiple retrieved passages. Despite these advancements, in-context learning with these models remains underexplored. \n\nIn this regard, we first conduct a comprehensive analysis of the state-of-the-art retrievalaugmented encoder-decoder language models by designing and experimenting with different prompting strategies. We find that these models exhibit a certain in-context learning ability; however, due to a mismatch between pretraining and inference and a limited context length-issues that are common to existing encoder-decoder LMs trained with masked language modeling-its few-shot performance is not stable and providing more than, e.g., 8-shot, examples does not lead to further improvement.",
            "score": 0.38352033664462704,
            "section_title": "Introduction",
            "char_start_offset": 1268,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 293
                },
                {
                    "start": 296,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1765
                }
            ],
            "ref_mentions": [
                {
                    "start": 55,
                    "end": 76,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 132,
                    "end": 153,
                    "matchedPaperCorpusId": "237421373"
                },
                {
                    "start": 153,
                    "end": 170,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 170,
                    "end": 188,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 457,
                    "end": 476,
                    "matchedPaperCorpusId": "249063119"
                },
                {
                    "start": 476,
                    "end": 493,
                    "matchedPaperCorpusId": "244117116"
                },
                {
                    "start": 493,
                    "end": 511,
                    "matchedPaperCorpusId": "248476156"
                },
                {
                    "start": 568,
                    "end": 590,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 612,
                    "end": 630,
                    "matchedPaperCorpusId": "258170263"
                },
                {
                    "start": 821,
                    "end": 843,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1001,
                    "end": 1024,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49853515625
        },
        {
            "corpus_id": "253441220",
            "title": "Pretrained Transformers for Text Ranking: BERT and Beyond",
            "text": "Text ranking takes a central place in Information Retrieval (IR), with Web search as its best-known application. More generally, text ranking models are applicable to any Natural Language Processing (NLP) task in which relevance of information plays a role, from filtering and recommendation applications to question answering and semantic similarity comparisons. Since the rise of BERT in 2019, Transformer models have become the most used and studied architectures in both NLP and IR, and they have been applied to basically any task in our research fields\u2014including text ranking. In a fast-changing research context, it can be challenging to keep lecture materials up to date. Lecturers in NLP are grateful for Dan Jurafsky and James Martin for yearly updating the 3rd edition of their textbook, making Speech and Language Processing the most comprehensive, modern textbook for NLP. The IR field is less fortunate, still relying on older textbooks, extended with a collection of recent materials that address neural models. The textbook Pretrained Transformers for Text Ranking: BERT and Beyond by Jimmy Lin, Rodrigo Nogueira, and Andrew Yates is a great effort to collect the recent developments in the use of Transformers for text ranking. The introduction of the book is well-scoped with clear guidance for the reader about topics that are out of scope (such as user aspects). This is followed by an excellent history section, stating for example:",
            "score": 0.3834368458271924,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.429931640625
        },
        {
            "corpus_id": "265594594",
            "title": "Semantic Embeddings for Arabic Retrieval Augmented Generation (ARAG)",
            "text": "Retrieval Augmented Generation (RAG), introduced by Facebook Researchers in 2020 [1], is a pivotal AI framework facilitating information retrieval for Generative AI models, thereby enhancing their accuracy and capabilities. RAG empowers Large Language Models (LLMs) by granting them access to external knowledge sources, augmenting the content generation process. This dual functionality entails retrieval, wherein RAG meticulously selects pertinent information from provided sources and generation, whereby LLMs craft contextually relevant responses based on user input. \n\nThe advantages of RAG are multi-fold. Firstly, it bolsters the performance by grounding LLMs with factual, up-todate information from external knowledge repositories. Furthermore, RAG maintains contextual relevance in responses, contributing to a more engaging user experience in conversational AI applications. Its scalability is noteworthy, as RAG models seamlessly handle copious volumes of information, proving invaluable for data-intensive tasks. Additionally, the adaptability of RAG models allows fine-tuning for specific applications [2], rendering them versatile across diverse data and use cases. Customizability is another hallmark, permitting RAG models to specialize in particular domains or subjects through customization and fine-tuning on specific knowledge bases. Due to the importance of such a framework for enterprises, extensive research is currently being pursued to discover new algorithms and techniques to enhance the performance of such models bounded by the context-window limitations of LLMs. Although there is ongoing research to expand the window size for LLM to be able to ingest more data in the prompt, the use of techniques like RAG is still of great practical importance, not only on homogeneous unstructured data but also on heterogeneous data [3]. \n\nIn principle, at the heart of the information retrieval module is the semantic embedding module which converts a piece of text, whether a query or a context text chunk to a numeric feature vector that embodies all semantic features of the text. The development of word and sentence embeddings is a relatively recent area of research in natural language processing (NLP) and information retrieval. \n\nMost of the semantic models are English language-centred; however, in recent years, Multilingual embedding models were released [4].",
            "score": 0.3832814172581209,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2257
                },
                {
                    "start": 2260,
                    "end": 2392
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.720703125
        },
        {
            "corpus_id": "265308606",
            "title": "Adapting LLMs for Efficient, Personalized Information Retrieval: Methods and Implications",
            "text": "The quest for better ways to extract information from large data repositories has been a constant theme in computational linguistics and computer science, especially with the complexity of the Internet's hyperlink networks [7]. Algorithms like PageRank [8] have tackled this challenge, navigating this network to evaluate the importance of web pages and bring some order to the vastness of the Internet. Early on, notable work by researchers like Salton and McGill introduced the Vector Space Model (VSM) [9] to the global academic and scientific community, setting the stage for many advancements to come. This model conceptualized text documents as vectors within a multi-dimensional space, igniting curiosity that subsequently catalyzed the advent of neural network-based language models [10,11,12]. The journey from early sequence modeling, represented by RNNs and LSTMs [13], to the transformative era brought about by transformer models [14], has been swift. Large Language Models (LLMs) like OpenAI's GPT series and Google's BERT have gone beyond just text retrieval or recognition, evolving to understand and generate text in a way that is much like human communication [15,16,17]. \n\nIn the sections that follow, we delve into the crucial role of LLMs in enhancing information retrieval, with a special focus on employing Retrieval-Augmented Generation (RAG) [18] to address the prevalent challenges associated with language models. We outline the architecture of RAG models and discuss their applicability and efficacy in various settings. Furthermore, we explore the integration of knowledge bases with information retrieval systems to augment the richness of contextual understanding and provide a more comprehensive and accurate retrieval of information. This integration is pivotal in bridging the gap between structured and unstructured data, thereby facilitating a more informed and insightful interaction for users [19,20].",
            "score": 0.3832814172581209,
            "section_title": "Related work",
            "char_start_offset": 3366,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1189
                },
                {
                    "start": 1192,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1939
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "15771935"
                },
                {
                    "start": 505,
                    "end": 508,
                    "matchedPaperCorpusId": "6473756"
                },
                {
                    "start": 795,
                    "end": 798,
                    "matchedPaperCorpusId": "259837016"
                },
                {
                    "start": 798,
                    "end": 801,
                    "matchedPaperCorpusId": "238667617"
                },
                {
                    "start": 875,
                    "end": 879,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1178,
                    "end": 1182,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1367,
                    "end": 1371,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1931,
                    "end": 1935,
                    "matchedPaperCorpusId": "259076468"
                },
                {
                    "start": 1935,
                    "end": 1938,
                    "matchedPaperCorpusId": "262078274"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68359375
        },
        {
            "corpus_id": "273812370",
            "title": "Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors",
            "text": "Despite the remarkable success of large language models (LLMs) across various natural language processing (NLP) tasks (Achiam et al., 2023;Touvron et al., 2023;Team et al., 2024), they still face significant limitations. One key issue is the reliance on static training data, which can quickly become outdated, leading to a lack of up-to-date knowledge, especially in fast-evolving fields like news (Nakshatri et al., 2023). Moreover, LLMs often struggle in domain-specific contexts, such as healthcare (Wang et al., 2023) and finance (Wu et al., 2023), where specialized knowledge is required, resulting in inaccurate or incomplete answers. Another critical challenge is hallucination, where LLMs generate plausible-sounding but factually incorrect information (Maynez et al., 2020;Ji et al., 2023). These limitations underscore the need for methodologies to enhance the reliability and accuracy of LLM outputs, particularly in dynamic and specialized domains. \n\nRetrieval-augmented generation (RAG) has emerged as a promising approach to address the limitations of LLMs (Lewis et al., 2020;Cheng et al., 2024;Zhang et al., 2024a). Unlike traditional LLMs that rely solely on pre-trained model parameters, RAG integrates an external knowledge retrieval component, which dynamically searches a large corpus of documents (referred to as a knowledge database) for relevant information. RAG typically retrieves the top-k most relevant documents based on the input query. This allows RAG systems to provide up-to-date and accurate responses by referencing these contextually relevant documents. RAG has already been applied in various real-world applications, such as Bing Search, Google Search, and WikiChat (Semnani et al., 2023). These systems offer a more reliable by leveraging external knowledge bases to generate informed responses.",
            "score": 0.3832814172581209,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1835
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 423,
                    "matchedPaperCorpusId": "266166437"
                },
                {
                    "start": 762,
                    "end": 783,
                    "matchedPaperCorpusId": "218487034"
                },
                {
                    "start": 783,
                    "end": 799,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 1072,
                    "end": 1092,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1092,
                    "end": 1111,
                    "matchedPaperCorpusId": "258479968"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.499267578125
        },
        {
            "corpus_id": "276885240",
            "title": "Path Pooling: Train-Free Structure Enhancement for Efficient Knowledge Graph Retrieval-Augmented Generation",
            "text": "Large Language Models (LLMs), pre-trained on vast corpora, have excelled in various natural language processing tasks (Shen et al., 2024;Naveed et al., 2023;Ge et al., 2023). However, outdated information or missing domain-specific knowledge in public training corpora often lead to hallucinations in real-world applications (Wang et al., 2023;Hong et al., 2023). To mitigate this, many methods (Pan et al., 2024;Peng et al., 2024;Edge et al., 2025) integrate high-quality Knowledge Graphs (KGs) for retrieval-augmented generation (RAG), enhancing credibility. * Equal contribution. \u2020 Corresponding author. \n\nKGs organize large collections of knowledge triples in a well-structured graph form and serve as core knowledge bases across various domains (Chein and Mugnier, 2008;Robinson et al., 2015). Compared to traditional text-based knowledge bases, KGs not only capture rich semantic information but also offer a clear structure organization, enabling efficient knowledge management and updates. Therefore, KG-RAG provides higher-quality domain knowledge in many applications, significantly mitigating LLMs' hallucinations (Jiang et al., 2023;Baek et al., 2023). \n\nHowever, the complex knowledge structures in KGs pose challenges for effective knowledge retrieval in KG-RAG systems. Current mainstream KG-RAG methods can be categorized into pathbased KG-RAG and triple-based KG-RAG based on their retrieval paradigms. Earlier path-based KG-RAG paradigm employs LLMs for path traversal on KGs (Sun et al., 2024;Ma et al., 2024), subsequent path-based works try to fine-tune LLMs with KG information to retrieve knowledge graph paths (Luo et al., 2024;Mavromatis and Karypis, 2024).",
            "score": 0.3832814172581209,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1164
                },
                {
                    "start": 1167,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1682
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 173,
                    "matchedPaperCorpusId": "258049306"
                },
                {
                    "start": 413,
                    "end": 431,
                    "matchedPaperCorpusId": "263620134"
                },
                {
                    "start": 1125,
                    "end": 1145,
                    "matchedPaperCorpusId": "258714753"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58154296875
        },
        {
            "corpus_id": "270702738",
            "title": "Ragnar\u00f6k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track",
            "text": "Ragnar\u00f6k is an open-source, reproducible, and reusable framework implementing an end-to-end retrieval-augmented generation (RAG) pipeline, comprising two modules applied sequentially: (1) (R) retrieval and (2) (AG) augmented generation.Through the Ragnar\u00f6k framework, we will provide several baselines to all participants in the upcoming TREC 2024 RAG track.An overview of the framework is provided in Figure 1.We first describe both modules and expand on the I/O specifications in our framework.\n\nRetrieval Module.This module retrieves the relevant segments for a user topic as the input.It supports (i) first-stage lexical retrieval models such as BM25 [49] and (ii) reranking models such as RankZephyr [47].The retrieval system searches for relevant segments in the document collection and retrieves the top-100 segments further reranked by the reranker model to filter out the top-20 relevant segments for the next stage.\n\nAugmented Generation Module.This module takes in the user topic and the top-20 retrieved segments (from the retrieval module) as the input and a prompting strategy to the large language model (LLM) to generate the answer response with in-context citations for the topic.The answer response is divided into individual sentences, each sentence within the answer contains text and is grounded on retrieved documents provided as references.",
            "score": 0.3832814172581209,
            "section_title": "OUR FRAMEWORK",
            "char_start_offset": 5759,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 236,
                    "end": 358
                },
                {
                    "start": 358,
                    "end": 411
                },
                {
                    "start": 411,
                    "end": 496
                },
                {
                    "start": 498,
                    "end": 515
                },
                {
                    "start": 515,
                    "end": 589
                },
                {
                    "start": 589,
                    "end": 710
                },
                {
                    "start": 710,
                    "end": 925
                },
                {
                    "start": 927,
                    "end": 955
                },
                {
                    "start": 955,
                    "end": 1197
                },
                {
                    "start": 1197,
                    "end": 1363
                }
            ],
            "ref_mentions": [
                {
                    "start": 655,
                    "end": 659,
                    "matchedPaperCorpusId": "207178704"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7353515625
        },
        {
            "corpus_id": "277596247",
            "title": "Towards Effective EU E-Participation: The Development of AskThePublic",
            "text": "Artificial Intelligence (AI) has seen significant advancements in deep learning. In Natural Language Processing (NLP), early static embeddings like GloVe [34] were replaced by context-aware methods following the introduction of the Transformer architecture [44], which employs self-attention for parallel sequence processing. Models like BERT [13] leverage this architecture to enable tasks such as translation, summarization, and question-answering. Generative models further extended NLP capabilities through pre-training on large corpora followed by fine-tuning [35], culminating in LLMs like GPT-4 [1], which integrate multimodal processing and reinforcement learning for alignment with human preferences. Retrieval-Augmented Generation (RAG) methods enhance these models by incorporating external knowledge for more accurate and context-rich outputs [26]. In a typical RAG workflow, the input query is first converted into a vector embedding using an encoder, and all documents in the external database are similarly embedded and stored for efficient retrieval. When a query is made, its embedding is used to search the database for the most relevant embeddings, and the retrieved information is then combined with the language model to generate a more accurate and context-rich response. \n\nLLMs and NLP techniques have been applied in various e-participation contexts to enhance public engagement, streamline service delivery, and support decision-making processes. In citizen input analysis, AI-based systems have automated feedback processing, reducing manual workload while ensuring consistent and scalable results [8]. In public administration, NLP tools have facilitated policy-making by bridging communication gaps between policymakers and citizens [19]. Similarly, LLM-powered chatbots have been employed in public services to improve service accessibility, though challenges linked to privacy, transparency, and trust remain critical concerns [14]. Moreover, citizen complaint management systems using contextual feedback mechanisms driven by LLMs have enhanced user trust and engagement by providing meaningful, real-time responses [24]. These applications demonstrate the practical potential of LLMs in enabling more interactive, transparent, and efficient e-participation systems.",
            "score": 0.3822788957543374,
            "section_title": "Large Language Models",
            "char_start_offset": 9274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1293
                },
                {
                    "start": 1296,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2152
                },
                {
                    "start": 2153,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 257,
                    "end": 261,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 343,
                    "end": 347,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 855,
                    "end": 859,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 2147,
                    "end": 2151,
                    "matchedPaperCorpusId": "270983197"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6220703125
        },
        {
            "corpus_id": "237732031",
            "title": "The Classical Language Toolkit: An NLP Framework for Pre-Modern Languages",
            "text": "Two software architectural patterns, the framework and the pipeline, are most relevant to the CLTK's design. \n\nAs NLP matured in the early 2000's, frameworks (or toolkits) emerged with the purpose of making the technology easier for non-specialists to use. To this end, these frameworks generally have documentation friendly for beginners, value diversity in algorithms, treat multiple languages, provide data sets, help with text preprocessing, and provide pre-trained models.8 Of these characteristics, the CLTK especially values multilingual and multi-algorithmic NLP, the latter of which being necessary to accommodate the varying state of data sets of pre-modern languages. The CLTK shows some especial similarity to the quanteda library for the R language (Benoit et al., 2018), as it contains novel algorithms yet also \"wraps\" other NLP libraries. \n\nSeveral NLP frameworks have popularized the pipeline processing architecture, in which default algorithms (tokenization, POS tagging, dependency parsing, etc.) are run in series upon input text. Algorithms may be added or removed from a default pipeline. Increasingly, frameworks use identical algorithms for every language, without special consideration for a language's nuances. \n\nAside from the CLTK, NLP tools for premodern languages have been uncommon, 9 despite a steady growth of language resources. 10 Premodern languages are often low-resource. Lowresource software applications, however, have tended toward transcription 11 and, in the case of en-dangered languages, language preservation. 12 An interesting exception may be UralicNLP (H\u00e4m\u00e4l\u00e4inen, 2019), which provides algorithms intended for relatively small data sets in Finnish and related languages.",
            "score": 0.38169042023280664,
            "section_title": "Previous Work",
            "char_start_offset": 5672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 111,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 854
                },
                {
                    "start": 857,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1237
                },
                {
                    "start": 1240,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1721
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.167724609375
        },
        {
            "corpus_id": "269816307",
            "title": "AI Chaperone: Awareness Chatbot for Alzheimer\u2019s Disease",
            "text": "The application aims to assist individuals with Alzheimer's in their daily lives by offering reminders for tasks, tracking their location, and engaging in memory training exercises. Notably, the chatbot provides personalized support, actively monitors activities, and enhances memory skills. What stands out is the innovative integration of technology to address the specific needs of Alzheimer's patients. Insights into the user interface, data security measures, and the overall effectiveness of the chatbot contribute to a comprehensive understanding of its potential impact on the care and well-being of individuals in the early stages of Alzheimer's disease.Both papers underscore the importance of leveraging chatbots to enhance the quality of life and support for those affected by dementia and Alzheimer's. \n\n1.2 Technologies Used Large Language Models: Large Language Models (LLMs) are advanced Artificial Intelligence Systems designed for understanding and generating human text. These Generative AI models focus on Natural Language Processing (NLP) tasks, interpreting natural language instructions and performing tasks akin to human capabilities. By leveraging extensive data and sophisticated neural network architectures, LLMs generate coherent and contextually relevant text across various NLP tasks. Notably, the publication of the paper \"Attention Is All You Need\" by Google and the University of Toronto [6] introduced the Transformers Architecture, which significantly impacted AI progress, particularly in NLP. Transformers enable efficient model scaling, parallel computation, and more effective training on large datasets, facilitating the development of foundational models. They excel in understanding language patterns, making accurate predictions, and generating textual content. LLMs, such as GPT2, FLAN-T5, LLaMa, BLOOM, and Google PaLM, are widely used for text generation, summarization, translation, and question answering. Their commercial deployment includes applications like OpenAI ChatGPT, Google Gemini, and Microsoft Copilot. Retrieval Augmented Generation: Retrievalaugmented generation (RAG) is an advanced approach in natural language processing (NLP) that combines the strengths of retrieval-based and generative models. LLMs (Large Language Models) are highly accurate at answering user queries but may provide outdated or generic information. To address this, RAG augments LLMs with retriever models that fetch relevant facts from an up-to-date database. By integrating contextually relevant information, RAG enhances LLM performance.",
            "score": 0.3815201675818812,
            "section_title": "Introduction",
            "char_start_offset": 2357,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2262
                },
                {
                    "start": 2263,
                    "end": 2386
                },
                {
                    "start": 2387,
                    "end": 2498
                },
                {
                    "start": 2499,
                    "end": 2578
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56787109375
        },
        {
            "corpus_id": "270341565",
            "title": "Utilizing passage\u2010level relevance and kernel pooling for enhancing BERT\u2010based document reranking",
            "text": "2][3][4][5] The success in applying these semantic-based language models may lead to another rapid rise in search engines.In practice, the process of information retrieval can summarize how to retrieve relevant documents or chunks quickly and accurately from a large amount of text related to queries consisting of only a few words submitted by users. 6Existing information retrieval frameworks that using PLMS includes two components: the retrieval phase and the rerank phase. 7This \"retrieval-then-rerank\" multistage retrieval pipeline outperforms well in a diversity of downstream NLP tasks, such as Question Answering, Recommendation system, 8 and Information Retrieval. 7,9Both recall and rerank phases influence the results jointly.\n\nConsidering time and memory costs during retrieval, using an unsupervised model such as BM25 + RM3 or DPH + KL for the first-stage retrieval is a cost-effective approach.BM25 + RM3 is an unsupervised ranking model using pseudo relevance feedback signals. 10Derived from the divergence-from-randomness framework, DPH is an unsupervised retrieval model, DPH + KL uses Kullback-Leibler (KL) divergence and Rocchio model to expand the original query, and then uses DPH to rank documents. 11,12The pre-trained model like BERT is more often adopted to further improve the retrieval results in rerank stage due to its ability to obtain the interaction between query and document.The effectiveness of BERT is mainly attributed to the complex architecture of transformer encoder 13 and its ability to compute deeply contextualized semantic interaction of input sequence.A challenge needs to be addressed when applying BERT to document retrieval.BERT uses the Transformer structure, where the self-attention mechanism is one of the core components of the model.In the self-attention mechanism, each position needs to calculate attention weights relative to all other positions in the sequence, and the complexity of this calculation is the square of the input sequence length.Therefore, as the length of the input sequence increases, the computational cost shows a quadratic growth.",
            "score": 0.3813869523206236,
            "section_title": "Motivation",
            "char_start_offset": 27,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 122,
                    "end": 353
                },
                {
                    "start": 353,
                    "end": 479
                },
                {
                    "start": 479,
                    "end": 678
                },
                {
                    "start": 678,
                    "end": 738
                },
                {
                    "start": 740,
                    "end": 910
                },
                {
                    "start": 910,
                    "end": 997
                },
                {
                    "start": 997,
                    "end": 1229
                },
                {
                    "start": 1229,
                    "end": 1412
                },
                {
                    "start": 1412,
                    "end": 1601
                },
                {
                    "start": 1601,
                    "end": 1676
                },
                {
                    "start": 1676,
                    "end": 1791
                },
                {
                    "start": 1791,
                    "end": 2006
                },
                {
                    "start": 2006,
                    "end": 2112
                }
            ],
            "ref_mentions": [
                {
                    "start": 2,
                    "end": 5,
                    "matchedPaperCorpusId": "218539531"
                },
                {
                    "start": 5,
                    "end": 8,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 352,
                    "end": 353,
                    "matchedPaperCorpusId": "263297581"
                },
                {
                    "start": 646,
                    "end": 647,
                    "matchedPaperCorpusId": "215814408"
                },
                {
                    "start": 677,
                    "end": 678,
                    "matchedPaperCorpusId": "81977235"
                },
                {
                    "start": 995,
                    "end": 997,
                    "matchedPaperCorpusId": "14116318"
                },
                {
                    "start": 1224,
                    "end": 1227,
                    "matchedPaperCorpusId": "7446821"
                },
                {
                    "start": 1227,
                    "end": 1229,
                    "matchedPaperCorpusId": "61859400"
                },
                {
                    "start": 1510,
                    "end": 1512,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4453125
        },
        {
            "corpus_id": "273707796",
            "title": "RAGraph: A General Retrieval-Augmented Graph Learning Framework",
            "text": "Graph Neural Networks (GNNs) [5,48,97,63,126] have recently burgeoned a surge of interest in both academic and industry communities due to their robust capability to model complex, real-world data in diverse domains, including societal [72,55,80], biochemical [17,111,107], and traffic-related [54,23,44,21] fields and etc [53,37,68,15,25,24]. Utilizing a message-passing mechanism [48,29], GNNs have transcended traditional node embedding approaches [28,79,95], enabling the capture of intricate relationships within data through sophisticated architectures and advanced graph representation learning techniques [48,50,54,18,97]. However, the challenge of generalizing GNNs across different modalities, domains [62,61], and tasks remains largely unexplored [56,113]. This is in stark contrast to the significant successes of large models such as GPTs [74,75] in NLP and Sora [64] in CV, presenting a crucial frontier for further research and realms for graph data generalizing. \n\nIn graph learning tasks, providing the necessary context is crucial for graph generalization [129,51,73,134], i.e., retrieve similar shopping context as illustrated in Figure 1 (c). Therefore, our insight is to enhance the model's generalization ability and prediction accuracy by retrieving necessary contexts during graph learning through retrieval. Retrieval-Augmented Generation (RAG) represents a prominent methodology, significantly augmenting language model functionalities through the integration of a dynamic retrieval mechanism during the generation process [135,77] (e.g., a person asks what animal it is, and we use some visual [138] or text retrieval [2] methods to retrieve more descriptive features or even the wanted category). RAG enriches not only accurate and reliable content but also reduces factual errors, addressing challenges such as incorrect answers, hallucinations, and limited interpretability in knowledge-intensive tasks [40,2,1], obviating the need for updating model parameters and could be generalized even in unseen scenarios.",
            "score": 0.3809553618000885,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 2042
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 32,
                    "matchedPaperCorpusId": "13999578"
                },
                {
                    "start": 32,
                    "end": 35,
                    "matchedPaperCorpusId": "3144218"
                },
                {
                    "start": 38,
                    "end": 41,
                    "matchedPaperCorpusId": "235324671"
                },
                {
                    "start": 236,
                    "end": 240,
                    "matchedPaperCorpusId": "260859554"
                },
                {
                    "start": 240,
                    "end": 243,
                    "matchedPaperCorpusId": "248266904"
                },
                {
                    "start": 243,
                    "end": 246,
                    "matchedPaperCorpusId": "250340384"
                },
                {
                    "start": 264,
                    "end": 268,
                    "matchedPaperCorpusId": "259730669"
                },
                {
                    "start": 268,
                    "end": 272,
                    "matchedPaperCorpusId": "258333694"
                },
                {
                    "start": 294,
                    "end": 298,
                    "matchedPaperCorpusId": "251518220"
                },
                {
                    "start": 301,
                    "end": 304,
                    "matchedPaperCorpusId": "259187717"
                },
                {
                    "start": 304,
                    "end": 307,
                    "matchedPaperCorpusId": "260171424"
                },
                {
                    "start": 323,
                    "end": 327,
                    "matchedPaperCorpusId": "250243834"
                },
                {
                    "start": 336,
                    "end": 339,
                    "matchedPaperCorpusId": "261681823"
                },
                {
                    "start": 382,
                    "end": 386,
                    "matchedPaperCorpusId": "3144218"
                },
                {
                    "start": 386,
                    "end": 389,
                    "matchedPaperCorpusId": "4755450"
                },
                {
                    "start": 451,
                    "end": 455,
                    "matchedPaperCorpusId": "207238980"
                },
                {
                    "start": 455,
                    "end": 458,
                    "matchedPaperCorpusId": "3051291"
                },
                {
                    "start": 613,
                    "end": 617,
                    "matchedPaperCorpusId": "3144218"
                },
                {
                    "start": 620,
                    "end": 623,
                    "matchedPaperCorpusId": "251518220"
                },
                {
                    "start": 623,
                    "end": 626,
                    "matchedPaperCorpusId": "222130583"
                },
                {
                    "start": 712,
                    "end": 716,
                    "matchedPaperCorpusId": "260499690"
                },
                {
                    "start": 716,
                    "end": 719,
                    "matchedPaperCorpusId": "251518440"
                },
                {
                    "start": 758,
                    "end": 762,
                    "matchedPaperCorpusId": "262464639"
                },
                {
                    "start": 1082,
                    "end": 1085,
                    "matchedPaperCorpusId": "259129271"
                },
                {
                    "start": 1085,
                    "end": 1089,
                    "matchedPaperCorpusId": "266650197"
                },
                {
                    "start": 1554,
                    "end": 1557,
                    "matchedPaperCorpusId": "267094920"
                },
                {
                    "start": 1621,
                    "end": 1626,
                    "matchedPaperCorpusId": "269214275"
                },
                {
                    "start": 1645,
                    "end": 1648,
                    "matchedPaperCorpusId": "264288947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54931640625
        },
        {
            "corpus_id": "273346485",
            "title": "Integrating Reinforcement Learning and Large Language Models for Crop Production Process Management Optimization and Control through A New Knowledge-Based Deep Learning Paradigm",
            "text": "In recent years, the field of computational natural language processing (NLP) has undergone a transformation with the advent of LLMs (Brown et al., 2020;Devlin et al., 2018). Leveraging the power of deep learning, LLMs have revolutionized NLP tasks by modeling and understanding language on an unprecedented scale. These models are typically built on DNNs that can learn context and meaning from large amounts of text data, enabling them to perform tasks ranging from language translation and text summarization to question-answering and dialogue generation, and beyond (Li et al., 2023b). \n\nOne of the key milestones in the rise of LLMs has been the development of Transformer-based architectures (Vaswani, 2017), which have paved the way for more powerful and versatile language models. A prominent example of this innovation is the Bidirectional Encoder Representations from Transformers (BERT, (Devlin et al., 2018)), which comes in two variants: BERT base with 110M parameters and BERT large with 340M parameters. By simply adding an output layer for task-specific fine-tuning, BERT can be adapted for a variety of NLP tasks like question answering and natural language inference. This adaptability allows BERT to achieve superior performance across tasks without major alterations to the model's architecture, often surpassing task-specific designs. Following the success of BERT, Google introduced the Pathways Language Model (PaLM) (Wei et al., 2022), followed by the Language Model for Dialogue Applications (LaMDA) (Thoppilan et al., 2022), PaLM2 (Anil et al., 2023), and, most recently, the advanced LLMs, Gemini and Gemma 2 (formerly known as Google Bard) (Team et al., 2023(Team et al., , 2024)). PaLM, with 540 billion parameters, and its successor PaLM2, featuring 340 billion parameters, are built as dense decoder-only Transformer models and trained using the Pathways system (Barham et al., 2022) for efficient learning. Gemma 2 offers multiple model sizes, including 2B, 9B, and 27B parameters, further expanding the capabilities of Google's LLMs.",
            "score": 0.38032381012961997,
            "section_title": "Preliminaries of LLMs",
            "char_start_offset": 16543,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2066
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 153,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 698,
                    "end": 713,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1440,
                    "end": 1458,
                    "matchedPaperCorpusId": "208910339"
                },
                {
                    "start": 1893,
                    "end": 1914,
                    "matchedPaperCorpusId": "247618786"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32373046875
        },
        {
            "corpus_id": "272370243",
            "title": "Improving viral annotation with artificial intelligence",
            "text": "The architecture of a model, or its structure, affects its training time, memory usage, accessibility, and accuracy. The choice of training architecture, as well as training data, also changes the model's bias, creating tradeoffs in what parts of the data the model is able to focus on while learning. Major examples of older language model architectures include n-gram, skip-gram, and bag-of-words language models. These language models are all statistical language models, which apply statistical estimation techniques to determining patterns in languages (67). \n\nN-gram language models are language models that determine the probability of a word being used in a sentence depending on the n-1 words preceding it (68). Skip-gram language models use a current word to predict its context given surrounding words. \n\nConversely, bag-of-words language models use the context provided by surrounding words to predict a current word (69). \n\nWord2Vec is one of the earliest examples of a successful NLP utilizing a neural network to produce embeddings and uses several of the older model architectures listed above. Specifically, Word2Vec consists of (i) a skip-gram model and (ii) a continuous bag-of-words model (69). ProtVec is a pLM based on the model architecture of Word2Vec. However, instead of being trained on words and sentences, ProtVec was trained on AA sequences from SwissProt, which were broken up into \"words\" using an n-gram model of size 3 (56). \n\nMajor examples of current pLM architectures include recurrent neural networks, long short-term memory models, encoder-decoder models (which include transformer models), large language models, and small-scale language models. \n\nRecurrent neural networks (RNNs) are a form of neural network where the model passes outputs from previous states into new states. The recurrence allows these models to process sequences of indefinite length. Consequently, RNNs are good for processing AA sequences, which can have variable lengths from tens to thousands of amino acids. However, RNNs cannot learn patterns in the AA sequence data as deeply (70). \n\nLong short-term memory models (LSTMs) are RNNs with an additional memory cell that stores information from previous states.",
            "score": 0.3800762839318128,
            "section_title": "Model architectures",
            "char_start_offset": 14206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 813
                },
                {
                    "start": 816,
                    "end": 934
                },
                {
                    "start": 937,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1685
                },
                {
                    "start": 1688,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2100
                },
                {
                    "start": 2103,
                    "end": 2226
                }
            ],
            "ref_mentions": [
                {
                    "start": 558,
                    "end": 562,
                    "matchedPaperCorpusId": "10959945"
                },
                {
                    "start": 929,
                    "end": 933,
                    "matchedPaperCorpusId": "4508796"
                },
                {
                    "start": 1209,
                    "end": 1213,
                    "matchedPaperCorpusId": "4508796"
                },
                {
                    "start": 1453,
                    "end": 1457,
                    "matchedPaperCorpusId": "269010973"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.578125
        },
        {
            "corpus_id": "276733154",
            "title": "Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models",
            "text": "A Retrieval-Augmented Generation (RAG) architecture will be utilized in order to accomplish the objective of this study, which is to evaluate large language models (LLMs) in the context of answering questions related to quranic studies [24]. The RAG approach combines LLMs with semantic retrieval to provide contextually relevant and authoritative responses from a descriptive dataset. The primary tasks and evaluation guidelines used to assess the system's performance are outlined in full below [14]. \n\n1) NLP Tasks: The research's primary NLP task is to generate semantically pertinent and contextually accurate responses to inquiries regarding quranic studies. The system employs a Retrieval-Augmented Generation (RAG) architecture, combining retrieval-based and generative methodologies, to ensure that responses are both dataset-based and linguistically coherent. The system executes the following tasks: \n\n\u2022 Semantic Search and Retrieval: Upon a user's query submission, the system does a semantic similarity search over the vectorized dataset obtained from Qur'anic surah descriptions [19]. This procedure determines the most contextually pertinent entries from the dataset to respond to the query. \n\n\u2022 Response Generation: The retrieved descriptions are submitted to the LLMs, which produce a comprehensive response [14]. This response integrates the retrieved information and provides explanatory content to address the query. \n\n\u2022 Citations and Contextualization: Each generated response includes references to the original dataset entries (e.g., surah descriptions or specific virtues), allowing users to trace the information back to its source [25]. \n\n2) Evaluation Guidelines: To assess the quality of the responses generated by the system, human evaluators followed a structured set of evaluation guidelines. These guidelines provided a consistent framework for scoring responses across three key dimensions: Context Relevance, Answer Faithfulness, and Answer Relevance [14]. Each dimension is explained below, along with its calculation method and examples. \n\n\u2022 Context Relevance evaluates how precisely the retrieved and generated responses align with the user query while avoiding irrelevant or extraneous information. The relevance score is calculated using the precision@k metric, where k represents the number of top retrieved results considered as shown in Equation 1. \n\nExample: \n\n\u2022 Query: \"What is the reason for Surah Al-Fatihah being named Umm Al-Kitab?\"",
            "score": 0.37981716541958344,
            "section_title": "B. NLP Tasks and Evaluation Guidelines",
            "char_start_offset": 9168,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1436
                },
                {
                    "start": 1439,
                    "end": 1662
                },
                {
                    "start": 1665,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2073
                },
                {
                    "start": 2076,
                    "end": 2236
                },
                {
                    "start": 2237,
                    "end": 2390
                },
                {
                    "start": 2393,
                    "end": 2401
                },
                {
                    "start": 2404,
                    "end": 2480
                }
            ],
            "ref_mentions": [
                {
                    "start": 236,
                    "end": 240,
                    "matchedPaperCorpusId": "252735056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52197265625
        },
        {
            "corpus_id": "253441220",
            "title": "Pretrained Transformers for Text Ranking: BERT and Beyond",
            "text": "Text ranking takes a central place in Information Retrieval (IR), with Web search as its best-known application. More generally, text ranking models are applicable to any Natural Language Processing (NLP) task in which relevance of information plays a role, from filtering and recommendation applications to question answering and semantic similarity comparisons. Since the rise of BERT in 2019, Transformer models have become the most used and studied architectures in both NLP and IR, and they have been applied to basically any task in our research fields-including text ranking. \n\nIn a fast-changing research context, it can be challenging to keep lecture materials up to date. Lecturers in NLP are grateful for Dan Jurafsky and James Martin for yearly updating the 3rd edition of their textbook, making Speech and Language Processing the most comprehensive, modern textbook for NLP. The IR field is less fortunate, still relying on older textbooks, extended with a collection of recent materials that address neural models. The textbook Pretrained Transformers for Text Ranking: BERT and Beyond by Jimmy Lin, Rodrigo Nogueira, and Andrew Yates is a great effort to collect the recent developments in the use of Transformers for text ranking. \n\nThe introduction of the book is well-scoped with clear guidance for the reader about topics that are out of scope (such as user aspects). This is followed by an excellent history section, stating for example: \n\nWe might take for granted today the idea that automatically extracted terms from a document can serve as descriptors or index terms for describing the contents of those documents, but this was an important conceptual leap in the development of information retrieval. (p. 11) \n\nChapter 2 is a complete yet compact overview of the IR field and the context of Transformer-based ranking models, with a substantial section devoted to text collections and a good introduction to keyword search. This includes a discussion of variants of BM25 leading to different results, and a mention of pseudo-relevance feedback. Also, this chapter pays attention to terminology differences between fields, clearly aligning terms such as performance versus effectiveness. \n\nChapter 3 is extensive. It starts with an overview of BERT. This is an excellent introduction to the topic, not as detailed as Jurafsky and Martin's but sufficiently specific for students to understand it on a conceptual level.",
            "score": 0.37975988998067284,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1246
                },
                {
                    "start": 1249,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1457
                },
                {
                    "start": 1460,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1734
                },
                {
                    "start": 1737,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2211
                },
                {
                    "start": 2214,
                    "end": 2237
                },
                {
                    "start": 2238,
                    "end": 2273
                },
                {
                    "start": 2274,
                    "end": 2441
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.449462890625
        },
        {
            "corpus_id": "270562615",
            "title": "Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs for Information Retrieval",
            "text": "The rapid growth and superior performance of large language models (LLMs) (Ouyang et al., 2022;OpenAI, 2023;Wang et al., 2024b) have made them a preferred choice for a wide range of NLP applications (Xi et al., 2023;Wang et al., 2024b;Wu et al., 2023;Zhang et al., 2023a,b).LLMs have demonstrated robust zero-shot ranking abilities in English and various low-resource languages (Adeyemi et al., 2023;Sun et al., 2023).Consequently, researchers have applied LLMs to the task of information retrieval, where they outperform previous text search and similarity measurement methods (Ma et al., 2023;Xu et al., 2024).\n\nThe retrieval-augmented generation (RAG) framework has been widely adopted to alleviate hallucination problems in LLMs generation, especially for knowledge-intensive tasks (Lewis et al., 2020).The RAG framework consists of two key components: a retriever to locate relevant information from a large corpus based on a given input, and a reader, typically a LLM, to integrate this information into its generation (Izacard et al., 2023;Shi et al., 2023).How to distill knowledge from LLMs to optimize the retriever in the RAG framework with indomain data has been a crucial challenge.Early efforts proposed training the retriever with whitebox LLM readers by extracting supervision signals directly from the LLMs' weights (Izacard et al., 2023;Rubin and Berant, 2023;Guu et al., 2020).However, this approach becomes more computationally intensive and time-consuming as LLMs increase in size.Meanwhile, it is incompatible with closed-source models.\n\nRecently, researchers have also turned to knowledge distillation for the retriever from black-box LLMs by training the retriever directly from generated outputs, such as RePLUG (Shi et al., 2023) and In-Context RALM (Ram et al., 2023).",
            "score": 0.37955109953353117,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 274,
                    "end": 418
                },
                {
                    "start": 418,
                    "end": 612
                },
                {
                    "start": 614,
                    "end": 807
                },
                {
                    "start": 807,
                    "end": 1065
                },
                {
                    "start": 1065,
                    "end": 1195
                },
                {
                    "start": 1195,
                    "end": 1396
                },
                {
                    "start": 1396,
                    "end": 1502
                },
                {
                    "start": 1502,
                    "end": 1558
                },
                {
                    "start": 1560,
                    "end": 1795
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 95,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 108,
                    "end": 127,
                    "matchedPaperCorpusId": "261064713"
                },
                {
                    "start": 216,
                    "end": 235,
                    "matchedPaperCorpusId": "261064713"
                },
                {
                    "start": 786,
                    "end": 806,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1025,
                    "end": 1047,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1333,
                    "end": 1355,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1378,
                    "end": 1395,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58251953125
        },
        {
            "corpus_id": "271571401",
            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
            "text": "The development of RAG technology can be summarized in three stages. Initially, retrieval-augmented techniques were introduced to improve the performance of pre-trained language models on knowledge-intensive tasks [19], [20]. In specific implementations, Retro [21] optimized pre-trained autoregressive models through retrieval augmentation, while Atlas [22] utilized a retrieval-augmented few-shot fine-tuning method, enabling language models to adapt to diverse tasks. IRCOT [23] further enriched the reasoning process during the inference phase by combining chain-of-thought and multistep retrieval processes. Entering the second stage, as the language processing capabilities of LLMs significantly improved, retrieval-augmented techniques began to serve as a means of supplementing additional knowledge and providing references, aiming to reduce the hallucination. For instance, RRR [24] improved the rewriting phase, and LLMlingua [25] removed redundant tokens in retrieved document chunks. With the continuous progress of RAG technology, research has become more refined and focused, while also achieving innovative integration with other technologies such as graph neural networks [26] and fine-tuning techniques [27]. The overall pipeline has also become more flexible, such as using LLMs to proactively determine the timing of retrieval and generation [14], [28]. \n\nThe development of RAG technology has been accelerated by LLM technology and practical application needs. Researchers are examining and organizing the RAG framework and development pathways from different perspectives. Building upon the enhanced stages of RAG, Gao et al., [2] subdivided RAG into enhancement during pre-training, inference, and fine-tuning stages. Based on the main processes of RAG, relevant works on RAG were organized from the perspectives of retrieval, generation, and augmentation methods. Huang et al., [29] categorize RAG methods into four main classes: pre-retrieval, retrieval, post-retrieval, generation, and provide a detailed discussion of the methods and techniques within each class. Hu et al., [30] discuss Retrieval-Augmented Language Models (RALMs) form three key components, including retrievers, language models, augmentations, and how their interactions lead to different model structures and applications.",
            "score": 0.37955109953353117,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 8318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2318
                }
            ],
            "ref_mentions": [
                {
                    "start": 220,
                    "end": 224,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 261,
                    "end": 265,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 936,
                    "end": 940,
                    "matchedPaperCorpusId": "252186384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7841796875
        },
        {
            "corpus_id": "276317808",
            "title": "Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation",
            "text": "In recent years, significant breakthroughs have been achieved in language models, driven primarily by the advent of transformers (Vaswani et al., 2017), enhanced computational capabilities, and the availability of large-scale training data (Naveed et al., 2024). The emergence of foundational Large Language Models (LLMs) (Ouyang et al., 2022; * These authors contributed equally. Grattafiori et al., 2024;Touvron et al., 2023;Qwen et al., 2025;Anil et al., 2023) has revolutionized natural language processing (NLP), demonstrating unprecedented capabilities in a wide range of tasks including instruction following (Qin et al., 2024), sophisticated reasoning (Wei et al., 2024c), In-context Learning (Brown et al., 2020), and multilingual machine translation (Zhu et al., 2024a). These advancements have elevated the performance of various NLP tasks, opening new avenues for research and application. Despite their remarkable achievements, LLMs face significant challenges, including hallucination, outdated internal knowledge, and a lack of verifiable reasoning (Huang et al., 2024;Xu et al., 2024b). Their reliance on parametric memory restricts their ability to access up-to-date knowledge, making them less effective for knowledge-intensive tasks compared to taskspecific architectures. Moreover, providing provenance for their decisions and updating their world knowledge remain critical open problems (Lewis et al., 2020). \n\nRetrieval-Augmented Generation (RAG) RAG (Lewis et al., 2020) has emerged as a promising solution to these limitations by enabling LLMs to retrieve and incorporate external knowledge, improving factual accuracy and reducing hallucinations (Shuster et al., 2021;Ding et al., 2024a). By dynamically accessing vast external knowledge repositories, RAG systems enhance knowledge-intensive tasks while ensuring responses remain grounded in verifiable sources (Gao et al., 2023). \n\nIn practice, RAG systems operate through a retriever-generator pipeline.",
            "score": 0.37955109953353117,
            "section_title": "Introduction & Background",
            "char_start_offset": 28,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1429
                },
                {
                    "start": 1432,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1905
                },
                {
                    "start": 1908,
                    "end": 1980
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 151,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 322,
                    "end": 343,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 616,
                    "end": 634,
                    "matchedPaperCorpusId": "266844311"
                },
                {
                    "start": 660,
                    "end": 679,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1064,
                    "end": 1084,
                    "matchedPaperCorpusId": "265067168"
                },
                {
                    "start": 1408,
                    "end": 1428,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1473,
                    "end": 1493,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1671,
                    "end": 1693,
                    "matchedPaperCorpusId": "233240939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5400390625
        },
        {
            "corpus_id": "271534572",
            "title": "MindSearch: Mimicking Human Minds Elicits Deep AI Searcher",
            "text": "RAG demonstrates significant advantages in addressing knowledge-intensive problems, especially in open-domain scenarios with the integration of search engines (Chen et al., 2017;Li et al., 2017). RAG allows LLMs to integrate with the retriever, providing timely information and offering effective solutions. Moreover, RAG is also applied in various tasks such as reducing hallucinations (Shuster et al., 2021;Gu et al., 2024), code generation (Zhou et al., 2022), and question answering (Lewis et al., 2020). Recently, some work (Karpukhin et al., 2020;Xiong et al., 2020;Qu et al., 2020) focuses on enhancing the retrieval component of RAG systems, while others (Izacard & Grave, 2020;Borgeaud et al., 2022;Yu et al., 2021;Lei et al., 2017) enhances the language model's ability as a reader to optimize the framework. \n\nWith the advancement of LLM capabilities, some researchers have begun to reoptimize frameworks and redesign methodologies for model training. SAIL (Luo et al., 2023) trains LLM to be more focused on credible and informative search results. Self-RAG (Asai et al., 2023) enables LMMs to independently fetch, introspect, and augment their text generation capabilities. RQ-RAG (Chan et al., 2024) enhances query formulation by learning to refine queries through an iterative process. \n\nOur work integrates web search capabilities into LLMs, enhancing response quality by retrieving valuable information from the Internet.",
            "score": 0.37955109953353117,
            "section_title": "RAG with LLM",
            "char_start_offset": 16627,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 818
                },
                {
                    "start": 821,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1300
                },
                {
                    "start": 1303,
                    "end": 1438
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 194,
                    "matchedPaperCorpusId": "4282343"
                },
                {
                    "start": 487,
                    "end": 507,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 686,
                    "end": 708,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 724,
                    "end": 741,
                    "matchedPaperCorpusId": "4358190"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33935546875
        },
        {
            "corpus_id": "270710808",
            "title": "Entropy-Based Decoding for Retrieval-Augmented Large Language Models",
            "text": "Retrieval-Augmented Language Models. Enhancing large language models (LLMs) with information retrieved from external knowledge bases has proven effective for various knowledgeintensive tasks. Initially, mainstream research in retrieval-augmented language models (RALM) focused on leveraging retrieved knowledge during the pre-training phase of LLMs (Guu et al., 2020;Izacard et al., 2023;Borgeaud et al., 2022). To mitigate the computational costs, some studies have concentrated on lightweight fine-tuning methods to integrate retrieval capabilities into LLMs (Lewis et al., 2020;Lin et al., 2023;Zhang et al., 2024). Notably, models like FiD (Izacard and Grave, 2020) and CEPE (Yen et al., 2024) perform parallel encoding of multiple retrieved documents using a fine-tuned encoder, enabling decoder-only LLMs to more effectively capture and utilize external knowledge. \n\nAnother approach leverages the in-context learning abilities of LLMs to incorporate external knowledge in a training-free manner (Ram et al., 2023;Shi et al., 2023c). The work most closely related to ours is REPLUG (Shi et al., 2023c), which utilizes the RAG-token model (Lewis et al., 2020) to perform parallel retrieval augmentation based on retrieval scores. However, we empirically demonstrate that focusing on the inherent uncertainty within the LLM's output distribution, rather than relying solely on pre-existing retrieval scores, can significantly improve the factual accuracy of content generated from retrieved documents. \n\nContrastive Decoding. The idea of contrastive decoding (CD) has been previously applied in controllable text generation to produce non-toxic by DExperts (Liu et al., 2021)",
            "score": 0.37955109953353117,
            "section_title": "Related Works",
            "char_start_offset": 12516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 870
                },
                {
                    "start": 873,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1505
                },
                {
                    "start": 1508,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1679
                }
            ],
            "ref_mentions": [
                {
                    "start": 349,
                    "end": 367,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 367,
                    "end": 388,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 388,
                    "end": 410,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 561,
                    "end": 581,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1002,
                    "end": 1020,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 1144,
                    "end": 1164,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1661,
                    "end": 1679,
                    "matchedPaperCorpusId": "235313967"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66357421875
        },
        {
            "corpus_id": "268856642",
            "title": "FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion",
            "text": "Recently, a series of retrieval-augmented language models [14,24,47] have been proposed to augment language models with external knowledge [9,17,53].Retrieval-augmented techniques can generally be divided into two types.The first type is at the input layer [14,20,42], where the retrieved information is text chunks.The second type is at the output layer [7,24,47], where the retrieved information is tokens.By combining the retrieved tokens with the tokens generated by the original model, the accuracy of the retrievalaugmented model's generation for each token can be improved.\n\nThe first type of method can provide the model with more external knowledge, making it adept at handling tasks in the NLP field such as knowledge-based question answering [27,45,49].The second type of method can refer to the retrieved information to correct the generated tokens, making it more suited for handling strictly structured generative tasks, such as code completion [7,10,11].In this work, we mainly focus on the second category.\n\nTo better understand the mechanism, we take kNN-LM [24] as an example for a detailed explanation.Given a context sequence   = ( 1 , . . .,   \u22121 ), the language models (LMs) estimate   (  |  ), i.e., the probability distribution over the next token   .kNN-LM is designed to augment a pre-trained language model with a set of nearest neighbours retrieved from an external text collection, which can be the training set . Different from fine-tuning, retrieval augmentation does not need any retraining.In particular, RaLM includes two tasks, i.e., building a datastore and retrieval-augmented inference.Datastore: The datastore is a retrieval set, which can be built with a forward pass by LM on the prepared text collection to store the context-target pairs as the subject of a query.We denote a function  (\u2022) to map a context  to a fixed-length vector representation computed by a pre-trained LM.Given an example (  ,   ) \u2208 , we can pass   to a LM to get its vector representation, i.e.,   =  (  ).",
            "score": 0.37955109953353117,
            "section_title": "BACKGROUND AND PROBLEM 2.1 Retrieval-Augmented Language Models",
            "char_start_offset": 7770,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 149,
                    "end": 220
                },
                {
                    "start": 220,
                    "end": 316
                },
                {
                    "start": 316,
                    "end": 408
                },
                {
                    "start": 408,
                    "end": 580
                },
                {
                    "start": 582,
                    "end": 764
                },
                {
                    "start": 764,
                    "end": 969
                },
                {
                    "start": 969,
                    "end": 1022
                },
                {
                    "start": 1024,
                    "end": 1121
                },
                {
                    "start": 1121,
                    "end": 1161
                },
                {
                    "start": 1161,
                    "end": 1275
                },
                {
                    "start": 1275,
                    "end": 1523
                },
                {
                    "start": 1523,
                    "end": 1624
                },
                {
                    "start": 1624,
                    "end": 1806
                },
                {
                    "start": 1806,
                    "end": 1919
                },
                {
                    "start": 1919,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 62,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 62,
                    "end": 65,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 139,
                    "end": 142,
                    "matchedPaperCorpusId": "249191271"
                },
                {
                    "start": 142,
                    "end": 145,
                    "matchedPaperCorpusId": "252568176"
                },
                {
                    "start": 257,
                    "end": 261,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 355,
                    "end": 358,
                    "matchedPaperCorpusId": "246431219"
                },
                {
                    "start": 358,
                    "end": 361,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 753,
                    "end": 757,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 959,
                    "end": 962,
                    "matchedPaperCorpusId": "246431219"
                },
                {
                    "start": 1075,
                    "end": 1079,
                    "matchedPaperCorpusId": "207870430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81396484375
        },
        {
            "corpus_id": "227231605",
            "title": "Deep Learning Brasil - NLP at SemEval-2020 Task 9: Sentiment Analysis of Code-Mixed Tweets Using Ensemble of Language Models",
            "text": "Nowadays, there are many advances in NLP, but the majority of researches is based on the English language, and those advances can be slow to transfer beyond English. The MultiFiT (Eisenschlos et al., 2019) method is based on Universal Language Model Fine-tuning (ULMFiT) (Howard and Ruder, 2018) and the goal of this model is to make it more efficient for modeling languages others than English. \n\nThere are two changes compared to the old model: it utilizes tokenization based on sub-words rather than words, and it also uses a QRNN (Bradbury et al., 2016) rather than an LSTM. The model architecture can be seen in Figure 2. \n\nThe architecture of the model consists of a subword embedding layer, four QRNN layers, an aggregation layer, and two linear layers. In special this architecture, subword tokenization has two very important properties: \n\n\u2022 Subwords more easily represent inflections and this includes common prefixes and suffixes. For morphologically rich languages this is well-suited. \n\n\u2022 It is a common problem out-of-vocabulary tokens and Subword tokenization is a good solution to prevent this problem. The implementation of BERT there has two steps: pre-training and fine-tuning. In the pre-training step, the model is trained on unlabeled data over different pre-training tasks using a corpus in a specific language or in multiples corpus with different languages. For the fine-tuning step, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the specific tasks. \n\nDataset repositories like NLP-progress4 track different model results and progress in many Natural Language Processing (NLP) benchmarks, and also the current state for the most common NLP tasks. When doing a comparison between results available for reference in such repositories, BERT was able to achieve state-of-the-art in many NLP-related tasks, which gives an excellent reason to use BERT in our architecture, even while many reasons of BERT state-of-art performance are not fully understood (Kovaleva et al., 2019) (Clark et al., 2019). 1236",
            "score": 0.37951383393722954,
            "section_title": "MultiFiT",
            "char_start_offset": 4599,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 395
                },
                {
                    "start": 398,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 626
                },
                {
                    "start": 629,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 846
                },
                {
                    "start": 849,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1562
                },
                {
                    "start": 1565,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 2107
                },
                {
                    "start": 2108,
                    "end": 2112
                }
            ],
            "ref_mentions": [
                {
                    "start": 2062,
                    "end": 2085,
                    "matchedPaperCorpusId": "201645145"
                },
                {
                    "start": 2086,
                    "end": 2106,
                    "matchedPaperCorpusId": "184486746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.263671875
        },
        {
            "corpus_id": "232068875",
            "title": "PharmKE: Knowledge Extraction Platform for Pharmaceutical Texts using Transfer Learning",
            "text": "In [23], authors utilized word embedding techniques to capture the semantics of the words in the sentence and built a generic model based on long short-term memory network-conditional random field (LSTM-CRF), which outperforms state-of-the-art entity-specific NER tools. \n\nStarting from 2018, Sequence-to-Sequence (Seq2Seq) architectures which work with text became a popular topic in NLP, due to their powerful ability to transform a given sequence of elements into another sequence -a concept which fits well in machine translation. Transformers are models which implement Seq2Seq architecture by using an encoder-decoder structure. \n\nOne of the latest milestones in this development is the release of Google's BERT [8] which is based on a transformer architecture and integrates an attention mechanism [24]. It produces outstanding results on many NLP tasks, including NER, due to its ability to learn contextual relations between words (or sub-words) in a text, making it applicable in the biomedical and pharmaceutical domains. Hakala and Pyysalo [25] present an approach based on Conditional Random Fields (CRF) and multilingual BERT for biomedical named entity recognition on content in Spanish. In [26], authors explore feature-based and fine-tuning training strategies for the BERT model for NER in Portuguese. Lamurias and Couto [27] present an approach based on a transformer architecture for question answering in the biomedical domain. \n\nBioBERT [28] is a domain-specific language representation, pre-trained on large scale biomedical corpora. It is pre-trained on large general domain corpora (English Books, Wikipedia, etc.) and on biomedical domain corpora (PubMed abstracts, PMC full-text articles), using the BERT architecture. This language model provides improved results in various biomedical text mining tasks, including NER. \n\nTransfer learning, as a machine learning method, provides the concept of re-usability in neural networks, where one model developed for a task can be reused as the starting point of the training process of another problem that has a significantly smaller training set.",
            "score": 0.3791365294486736,
            "section_title": "Related Work",
            "char_start_offset": 8155,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 273,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1448
                },
                {
                    "start": 1451,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1847
                },
                {
                    "start": 1850,
                    "end": 2118
                }
            ],
            "ref_mentions": [
                {
                    "start": 805,
                    "end": 809,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1052,
                    "end": 1056,
                    "matchedPaperCorpusId": "207905114"
                },
                {
                    "start": 1339,
                    "end": 1343,
                    "matchedPaperCorpusId": "199379633"
                },
                {
                    "start": 1459,
                    "end": 1463,
                    "matchedPaperCorpusId": "59291975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4375
        },
        {
            "corpus_id": "274165965",
            "title": "RAG-Thief: Scalable Extraction of Private Data from Retrieval-Augmented Generation Applications with Agent-based Attacks",
            "text": "Despite the impressive performance of large language models (LLMs) in tasks like knowledge-based question answering and content generation, they still face limitations in specific areas, such as generating hallucinations [1], [2] and lacking access to the most current data. The emergence of Retrieval-Augmented Generation (RAG) [3], [4], [5], [6], [7], [8], [9] expands the capabilities of LLMs and becomes a popular method to enhanc their performance. RAG integrates information retrieval with text generation by using a retrieval module to extract the most relevant information chunks from external knowledge bases. These chunks are then used as contextual prompts for the language model, improving its ability to produce more accurate, relevant, and Figure 1: Attack scenario of RAG-Thief and demonstration on a real-world healthcare-related RAG application from OpenAI GPTs (For ethical reasons, the GPT is created by the authors and only contains public data). coherent responses. Currently, RAG technology is widely applied across various vertical industries, demonstrating significant value in fields like healthcare (e.g., SMART Health GPT [10], [11]), finance [12], law (AutoLaw [13], [14]) , and scientific research (MyCrunchGPT [15], [16], [17]) . For instance, in healthcare, RAG can be combined with proprietary case knowledge bases to build intelligent questionanswering systems. These systems not only provide more precise medical analyses but also offer personalized healthcare guidance. By supplementing knowledge with the latest medical literature and case data, such systems can assist doctors and patients in making more informed decisions. Moreover, OpenAI allows users to build and publish GPTs, a type of AI application, with private data. Currently, there are over 3 million custom GPTs on the ChatGPT platform. \n\nIntuitively, RAG systems should be relatively secure in terms of privacy, as the private knowledge base is merely an independent external file within the RAG system, and users can only interact with the LLM without direct access to the knowledge base content. However, some studies indicate that RAG systems pose data privacy risks related to the leakage of private knowledge bases.",
            "score": 0.3779046750067034,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1836
                },
                {
                    "start": 1839,
                    "end": 2098
                },
                {
                    "start": 2099,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 224,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 226,
                    "end": 229,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 329,
                    "end": 332,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 334,
                    "end": 337,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 339,
                    "end": 342,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 344,
                    "end": 347,
                    "matchedPaperCorpusId": "261822526"
                },
                {
                    "start": 349,
                    "end": 352,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 354,
                    "end": 357,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1149,
                    "end": 1153,
                    "matchedPaperCorpusId": "266336011"
                },
                {
                    "start": 1155,
                    "end": 1159,
                    "matchedPaperCorpusId": "260375927"
                },
                {
                    "start": 1170,
                    "end": 1174,
                    "matchedPaperCorpusId": "265128933"
                },
                {
                    "start": 1240,
                    "end": 1244,
                    "matchedPaperCorpusId": "264846949"
                },
                {
                    "start": 1252,
                    "end": 1256,
                    "matchedPaperCorpusId": "265609385"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55810546875
        },
        {
            "corpus_id": "247025748",
            "title": "Neural Program Repair : Systems, Challenges and Solutions",
            "text": "The purpose of Input Representing is to make the model understand the semantics of buggy programs with an encoder module, which is also called Encoding. To this aim, the input will be mapped into a special semantic vector space by the encoder module, which is composed of multi-layer neural networks. In this phase, to design a proper encoder architecture is the most important. Architectures of encoders that existing NPR approaches use can be categorized as Standard, Struture-aware. \n\nStandard encoders refer to those popular in the Natural Language Processing (NLP) field. Under this category, classic neural networks like Long Short-Term Memory (LSTM) architecture [20] and Transformer [49] are most commonly used [6,9,14,18,47]. Another popular choice is to reuse models that have already trained on a large corpus of codes, which is called pre-train [12]. Given the effectiveness of language models in the NLP domain, CURE [22] proposes to add a GPT [39] module pre-trained on software code to a Neural Machine Translation (NMT) architecture. TFix [3] and RewardRepair [57] leverage T5, a Transformer-based [49] model pre-trained on NLP tasks and fine-tune it for the task of generating code fixes. And the CodeBERT [16] model, a bimodal pre-trained language model for both natural and programming languages, is also used to adapt the APR task [32]. \n\nStructure-aware encoders are used to capture the AST-based features. For an original tree-structure AST, DLFix [26] encodes it with a Tree-LSTM [45] and Hoppity [13] adopts a Gated Graph Neural Network (GGNN) [1], treating the AST as a graph. For traverse results of ASTs, Tang [46] designs a Cross-Attention mechanism to make full use of token information and syntax information interactively. Recoder [62] uses a special encoder called Code Reader that combines traverse results and AST-based graph through three sub neural layers.",
            "score": 0.3774298137461053,
            "section_title": "Input Representing",
            "char_start_offset": 21161,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1356
                },
                {
                    "start": 1359,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1892
                }
            ],
            "ref_mentions": [
                {
                    "start": 670,
                    "end": 674,
                    "matchedPaperCorpusId": "7452865"
                },
                {
                    "start": 691,
                    "end": 695,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 719,
                    "end": 722,
                    "matchedPaperCorpusId": "221321491"
                },
                {
                    "start": 722,
                    "end": 724,
                    "matchedPaperCorpusId": "57573711"
                },
                {
                    "start": 724,
                    "end": 727,
                    "matchedPaperCorpusId": "221292854"
                },
                {
                    "start": 727,
                    "end": 730,
                    "matchedPaperCorpusId": "29157253"
                },
                {
                    "start": 730,
                    "end": 733,
                    "matchedPaperCorpusId": "56517510"
                },
                {
                    "start": 857,
                    "end": 861,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 930,
                    "end": 934,
                    "matchedPaperCorpusId": "232076119"
                },
                {
                    "start": 1055,
                    "end": 1058,
                    "matchedPaperCorpusId": "235672358"
                },
                {
                    "start": 1114,
                    "end": 1118,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1223,
                    "end": 1227,
                    "matchedPaperCorpusId": "211171605"
                },
                {
                    "start": 1351,
                    "end": 1355,
                    "matchedPaperCorpusId": "232307349"
                },
                {
                    "start": 1470,
                    "end": 1474,
                    "matchedPaperCorpusId": "222080996"
                },
                {
                    "start": 1503,
                    "end": 1507,
                    "matchedPaperCorpusId": "3033526"
                },
                {
                    "start": 1520,
                    "end": 1524,
                    "matchedPaperCorpusId": "213089769"
                },
                {
                    "start": 1568,
                    "end": 1571,
                    "matchedPaperCorpusId": "3495200"
                },
                {
                    "start": 1637,
                    "end": 1641,
                    "matchedPaperCorpusId": "236478113"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.278076171875
        },
        {
            "corpus_id": "268038008",
            "title": "Updated Primer on Generative Artificial Intelligence and Large Language Models in Medical Imaging for Medical Professionals",
            "text": "LLMs, such as ChatGPT and GPT-4 [16], are based on transformer architecture [42]. The transformer, the core component of LLMs, enables these models to understand and generate human-like text. Words are processed simultaneously in the transformer, rather than one after the other, which makes it a great tool for understanding the context of a language. Transformers use a mechanism known as \"attention\" to weigh the relevance of different words when generating responses or predictions. LLMs train on vast amounts of text data and scale the model architecture to a larger size. \n\nGPT is an LLM that uses a transformer decoder, a specific part of the transformer architecture. As it comprises a decoder-only architecture, it resembles an expert chef who does not have to learn a specific recipe (encoder) since they have already learned hundreds of recipes. Consequently, GPT-based LLMs [6][7][8]16], such as ChatGPT and GPT-4, can effectively generate human-like text by combining the power of the transformer decoder with the broad knowledge learned from extensive training data. GPT can generate texts based on a description or a single or few examples. Figure 5  Bidirectional Encoder Representations from Transformers (BERTs) [10] are pre-trained models that use the bidirectional encoder of the transformer architecture, which facilitates the consideration of the left and right contexts during training. BERT is trained on a process known as the masked language model, which masks random words in a sentence. The model predicts the masked words based on the context provided by the surrounding sentences and words. This process has enabled BERT to capture deep contextual representations, resulting in superior performance in various NLP tasks. BERT is a powerful tool for NLP tasks, such as language understanding, sentiment analysis, and question-answering systems, owing to its ability to understand context. GPT and BERT are built on the transformer architecture and utilize attention mechanisms, which significantly improve NLP tasks. These models have facilitated the completion of more accurate and context-aware language-modeling tasks, making them valuable assets in the domain of NLP.",
            "score": 0.37656306777991244,
            "section_title": "Large Language Model (LLM)",
            "char_start_offset": 13317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 577
                },
                {
                    "start": 580,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2200
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.322265625
        },
        {
            "corpus_id": "249068780",
            "title": "Plant phenotype relationship corpus for biomedical relationships between plants and phenotypes",
            "text": "As an application of the PPR corpus, we used NER and performed relation extraction (RE) tasks to the corpus. NER is one of the most widely known text mining-related tasks, which involves recognition of numerous domain-specific entities in the biomedical text, and RE is another commonly studied NLP task to classify relationships between the recognized named entities in text. For the NLP tasks with the best performance, most researchers previously used various combinations of hidden layers such as deep neural networks and conditional random fields architectures 10,11 . Recently established deep learning methods, especially contextualized language models such as BERT 12 , have resulted in significant improvements in many NLP tasks, including NER and RE. Therefore, we considered fine-tuning BERT-based models such as BERT 12 , BioBERT 45 , BlueBERT 46 , SciBERT 47 , and PubMedBERT 48 . \n\n\u2022 BERT 12 : BERT is a contextual language representation model using pre-training deep bidirectional representations from unlabeled text. Instead of conducting traditional left-to-right language modeling, BERT is trained on two tasks: a masked language model (MLM) by predicting randomly masked tokens and a next sentence prediction (NSP) by predicting whether two sentences follow each other. BERT demonstrates a simple architecture based on the transformer and shows powerful performance in various NLP tasks, while illustrating the potential of the fine-tuning approach. \u2022 BioBERT 45 : BioBERT is a domain-specific language representation model designed for biomedical text and is initialized with the checkpoint of BERT, followed by training of the BERT model using PubMed abstracts and PubMed Central full-text articles. BioBERT achieves SOTA performance in various biomedical NLP tasks with minimal task-specific fine-tuning, while requiring only minimal architectural modifications. \u2022 BlueBERT 46 : Similar to BioBERT, BlueBERT is recognized as another variant of BERT, which is initialized with BERT and is further pre-trained using information available in PubMed abstracts and clinical notes derived from MIMIC-III. The standard approach of utilization of BERT-based models, such as BioBERT, is initialized with application of the BERT model, followed by continuous conduction of the pre-training process with MLM and NSP using their respective corpora.",
            "score": 0.37640772216826945,
            "section_title": "The evaluation techniques.",
            "char_start_offset": 29204,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 893
                },
                {
                    "start": 896,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2121
                },
                {
                    "start": 2122,
                    "end": 2359
                }
            ],
            "ref_mentions": [
                {
                    "start": 842,
                    "end": 844,
                    "matchedPaperCorpusId": "59291975"
                },
                {
                    "start": 856,
                    "end": 858,
                    "matchedPaperCorpusId": "189762009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.322509765625
        },
        {
            "corpus_id": "212747830",
            "title": "Pre-trained models for natural language processing: A survey",
            "text": "To clarify the relations of existing PTMs for NLP, we build the taxonomy of PTMs, which categorizes existing PTMs from four different perspectives.\n\n(1) Representation type. According to the representation used for downstream tasks, we can divide PTMs into noncontextual and contextual models.\n\n(2) Architectures. The backbone network used by PTMs, including LSTM, Transformer encoder, Transformer decoder, and the full Transformer architecture. \"Transformer\" means the standard encoder-decoder architecture. \"Transformer encoder\" and \"Transformer decoder\" mean the encoder and decoder part of the standard Transformer architecture, respectively. Their difference is that the decoder part uses masked self-attention with a triangular matrix to prevent tokens from attending their future (right) positions.\n\n(3) Pre-training task types. The type of pre-training tasks used by PTMs. We have discussed them in Sect. 3.1.\n\n(4) Extensions. PTMs designed for various scenarios, including knowledge-enriched PTMs, multilingual or language-specific PTMs, multi-model PTMs, domainspecific PTMs and compressed PTMs. We will particularly introduce these extensions in Sect. 4. Figure 3 [5, 11-16, 34, 40-43, 45, 47-49, 55-94] shows the taxonomy as well as some corresponding representative PTMs. Besides, Table 2 distinguishes some representative PTMs in more detail.",
            "score": 0.37625845149430015,
            "section_title": "Taxonomy of PTMs",
            "char_start_offset": 31187,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53076171875
        },
        {
            "corpus_id": "265466391",
            "title": "Novel Preprocessing Technique for Data Embedding in Engineering Code Generation Using Large Language Model",
            "text": "Large Language Models (LLMs) have exhibited remarkable success across diverse applications, showcasing their proficiency in tasks such as domain-specific question answering, code generation, and more [1]. Their adaptability and versatility make them invaluable tools for addressing a wide array of challenges and scenarios. \n\nHowever, applying LLMs for code generation in the RedHawk-SC (RH-SC) domain presents unique challenges. First, generating RH-SC code requires knowledge of circuit design, chip design, and electrical engineering concepts, as well as familiarity with the RH-SC architecture. The documentation for RH-SC is not well-detailed, making it difficult to learn the architecture from the manual, as it assumes readers have a professional background in the field. \n\nThe user manual and API documentation are designed for experts in chip design and are not suitable for laypersons, such as LLMs, which may be considered novices in the chip design domain. Furthermore, there is a scarcity of available scripts, making the generation of domain-specific code challenging. \n\nEvaluating the generated code is also difficult due to the nature of the research topic, which focuses on generating code in domains where LLMs have not been trained. This implies that any publicly available information would have already been learned by LLMs, and thus the datasets must be generated by the researchers themselves to avoid any potential biases. To create such datasets, we must utilize the RH-SC tools and system concepts to generate corresponding test code. \n\nRetrieval-Augmented Generation (RAG) [2,3] is a commonly used technique that has been proven to reduce hallucinations and enhance the accuracy and reliability of LLMs by fetching facts from external sources. While RAG has achieved notable advancements in specific domains, several challenges still exist. \n\nRAG's effectiveness lies in the quality of information retrieved, and any shortcomings in this process can directly impact LLM performance and output. One critical concern is the potential for inappropriate data preprocessing, leading to chunks that may not accurately represent their semantic space positions. This discrepancy poses a challenge in effectively locating corresponding content. Even when relevant content is identified, another issue arises: essential information may only constitute a fraction of the retrieved data.",
            "score": 0.3762286321702622,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 323
                },
                {
                    "start": 326,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1082
                },
                {
                    "start": 1085,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1560
                },
                {
                    "start": 1563,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1867
                },
                {
                    "start": 1870,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2180
                },
                {
                    "start": 2181,
                    "end": 2262
                },
                {
                    "start": 2263,
                    "end": 2402
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.465576171875
        },
        {
            "corpus_id": "267770449",
            "title": "PIRB: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods",
            "text": "Text information retrieval is a process of retrieving relevant documents from a large collection of text data in response to a user's query. It is a fundamental task in natural language processing and plays a crucial role in various applications, including search engines, question-answering systems, and recommendation engines. Despite the fact that research in the field of information retrieval has a long history, we have recently observed increased interest in this topic. This is primarily related to the emergence of large language models (LLMs), whether offered as services like GPT-4 (OpenAI, 2023) or free and publicly available ones like LLaMA (Touvron et al., 2023a,b) or Falcon (Penedo et al., 2023). With the popularization of these solutions, more attention is also being given to retrieval augmented generation systems (Lewis et al., 2020;Cai et al., 2022), in which a large language model, along with a user's query, receives additional context from an external knowledge base. This context is created based on documents most relevant to the query, extracted using a retrieval algorithm. Correctly selected documents reduce the hallucinations of the language model (Shuster et al., 2021) and allow the injection of additional knowledge that the model may not possess. The quality of the response of such a system is therefore highly dependent on the performance of its retrieval component.",
            "score": 0.375857175152596,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1406
                }
            ],
            "ref_mentions": [
                {
                    "start": 835,
                    "end": 855,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1182,
                    "end": 1204,
                    "matchedPaperCorpusId": "233240939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5634765625
        },
        {
            "corpus_id": "270521566",
            "title": "HIRO: Hierarchical Information Retrieval Optimization",
            "text": "The advent of Large Language Models (LLMs) has brought about significant transformation in artificial intelligence and natural language processing, enabling advancements in tasks such as text generation, translation, and question-answering [3]. As these models increase in size and complexity, they have evolved into highly effective standalone knowledge repositories, embedding vast amounts of factual information within their parameters [12,19]. However, LLMs are inherently constrained by the static nature of their training datasets, which limits their adaptability to continuously evolving real-world information, where the breadth and depth of knowledge required far exceed any static training corpus [10]. This limitation underscores a critical gap in LLM design, where fine-tuning falls short, particularly when updates and domain-specific information are necessary [8]. \n\nIn response to these challenges, the field has witnessed the emergence of Retrieval-Augmented Generation (RAG) models, evolving into what are now known as Retrieval-Augmented Language Models (RALMs). These new paradigms enhance LLMs by integrating dynamic external knowledge through sophisticated retrieval mechanisms, effectively addressing the inherent limitations of static knowledge bases [8]. Inspired by the success of open-domain question answering systems, RALMs utilize indexed text databases to enhance model responses by presenting retrieved information alongside input queries [1,13]. This integration not only boosts the models' ability to provide current, domain-specific knowledge but also enhances interpretability and source traceability, offering a stark contrast to the opaque parametric knowledge of traditional LLMs. \n\nAmidst this evolving landscape, novel approaches to information retrieval, such as the use of hierarchical data structures for organizing documents, represent significant advancements. Techniques like RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval, which clusters and summarizes texts into a tree structure, demonstrate the potential to overcome traditional limitations by providing both granular and high-level insights [4,5,15,20]. However, the adoption of hierarchical data structures poses challenges, particularly the overwhelming amount of context they can return, which may not always align with the query requirements [15]. For queries demanding extensive information, this can result in insufficient data being returned, whereas for simpler inquiries, it might overwhelm LLMs with excessive context.",
            "score": 0.375857175152596,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 878
                },
                {
                    "start": 881,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1718
                },
                {
                    "start": 1721,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2177
                },
                {
                    "start": 2178,
                    "end": 2375
                },
                {
                    "start": 2376,
                    "end": 2552
                }
            ],
            "ref_mentions": [
                {
                    "start": 443,
                    "end": 446,
                    "matchedPaperCorpusId": "209515274"
                },
                {
                    "start": 874,
                    "end": 877,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1274,
                    "end": 1277,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1473,
                    "end": 1476,
                    "matchedPaperCorpusId": "256459451"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83447265625
        },
        {
            "corpus_id": "273962778",
            "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
            "text": "In this section, we introduce a novel retrieval-augmented language model architecture, Invar-RAG, which addresses the previously mentioned issues by using LLM-aligned retrieval combined with a specially designed invariance loss. We first present an overview of our proposed architecture, followed by a detailed explanation of its key components, and finally, we introduce how we construct the prompts.",
            "score": 0.375857175152596,
            "section_title": "Methodology",
            "char_start_offset": 5219,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 401
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4638671875
        },
        {
            "corpus_id": "264146725",
            "title": "RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling",
            "text": "Language models (LMs) have achieved state-ofthe-art performance on many NLP tasks (Zhu et al., 2021;Pang et al., 2021), which reveals that they store a large amount of world knowledge as implicit parameters. While this development is exciting, LMs still suffer from some problems (Li et al., 2022): 1) performance and model parameter size follow a power law relationship ( et al., 2020), which results in model parameters having to grow exponentially in order to gain more world knowledge; 2) difficulty in adjusting for timesensitive knowledge (Lewis et al., 2020); 3) may produce \"fact hallucination\" problem (Guu et al., 2020;Marcus, 2020). \n\nRecently, the advent of retrieval-augmented text generation has emerged as a novel paradigm aimed at addressing these pertinent issues (Borgeaud et al., 2022;Li et al., 2022;Shi et al., 2023). Compared to generative-only models, this paradigm not only explicitly exploits similar texts to generate more fluent sentences but also leverages expertise to generate difficult responses. Nonetheless, we contend that there are two primary challenges associated with current retrieval-augmented language models. Firstly, not only current semantic information, but also future semantic information need to be considered during retrieval. Previous studies (Khandelwal et al., 2020;Guu et al., 2020;Lewis et al., 2020) either directly use the entire text as key and value parts at the same time, and then use cosine similarity (Xu et al., 2023), TF-IDF and other indicators to search, which leads to the value part is only similar to the source text (query), and does not necessarily serve the best for generator. Another way is to divide a piece of text into two parts, where the first part and the second part are regarded as current information and future information, such as RETRO (Borgeaud et al., 2022). However, RETRO adds future information to value part, but ignores the future information in query and key, which leads to the fact that candidate documents with high similarity do not necessarily contain future information that can help the generator.",
            "score": 0.375857175152596,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 643
                },
                {
                    "start": 646,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 2098
                }
            ],
            "ref_mentions": [
                {
                    "start": 82,
                    "end": 100,
                    "matchedPaperCorpusId": "237502990"
                },
                {
                    "start": 100,
                    "end": 118,
                    "matchedPaperCorpusId": "231632824"
                },
                {
                    "start": 781,
                    "end": 804,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1293,
                    "end": 1318,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 1463,
                    "end": 1480,
                    "matchedPaperCorpusId": "258762869"
                },
                {
                    "start": 1822,
                    "end": 1845,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55859375
        },
        {
            "corpus_id": "259360590",
            "title": "Improving Retrieval-Augmented Large Language Models via Data Importance Learning",
            "text": "Large language models (LLMs) consisting of neural networks with billions of parameters and trained on vast quantities of unlabelled text are the basis of unprecented progress in natural language processing tasks [6,20,21,13]. With zero-shot or few-shot prompting, LLMs can be adopted for a wide range of diverse tasks, such as question answering [15] summarization [15,2] and data imputation [17]. \n\nDrawbacks of large language models. LLMs, however, have two widely acknowledged disadvantages [1,22]. Firstly, despite their impressive capabilities, LLMs actually perform badly on tail entities [1], which they have not seen at training time or cannot remember due to limitations of the network capacity. The second drawback is that with the ever-growing number of model parameters, training, and fine-tuning costs are exploding as well. As a rough estimate, it costs $80k -$1.6m to train a 1.5 billion parameter language model [25,22,29]. This makes it difficult to leverage LLMs for tasks that require regularly updated data or that regularly need to remove privacy-sensitive or copyright-protected data [3]. \n\nRetrieval-augmented models. To address such problems, retrieval-augmented (RAG) models have recently been proposed [12,14,8]. A typical retrieval-augmented model consists of two parts, a retriever f ret and a generator f gen . Given a retrieval corpus D ret = {d 1 , \u2022 \u2022 \u2022 , d M }, the retriever f ret retrieves K data points for an input x i as f ret (x i , D ret ) = {d \u03b11 , d \u03b12 , ..., d \u03b1 K }. Here, \u03b1 k denotes the rank of each data point in the retrieval corpus assigned by the retriever. The generator Figure 1: Data importance evaluation for retrieval-augmented models: The retriever f ret retrieves K data points from the retrieval corpus D ret and provides them to the answer generator f gen . Our data importance evaluator learns weights for the data sources in the retrieval corpus based on the performance on a validation set D val .",
            "score": 0.375857175152596,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 397
                },
                {
                    "start": 400,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1110
                },
                {
                    "start": 1113,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1959
                }
            ],
            "ref_mentions": [
                {
                    "start": 218,
                    "end": 221,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 935,
                    "end": 938,
                    "matchedPaperCorpusId": "249375466"
                },
                {
                    "start": 1232,
                    "end": 1235,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1235,
                    "end": 1237,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.763671875
        },
        {
            "corpus_id": "254247100",
            "title": "GNN-SL: Sequence Labeling Based on Nearest Examples via GNN",
            "text": "Retrieval Augmented Model Retrieval augmented models additionally use the input to retrieve information from the constructed datastore to the model performance. As described in Meng et al. (2021b), this process can be understood as \"an open-book exam is easier than a close-book exam\". The retrieval augmented model is more familiar in the question answering task, in which the model generates related answers from a constructed datastore (Karpukhin et al., 2020;Xiong et al., 2020;Yih, 2020). Recently other NLP tasks have introduced this approach and achieved a good performance, such as language modeling (LM) (Khandelwal et al., 2019;Meng et al., 2021b), dialog generation (Fan et al., 2020;Thulke et al., 2021), neural machine translation (NMT) (Khandelwal et al., 2020;Meng et al., 2021a;Wang et al., 2021).",
            "score": 0.375857175152596,
            "section_title": "Related Work",
            "char_start_offset": 3901,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 813
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56103515625
        },
        {
            "corpus_id": "268733288",
            "title": "Are Large Language Models Good at Utility Judgments?",
            "text": "LLMs for relevance judgments.With exhibited unprecedented proficiency in language understanding, large language models (LLMs) such as ChatGPT [29] and Llama 2 [42] have seen widespread applications across various tasks [10,12,27,43].IR is a representative work of LLMs applications, with many studies incorporating LLMs into relevance ranking [21,22,33,55].Research into LLMs in relevance ranking mainly contains the following three approaches: (i) pointwise [28,54], (ii) pairwise [14,33], and (iii) listwise [32,41,55].Zhuang et al. [54] employed LLMs in scoring fine-grained pointwise relevance labels.Qin et al. [33] employed a pairwise relevance comparison method to distinguish differences between candidate outputs.Previous works [32,41,55] analyzed the capabilities of LLMs in the relevance ranking task.Faggioli et al. [4] demonstrated LLMs' proficiency in relevance assessment in IR.However, relevance in IR and utility in answering specific questions are distinct concepts.This paper investigates whether LLMs excel in judging the utility of retrieved passages.[12,36,39,49] is mainly retrieval-augmented LLMs [5,6,34,36,50].Current researches on retrieval-augmented LLMs can be categorized into two main groups, i.e., independent architectures [25,46,49] and joint architectures [12,20,39,52].\n\nIn independent architectures, the retriever and LLMs operate independently, with the retriever's sole role being to provide relevant external knowledge to the LLMs [52].For example, Yu et al. [49] demonstrated that using retrieval-augmented methods can improve GPT-3 performance on open-domain question answering.However, these retrieval models are usually based on the probability ranking principle (PRP) [52], ranking passages based on their likelihood of being relevant to the question [50,52], which may not align with a retrieval-augmented framework.In the joint architecture, the LLMs actively engage in the training process of the retriever [12,17,39,52].",
            "score": 0.375857175152596,
            "section_title": "RELATED WORK",
            "char_start_offset": 36577,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 29,
                    "end": 233
                },
                {
                    "start": 233,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 521
                },
                {
                    "start": 521,
                    "end": 605
                },
                {
                    "start": 605,
                    "end": 722
                },
                {
                    "start": 722,
                    "end": 812
                },
                {
                    "start": 812,
                    "end": 893
                },
                {
                    "start": 893,
                    "end": 984
                },
                {
                    "start": 984,
                    "end": 1072
                },
                {
                    "start": 1072,
                    "end": 1136
                },
                {
                    "start": 1136,
                    "end": 1305
                },
                {
                    "start": 1307,
                    "end": 1476
                },
                {
                    "start": 1476,
                    "end": 1620
                },
                {
                    "start": 1620,
                    "end": 1862
                },
                {
                    "start": 1862,
                    "end": 1969
                }
            ],
            "ref_mentions": [
                {
                    "start": 229,
                    "end": 232,
                    "matchedPaperCorpusId": "256631106"
                },
                {
                    "start": 343,
                    "end": 347,
                    "matchedPaperCorpusId": "261049237"
                },
                {
                    "start": 347,
                    "end": 350,
                    "matchedPaperCorpusId": "258418289"
                },
                {
                    "start": 482,
                    "end": 486,
                    "matchedPaperCorpusId": "259075564"
                },
                {
                    "start": 828,
                    "end": 831,
                    "matchedPaperCorpusId": "258187001"
                },
                {
                    "start": 1121,
                    "end": 1124,
                    "matchedPaperCorpusId": "266348365"
                },
                {
                    "start": 1126,
                    "end": 1129,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 1132,
                    "end": 1135,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 1256,
                    "end": 1260,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 1295,
                    "end": 1298,
                    "matchedPaperCorpusId": "259144903"
                },
                {
                    "start": 1301,
                    "end": 1304,
                    "matchedPaperCorpusId": "264288748"
                },
                {
                    "start": 1471,
                    "end": 1475,
                    "matchedPaperCorpusId": "264288748"
                },
                {
                    "start": 1713,
                    "end": 1717,
                    "matchedPaperCorpusId": "264288748"
                },
                {
                    "start": 1796,
                    "end": 1800,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 1800,
                    "end": 1803,
                    "matchedPaperCorpusId": "264288748"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5908203125
        },
        {
            "corpus_id": "270738192",
            "title": "Poisoned LangChain: Jailbreak LLMs by LangChain",
            "text": "RAG was first proposed by Lewis et al. [12] in 2020, combining a pre-trained retriever with a pre-trained seq2seq model [15] and undergoing end-to-end fine-tuning to achieve more modular and interpretable ways of acquiring knowledge.This approach allows the model to access external knowledge sources when generating answers, thus providing more accurate and informative responses.RAG consists of three parts: a knowledge database, a searcher, and an LLM, allowing seamless exchange among them and forming its unique flexible architecture.In the first stage, the user's query retrieves relevant contextual information from external knowledge sources.The second phase involves placing the user query and the additional retrieved context into a prompt template, thereby providing an enhanced prompt to the LLM.\n\nIn the final step, the enhanced prompts are fed into a large language model (LLM) for generation, which effectively improves the speed of knowledge updates and alleviates the hallucination problem in large models.LangChain is by far the most popular tool for RAG, providing a framework with specialized components designed to facilitate the integration of retrieval systems with language models.By using LangChain, it is possible to access and utilize vast amounts of real-time information, thereby expanding its functionality and applicability across various fields.",
            "score": 0.375857175152596,
            "section_title": "Retrieval-Augmented Generation (RAG)",
            "char_start_offset": 5991,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 233,
                    "end": 381
                },
                {
                    "start": 381,
                    "end": 539
                },
                {
                    "start": 539,
                    "end": 650
                },
                {
                    "start": 650,
                    "end": 808
                },
                {
                    "start": 810,
                    "end": 1023
                },
                {
                    "start": 1023,
                    "end": 1205
                },
                {
                    "start": 1205,
                    "end": 1377
                }
            ],
            "ref_mentions": [
                {
                    "start": 39,
                    "end": 43,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 120,
                    "end": 124,
                    "matchedPaperCorpusId": "7672408"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46826171875
        },
        {
            "corpus_id": "266362497",
            "title": "Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP",
            "text": "Recently, many large language models have shown promising abilities to generate texts in fluent English and are able to perform a wide range of natural language processing tasks. (OpenAI, 2023;Chowdhery et al., 2022;Brown et al., 2020) However, large language models suffer from issues such as hallucination (Zhang et al., 2023b) and the difficulty in keeping parameters update-to-date which hinders the practical use of large language models. One of the methods that can alleviate these issues is Retrieval-Augmented Generation (RAG) (Lewis et al., 2021;Wang et al., 2023). By leveraging retrieved information from a trusted corpus, retrieval context can help reduce hallucination and let the model retrieve up-to-date information in the context (Lewis et al., 2020). \n\nRecent RAG approaches typically include two stages of retrieval pipeline. Sparse or dense retrieval as the first stage, and a reranker as a second stage. (Glass et al., 2022;Hofst\u00e4tter et al., 2022; Figure 1: A comparison of throughput of different methods on a single Nvidia A40. Our method (ETR) via Broadcasting Query Encoder (BQE) has a throughput improvement of 20x to 40x compared to vanilla passage rerankers, and at least 3x throughput improvement over normal title reranker. Best viewed in color. Bai et al., 2023) Dense retrieval leverages the distance between the embeddings of the query and the passage serves as the score (Lewis et al., 2020). This results in enhanced efficiency; however, it lacks the expressiveness to handle many-to-many relations. In comparison, a reranker scores each query and text pair individually, making it possible to express logical relation between each pair of query and text. Consequently, rerankers are leveraged for the expressibility in recent State-ofthe-Art RAG systems (Nogueira and Cho, 2020;Nogueira et al., 2019a;Glass et al., 2022).",
            "score": 0.375857175152596,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1858
                }
            ],
            "ref_mentions": [
                {
                    "start": 747,
                    "end": 767,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1406,
                    "end": 1426,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59521484375
        },
        {
            "corpus_id": "257771262",
            "title": "Typhoon: Towards an Effective Task-Specific Masking Strategy for Pre-trained Language Models",
            "text": "From translation to question answering to recommendations and sentence completion, the ability for models such as BERT (Devlin et al. 1) to perform natural language processing tasks effectively has emerged as one of the most impressive use-cases of modern-day deep learning. These models extend beyond traditional sentiment analysis, question-answering systems and machine translation tasks however, and have even been used by researchers in the life science to understand the network relationships between proteins, DNA and RNA sequences. We may be seeing a new \"Moore's Law\" for language models,2 where emerging large language models such as PaLM (Google Research 5), GPT (OpenAI 3), BLOOM (Google Research 5) -with an increasingly larger number of parameters -exhibit newfound \"emergent\" abilities that only arise after the models exceed a certain size (Google Research et al. 4). It suffices to say that there is a significant amount of academic and industry interest regarding these language models. Foundation models are model architectures that are trained on large amounts of data, which can later be adapted to a wide variety of downstream tasks. The field of Natural Language Processing (NLP) has found great benefit in the application of these models, and has resulted in exciting new use-cases with unprecedented performance. The typical recipe for the use of these models include two steps: self-supervised pre-training on large amounts of unlabeled data, and fine-tuning on a labeled task-specific dataset. \n\nCurrent state-of-the-art NLP models generally have taken one of two approaches in the goal towards machine language understanding: masked language modeling and causal language modeling. An example of a Causal Language Model (CLM) architecture is GPT, and an example of a Masked Language Model (MLM) architecture is BERT. Causal language models are typically used for natural language generation due to their auto-regressive nature, while masked language modeling is typically used for natural language understanding and representation. An example of how a MLM might process masked data is shown in the figure presented in Section 2. \n\nOur overall goal is to explore the nuances of how different types of masking affects the performance of Masked Language Models (MLMs).",
            "score": 0.37575391470108327,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2155
                },
                {
                    "start": 2158,
                    "end": 2292
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.439208984375
        },
        {
            "corpus_id": "268856971",
            "title": "Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts",
            "text": "Transformer-based (Vaswani et al., 2017) architectures with pre-training on large corpus have become popular in recent Natural Language Processing research (Brown et al., 2020;Workshop et al., 2022;Chowdhery et al., 2023).An increasing number of Natural Language Processing (NLP) tasks need to process long contexts such as Open-Domain Question Answering (ODQA) with Retrieval Augmented Generation (RAG) (Lewis et al., 2020;Izacard and Grave, 2020;Gu et al., 2018).However, the fine-tuning and inference stages in downstream tasks are still constrained by the input length, e.g., 2048 tokens for Bloomz (Muennighoff et al., 2022) and Llama-1 (Touvron et al., 2023).\n\nWith RAG, the input can easily surpass the maximum length the model can handle and it becomes Figure 1: A comparison of our method (lower) and retrieval augmented ODQA without vectorization (upper).In the upper part, limited retrieved contexts are processed by the task model to finish the task.The lower part illustrates our method in which an encoder is incorporated to encode overlong retrieved contexts.\n\nchallenging for the model to perform both finetuning and inference on overlong contexts.Moreover, in the in-context learning (ICL) (Dong et al., 2022;Kim et al., 2022) setting, the context will be much longer together with retrieved contexts.In such cases, the demand for the model to handle longer input text significantly increases.\n\nTo enable the model to cover longer context during both fine-tuning and inference stages, this paper proposes a method that leverages a 100 millionlevel encoder model in downstream ODQA tasks with a 1 billion-level language model as illustrated in the lower part of Fig. 1.With our method, the length of context that the model can cover increases from 2k (in text form) to a maximum of 10k (in dense form, which is condensed by the encoder).Experiments are designed under three settings to validate the effectiveness of our method.",
            "score": 0.3750691672158111,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 222,
                    "end": 465
                },
                {
                    "start": 465,
                    "end": 665
                },
                {
                    "start": 667,
                    "end": 865
                },
                {
                    "start": 865,
                    "end": 962
                },
                {
                    "start": 962,
                    "end": 1074
                },
                {
                    "start": 1076,
                    "end": 1164
                },
                {
                    "start": 1164,
                    "end": 1318
                },
                {
                    "start": 1318,
                    "end": 1410
                },
                {
                    "start": 1412,
                    "end": 1685
                },
                {
                    "start": 1685,
                    "end": 1853
                },
                {
                    "start": 1853,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 18,
                    "end": 40,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 156,
                    "end": 176,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 198,
                    "end": 221,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 404,
                    "end": 424,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 448,
                    "end": 464,
                    "matchedPaperCorpusId": "19206366"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4609375
        },
        {
            "corpus_id": "238226822",
            "title": "SlovakBERT: Slovak Masked Language Model",
            "text": "English is the most commonly used language in NLP and a de facto standard for experimental work. Most of the proposed LM variants are indeed trained and evaluated only on English. Other languages usually have at most only a few LMs trained, usually with a very safe choice of model architecture (e.g. BERT or RoBERTa). Languages with available native models are, to name only a few, French (Martin et al., 2020), Dutch (Delobelle et al., 2020), Greek (Koutsikakis et al., 2020), Arabic (Antoun et al., 2020), Czech (Sido et al., 2021) or Polish (Dadas et al., 2020). \n\nThere is no Slovak-specific large-scale LM available so far. There is a Slovak version of WikiB-ERT model (Pyysalo et al., 2021), but it is trained only on texts from Wikipedia, which is not a large enough corpus for proper language modeling at this scale. The limitations of this model will be shown in the results as well.",
            "score": 0.3746296529258056,
            "section_title": "Availability in Different Languages",
            "char_start_offset": 3638,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 893
                }
            ],
            "ref_mentions": [
                {
                    "start": 419,
                    "end": 443,
                    "matchedPaperCorpusId": "210714061"
                },
                {
                    "start": 675,
                    "end": 697,
                    "matchedPaperCorpusId": "219179429"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09039306640625
        },
        {
            "corpus_id": "271600473",
            "title": "Improving Retrieval-Augmented Generation in Medicine with Iterative Follow-up Questions",
            "text": "4][15][16] However, LLMs often generate plausible-sounding but inaccurate content, an issue commonly known as \"hallucination\" in the literature. 17 They also possess outdated knowledge obtained from a fixed set of training data. 18 Retrieval-augmented generation (RAG) provides a lightweight post-training solution to these issues by providing LLMs with relevant documents retrieved from up-to-date and trustworthy sources. 19,20 hile there have been several medical applications of RAG, such as Almanac, 21 Clinfo.ai, 22 nd MedRAG, 23 their RAG component is mainly beneficial to questions that have direct answers in a single document, such as those in the PubMedQA 24 and BioASQ 25 datasets. However, only marginal improvements are seen with RAG for questions that require multiple rounds of clinical reasoning like MedQA, 26 a dataset curated from medical license examinations. Fo  example, to recommend a treatment for a patient with certain symptoms, a system needs to first infer the potential diagnosis from the symptoms and then find a suitable treatment for the diagnosis. Nevertheless, only one round of retrieval is conducted in the conventional RAG architecture, prohibiting multiple rounds of information seeking that are required in complex clinical reasoning. \n\nIn this work, we propose i -MedRAG, a simple and effective framework for incorporating follow-up queries into RAG. Specifically, we prompt LLMs to iteratively generate follow-up queries to search for additional information from external medical corpora. The queries and the corresponding answers generated with RAG will be used to augment the answer generation of the original question. Empirical results demonstrate the effectiveness of i -MedRAG on both open-and close-source LLMs, which show improved performance on the United States Medical Licensing Examination (USMLE) subset of MedQA and medical questions from the Massive Multitask Language Understanding (MMLU) dataset. Our further analysis of the number of iterations and number of queries per iteration used in i -MedRAG reflects how its performance scales with different settings. Additionally, we present several case studies of i -MedRAG, showing how it overcomes the limitations in conventional RAG to find the correct answers. \n\nIn summary, our contributions are three-fold: \n\n2. Related Work",
            "score": 0.3746198178990835,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1274
                },
                {
                    "start": 1277,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2269
                },
                {
                    "start": 2272,
                    "end": 2317
                },
                {
                    "start": 2320,
                    "end": 2335
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 147,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 424,
                    "end": 427,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 505,
                    "end": 507,
                    "matchedPaperCorpusId": "258740478"
                },
                {
                    "start": 825,
                    "end": 827,
                    "matchedPaperCorpusId": "221970190"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.449462890625
        },
        {
            "corpus_id": "269757645",
            "title": "Prompt-based Code Completion via Multi-Retrieval Augmented Generation",
            "text": "Language Model for Code Completion.To generate code completions of arbitrary lengths, researchers view code as a distinct variant of language and have subsequently used natural language processing techniques (NLP) to model code statistically.Earlier work leveraged N-gram models [53], recurrent neural networks such as LSTM [52], and attention mechanisms [34] to encode programming languages.With the emergence of transformer-based models, language models (LMs) are trained on large-scale code datasets, which has significantly advanced code completion.CodeBERT [13], one of the pioneering code LMs, performs the code completion task through masked language modeling.To facilitate the generation capability, later LMs mainly adopt either a decoder-only or an encoder-decoder model, which is trained to predict the subsequent token in an auto-regressive manner.For example, CodeGPT [42], which follows the architecture of decoder-only GPT [51], outperforms GPT2 in the code completion task.UniXCoder [18], a mixed encoder-decoder model, integrates multi-task learning strategies and leverages code structures to enhance pre-training and further advances code completion performance.More recent LLMs, such as Codex [6], CodeGen [47], InCoder [14], StarCoder [36], and Code Llama [56], employ billions of parameters and trained on trillions of code tokens, significantly excel in code generation tasks.Notably, Code Llama [56] adopts the fill-in-the-middle pretraining objective [4], which resembles incomplete code contexts in code completion.This provides useful inductive bias, enabling Code Llama to substantially outperform prior models on infilling benchmarks [14].\n\nRetrieval Augmented Code Completion.Retrieval augmented generation [32] (RAG) has emerged as a technique to inject external knowledge into large language models (LLMs) to assist coherent text generation and mitigate hallucination for code completion.The RAG paradigm typically first retrieves the most relevant information using similarity measures such as BM25, dense embeddings such as SimCSE [16] or Dense Passage Retrieval [29] (DPR).",
            "score": 0.37448295028944223,
            "section_title": "RELATED WORK",
            "char_start_offset": 42335,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 35,
                    "end": 242
                },
                {
                    "start": 242,
                    "end": 392
                },
                {
                    "start": 392,
                    "end": 553
                },
                {
                    "start": 553,
                    "end": 667
                },
                {
                    "start": 667,
                    "end": 860
                },
                {
                    "start": 860,
                    "end": 989
                },
                {
                    "start": 989,
                    "end": 1181
                },
                {
                    "start": 1181,
                    "end": 1399
                },
                {
                    "start": 1399,
                    "end": 1541
                },
                {
                    "start": 1541,
                    "end": 1668
                },
                {
                    "start": 1670,
                    "end": 1706
                },
                {
                    "start": 1706,
                    "end": 1920
                },
                {
                    "start": 1920,
                    "end": 2108
                }
            ],
            "ref_mentions": [
                {
                    "start": 279,
                    "end": 283,
                    "matchedPaperCorpusId": "13040187"
                },
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "225584181"
                },
                {
                    "start": 999,
                    "end": 1003,
                    "matchedPaperCorpusId": "247315559"
                },
                {
                    "start": 1737,
                    "end": 1741,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62353515625
        },
        {
            "corpus_id": "252762146",
            "title": "Artificial Intelligence and Natural Language Processing and Understanding in Space: A Methodological Framework and Four ESA Case Studies",
            "text": "The design and development phase aims to define the general architecture and software components that make up the solution proposed for the use case, as well as the development of such components. In addition to the traditional software design and development we include activities to generate the required language technology resources, and their evaluation. We also sketch the general components in a high level architecture for NLP-based projects.\n\nDesign and Architecture. As in any software project the goal of the design process is to generate a specification of software components to fulfill the use case or project requirements. Since a general discussion about software design and architecture is out of the scope of this work, we focus on the most prominent components of software project involving NLP components in space. Following a layered architecture pattern (Richards, 2015), we define the next layers (bottom-up):\n\n\u2022 Data storage layer, where text, metadata, and dense representations are stored for efficient retrieval and similarity comparison in an inverted index 12 , an embedding index 13 or other document-oriented database.\n\n\u2022 Data access layer, including the components to extract data and text from external sources such as databases, file systems, FTP, web services, and web pages, and components to manage and query the data in the storage layer. Examples of the latter are traditional search engines like Elasticsearch, and neural retrievers such as ColBERT (Khattab and Zaharia, 2020) or DPR (Karpukhin et al., 2020). In the data access layer, data preprocessing is done including cleaning and formatting text, e.g. text from PDF documents, particularly prevalent in ESA, or web scraping, or generating dense representations for text documents using language models (Reimers and Gurevych, 2019).\n\n\u2022 Domain logic layer is where software components, including the ones supporting NLP tasks, are orchestrated according to the logic of the different functionalities required for the use case. NLP components include language models fine-tuned for specific task, available NLP APIs or statistical models. An example of an orchestrating component is one that generates questions using a language model, e.g., T5 (Raffel et al., 2020), and then attempts to answer the question using a RoBERTa model (Liu et al., 2019).\n\n\u2022 Presentation layer includes the user interface components necessary",
            "score": 0.3741975916760888,
            "section_title": "Design and Development",
            "char_start_offset": 26702,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17431640625
        },
        {
            "corpus_id": "267388279",
            "title": "A review of existing Machine Translation Approaches, their Challenges and Evaluation Metrics",
            "text": "The majority of NMT models assume that there are sufficient multilingual training data [45]. Designing a better MT framework with high accuracy is significantly challenging [46] Recently, the idea of using NMT to complete the MT task was offered. [47]- [48]. In a different study, authors offered a new encoder-decoder architecture called RNMT+ that combines the benefits of RNN-based and Transformer-based translation models to address the issues with machine translation [49]- [50]. Figure 5 shows the challenges with machine translation. The Figure 5 illustrates the relationship between diversity and Natural Language Processing (NLP). It presents three elements. Firstly, it emphasizes the array of languages worldwide, showing their unique grammatical and structural features. Secondly, it highlights the difficulty NLP faces in focusing on a language with rich linguistic resources while neglecting others that lack sufficient data for developing NLP tools. Lastly, it underscores the role played by Named Entity Recognition (NER). However, NER becomes complex due to forms of nouns in related languages, leading to ambiguity and challenges for NLP systems. In another investigation, the focus was on designing a much deeper Transformer model; after a deep analysis, the authors traced the various challenges in machine translation [51]. The common challenges in every translation approach are discussed below in Table 7. Below, Figure 6 represents the issues with machine transition in Natural Language Processing (NLP); techniques and processes are used to understand and work with human languages. One such technique is Morphological Analysis, which helps break down words into their root forms, making it easier to identify those forms based on word-level information. Part of Speech Tagging is another technique that assigns labels, like nouns, verbs, and adjectives, to words in sentences, helping us to understand how sentences are structured. Chunking is a process that groups words into phrases, such as noun or verb phrases, giving us an organized view of sentence content. Parsing involves the use of Parse Trees. Provides a detailed analysis of sentence structure by combining part of speech tagging and chunking data. Lastly, Word Sense Disambiguation helps resolve the meanings of words in context when they have interpretations. These various processes work together to ensure accurate language understanding in NLP applications.",
            "score": 0.37324854829595755,
            "section_title": "II. CHALLENGES",
            "char_start_offset": 10126,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2237
                },
                {
                    "start": 2238,
                    "end": 2350
                },
                {
                    "start": 2351,
                    "end": 2451
                }
            ],
            "ref_mentions": [
                {
                    "start": 247,
                    "end": 251,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 253,
                    "end": 257,
                    "matchedPaperCorpusId": "5590763"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2476806640625
        },
        {
            "corpus_id": "231717977",
            "title": "Adaptive Semiparametric Language Models",
            "text": "For example, attempts to incorporate extended local context to a neural network-such as those found in neural cache (Grave et al., 2017c), transformer-XL (Dai et al., 2019) compressive transformer (Rae et al., 2020), performers (Choromanski et al., 2021), longformer (Beltagy et al., 2020), and reformer (Kitaev et al., 2020)-can be seen as models of working memory. Models of episodic memory include kNN-LM (Khandelwal et al., 2020) and architectures that are designed for more complicated tasks such as question answering (de Masson d' Autume et al., 2019;Guu et al., 2020) and machine translation (Khandelwal et al., 2021). In machine learning and natural language processing, memory-augmented neural networks is used to refer to all types of memory systems. \n\nIn this paper, inspired by the modular design of human memory systems, we present a language model architecture (SPALM) with storage modules that resemble working and episodic memory systems, which we combine with a large parametric neural network that is responsible for computation ( \u00a72). Our hypothesis is that encouraging each arXiv:2102.02557v1 [cs.CL] 4 Feb 2021 component to focus on a specific function (e.g., storing long-term information, capturing extended context, modeling local information) facilitates easier training that produces an overall better language model. 2  Specifically, we follow transformer-XL (Dai et al., 2019) to capture extended context by caching hidden states in a temporary short-term memory. For long-term context, we use a persistent keyvalue database and perform sparse retrieval with (approximate) k-nearest neighbors.",
            "score": 0.3725602522702988,
            "section_title": "Introduction",
            "char_start_offset": 1721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 761
                },
                {
                    "start": 764,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1622
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 137,
                    "matchedPaperCorpusId": "8693672"
                },
                {
                    "start": 197,
                    "end": 215,
                    "matchedPaperCorpusId": "207930593"
                },
                {
                    "start": 228,
                    "end": 254,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 304,
                    "end": 325,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 408,
                    "end": 433,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 558,
                    "end": 575,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 600,
                    "end": 625,
                    "matchedPaperCorpusId": "222125236"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37744140625
        },
        {
            "corpus_id": "267750726",
            "title": "Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs",
            "text": "We conduct experiments on two open-source LLMs, Llama2-70B-Chat (Touvron et al., 2023) and Qwen-72b-Chat (Bai et al., 2023). The default proxy model, fine-tuned query rewriting model, and retrieval necessity judgment model are built on Llama2-7B-Chat. We build a search engine on the KILT dataset's document library, which is based on the 2019 Wikipedia mirror (Petroni et al., 2021). BM25 (Robertson and Zaragoza, 2009) is used as the retriever and E5 base (Wang et al., 2022) is employed as the reranker. More implementation details are provided in Appendix A.",
            "score": 0.3721991141151649,
            "section_title": "Implementation Details",
            "char_start_offset": 21144,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 562
                }
            ],
            "ref_mentions": [
                {
                    "start": 361,
                    "end": 383,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 390,
                    "end": 420,
                    "matchedPaperCorpusId": "207178704"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16357421875
        },
        {
            "corpus_id": "277349131",
            "title": "Bias-Aware Agent: Enhancing Fairness in AI-Driven Knowledge Retrieval",
            "text": "The internet has removed the physical barriers of information access as it delivers limitless knowledge to anyone's fingertips within seconds. With such unending sources of information, efficient information retrieval is a requirement. Information Retrieval (IR) can be defined as the process of obtaining relevant information from a large collection of data based on a user's query. It must be accurate, efficient, and adaptable to ever-evolving technologies and user behaviors, which are the key features of major IR usage today in search engines everywhere [13]. The IR process is defined by 2 steps: retrieval of documentation relevant to the user query and ranking the documents by relevancy score to give the most pertinent documents first [4]. In line with these goals, Brin and Page introduce the PageRank algorithm which ranks pages by the number of links and references from other pages, thus effectively ranking them by assigning proper weights based on these criteria [1]. The efficiency, reliability, and speed of this algorithm in retrieving relevant information is still the backbone of Google's search engine. Building on these foundations, advances in artificial intelligence have paved the way for IR systems that go beyond keyword matching, enabling a paradigm shift toward understanding context and intent. Recently, Large Language Models (LLMs) have disrupted the field of information retrieval. LLMs, such as ChatGPT, excel at retrieving and summarizing knowledge to deliver relevant, accurate, and context-sensitive responses. These advances not only transform communication and democratize knowledge, but also improve information retrieval efficiency, reshaping how we interact and access information in real-world applications [5] [20]. \n\nAlthough LLMs lead to significant advances in the field of IR, they suffer from problems such as hallucinations and stale information. Because LLMs are trained on static data, the context and data required to answer complex real-world questions where new information arises constantly results in stale and incomplete data and hence results in hallucinations to fill in the gap with plausible yet factually incorrect answers. Lewis et al. introduce Retrieval Augmented Generation (RAG) to address these challenges. RAG systems consist mainly of two components -Retriever and Generator.",
            "score": 0.3721991141151649,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1761
                },
                {
                    "start": 1764,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2277
                },
                {
                    "start": 2278,
                    "end": 2348
                }
            ],
            "ref_mentions": [
                {
                    "start": 560,
                    "end": 564,
                    "matchedPaperCorpusId": "260972090"
                },
                {
                    "start": 746,
                    "end": 749,
                    "matchedPaperCorpusId": "256105432"
                },
                {
                    "start": 980,
                    "end": 983,
                    "matchedPaperCorpusId": "7587743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46826171875
        },
        {
            "corpus_id": "278714952",
            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) augments large language models by incorporating external documents into the generation process. Rather than relying solely on the model's internal parameters, RAG retrieves relevant passages from a knowledge base and feeds them, along with the user query, into the model to guide its response (Zhou et al., 2024). \n\nA standard RAG system operates in two phases: \n\n\u2022 Retrieval. A retriever model selects the top-k most relevant documents for a given query. \n\n\u2022 Generation. A language model generates a response conditioned on both the query and the retrieved documents. \n\nThe appeal of RAG lies in its ability to dynamically access up-to-date or domain-specific information, which is especially useful in fast-changing or specialized fields. However, it also introduces new failure modes, particularly when the retrieval quality is imperfect (Barnett et al., 2024).",
            "score": 0.3721991141151649,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 4504,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 398
                },
                {
                    "start": 401,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 492
                },
                {
                    "start": 495,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 605
                },
                {
                    "start": 608,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 901
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76171875
        },
        {
            "corpus_id": "276928032",
            "title": "OpenRAG: Optimizing RAG End-to-End via In-Context Retrieval Learning",
            "text": "As large language models (LLMs) (Zhao et al., 2023;Minaee et al., 2024) scale, they face a data bottleneck where the high-quality internet data unable to meet growing training demands. Meanwhile, the volume of downstream data is expanding rapidly but often remains unusable for pre-training due to their real-time availability (Wang et al., 2024b;Liu et al., 2023), privacy concerns (Arora et al., 2023), licensing restrictions (Min et al., 2024), and ethical concern (Serouis & S\u00e8des, 2024;Ayyamperumal & Ge, 2024). \n\nRetrieval-augmented generation (RAG) (Lewis et al., 2020;Guu et al., 2020;Gao et al., 2023) emerges as a promising solution to this challenge. Rather than relying solely on well-curated internet data, RAG leverages information retrieval (IR) to fetch relevant data from external sources and incorporates it as context to enhance generation quality. This is valuable as RAG enables the use of rapidly expanding yet often inaccessible downstream data, which are more scalable and up-to-date than the heavily processed and regulated internet data used in pre-training. Despite their success, existing RAG frameworks typically rely on off-the-shelf retrievers trained on QA datasets, which can lead to inconsistencies between the learned retrieval relevance and the needs of downstream tasks. This discrepancy highlights key relevance gaps between IR and RAG scenarios. We explore these gaps in detail below, drawing on insights from prior research. First, there is the broadening of tasks: traditional IR datasets (Kwiatkowski et al., 2019;Bajaj et al., 2016) are designed mainly for open-domain question-answering (OpenQA), while RAG framework are applied to a wider range of tasks, such as recommendation (Manzoor & Jannach, 2022), dialog systems (Liu et al., 2024), and role-playing (Wang et al., 2023), where task requirements can be flexibly written as instructions.",
            "score": 0.3721991141151649,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1887
                }
            ],
            "ref_mentions": [
                {
                    "start": 327,
                    "end": 347,
                    "matchedPaperCorpusId": "261064713"
                },
                {
                    "start": 383,
                    "end": 403,
                    "matchedPaperCorpusId": "247593883"
                },
                {
                    "start": 428,
                    "end": 446,
                    "matchedPaperCorpusId": "260704206"
                },
                {
                    "start": 556,
                    "end": 576,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 576,
                    "end": 593,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53564453125
        },
        {
            "corpus_id": "256389797",
            "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
            "text": "Large language models (LLMs) such as GPT-3 (Brown et al., 2020a) and Codex (Chen et al., 2021a), have demonstrated impressive performance on a wide range of language tasks. These models are typically trained on very large datasets and store a substantial amount of world or domain knowledge implicitly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020;Borgeaud et al., 2022;Izacard et al., 2022b;Yasunaga et al., 2022), in contrast, can retrieve knowledge from an external datastore when needed, potentially reducing hallucination and increasing coverage. Previous approaches of retrieval-augmented language models require access to the internal LM representations (e.g., to train the model (Borgeaud et al., 2022; Figure 1. Different from previous retrieval-augmented approaches (Borgeaud et al., 2022) that enhance a language model with retrieval by updating the LM's parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., >100B parameters), which are often served via APIs. Izacard et al., 2022b) or to index the datastore (Khandelwal et al., 2020)), and are thus difficult to be applied to very large LMs. In addition, many best-in-class LLMs can only be accessed through APIs. Internal representations of such models are not exposed and fine-tuning is not supported. \n\nIn this work, we introduce REPLUG (Retrieve and Plug), a new retrieval-augmented LM framework where the language model is viewed as a black box and the retrieval component is added as a tuneable plug-and-play module. Given an input context, REPLUG first retrieves relevant documents from an external corpus using an off-the-shelf retrieval model.",
            "score": 0.3721991141151649,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1555
                },
                {
                    "start": 1558,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1904
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 64,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 484,
                    "end": 509,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 509,
                    "end": 531,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 848,
                    "end": 871,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 937,
                    "end": 960,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1310,
                    "end": 1335,
                    "matchedPaperCorpusId": "207870430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71484375
        },
        {
            "corpus_id": "266693848",
            "title": "keqing: knowledge-based question answering is a nature chain-of-thought mentor of LLM",
            "text": "Large language models (LLMs) [1][2][3][4][5] have recently become the new darling of academia and industry due to their remarkable performance in a variety of natural language processing (NLP) tasks. With the blessing of techniques such as large-scale pre-training [6], instruction tuning [7], and reinforcement learning from human feedback (RLHF) [8,9], existing pretrained LLMs have demonstrated unique capabilities in language understanding, generation, interaction, and reasoning. These powerful capabilities of LLMs also drive many emergent research topics (e.g., instruction learning [10], in-context learning [1], chain-of-thought prompting [11], etc.) to further investigate their huge potentials, and bring unlimited possibilities for humans to build advanced artificial intelligence systems. However, alongside these advancements, a pressing issue that plagues LLMs has been widely criticized as \"hallucination\", referred to as a phenomenon where LLMs tend to generate text that is incorrect, nonsensical, or not real [12]. \n\nTo alleviate the phenomenon of \"hallucination\" during the generation of LLMs, a promising direction is to retrieve the factual knowledge that are highly relevant to the user query, and then guide LLMs to generate response according to the retrieved context, resulting in retrieval-augmented LMs [13,14] that have recently demonstrated strong performance in knowledge intensive tasks, especially for knowledge-based question answering (KBQA). The workflow of existing retrieval-augmented LMs [15,16] mainly relies on embedding-based retrieval methods, which will first encode various forms of knowledge base and also the user query into the same latent space, then use a semantic similarity metric to retrieve the top-K most relevant documents as prompt, and finally instruct LLMs to only use the provided context to answer the user query. Due to the fact that embedding-based corpus retrieval often brings redundant context input, where repeated or irrelevant content will occupy a large number of tokens in the prompt, influencing the quality of response generated by LLMs [15].",
            "score": 0.3721991141151649,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1033
                },
                {
                    "start": 1036,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2115
                }
            ],
            "ref_mentions": [
                {
                    "start": 351,
                    "end": 353,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 648,
                    "end": 652,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6181640625
        },
        {
            "corpus_id": "269484731",
            "title": "CookingSense: A Culinary Knowledgebase with Multidisciplinary Assertions",
            "text": "To assess the effectiveness of our KB, we adopt the context-augmented language model setup inspired by the work of Retrieval Augmented Generation (RAG; Lewis et al., 2020b), where a context retrieved from a retriever system is augmented with the input to generate texts.We use baseline KBs and the CookingSense as sources for retrieval to measure how differently knowledge assertions from other KBs enrich the input.\n\nRetriever system: We adopt Okapi-BM25 (Robertson et al., 2009) for the retriever system for RAG evaluation, using the retriv.6BM25 is a simple yet powerful ranking algorithm based on term and document frequency, which is widely used in various work (Trotman et al., 2014).The motivation behind choosing BM25 for our retriever is, to keep a retrieval algorithm as simple as possible so that the generation quality depends more on KB's quality, not the performance of a retrieval algorithm.\n\nLanguage model: We utilized the Flan-T5 (flan-t5-large) language model (Chung et al., 2022) for text generation purposes.Flan-T5 is a language model based on T5 (Raffel et al., 2020) fine-tuned with instruction guides (Wei et al., 2022a;Ouyang et al., 2022;Sanh et al., 2022, inter alia).It can respond to a wide range of question types without the need for additional fine-tuning specific to a benchmark format.",
            "score": 0.3721991141151649,
            "section_title": "Evaluation",
            "char_start_offset": 17608,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 270,
                    "end": 416
                },
                {
                    "start": 418,
                    "end": 544
                },
                {
                    "start": 544,
                    "end": 690
                },
                {
                    "start": 690,
                    "end": 906
                },
                {
                    "start": 908,
                    "end": 1029
                },
                {
                    "start": 1029,
                    "end": 1196
                },
                {
                    "start": 1196,
                    "end": 1320
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 172,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 456,
                    "end": 480,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 667,
                    "end": 689,
                    "matchedPaperCorpusId": "207220720"
                },
                {
                    "start": 1069,
                    "end": 1090,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1145,
                    "end": 1165,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.404296875
        },
        {
            "corpus_id": "268819923",
            "title": "ARAGOG: Advanced RAG Output Grading",
            "text": "Our investigation into Retrieval-Augmented Generation (RAG) techniques has identified HyDE and LLM reranking as notable enhancers of retrieval precision in LLMs.These approaches, however, necessitate additional LLM queries, incurring greater latency and cost.Surprisingly, established techniques like MMR and Cohere rerank did not demonstrate significant benefits, and Multi-query was found to be less effective than baseline Naive RAG.\n\nThe results demonstrate the efficacy of the Sentence Window Retrieval technique in achieving high precision for retrieval tasks, although a discrepancy was observed between retrieval precision and answer similarity scores.Given its conceptual similarity to Sentence Window retrieval, we suggest that Auto-merging retrieval (Phaneendra, 2023) might offer comparable benefits, warranting future investigation.The Document Summary Index approach also exhibited satisfactory performance, however, it requires an upfront investment in generating summaries for each document in the corpus.\n\nDue to constraints such as dataset singularity, limited questions, and the use of GPT-3.5-turbo for evaluation, the results may not fully capture the potential of more advanced models.Future studies with broader datasets and higher-capability LLMs could provide more comprehensive insights.This research contributes a foundational perspective to the field, encouraging subsequent works to refine, validate, and expand upon our findings.\n\nTo facilitate this continuation of research and allow for the replication and extension of our work, we have made our experimental pipeline available through a publicly accessible GitHub repository.(Predlico, 2024) 7 Future Work\n\n\u2022 Knowledge Graph RAG: Integrating Knowledge Graphs (KGs) with RAG systems represents a promising direction for enhancing retrieval precision and contextual relevance.KGs offer a well-organized framework of relationship-rich data that could refine the retrieval phase of RAG systems (Bratanic, 2023).Although setting up such systems is resource-demanding, the potential for significantly improved retrieval processes justifies further investigation.\n\n\u2022 Unfrozen RAG systems: Unlike the static application of RAG systems in our study, future investigations can benefit from adapting RAG components, including embedding models and rerankers, directly to specific datasets (Gao et al., 2024;Kiela, 2024).",
            "score": 0.3721991141151649,
            "section_title": "Conclusion",
            "char_start_offset": 22589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 161,
                    "end": 259
                },
                {
                    "start": 259,
                    "end": 436
                },
                {
                    "start": 438,
                    "end": 660
                },
                {
                    "start": 660,
                    "end": 845
                },
                {
                    "start": 845,
                    "end": 1021
                },
                {
                    "start": 1023,
                    "end": 1207
                },
                {
                    "start": 1207,
                    "end": 1313
                },
                {
                    "start": 1313,
                    "end": 1459
                },
                {
                    "start": 1461,
                    "end": 1659
                },
                {
                    "start": 1659,
                    "end": 1689
                },
                {
                    "start": 1691,
                    "end": 1858
                },
                {
                    "start": 1858,
                    "end": 1991
                },
                {
                    "start": 1991,
                    "end": 2140
                },
                {
                    "start": 2142,
                    "end": 2392
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38525390625
        },
        {
            "corpus_id": "277955205",
            "title": "Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey",
            "text": "RAG is a technical framework that enhances NLP systems by integrating external knowledge retrieval, whose core innovation enables extra non-parametric optimization of parameterfixed neural language models after training, effectively expanding their operational domains while maintaining architectural stability [22]. Prior to the widespread adoption of LLM, scholarly investigations had already established methods for enhancing NLP tasks through external knowledge infusion [23]. Initial researches on RAG adhered to an elementary indexing and reading paradigm [24,25]. Later formulations delineated two core components: (1) the retriever, which identifies, indexes, filters, and structures relevant knowledge fragments from external data sources; (2) the generator, which synthesizes the curated segments through analysis and logical reasoning to produce outputs [9]. Figure 1 shows the workflow of an RAG system with recommendations of components implementation using LLMs at present. We provide a concise description of each module's process below. \n\nThe retrieval component of RAG systems is inspired by the retrieval technologies in multiple domains, such as information retrieval [26], open-domain question answering [27], and recommender systems [28,29]. Before the retrieval, it is necessary to construct a suitable corpus for the retrieval component at the beginning. The sources of data are diverse, such as domain-specific datasets like Wikipedia, specialized corpora (e.g., scientific articles, financial reports) [30], or realtime data gathered from web scraping or search engines [31]. The corpus is subsequently filtered and preprocessed to conform to the retrieval-friendly structure via offline chunking and embedding. Chunking involves segmenting large documents into smaller, more manageable units guided by the original structure or context information [32][33][34]. Embedding (or text vectorization) aims to represent the textual con- Red Team \n\nFig. 1 The workflow of the RAG system and component implementation in the LLM era. \n\ntent in a high-dimensional, dense semantic space for efficient retrieval computation [5,35]. Typically, RAG assessments convert the task into a conversational format of Question Answering (QA) comprising question and the ground-true answers with doc candidates [36,37].",
            "score": 0.3721991141151649,
            "section_title": "Retrieval Augmented Generation (RAG)",
            "char_start_offset": 6155,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 1965
                },
                {
                    "start": 1968,
                    "end": 2050
                },
                {
                    "start": 2053,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2322
                }
            ],
            "ref_mentions": [
                {
                    "start": 311,
                    "end": 315,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 566,
                    "end": 569,
                    "matchedPaperCorpusId": "174801285"
                },
                {
                    "start": 1187,
                    "end": 1191,
                    "matchedPaperCorpusId": "3710903"
                },
                {
                    "start": 1224,
                    "end": 1228,
                    "matchedPaperCorpusId": "249049410"
                },
                {
                    "start": 1527,
                    "end": 1531,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1878,
                    "end": 1882,
                    "matchedPaperCorpusId": "269740933"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.658203125
        },
        {
            "corpus_id": "267061013",
            "title": "Dynamic Q&A of Clinical Documents with Large Language Models",
            "text": "Probabilistic models [5] rank results based on keyword occurrences, which can be effective for certain tasks, such as information retrieval in search engines [45], spam email detection [32], and sentiment analysis in text data [9], but they struggle to grasp the underlying meaning of the text. TF-IDF [41] is another widely used method that assigns weights to keywords in queries and documents based on their rarity. This approach helps identify important terms, but it still falls short in capturing the full semantic context and relationships between words. The shared limitations across these approaches are the lack of semantic understanding and the ability to interpret the context holistically. \n\nAs a result, these traditional techniques often miss relevant information when applied to complex natural language text. They fail to recognize synonyms, antonyms, word variations, context, and other language intricacies that are essential for accurate and comprehensive information retrieval. \n\nLarge language models (LLMs) based on the transformer architecture [49] can be used to overcome these limitations and unlock the deeper meaning. The rapidly advancing LLMs, like , have revolutionized the field of natural language processing, with their contextual understanding, generative capabilities, and fine-tuning adaptability [24,57]. \n\nIntegrating LLMs into information retrieval systems holds the potential to deliver more accurate, insightful, and context-aware results, transforming the way users access information in a more personalized and efficient manner. \n\nEmbedding models (EMs) serve as powerful tools in natural language processing by representing words or phrases in a continuous vector space where the geometric distances between vectors capture semantic relationships between words. EMs, such as word embeddings [4], play a crucial role in capturing intricate relationships and contextual nuances within the text. \n\nRetrieval augmented generation (RAG) refers to a novel technique where a model first retrieves relevant information from a large corpus or dataset and then generates responses or outputs based on the retrieved information. This approach combines the benefits of both retrieval-based and generation-based methods, allowing for more contextually relevant and informative responses in conversational systems. Recent research works are exploring the trade-offs between this method and traditional fine-tuning approaches for language models, with some favoring RAG for certain contexts [7,23].",
            "score": 0.3721991141151649,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2187,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 701
                },
                {
                    "start": 704,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1341
                },
                {
                    "start": 1344,
                    "end": 1571
                },
                {
                    "start": 1574,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1936
                },
                {
                    "start": 1939,
                    "end": 2161
                },
                {
                    "start": 2162,
                    "end": 2344
                },
                {
                    "start": 2345,
                    "end": 2527
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 24,
                    "matchedPaperCorpusId": "7446821"
                },
                {
                    "start": 158,
                    "end": 162,
                    "matchedPaperCorpusId": "15993277"
                },
                {
                    "start": 185,
                    "end": 189,
                    "matchedPaperCorpusId": "207191509"
                },
                {
                    "start": 227,
                    "end": 230,
                    "matchedPaperCorpusId": "33573440"
                },
                {
                    "start": 1067,
                    "end": 1071,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.728515625
        },
        {
            "corpus_id": "272600014",
            "title": "Experimenting with Legal AI Solutions: The Case of Question-Answering for Access to Justice",
            "text": "Retrieval Augmented Generation. Large language models often hallucinate and contain outdated information (Zhang et al., 2023). Retrieval augmented generation (RAG) is an emerging approach to reduce the prevalence of hallucinations by grounding a model's generations in a data source besides the model's weights. Retrieval has been used extensively in single-hop (Ke et al., 2024), multi-hop (Sun et al., 2023), and long-form open-ended question answering (Lin et al., 2023). With the rise of instruction-following language models (Touvron et al., 2023;Chung et al., 2022;Brown et al., 2020), retrieval methods often insert context directly into the context of the language model (Ma et al., 2023;Chen et al., 2023). We focus on this setting because it is possible to integrate with existing well-performing models (such as OpenAI's GPT-3.5), further supporting our humancentric goal of making legal AI more accessible. \n\nVery recently, commercial RAG efforts such as Cohere's Command R+ models, have been applied to legal domains with a focus on trustworthiness and data privacy (Gainer & Starostin, 2024). Their retrieval method passes the retrieved documents directly into the context of a language model, such that the model generations are grounded in the context provided. In this work, we build off this line of research by focusing on retrieval from a trusted source. \n\nDatasets and Legal AI Benchmarks. NLP has been applied to various fields in law, such as question answering, relation extraction, or text summarization (Zhong et al., 2020). \n\nPreviously, work was focused on domain-specific fine-tuned models (Chalkidis et al., 2020;Zheng et al., 2021). Recently, existing work has focused more on the ability of general LLMs to perform legal reasoning (Yu et al., 2022;Jiang & Yang, 2023;Blair-Stanek et al., 2023;Yu et al., 2023).",
            "score": 0.3721991141151649,
            "section_title": "Prior Work",
            "char_start_offset": 2956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 918
                },
                {
                    "start": 921,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1374
                },
                {
                    "start": 1377,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1550
                },
                {
                    "start": 1553,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1842
                }
            ],
            "ref_mentions": [
                {
                    "start": 391,
                    "end": 409,
                    "matchedPaperCorpusId": "252692968"
                },
                {
                    "start": 679,
                    "end": 696,
                    "matchedPaperCorpusId": "258841283"
                },
                {
                    "start": 1529,
                    "end": 1549,
                    "matchedPaperCorpusId": "216552897"
                },
                {
                    "start": 1643,
                    "end": 1662,
                    "matchedPaperCorpusId": "233296302"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38134765625
        },
        {
            "corpus_id": "270764768",
            "title": "RAVEN: Multitask Retrieval Augmented Vision-Language Learning",
            "text": "Retrieval augmentation has become an important technique for improving natural language processing models.One of the first works in this area was kNN-LM by Khandelwal et al. (Khandelwal et al., 2020) who showed how interpolating over nearest neighbors from any text collection could improve generalization.This was followed by RETRO (Borgeaud et al., 2021), which scaled up the retrieval corpus to trillions of tokens.Another line of work has focused on integrating Wikipedia passages directly into models like REALM (Guu et al., 2020), RAG (Lewis et al., 2020), and FiD (Izacard and Grave, 2021).By retrieving and conditioning on relevant Wikipedia passages, these models can better perform knowledge-intensive downstream tasks like question answering.Overall, retrieval augmentation has proven to be a highly effective way of injecting knowledge into language models to improve their capabilities.The techniques have progressed from simple corpus retrieval to integrated and scalable architectures that retrieve from large knowledge bases like Wikipedia.\n\nFigure 1: Illustration of our RAVEN framework.Given an input image, we retrieve image-text pairs from an external memory.Subsequently, we use a multitask pretrained base vision-language model (VLM) to encode the retrieved samples along with the query and decode to generate an output by attending over both the query and retrieved samples.",
            "score": 0.3721991141151649,
            "section_title": "Retrieval Augmented Generation in NLP",
            "char_start_offset": 5638,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 106,
                    "end": 306
                },
                {
                    "start": 306,
                    "end": 418
                },
                {
                    "start": 418,
                    "end": 597
                },
                {
                    "start": 597,
                    "end": 753
                },
                {
                    "start": 753,
                    "end": 899
                },
                {
                    "start": 899,
                    "end": 1056
                },
                {
                    "start": 1058,
                    "end": 1104
                },
                {
                    "start": 1104,
                    "end": 1179
                },
                {
                    "start": 1179,
                    "end": 1397
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 199,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 333,
                    "end": 356,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 541,
                    "end": 561,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 571,
                    "end": 596,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8125
        },
        {
            "corpus_id": "250644189",
            "title": "Benchmarking Transformers-based models on French Spoken Language Understanding tasks",
            "text": "The development of pre-trained language models based on Transformer architectures [1], such as BERT [2] has recently led to significant progress in the field of natural language processing (NLP). However, the trend of training large pre-trained language models on ever larger corpora, with an ever-increasing amount of parameters has raised questions related to the usability of these approaches [3]. Because these models require considerable computational resources, major efforts have been made to develop compact models in order to reduce the cost of using them. Compact models offer alternatives to the energyintensive models with comparable performances while reducing their computational complexity and size. These models also allow to solve some industrial problems related to online speech processing. In particular, some applications (speech recognition, speech to text, etc.) have some known problems associated to network latency, transmission path difficulties, or privacy concerns. \n\nSpoken Language Understanding (SLU) in language dialogue systems refers to the task of producing a semantic analysis and a formalization of the user's utterance. SLU traditionally encompasses the processes of determining a broad range of information conveyed in dialogue such as identifying the domain, the intent and the concepts of the conversation. \n\nBenchmarking models have been extensively used in the performance evaluation of NLP-based systems [4,5,6]. With respect to our task of interest, recent research has focused on evaluating word embeddings representations. Word embeddings have proven to be effective in capturing semantic relationships between words. They are also an essential element of deep learning-based architectures. \n\nIn [7], contextual (ELMo [8]) and flat (Word2Vec [9], GloVe [10] and Fast-Text [11]) representations have been evaluated in order to investigate different input representations and their influence on the results obtained in SLU tasks. They highlighted the competitiveness of Word2Vec and ELMO on the French SLU corpus: MEDIA. More recently, [12] investigated the transferability of two French pre-trained BERT models [2] and their integration in BiLSTM and BiLSTM+CNN-based architectures. They obtained state-of-the-art results on MEDIA using CamemBERT [13] model.",
            "score": 0.37201741888877354,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 994
                },
                {
                    "start": 997,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1738
                },
                {
                    "start": 1741,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2229
                },
                {
                    "start": 2230,
                    "end": 2305
                }
            ],
            "ref_mentions": [
                {
                    "start": 82,
                    "end": 85,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1449,
                    "end": 1452,
                    "matchedPaperCorpusId": "226262340"
                },
                {
                    "start": 1452,
                    "end": 1454,
                    "matchedPaperCorpusId": "209202658"
                },
                {
                    "start": 1454,
                    "end": 1456,
                    "matchedPaperCorpusId": "233365261"
                },
                {
                    "start": 1744,
                    "end": 1747,
                    "matchedPaperCorpusId": "215844346"
                },
                {
                    "start": 1766,
                    "end": 1769,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1801,
                    "end": 1805,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 2082,
                    "end": 2086,
                    "matchedPaperCorpusId": "227230557"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.309814453125
        },
        {
            "corpus_id": "246895957",
            "title": "Applying Recurrent Networks For Arabic Sentiment Analysis",
            "text": "NLP considers many tasks that aim at analyzing text structures and understanding text semantics. The extracted syntactic and semantic information is then exploited for a higher level target. Examples of NLP tasks are Named entity recognition (NER) [1], Part-of-speech Tagging (POS) [1], Chunking or shallow parsing [2], Parsing [3], Word-sense disambiguation [4], Anaphora resolution (pronoun resolution) [5], Semantic role labeling (SRL) [1], Sentence classification [6], Sentiment analysis [7], Emotion detection (ED) [8,9], Document classification [10], Text summarization [11], Machine translation [3], and Question answering (QA) [2]. \n\nRecently, deep architectures have been extensively applied in NLP. Models that employ deep structures to identify and extract relevant features from large data corpora have reported enhanced performance in many fields [12]. In addition to NLP, deep structures have been employed in various fields as computer vision, handwriting recognition, speech recognition, object detection, cancer detection, biological image classification, face recognition, stock market analysis, and others [13]. \n\nRNNs are commonly used for sequence modelling. The recurrence connection enables memorizing information as the context in natural language tasks [14]. RNNs are widely implemented in NLP as they can consider the word order which enables preserving the context [15]. Unlike feedforward neural networks that use the learned weights for output prediction, RNN makes use of the learned weights and a state vector for output generation [16]. RNNs have two variants Long-Short Term Memory (LSTM) and Gated Recurrent Unit (GRU). The two variants are based on the notion of gates [16,17]. On contrary to RNN, gated variants are capable of handling long-term dependencies. Also, they can combat vanishing and exploding gradients by the gating technique [14]. LSTM is the most widespread deep architecture applied to NLP as it has shown the ability to capture far distance dependency of terms [15].",
            "score": 0.3717990284906087,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2020
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 251,
                    "matchedPaperCorpusId": "351666"
                },
                {
                    "start": 282,
                    "end": 285,
                    "matchedPaperCorpusId": "351666"
                },
                {
                    "start": 328,
                    "end": 331,
                    "matchedPaperCorpusId": "63292654"
                },
                {
                    "start": 359,
                    "end": 362,
                    "matchedPaperCorpusId": "461624"
                },
                {
                    "start": 405,
                    "end": 408,
                    "matchedPaperCorpusId": "22155438"
                },
                {
                    "start": 439,
                    "end": 442,
                    "matchedPaperCorpusId": "351666"
                },
                {
                    "start": 468,
                    "end": 471,
                    "matchedPaperCorpusId": "18183798"
                },
                {
                    "start": 492,
                    "end": 495,
                    "matchedPaperCorpusId": "212462785"
                },
                {
                    "start": 520,
                    "end": 523,
                    "matchedPaperCorpusId": "14291749"
                },
                {
                    "start": 576,
                    "end": 580,
                    "matchedPaperCorpusId": "8399914"
                },
                {
                    "start": 602,
                    "end": 605,
                    "matchedPaperCorpusId": "63292654"
                },
                {
                    "start": 1125,
                    "end": 1129,
                    "matchedPaperCorpusId": "191166260"
                },
                {
                    "start": 1278,
                    "end": 1282,
                    "matchedPaperCorpusId": "46966003"
                },
                {
                    "start": 1392,
                    "end": 1396,
                    "matchedPaperCorpusId": "22820646"
                },
                {
                    "start": 1876,
                    "end": 1880,
                    "matchedPaperCorpusId": "46966003"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.445068359375
        },
        {
            "corpus_id": "277151168",
            "title": "PersonaAI: Leveraging Retrieval-Augmented Generation and Personalized Context for AI-Driven Digital Avatars",
            "text": "Personalized AI systems have garnered significant attention in recent years, driven by advancements in natural language processing (NLP), retrieval-augmented generation, and memory-based neural architectures. Despite these advancements, existing solutions often fall short in delivering deeply personalized and scalable interactions, presenting opportunities for innovation. \n\nLarge language models (LLMs) such as OpenAI's GPT series [1] have redefined the capabilities of AI systems in generating coherent and contextually appropriate text. GPT-3 and GPT-4, for example, exhibit exceptional performance in natural language understanding and generation. However, these models typically operate as general-purpose assistants, requiring users to repeatedly provide context to produce personalized responses. This dependence on manual input limits their effectiveness in scenarios demanding dynamic and user-centric personalization. \n\nRetrieval-Augmented Generation (RAG), introduced by Facebook AI [2], addresses some of these limitations by combining external retrieval mechanisms with generative models. RAG enables the dynamic integration of contextspecific information into the response generation process, significantly enhancing relevance and accuracy. Despite its promise, existing RAG implementations often focus on narrow, domain-specific applications, leaving a gap in its application to user-driven, personalized interactions at scale. \n\nEfforts to develop personalized virtual assistants, such as Google's Meena [3] and Amazon Alexa [4], have incorporated user preferences and intent recognition to tailor interactions. These systems rely on static user profiles and predefined intent libraries, which can hinder their ability to adapt to nuanced or evolving user behaviors. Furthermore, their personalization strategies are typically limited to specific use cases, lacking the flexibility to generalize across diverse user contexts. \n\nMemory-augmented neural networks [5], such as those pioneered by Weston et al., have advanced the capability of AI systems to retain and recall historical user data. These networks facilitate long-term personalization by enabling systems to adapt to user preferences over time. However, challenges remain in scaling memory networks efficiently, particularly as the volume of stored user data grows, which can lead to computational bottlenecks and diminished response times. \n\nEthical considerations have emerged as a critical area of research in personalized AI.",
            "score": 0.3716186536827062,
            "section_title": "Related Works",
            "char_start_offset": 3653,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 374
                },
                {
                    "start": 377,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 929
                },
                {
                    "start": 932,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1444
                },
                {
                    "start": 1447,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1943
                },
                {
                    "start": 1946,
                    "end": 2111
                },
                {
                    "start": 2112,
                    "end": 2223
                },
                {
                    "start": 2224,
                    "end": 2419
                },
                {
                    "start": 2422,
                    "end": 2508
                }
            ],
            "ref_mentions": [
                {
                    "start": 996,
                    "end": 999,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1522,
                    "end": 1525,
                    "matchedPaperCorpusId": "269498086"
                },
                {
                    "start": 1979,
                    "end": 1982,
                    "matchedPaperCorpusId": "3315224"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66064453125
        },
        {
            "corpus_id": "222134030",
            "title": "NLP Service APIs and Models for Efficient Registration of New Clients",
            "text": "State-of-the-art NLP inference uses enormous neural architectures and models trained for GPU-months, well beyond the reach of most consumers of NLP. This has led to one-size-fits-all public API-based NLP service models by major AI companies, serving millions of clients. They cannot afford traditional fine tuning for individual clients. Many clients cannot even afford significant fine tuning, and own little or no labeled data. Recognizing that word usage and salience diversity across clients leads to reduced accuracy, we initiate a study of practical and lightweight adaptation of centralized NLP services to clients. Each client uses an unsupervised, corpus-based sketch to register to the service. The server modifies its network mildly to accommodate client sketches, and occasionally trains the augmented network over existing clients. When a new client registers with its sketch, it gets immediate accuracy benefits. We demonstrate the proposed architecture using sentiment labeling, NER, and predictive language modeling.",
            "score": 0.3712678302863338,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1927490234375
        },
        {
            "corpus_id": "269762872",
            "title": "Bi-CAT: Improving Robustness of LLM-based Text Rankers to Conditional Distribution Shifts",
            "text": "Retrieval and ranking lie at the heart of several applications like search, question-answering, and recommendations. The use of Large language models (LLMs) such as BERT in these applications have shown promising results in recent times. Recent works on text-based retrievers and rankers show promising results by using bi-encoders (BE) architecture with BERT like LLMs for retrieval and a cross-attention transformer (CAT) architecture BERT or other LLMs for ranking the results retrieved. Although the use of CAT architecture for re-ranking improves ranking metrics, their robustness to data shifts is not guaranteed. In this work we analyze the robustness of CAT-based rankers. Specifically, we show that CAT rankers are sensitive to item distribution shifts conditioned on a query, we refer to this as conditional item distribution shift (CIDS). CIDS naturally occurs in large online search systems as the retrievers keep evolving, making it challenging to consistently train and evaluate rankers with the same item distribution. In this paper, we formally define CIDS and show that while CAT rankers are sensitive to this, BE models are far more robust to CIDS. We propose a simple yet effective approach referred to as BI-CAT which augments BE model outputs with CAT rankers, to significantly improve the robustness of CAT rankers without any drop in in-distribution performance. We conducted a series of experiments on two publicly available ranking datasets and one dataset from a large e-commerce store. Our results on dataset with CIDS demonstrate that the BI-CAT model significantly improves the robustness of CAT rankers by roughly 100-1000bps in F1 without any reduction in in-distribution model performance.",
            "score": 0.37103347344319715,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50048828125
        },
        {
            "corpus_id": "267675587",
            "title": "A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges",
            "text": "The significant advancement in LLMs systems occurred when the transformer architecture was introduced in the seminal work [9]. The transformer model is built around the self-attention mechanism, enabling parallelization and efficient handling of long-range dependencies. Furthermore, LLM architectures served as the basis for models such as Google's Bidirectional Encoder Representations from Transformers (BERT) [10] and open AI's Generative Pretrained Transformer (GPT) series, which excelled at various language tasks. The pipeline of the basic LLMs architecture is shown in Figure 1. LLMs architecture receives text data from multiple sources and then the architecture forwards text to the subsequent stage for preprocessing. It then completes its training process by executing a series of stages, including random parameter initialization, numerical data input, loss function calculation, parameter optimization, and iterative training. They offer text translation, text summarization, sentiment analysis, and other services following the training phase. Prior research has shown the potential of LLMs in many NLP tasks, including specialized applications in domains such as the medical and health sciences [11] and politics [12]. Moreover, after inventing the most sophisticated GPT model [13], developing the state-of-the-art models (LLaMa and Bard [14]), and exploring their capabilities, such as Alpaca and GPTHuggingface [15], LLM has become a crucial and effective domain. As a result, a trustworthy assessment of current LLMs research is becoming increasingly important, and prior research has shown the potential and superiority of LLMs in NLP tasks. Despite this, only a few studies [3], [16], [17] have thoroughly reviewed latest LLMs developments, possibilities, and limitations in their research. \n\nBesides, researchers have presented various aspects of the LLMs domain in several studies [3], [16], [17], [18]; but their work still has several limitations. These studies miss many aspects of LLM including high-level architecture and configurations, taxonomies, API and domain-specific applications, and datasets of LLMs.",
            "score": 0.370843660847286,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1813
                },
                {
                    "start": 1816,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 1212,
                    "end": 1216,
                    "matchedPaperCorpusId": "233024902"
                },
                {
                    "start": 1230,
                    "end": 1234,
                    "matchedPaperCorpusId": "233476528"
                },
                {
                    "start": 1295,
                    "end": 1299,
                    "matchedPaperCorpusId": "257580633"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41796875
        },
        {
            "corpus_id": "267675587",
            "title": "A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges",
            "text": "In this subsection, we present a detailed overview on the architecture of LLMs. Table 7 presents a description and architecture of LLMs such as GPT-1, BERT, RoBERta, and T5. The table assists researchers in selecting the optimal model for a NLP task. GPT-1, BERT base, and BERT large contain 12, 12, and 24 layers, respectively, in LLMs. RoBERta is an enhanced variant of BERT, while T5 is a decoder and encoder transformer. Diagram illustrating BERT's input token processing, context-aware embedding, and masked language modeling tasks, where the masked words are intended to predict the model. T5 demonstrates the sequential layers of the transformer model, including the feedforward neural network, and self-attention. T5 explains how information flows and structures text. GPT-1 passes data input embedding and positional encoding through multiple transformer layers.",
            "score": 0.3708054763044048,
            "section_title": "D. ARCHITECTURAL OVERVIEW OF LARGE LANGUAGE MODELS",
            "char_start_offset": 58809,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 871
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4716796875
        },
        {
            "corpus_id": "265552007",
            "title": "The Efficiency Spectrum of Large Language Models: An Algorithmic Survey",
            "text": "Language modeling, a cornerstone in the field of NLP, aims to model the generative likelihood of word sequences and predict the probabilities of subsequent or missing tokens. This area has evolved significantly over several decades. Initially rooted in statistical language models [15,29,118,227,323], the focus has gradually shifted to pre-trained neural language models [130,156,189,190,207,215] and, more recently, to Large Language Models (LLMs) [27,113,241,308,335]. While there is no standardized definition for LLMs, they are typically distinguished by their extensive parameter sizes and extraordinary learning capabilities. In this section, we adopt the criteria outlined in [335], focusing on language models with more than one billion parameters, and discuss their core concepts in detail. \n\nArchitectural Foundations. LLMs can generally be categorized into two main paradigms: encoder-decoder models [149,151,175,207,217,233], exemplified by BERT [130], and decoder-only models [12, 23, 53, 72, 107, 200, 214-216, 235, 269-271, 293, 331], such as the GPT series [23,200,214,215]. BERT is trained using masked language modeling, enabling it to excel in contextual understanding by predicting masked or missing tokens. On the other hand, GPT models are trained using autoregressive modeling, which equips them with strong generative capabilities by predicting subsequent tokens in a sequence. Despite these differences, both types of models commonly rely on the Transformer [275] architecture, which is particularly noteworthy for its self-attention mechanism. In self-attention, each token is represented as a key, value, and query. The query weighs the importance of other tokens, represented as keys in understanding a particular token. These weights are applied to the values to create context-aware representations. This mechanism allows each token in the sequence to consider all other tokens simultaneously, facilitating parallel processing of sequential data and effective capture of long-sequence dependencies. \n\nAs a result, multi-head attention layers are often stacked to form deep networks in LLMs.",
            "score": 0.3704665751942553,
            "section_title": "Core Concepts of LLMs",
            "char_start_offset": 5292,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 800
                },
                {
                    "start": 803,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 2029
                },
                {
                    "start": 2032,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 281,
                    "end": 285,
                    "matchedPaperCorpusId": "40552549"
                },
                {
                    "start": 292,
                    "end": 296,
                    "matchedPaperCorpusId": "10959945"
                },
                {
                    "start": 296,
                    "end": 300,
                    "matchedPaperCorpusId": "61572040"
                },
                {
                    "start": 372,
                    "end": 377,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 377,
                    "end": 381,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 381,
                    "end": 385,
                    "matchedPaperCorpusId": "17048224"
                },
                {
                    "start": 385,
                    "end": 389,
                    "matchedPaperCorpusId": "14850173"
                },
                {
                    "start": 393,
                    "end": 397,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 912,
                    "end": 917,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 917,
                    "end": 921,
                    "matchedPaperCorpusId": "59291975"
                },
                {
                    "start": 929,
                    "end": 933,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 959,
                    "end": 964,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1074,
                    "end": 1078,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1086,
                    "end": 1090,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1484,
                    "end": 1489,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64599609375
        },
        {
            "corpus_id": "273482756",
            "title": "Assistive AI for Augmenting Human Decision-making",
            "text": "The finance domain poses several challenges to the application of generic NLP techniques. While most tasks, e.g., financial product recommendation and financial question answering have analogues in other domains, its high complexity, special terminology and time sensitivity have always required domain-specific solutions. As current state-of-the-art NLP solutions are built on large language models (LLMs), their domain adaptation requires financial LLMs that are specifically trained on financial linguistic data. While the first significant financial LLM, Bloomberg's proprietary BloombergGPT [173], was trained from scratch on a mixture of domain-specific and general-purpose texts, most approaches start with general LLMs and fine-tune them on relatively small but carefully assembled financial datasets, with frequent emphasis on examples of financial instruction following and assistance (see, e.g., [27,176]). While both approaches result in domain-adapted financial LLMs that outperform their general counterparts by a significant margin, fine-tuning, especially with parameter-efficient methods (PEFT) achieves competitive results far more cost-effectively (see, e.g., Xie et al. 176), and makes it viable to update the models frequently, which is essential in highly time-sensitive applications. \n\nLarge Language Models (LLMs) experience a wide range of research concerning coupling them with external tools. \n\nHere the LLM is the main interpreter, or interface that contacts the end user, while it also interacts with tools available through APIs [177]. These tools could include program calls, external applications, web sources, information retrieval pipelines, or even other LLMs with different configurations. Using tools can improve task-specific performance and user experience as the tools only need to be active when a user request arises [126]. \n\nIn our financial product recommendation these tools could be: \n\n\u2022 Customer information retrieval from the database, if authorized; \n\n\u2022 Task-oriented LLM instance for filling in application forms; \n\n\u2022 Recommendation system call; \n\n\u2022 Retrieval augmented generation system for detailed financial product information. This way the system will be able to retrieve related information if needed [53]. It can enter a form-filling process and can also call for the recommendation system if needed, while it can provide detailed information by accessing that financial product's description which is to be explained.",
            "score": 0.3700948676865033,
            "section_title": "Large Language Models in financial product recommendations",
            "char_start_offset": 108106,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1306
                },
                {
                    "start": 1309,
                    "end": 1419
                },
                {
                    "start": 1422,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1865
                },
                {
                    "start": 1868,
                    "end": 1929
                },
                {
                    "start": 1932,
                    "end": 1998
                },
                {
                    "start": 2001,
                    "end": 2063
                },
                {
                    "start": 2066,
                    "end": 2095
                },
                {
                    "start": 2098,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2262
                },
                {
                    "start": 2263,
                    "end": 2475
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3408203125
        },
        {
            "corpus_id": "275212986",
            "title": "Decoupling Knowledge and Reasoning in Transformers: A Modular Architecture with Generalized Cross-Attention",
            "text": "Our work draws upon and contributes to several areas of research, including Transformer architectures, knowledge retrieval, the connection between symbolic and neural AI, modular neural networks, interpretability, and generalized attention. \n\nTransformer Architectures. The Transformer architecture [19] has revolutionized NLP and other fields, leading to various modifications and extensions for improved efficiency and performance [18,15] and influential architectures like BERT [6] and GPT [14]. While much work has focused on the attention mechanism, the role of Feed-Forward Networks (FFNs) has received less attention. Geva et al. [8] addressed this by proposing that FFNs function as key-value memories, implicitly storing and retrieving knowledge. Our work builds directly on this insight, providing a formal mathematical derivation demonstrating that FFNs are a specialized case of generalized cross-attention. This formalization, further supported by empirical analyses of FFNs in code language models by Haider et al. [10], offers a deeper understanding of the functional role of FFNs. \n\nKnowledge Retrieval and Symbolic AI. Integrating external knowledge into neural networks is a long-standing goal in bridging symbolic and neural AI [7]. Approaches like Memory Networks [21] and Neural Turing Machines [9] introduced explicit memory components accessed through attention. Retrieval-augmented language models (RAG) directly incorporate external knowledge into the input context. In contrast to these explicit methods, and building on the understanding of FFNs as implicit knowledge stores [8], our work proposes an architecture that explicitly decouples knowledge and reasoning, bridging the gap between implicit and explicit knowledge representation. \n\nModular Neural Networks. Modularity in neural networks has been shown to improve learning, generalization, and interpretability [12]. Previous work has explored task-specific modularity [1,11] and parameterized Transformers [22], introducing modularity at a higher level (e.g., different modules for different tasks). Our work focuses on modularity within the Transformer architecture, formalizing the FFN as a module dedicated to implicit knowledge retrieval via cross-attention. This formalization lays the groundwork for explicitly decoupling knowledge into a separate module, distinguishing our approach from previous modular neural network designs. \n\nInterpretability of Neural Networks.",
            "score": 0.369733031210257,
            "section_title": "Related Work",
            "char_start_offset": 30885,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1764
                },
                {
                    "start": 1767,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2247
                },
                {
                    "start": 2248,
                    "end": 2420
                },
                {
                    "start": 2423,
                    "end": 2459
                }
            ],
            "ref_mentions": [
                {
                    "start": 299,
                    "end": 303,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 433,
                    "end": 437,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1284,
                    "end": 1288,
                    "matchedPaperCorpusId": "2926851"
                },
                {
                    "start": 1895,
                    "end": 1899,
                    "matchedPaperCorpusId": "572361"
                },
                {
                    "start": 1953,
                    "end": 1956,
                    "matchedPaperCorpusId": "5276660"
                },
                {
                    "start": 1956,
                    "end": 1959,
                    "matchedPaperCorpusId": "2213896"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.578125
        },
        {
            "corpus_id": "258822859",
            "title": "Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights",
            "text": "LMs for Text. In recent years, significant advancements have been made in the development of LMs, with several landmark breakthroughs that have helped to shape the field of NLP. Word2vec, developed in 2013, revolutionized NLP by providing a scalable and efficient way of learning word embeddings from large text corpora. Since then, numerous improvements have been made to word representation models, such as GloVe [32], TextCNN [21], ELMo [33], and ULMFiT [16], etc. In 2018, the Bidirectional Encoder Representations from Transformers (BERT) model demonstrated state-of-the-art performance on a range of NLP tasks by introducing a pre-training approach based on a masked language modeling objective. BERT and its variants (RoBERTa [29], ALBERT [24], XLNet [53], TinyBERT [19], T5 [36], etc.) have become a dominant paradigm in the NLP community in recent years. More recently, ChatGPT, a conversational AI model based on the GPT-3 architecture, has gained significant attention due to its remarkable performance in various language tasks. Along this line, several other notable works have contributed to the advancement of LMs, including the Transformer architecture and the GPT series of models [34,35,4]. These advancements have not only improved the accuracy of NLP models but also opened up new avenues for research and applications in a wide range of domains outside of NLP. \n\nLMs for Recommender Systems. Over the past decade, language models have been widely used in item recommendation tasks [17,49], with two main lines of research in this area. The first involves using LMs to represent textual items [49,48,59,58,50], while the second involves using LMs as user encoders or recommendation backbones, such as SASRec, BERT4Rec [41], GRU4Rec [13], NextItNet [56], and Caser [42]. In this paper, we focus primarily on the first line of research. Among the various item encoders, lightweight word2vec and medium-sized BERT are the two most popular.",
            "score": 0.36963238309764035,
            "section_title": "Background",
            "char_start_offset": 5271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 13
                },
                {
                    "start": 14,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1381
                },
                {
                    "start": 1384,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1956
                }
            ],
            "ref_mentions": [
                {
                    "start": 415,
                    "end": 419,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 429,
                    "end": 433,
                    "matchedPaperCorpusId": "9672033"
                },
                {
                    "start": 440,
                    "end": 444,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 782,
                    "end": 786,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1202,
                    "end": 1205,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1205,
                    "end": 1207,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1506,
                    "end": 1509,
                    "matchedPaperCorpusId": "233241208"
                },
                {
                    "start": 1613,
                    "end": 1617,
                    "matchedPaperCorpusId": "233241208"
                },
                {
                    "start": 1617,
                    "end": 1620,
                    "matchedPaperCorpusId": "202774468"
                },
                {
                    "start": 1620,
                    "end": 1623,
                    "matchedPaperCorpusId": "237101122"
                },
                {
                    "start": 1626,
                    "end": 1629,
                    "matchedPaperCorpusId": "220046458"
                },
                {
                    "start": 1738,
                    "end": 1742,
                    "matchedPaperCorpusId": "119181611"
                },
                {
                    "start": 1768,
                    "end": 1772,
                    "matchedPaperCorpusId": "53977298"
                },
                {
                    "start": 1784,
                    "end": 1788,
                    "matchedPaperCorpusId": "39847715"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5146484375
        },
        {
            "corpus_id": "269340390",
            "title": "Anticipating Job Market Demands - A Deep Learning Approach to Determining the Future Readiness of Professional Skills",
            "text": "The Transformer architecture [27] has changed the landscape of NLP, as almost all the classic NLP tasks (e.g., classification, dependency parsing, sentiment analysis, named entity recognition, or question answering) can be implemented with it. Transformer-based language models (e.g., BERT [28], DistilBERT [29], or RoBERTa [30]) employ a self-attention mechanism to capture the context and relationships among words. This self-attention mechanism enables them to assess the significance of individual words within a sentence, prioritizing semantically meaningful tokens while filtering out irrelevant noise [31]. Bommasani [32] even names pre-trained Transformer foundation models because most NLP tasks can be designed around them. Still, they also require adaptation to domain-specific tasks (e.g., text classification, sentiment analysis, etc.). \n\nA large-scale survey by Minaee et al. [33] presents most of the deep learning architectures widely used for text classification, from LSTMs to Transformers. A taxonomy of Transformers can be found in [27] and includes most models used for classification until late 2021. Some specialized surveys are also available. One survey [34] examines the role of embeddings in text classification. The last few years have also seen the rise of hybrid architectures that combine sequence-to-sequence or graph neural networks with Transformers, as described in Pham et al. [35]. Another recent survey [36] examines text classification models in the context of designing spam filters. \n\nLarge Language Models (LLMs) exceed Transformers in size (i.e., over 10 billion parameters [37]) and apply training strategies such as instruction tuning and adaptation tuning to enable instruction following and zero-shot capabilities. The GPT 3/4 models (https://chat.openai.com, accessed on 1 March 2024) [38] inhibit so-called emerging capabilities which further improve their capability to correctly interpret human language and, therefore, pave the way for even more advanced text classification systems [37]. \n\nUsing AI tools for classification or related generative processes requires considering issues such as transparency and accountability.",
            "score": 0.36924144646424534,
            "section_title": "Deep Learning for Text Classification",
            "char_start_offset": 13002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1523
                },
                {
                    "start": 1526,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 2040
                },
                {
                    "start": 2043,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 33,
                    "matchedPaperCorpusId": "235368340"
                },
                {
                    "start": 290,
                    "end": 294,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 307,
                    "end": 311,
                    "matchedPaperCorpusId": "203626972"
                },
                {
                    "start": 1052,
                    "end": 1056,
                    "matchedPaperCorpusId": "235368340"
                },
                {
                    "start": 1179,
                    "end": 1183,
                    "matchedPaperCorpusId": "257796671"
                },
                {
                    "start": 1833,
                    "end": 1837,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.339111328125
        },
        {
            "corpus_id": "267947932",
            "title": "Exploratory analysis on the natural language processing models for task specific purposes",
            "text": "NLP or more commonly referred to as NLP is a part of AI that deals with the communication gap between humans and machines. It is used to help people communicate with the machines in their own language while also assisting the machines to understand it. Alexa by Amazon, Google Assistant, Siri by Apple are the results of the latest advancements in the field of NLP. While it is important to keep up the accuracy, it's not possible to keep up the same level of performance for all NLP tasks like semantic analysis, summarization, question answering, and NLI. The above-mentioned techs do really well with NLI, none of them can summarize an essay. So, it is important to understand which model we have to choose for our task, and that is what we aim to do in this study. \n\nWe have selected a wide range of NLP models of different architectures with various embedding techniques to diversify our study. And, we have chosen some of the most commonly used and important tasks i.e., semantic analysis, question and answering, natural language inference and text classification. We will be using the models as suggested in the Objectives to see and identify which model suits the best for these tasks. In order to evaluate the models for the chosen NLP tasks, we are measuring their performance on standard datasets that are used for the particular task. For the study, an algorithm was devised to maintain the uniformity in the process and consistent results. The algorithm used is as:",
            "score": 0.3690741037294391,
            "section_title": "PROPOSED METHOD",
            "char_start_offset": 12718,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1479
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39208984375
        },
        {
            "corpus_id": "278129318",
            "title": "Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family",
            "text": "In 2024, language models scaled down. Major series of open weights models like Llama (Touvron, 2024), Qwen (Qwen, 2025), or Gemma (Team, 2025) have extended their size range below 7-8 billion parameters, sometimes even as low as 500 million. This new generation of small LLMs has been termed \"Small Language Models\" (SLM), which \"typically range from a few million to a few billion\" (Wang, 2025). This trend might seem counterintuitive since the history of deep learning has been mostly driven by ever-increasing scaling of compute, weights, and data. Models in the 1 billion parameter size range and below seem almost like a throwback to the GPT-2 era, when the \"large\" model had no more than 1.5 billion parameters (Radford et al.). \n\nSLMs have been driven by a significant demand for on-device and local AI: \"SLMs are downloaded more frequently than larger models in the Hugging Face community\" (Wang et al., 2024). Larger open-weight models are most commonly available through remote API, which creates a range of data issues and data frictions already encountered with proprietary models: privacy, lack of broadband connection, compatibility with secured infrastructures in professional settings. \n\nDespite continuous improvements in architecture, data quality, and training schedule, Small Language Models suffer from inherent limitations: for models used as \"latent databases\", the quality of memorization and recall correlates with parameter counts (Lu et al., 2024). Smaller models in the phone-sized range (125-500 million parameters) entail a higher risk of hallucinations. Without a significant amount of data preparation and specialized training, a lack of accuracy challenges all the most common use cases of generative AI be it retrieval augmented-generation (RAG), user support, or conversational chat. \n\nWe introduce two new reasoning SLMs designed for information retrieval and source synthesis with an unprecedented level of accuracy for their size range: Pleias-RAG-350m and Pleias-RAG-1B.",
            "score": 0.36892069956105916,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1201
                },
                {
                    "start": 1204,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1818
                },
                {
                    "start": 1821,
                    "end": 2009
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13037109375
        },
        {
            "corpus_id": "269635804",
            "title": "Automated Program Repair: Emerging Trends Pose and Expose Problems for Benchmarks",
            "text": "Commonly used in NLP, language models are used to learn contextual representations of text and are usually implemented as deep neural networks.A pre-trained language model (PLM) has parameters initialized from training on large corpora for generic text representation.They amortize much of the effort required to train on such large datasets and can be fine-tuned later for specific tasks [32,106].Large language models (LLMs) [136] are, as the name suggests, very large-scaled language models, usually with a billion or more parameters.LLMs inherit the pre-training and finetuning paradigms and architectures of PLMs.LLMs are seeing quick adoption in many fields, including APR, where their better performance at complex generative tasks is a relevant competitive advantage.",
            "score": 0.36869567886433363,
            "section_title": "Language Models",
            "char_start_offset": 4664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 143,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 398
                },
                {
                    "start": 398,
                    "end": 537
                },
                {
                    "start": 537,
                    "end": 618
                },
                {
                    "start": 618,
                    "end": 775
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.401611328125
        },
        {
            "corpus_id": "271162171",
            "title": "Human-like Episodic Memory for Infinite Context LLMs",
            "text": "In our experiments with Retrieval-Augmented Generation (RAG) baselines, we implemented a standard RAG pipeline consisting of a retriever and a downstream LLM. For each example in a benchmark task, the example context is split into chunks of l words each and encoded using the retriever's embedding model into a vector database. A similarity lookup into the vector database is used to retrieve the top k most relevant chunks, which are then fed to the downstream LLM alongside the query and task description. For all experiments, we set l = 300 and k = 5, following the protocol of Li et al. (2024c). \n\nWe conducted experiments using two retriever models-NV-Embed-v2 (Lee et al., 2024) and allmpnet-base-v2 (Reimers, 2022). NV-Embed-v2 is a SOTA LLM-based retriever that uses that, as of September 2024, ranks first on the Massive Text Embedding Benchmark (MTEB) Leaderboard (Muennighoff et al., 2022). It is a fine-tuned Mistral-7Bv0.1 model with an embedding dimension of 4096, trained using contrastive instruction-tuning on both retrieval and non-retrieval datasets. all-mpnet-base-v2 is a smaller 110M parameter model with an embedding size of 768, built on the BERT-base-uncased architecture, trained using contrastive learning on a dataset of over 1 billion sentence pairs. For each embedding model, we ran experiments using LLaMa-3-8B and LLaMa-3.1-8B as the downstream LLM.",
            "score": 0.3685763966626367,
            "section_title": "A.2 COMPARISON WITH RAG EXPERIMENT DETAILS",
            "char_start_offset": 36605,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 599
                },
                {
                    "start": 602,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1381
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.421875
        },
        {
            "corpus_id": "252815806",
            "title": "Retrieval Augmentation for T5 Re-ranker using External Sources",
            "text": "Retrieval augmentation for NLP tasks. Due to the opaque nature of knowledge stored in the parameters of LLMs, retrieval augmentation has been introduced for a variety of different NLP tasks. For example, on question answering tasks, REALM (Guu et al., 2020), RAG (Lewis et al., 2020) augment inputs with a document corpus, enriching the representation using top-ranked retrieved items via Maximum Inner Product Search (MIPS). Meanwhile, RETRO (Borgeaud et al., 2021) uses retrieval to augment at the granularity of small chunks of tokens. It has also been shown that retrieval augmentation can help provide bettergrounded text in dialogue systems (Shuster et al., 2021;Cohen et al., 2022) and in the evaluation of hallucination (Honovich et al., 2021). Inspired arXiv:2210.05145v1 [cs.IR] 11 Oct 2022 by these successes, our work investigates retrieval augmentation for re-ranking using a fixed retrieval component. Query expansion and pseudo-relevance feedback (PRF). In early work, Diaz and Metzler (2006) showed it is effective to incorporate information from an external corpus into a non-neural language modeling framework. We exploit such information when using a pre-trained language model for re-ranking by directly augmenting the original query with the top-ranked results from an external corpus. An orthogonal research direction is to improve re-ranking models by incorporating pseudorelevance feedback (PRF) signals as in (Li et al., 2018;Padaki et al., 2020;Zheng et al., 2020;Yu et al., 2021;Naseri et al., 2021). One essential component therein identifies the relevant information from the pseudo relevance, avoiding the topic shift. Besides, these methods are involved with expensive multiple iterations to collect the PRF and use that for re-ranking. In contrast, our model consumes high-quality external augmentation text and requires one single iteration.",
            "score": 0.3685763966626367,
            "section_title": "Related Work",
            "char_start_offset": 2193,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1874
                }
            ],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 283,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 647,
                    "end": 669,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 984,
                    "end": 1007,
                    "matchedPaperCorpusId": "9130375"
                },
                {
                    "start": 1434,
                    "end": 1451,
                    "matchedPaperCorpusId": "53081945"
                },
                {
                    "start": 1451,
                    "end": 1471,
                    "matchedPaperCorpusId": "215746655"
                },
                {
                    "start": 1471,
                    "end": 1490,
                    "matchedPaperCorpusId": "221703727"
                },
                {
                    "start": 1490,
                    "end": 1506,
                    "matchedPaperCorpusId": "231648324"
                },
                {
                    "start": 1506,
                    "end": 1526,
                    "matchedPaperCorpusId": "219162111"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6650390625
        },
        {
            "corpus_id": "270620489",
            "title": "Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation",
            "text": "Large language models (LLMs) have become a cornerstone of natural language processing (NLP) systems due to their impressive capabilities in understanding and generating human language (Brown et al., 2020;Ouyang et al., 2022;OpenAI, 2023).Despite their success, LLMs often suffer from temporal misalignment (R\u00f6ttger and Pierrehumbert, 2021;Luu et al., 2022)or generating hallucinatory content (Ji et al., 2023;Shi et al., 2023;Bang et al., 2023).This impacts the dependability of LLMs and limits their broader practical use, as the alignment between LLM outputs and real-world information still requires further validation.Augmenting LLMs with retrieval mechanisms to fetch relevant information from external sources has emerged as a promising approach to mitigate these issues (Khandelwal et al., 2019;Izacard et al., 2023).\n\nRetrieval-augmented language models (LMs) typically operate using a retrieve-and-generate framework.This process begins by retrieving relevant documents based on the user's input.Subsequently, the model generates a comprehensive response that is conditioned on the information contained within these retrieved documents.This approach leverages the synergy between information retrieval and natural language generation, enhancing the model's ability to provide accurate and contextually relevant answers.(Chen et al., 2017;Guu et al., 2020;Lewis et al., 2021;Izacard and Grave, 2021;Sachan et al., 2021;Lee et al., 2022;Jiang et al., 2022;Izacard et al., 2023;Nakano et al., 2022;Qian et al., 2023;Lazaridou et al., 2022;Shi et al., 2023).\n\nStandard RAG methods often involve a single retrieval step, which can be insufficient for complex problems requiring multi-step reasoning.(Yoran et al., 2024).",
            "score": 0.3685763966626367,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 238,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 622
                },
                {
                    "start": 622,
                    "end": 824
                },
                {
                    "start": 826,
                    "end": 926
                },
                {
                    "start": 926,
                    "end": 1005
                },
                {
                    "start": 1005,
                    "end": 1146
                },
                {
                    "start": 1146,
                    "end": 1329
                },
                {
                    "start": 1329,
                    "end": 1564
                },
                {
                    "start": 1566,
                    "end": 1704
                },
                {
                    "start": 1704,
                    "end": 1725
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 204,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 204,
                    "end": 224,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 306,
                    "end": 339,
                    "matchedPaperCorpusId": "233289460"
                },
                {
                    "start": 339,
                    "end": 356,
                    "matchedPaperCorpusId": "244117116"
                },
                {
                    "start": 392,
                    "end": 409,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 426,
                    "end": 444,
                    "matchedPaperCorpusId": "256662612"
                },
                {
                    "start": 802,
                    "end": 823,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1329,
                    "end": 1348,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 1384,
                    "end": 1408,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 1464,
                    "end": 1485,
                    "matchedPaperCorpusId": "251371732"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56884765625
        },
        {
            "corpus_id": "271769496",
            "title": "Retrieval-Augmented Knowledge Integration into Language Models: A Survey",
            "text": "While the previous sections describe incorporating structured information, most RAG systems retrieve natural language (NL) documents, mainly because there is more knowledge available in text form than in structured form such as knowledge graph, and converting text to knowledge graph is challenging (Melnyk et al., 2022). \n\nFormally, we define a natural language (NL) source to be the composite of text resources: \n\nwhere each D i is a document consisting of a sequence of tokens. While text is widely considered as unstructured (Hu et al., 2024;Mo et al., 2022), some works see that text can be semistructured, because of the sentence and paragraph structure (Ruan et al., 2022) by its nature, as well as handcrafted structural clues (Arivazhagan et al., 2023) such as headings and meta information. Despite their differences in structure, unstructured and semi-structured texts are predominately treated equally in the reader stage following the concatenation and/or compression of retrieved texts. NL-based RAG systems like LangChain (Chase, 2022) and LlamaIndex (Liu, 2022) usually incorporate the following steps: (1) preparation including chunking and indexing, (2) (first-)retrieval, (3) reranking and (4) generation. Respectively, in this RAKI survey, we will describe (1), ( 2) and (3) in Section 3.3.1 (NL retrieval) and final prediction/generation in Section 3.3.2 (NL integration).",
            "score": 0.3685763966626367,
            "section_title": "Natural Language",
            "char_start_offset": 19894,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 321
                },
                {
                    "start": 324,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1393
                }
            ],
            "ref_mentions": [
                {
                    "start": 299,
                    "end": 320,
                    "matchedPaperCorpusId": "253734817"
                },
                {
                    "start": 546,
                    "end": 562,
                    "matchedPaperCorpusId": "250390946"
                },
                {
                    "start": 660,
                    "end": 679,
                    "matchedPaperCorpusId": "247594288"
                },
                {
                    "start": 735,
                    "end": 761,
                    "matchedPaperCorpusId": "259858975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51806640625
        },
        {
            "corpus_id": "263908865",
            "title": "Fine-Tuning LLaMA for Multi-Stage Text Retrieval",
            "text": "Text retrieval, which entails identifying and ranking the most relevant documents or text snippets in response to a query, is crucial in various opendomain language comprehension tasks (Petroni et al., 2021), including web search (Bajaj et al., 2016), open-domain question answering (Chen et al., 2017), and fact verification (Thorne et al., 2018). Retrieval also plays an important role in enhancing the effectiveness of large language models (LLMs) in a retrieval-augmented generation (RAG) pipeline (Lewis et al., 2020b;Shi et al., 2023). This approach not only mitigates hallucinations but also enables LLMs to access knowledge that is not captured within their parameters (Yang et al., 2023;Jiang et al., 2023). \n\n1 https://huggingface.co/castorini A typical multi-stage text retrieval pipeline consists of a retriever, designed to efficiently locate the top-k relevant texts from a corpus, and a reranker, which further refines the order of the retrieved candidates to improve output quality (Nogueira and Cho, 2019). Both retrievers and rerankers have significantly benefited from the advent of pre-trained language models based on Transformers (Vaswani et al., 2017) such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020). These models are trained to encode queries and documents into vector representations for retrieval (Karpukhin et al., 2020;Lin, 2021) or to directly score the relevance between a query and a document for reranking (Nogueira et al., 2019;Zhuang et al., 2023). \n\nRecent large language models with billions of parameters, fine-tuned to follow instructions, such as InstructGPT (Ouyang et al., 2022), GPT-4 (Open-AI, 2023), and LLaMA (Touvron et al., 2023a,b), have exhibited extraordinary capabilities in many NLP tasks, surpassing previous smaller pre-trained language models (Zhao et al., 2023).",
            "score": 0.3685763966626367,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 716
                },
                {
                    "start": 719,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1498
                },
                {
                    "start": 1501,
                    "end": 1834
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 207,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 283,
                    "end": 302,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 326,
                    "end": 347,
                    "matchedPaperCorpusId": "4711425"
                },
                {
                    "start": 502,
                    "end": 523,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1152,
                    "end": 1174,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1188,
                    "end": 1209,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1217,
                    "end": 1238,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1339,
                    "end": 1363,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1477,
                    "end": 1497,
                    "matchedPaperCorpusId": "252993059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.437255859375
        },
        {
            "corpus_id": "276574836",
            "title": "Language Model Re-rankers are Steered by Lexical Similarities",
            "text": "Retrieval-augmented generation (RAG) is used to alleviate problems arising from imperfect parametric knowledge of language models (LMs) (Gao et al., 2024;Vu et al., 2024). However, the efficiency of RAG hinges on the retrieval of useful information (Wang et al., 2024b). To this end, LM re-rankers are increasingly used to provide more accurate retrieval results for RAG, superseding simpler methods based on keyword matching, such as BM25 (see Figure 1). While there are many benchmark results for LM re-rankers (Thakur et al., 2021;Petroni et al., 2021), few extensive inspections of LM re-rankers have been performed. Little is known about when the computationally expensive LM re-rankers are worth the cost and whether they always can be expected to outperform simpler methods. \n\nIn this paper, we evaluate LM re-rankers to better understand when they work well and when they fail to outperform less expensive alternatives. The contributions of this paper are as follows: \n\n\u2022 We evaluate 6 LM re-rankers of varying design on the NQ, LitQA2 and DRUID datasets to compare re-ranker performance for scenarios of varying aspects of difficulty and domain. \n\n\u2022 We explain variations in LM re-ranker performance using passage-query similarities, leveraging BM25 scores and our novel separation metric D S . All LM re-rankers underperform on samples corresponding to low D S values and we tie these to high rates of distractors (non-gold passages with high lexical similarity to the query) and lack of document context. \n\n\u2022 We evaluate a set of methods for improving LM re-ranker performance, such as adding contextual information. Our results show that while most methods work well on NQ, they are less effective for LitQA2 and DRUID. \n\nTaken together, our paper identifies and measures novel aspects of difficulty for LM re-rankers; distractors and lack of contextual information. These aspects are likely to occur in real-world scenarios relying on e.g. information retrieval from the Internet.",
            "score": 0.3685763966626367,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 781
                },
                {
                    "start": 784,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1154
                },
                {
                    "start": 1157,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1515
                },
                {
                    "start": 1518,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1731
                },
                {
                    "start": 1734,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 170,
                    "matchedPaperCorpusId": "263672149"
                },
                {
                    "start": 513,
                    "end": 534,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 534,
                    "end": 555,
                    "matchedPaperCorpusId": "221507798"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67333984375
        },
        {
            "corpus_id": "266999855",
            "title": "Inroads to a Structured Data Natural Language Bijection and the role of LLM annotation",
            "text": "It would be an understatement to say there has been an explosion in interest in Large Language Models (LLMs) for assisting with knowledge work. At the same time, we are grappling with the \"hallucination\" problem (Gabriel et al., 2021;Kryscinski et al., 2020). To address this, have proposed Retrieval-Augmented generation (RAG), placing unstructured documents into the context windows of Large Language Models. Still, there is a relative dearth of researching structured queries for RAG from structured sources compared to unstructured sources (Li et al., 2022;Shuster et al., 2021). It may one day be possible for smaller language models to match or best larger models for factual data recall tasks by iteratively querying databases of facts with their sources. This work is a first step towards small pre-trained Language Models (PLMs) which add structure to text documents. Further aspirational use-cases follow in Appendix E.1",
            "score": 0.3685763966626367,
            "section_title": "Motivation",
            "char_start_offset": 30,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 930
                }
            ],
            "ref_mentions": [
                {
                    "start": 212,
                    "end": 234,
                    "matchedPaperCorpusId": "225067529"
                },
                {
                    "start": 234,
                    "end": 258,
                    "matchedPaperCorpusId": "204976362"
                },
                {
                    "start": 561,
                    "end": 582,
                    "matchedPaperCorpusId": "233240939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31884765625
        },
        {
            "corpus_id": "268667010",
            "title": "Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs",
            "text": "With the rapid development of Large Language Models(LLMs), the Retrieval-Augmented Generation(RAG) techniques have been widely used in various downstream tasks.In QA and dialogue situations, we can get answers through retrieval and generation.The retrieval-augmented generation core idea is a mixture framework that integrates the retrieval model and generation model, which generates text that is not only accurate but also rich in information.The RAG core idea is to first collect sufficient evidence related to the question, and then find the answer by the Large Language Model.Therefore, for template design, our research focuses on how to use LLMs to select appropriate modifiers for the trigger words.In the design of verbalizers, our main focus is on how to utilize large language models (LLMs) to establish an appropriate mapping from the vocabulary space to the label space.\n\nIn this paper, we propose a novel RAG-based model for Event Temporal Relation(RETR), which is a prompt-based neural network method that focuses on addressing the task of the TempRel framework.The RETR consists of two parts, the rough selection stage and the fine-tuning selection stage.In the first stage, we list various strategies with prompts in terms of PLMs selection, template style design, and tuning modes.In the second stage, our goal is to search for suitable PVP pairings, which utilize the Retrieval-Augmented Generation techniques for the prompt template to optimize the trigger word modifier word and find the suitable verbalizer for each TempRel dataset using the LLMs.In the phase of searching for suitable PVP pairings, we designed an algorithm to find the best PVP pairings based on the optimal strategy of RETR.After the above two stages, we can obtain the best extraction performance and get the best F1 value.Experimental results show that our method consistently achieves good performances on three widely used TempRel datasets.Besides its effectiveness, the model is universal, which is valuable for future related applications.Our contributions can be summarized as follows:\n\n\u2022 We are the first to integrate Retrieval-Augmented Generation (RAG) with the prompt-based learning paradigm, mitigating the problem of relation ambiguity in the task of event TempRel extraction.",
            "score": 0.3685763966626367,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 160,
                    "end": 243
                },
                {
                    "start": 243,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 581
                },
                {
                    "start": 581,
                    "end": 707
                },
                {
                    "start": 707,
                    "end": 883
                },
                {
                    "start": 885,
                    "end": 1077
                },
                {
                    "start": 1077,
                    "end": 1171
                },
                {
                    "start": 1171,
                    "end": 1299
                },
                {
                    "start": 1299,
                    "end": 1569
                },
                {
                    "start": 1569,
                    "end": 1715
                },
                {
                    "start": 1715,
                    "end": 1815
                },
                {
                    "start": 1815,
                    "end": 1935
                },
                {
                    "start": 1935,
                    "end": 2083
                },
                {
                    "start": 2085,
                    "end": 2280
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7158203125
        },
        {
            "corpus_id": "272525276",
            "title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation",
            "text": "A state-of-the-art retrieval method that ranks in the top 3 on the MTEB leaderboard at the time of writing this paper. Jina-emb-v3 [48]: A newly released frontier multilingual retrieval model, which claims to perform well in various scenarios, particularly in RAG tasks. (3) Advanced RAG Methods: RQ-RAG [6]: RQ-RAG prompts LLMs to refine the input query into several sub-queries that are more effective for retrieval by explicit rewriting, decomposition, and disambiguation. The supporting passages are retrieved using both the original and refined queries. HyDE [15]: Directly prompts LLMs to generate hypothetical documents based solely on the query, and then retrieves relevant passages using these documents. The final answer is generated based on the retrieved passages. GraphRAG [13]: A graph-based RAG framework that transforms unstructured data into graph structures, enabling the system to perform more complex question-answering tasks based on graph-based information retrieval. \n\nIn the main experiments, the memory model is trained on Mistral-7B-Instruct-v0.2-32K. By default, MemoRAG uses the underlying LLM of the memory model as the generator. But Mistral's 32K context window is insufficient for most evaluation dataset contexts. To avoid context truncation, we use Phi-3-mini-128K-instruct [1] as the generator for MemoRAG and all baseline methods except for SelfExtend, which is specifically designed to enable LLMs to process contexts much longer than their native window. SelfExtend utilizes Phi-3-mini-4K-instruct as the generator and adjusts its effective context window according to the maximum context length required by different tasks. For GraphRAG, we utilize OpenAI's GPT-4o API for all requests during both the indexing and searching processes. \n\nx and used as the grounding evidence for answer generation 1 . See Appendix A for more implementation details.",
            "score": 0.3685763966626367,
            "section_title": "Stella-en-1.5B-v5[12]:",
            "char_start_offset": 21480,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1774
                },
                {
                    "start": 1777,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1887
                }
            ],
            "ref_mentions": [
                {
                    "start": 564,
                    "end": 568,
                    "matchedPaperCorpusId": "254877046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4296875
        },
        {
            "corpus_id": "247941966",
            "title": "kNN-NER: Named Entity Recognition with Nearest Neighbor Search",
            "text": "Retrieval Augmented Model Retrieval augmented models additionally use the input to re-Figure 1: An example for the process of kNN-NER. The datastore contains a set of representation-label pairs, which are extracted from the hidden states of the vanilla NER model. By given an inference sentence: Obama lives in Washington, suppose that at current test time t we need to assign named entity to the word Washington. The word representation of Washington is used to query k nearest neighbors from the datastore according to the similarity distance, and through the softmax function, the similarity distances are converted to kNN entity distribution. Interpolating the kNN distribution with the vanilla NER model distribution, we get the final distribution for the assigned named entities. \n\ntrieve a set of relevant information to improve the model performance under the merit that an openbook exam is easier than a close-book exam. Recent success on various NLP tasks has shown the effectiveness of retrieval augmented models in improving the quality of neural NLP models, such as language modeling (Khandelwal et al., 2019;Meng et al., 2021b), question answering (Guu et al., 2020;Lewis et al., 2020a,b;Xiong et al., 2020), text classification (Lin et al., 2021b), dialog generation (Fan et al., 2020;Thulke et al., 2021;Weston et al., 2018) and neural machine translation (Khandelwal et al., 2019;Meng et al., 2021a;Wang et al., 2021).",
            "score": 0.3685763966626367,
            "section_title": "Related Work",
            "char_start_offset": 2735,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1435
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5498046875
        },
        {
            "corpus_id": "267783055",
            "title": "Assessing generalization capability of text ranking models in Polish",
            "text": "Text information retrieval is one of the most active research areas in natural language processing.The popularity of this field primarily stems from its broad applications and the challenges posed by steadily increasing volumes of unstructured data, whether publicly available on the Internet or processed internally by companies.Information retrieval methods are applied to various problems, including search, question answering, summarization, clustering, or plagiarism detection.Text retrieval originally relied on lexical methods such as BM25 [17], which, with the advancement of deep learning, began to be replaced or supplemented by neural approaches, particularly those leveraging neural language models [31,32].\n\nNowadays, a lot of attention is being paid to building end-to-end systems that are designed to answer a user's question based on data in a local knowledge base.This challenge is typically addressed by integrating multiple cooperating models, each specialized for a specific step in the process.Such approaches are referred to as retrieval-augmented generation (RAG) [11,12].Figure 1 illustrates the typical architecture of the RAG system, consisting of three models: retriever, reranker, and reader.The retriever is responsible for the initial extraction of a set of documents relevant to the user's query.At this stage, the focus is primarily on the model's efficiency -it should be able to quickly select matching documents from a large collection, potentially numbering in the millions or billions.Therefore, lexical methods based on full-text indexes or neural text encoders along with vector indexes are commonly employed.This allows for precomputing representations for documents and storing them in a data structure that enables efficient searching.In the next step, the retrieved documents are sorted using the reranker model.Since the input consists of a small set of preselected documents, models used at this stage may offer higher prediction quality at the expense of efficiency.Frequently, employed methods compute similarity for each query-document pair, requiring n \u00d7 m comparisons for n queries and m documents per query.In the final step of the process, the query along with the content of the documents is passed to the reader, which generates the final answer.Currently, large language models (LLMs) are most commonly used for this purpose.Fig. 1.",
            "score": 0.3685763966626367,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 99,
                    "end": 330
                },
                {
                    "start": 330,
                    "end": 482
                },
                {
                    "start": 482,
                    "end": 719
                },
                {
                    "start": 721,
                    "end": 881
                },
                {
                    "start": 881,
                    "end": 1015
                },
                {
                    "start": 1015,
                    "end": 1095
                },
                {
                    "start": 1095,
                    "end": 1220
                },
                {
                    "start": 1220,
                    "end": 1327
                },
                {
                    "start": 1327,
                    "end": 1522
                },
                {
                    "start": 1522,
                    "end": 1648
                },
                {
                    "start": 1648,
                    "end": 1777
                },
                {
                    "start": 1777,
                    "end": 1855
                },
                {
                    "start": 1855,
                    "end": 2012
                },
                {
                    "start": 2012,
                    "end": 2158
                },
                {
                    "start": 2158,
                    "end": 2300
                },
                {
                    "start": 2300,
                    "end": 2380
                },
                {
                    "start": 2380,
                    "end": 2387
                }
            ],
            "ref_mentions": [
                {
                    "start": 547,
                    "end": 551,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1087,
                    "end": 1091,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6865234375
        },
        {
            "corpus_id": "270285886",
            "title": "XL-HeadTags: Leveraging Multimodal Retrieval Augmentation for the Multilingual Generation of News Headlines and Tags",
            "text": "Retrieval-Augmented Generation (RAG) represents a pivotal advancement in Natural Language Generation (NLG), addressing the issue of neural models' limited contextual understanding.Traditional neural models often falter when the input lacks comprehensive information for generating accurate outputs, particularly in complex realworld applications (Yu et al., 2022).To bridge this gap, KNNLM (Khandelwal et al., 2020) introduced a technique for augmenting language models with examples retrieved from a training text dataset, enhancing contextual relevance.Building on this, RETRO (Borgeaud et al., 2021) leveraged a vastly expanded text corpus, enabling models with a smaller footprint to achieve performance on par with GPT-3 (Brown et al., 2020).\n\nModels such as REALM (Guu et al., 2020) and RAG (Lewis et al., 2020b) incorporate Wikipedia passages as external knowledge bases, significantly boosting their efficacy in tasks like Question Answering.REALM focuses on encoding information through masked language modeling, whereas RAG employs an encoder-decoder structure for generative language tasks.\n\nExpanding on these concepts, MuRAG (Chen et al., 2022) stands out by integrating multimodal knowledge sources, encompassing both visual and textual data.This innovation extends the capabilities of knowledge-enhanced text generation, catering to the nuanced demands of intricate information landscapes.-w/I (K=15) 31.37 (-0.09) 12.63 (-0.10) 28.13 (-0.02) 8.58 (-0.17) 24.40 (-0.21) 0.71 (0.00) 70.84 (-0.03)Table 8: Headline Generation Evaluation.Selected Content (Only Important Sentences).The best results compared to their respective baseline models are marked in bold, and \u2206 gains are shown in round brackets and highlighted with green and red colors.",
            "score": 0.3685763966626367,
            "section_title": "G.4 Retrieval-Augmented Generation",
            "char_start_offset": 38590,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 364
                },
                {
                    "start": 364,
                    "end": 555
                },
                {
                    "start": 555,
                    "end": 747
                },
                {
                    "start": 749,
                    "end": 950
                },
                {
                    "start": 950,
                    "end": 1101
                },
                {
                    "start": 1103,
                    "end": 1256
                },
                {
                    "start": 1256,
                    "end": 1404
                },
                {
                    "start": 1404,
                    "end": 1510
                },
                {
                    "start": 1510,
                    "end": 1550
                },
                {
                    "start": 1550,
                    "end": 1594
                },
                {
                    "start": 1594,
                    "end": 1758
                }
            ],
            "ref_mentions": [
                {
                    "start": 346,
                    "end": 363,
                    "matchedPaperCorpusId": "222272210"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51025390625
        },
        {
            "corpus_id": "268248911",
            "title": "Reliable, Adaptable, and Attributable Language Models with Retrieval",
            "text": "Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose retrieval-augmented LMs. This involves a reconsideration of datastores and retrievers, the exploration of pipelines with improved retriever-LM interaction, and significant investment in infrastructure for efficient training and inference.",
            "score": 0.3685763966626367,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86865234375
        },
        {
            "corpus_id": "277824631",
            "title": "NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes",
            "text": "Retrieval-augmented generation Retrieval-Augmented Generation (RAG) systems (Gupta et al., 2024) enhance the performance of large language models (LLM) by retrieving relevant information from external documents, grounding responses in domain-specific knowledge. Traditional RAG approaches (Zhao et al., 2024) embed user queries and entries from a knowledge base into a shared vector space and then compare query vectors to knowledge base vectors to retrieve the top-K most similar contexts based on cosine similarity or similar variants (Fan et al., 2024;Lewis et al., 2020) (Hsia et al., 2024). Despite these advancements, traditional RAG systems still face significant challenges. \n\nThe context window limitations (Cheng et al., 2024;Su et al., 2024) of LLMs constrain their ability to process extensive external documents holistically (Jiang et al., 2024b). RAG has been applied to various domain-specific knowledge bases, such as BioRAG and MedicalRAG (Wang et al., 2024a;Wu et al., 2024;Jiang et al., 2024a). RAG also struggles with corpus-wide understanding tasks, like query-focused abstractive summarization, which require synthesizing knowledge across large datasets. The table 3 presents the system performance of mainstream graph-based RAG methods and our proposed approach. Compared to previous work, our method demonstrates superior performance across multiple datasets and in open-ended head-to-head evaluations, while also achieving better system-level efficiency.",
            "score": 0.3685763966626367,
            "section_title": "Related Works",
            "char_start_offset": 22847,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1479
                }
            ],
            "ref_mentions": [
                {
                    "start": 537,
                    "end": 555,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 716,
                    "end": 736,
                    "matchedPaperCorpusId": "258479968"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.646484375
        },
        {
            "corpus_id": "252111085",
            "title": "Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence",
            "text": "In this section, we will introduce the details of building Fengshenbang Models. As shown in   Advances in NLP have developed numerous powerful models from different perspectives, including research and applications. To understand the difference and track progress, there is an opportunity to standardize taxonomy in this field. However, models are often difficult to be categorized due to their complexity. For example, TinyBERT (Jiao et al., 2020) can be classified as \"Encoder-only\" in the model architecture, or it can be assigned as \"Distillation\" under model parameter reduction methods. To reduce misunderstanding, we introduce the User-centered Taxonomy (UCT), which consults numerous NLPers. In general demands, there are common NLP tasks, which are classified into Natural Language Understanding (NLU), Natural Language Generation (NLG), and Natural Language Transformation (NLT). Due to the fast development, NLP community brings special demands to the entire AI community, which are often assigned to MultiModal (MM), Domains and Exploration. Moreover, we assign a series name for each task. Note that we will update UCT timely according to the development of the NLP field. \n\nNatural Language Understanding (NLU) NLU tasks make use of syntactic and semantic analysis of text to understand the meaning of sentences. The syntax is related to the grammatical structure of a sentence, and semantics refers to its intended meaning. Relationships between words and phrases are also important as these will lead to different concepts. In addition, some problems in this task are difficult to solve even for humans. To evaluate the performance of NLU, several tasks are developed to ensure reliability: \n\n\u2022 Semantic Matching Different from computer reading comprehension in NLU, NLG is concerned with developing computer systems that produce understandable writing. An NLG-capable system should be able to generate natural language by forming its ideas as opposed to transforming existing data. And, the generated text needs to be coherent and understandable to humans. We assign several NLG tasks as follows. \n\n\u2022 Creative Writing We define NLT tasks as source-to-target transformation tasks. In contrast to NLG, NLT is based on the source objects and target objects. Language models require generating or transforming target objects by understanding source objects.",
            "score": 0.3685483435081328,
            "section_title": "Fengshenbang Model",
            "char_start_offset": 3707,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1185
                },
                {
                    "start": 1188,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1706
                },
                {
                    "start": 1709,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2113
                },
                {
                    "start": 2116,
                    "end": 2196
                },
                {
                    "start": 2197,
                    "end": 2271
                },
                {
                    "start": 2272,
                    "end": 2370
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2109375
        },
        {
            "corpus_id": "265050661",
            "title": "Evaluating Generative Ad Hoc Information Retrieval",
            "text": "'Generative retrieval' or 'generative IR' are umbrella terms for a diversity of approaches that use generative models to solve retrieval tasks. 5 Following Arora et al. [6], Figure 3 categorizes these approaches into generation-augmented retrieval (GAR) and retrievalaugmented generation (RAG). Notably, GAR approaches create traditional list SERPs, while RAG approaches generate text SERPs. \n\nIn GAR approaches, generative models are used to enhance the traditional search architecture at indexing time or at query time. At indexing time, generative models can be used for augmenting documents [37,44,78,94,152] with confabulated or hallucinated content, or for replacing the standard indexing process with what are commonly termed 'differentiable indices' by, for instance, generating document identifiers like page titles [20,31,125], URLs [153], or (structured) string identifiers [124,128,148,151]. At query time, generative models can be used for augmenting queries [2,77], or for modeling relevance by, for instance, generating parts of existing documents from the query and retrieving the documents by string matching [11], by predicting a (re-)ranking directly [123], or by using special tokens as relevance signal [76,93,99,150].",
            "score": 0.3682940439583279,
            "section_title": "A Taxonomy of Generative Retrieval",
            "char_start_offset": 11570,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 391
                },
                {
                    "start": 394,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1239
                }
            ],
            "ref_mentions": [
                {
                    "start": 599,
                    "end": 602,
                    "matchedPaperCorpusId": "255545874"
                },
                {
                    "start": 602,
                    "end": 605,
                    "matchedPaperCorpusId": "216641912"
                },
                {
                    "start": 825,
                    "end": 829,
                    "matchedPaperCorpusId": "248118757"
                },
                {
                    "start": 829,
                    "end": 832,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 843,
                    "end": 848,
                    "matchedPaperCorpusId": "258714822"
                },
                {
                    "start": 885,
                    "end": 890,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 890,
                    "end": 894,
                    "matchedPaperCorpusId": "249395549"
                },
                {
                    "start": 894,
                    "end": 898,
                    "matchedPaperCorpusId": "255879096"
                },
                {
                    "start": 972,
                    "end": 975,
                    "matchedPaperCorpusId": "259123956"
                },
                {
                    "start": 1126,
                    "end": 1130,
                    "matchedPaperCorpusId": "248366293"
                },
                {
                    "start": 1170,
                    "end": 1175,
                    "matchedPaperCorpusId": "258212638"
                },
                {
                    "start": 1228,
                    "end": 1231,
                    "matchedPaperCorpusId": "212725651"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6826171875
        },
        {
            "corpus_id": "257952074",
            "title": "Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing",
            "text": "The Transformer architecture is used as the main structure for several cutting-edge models, including GPT-3 [12], DALL-E-2 [13] and Codex [14]. It was created to overcome the limitations of traditional models like RNNs in managing sequences of varying lengths and contextual information. The Transformer relies on a self-attention mechanism, which enables the model to focus on different segments of the input sequence. The Transformer comprises an encoder and a decoder. The encoder processes the input sequence and produces hidden representations, while the decoder uses these hidden representations to generate the output sequence. Each layer of the encoder and decoder contains a multi-head attention mechanism and a feed-forward neural network. The multi-head attention is the most important part of the Transformer, as it determines how tokens are weighted based on their relevance. This method of information routing allows the model to better handle long-term dependencies, leading to improved performance across various NLP tasks. Another advantage of the Transformer is its parallelizability, which allows it to handle large-scale pre-training and adaptability to different downstream tasks without inductive biases. Figure 4 presents the architecture of the transformer. The Transformer architecture has become the primary choice in natural language processing due to its ability to learn and parallelize. Pre-trained language models that use the Transformer architecture can be divided into two categories based on their training tasks: autoregressive language modeling and masked language modeling. \n\nMasked language modeling: used in models like BERT [15] and RoBERTa [16], involves predicting the probability of a masked token given its context within a sentence. \n\nAutoregressive language modeling: used in models like GPT-3 and Open pre-trained transformer language models (OPT) [17], involves modeling the probability of the next token in a sentence given the preceding tokens, making it a left-to-right language modeling approach. Autoregressive models are better suited for generative tasks than masked language models. RoBERTa uses the same architecture as BERT but performs better by increasing the amount of pre-training data and incorporating more challenging pre-training objectives.",
            "score": 0.3680946420146871,
            "section_title": "Transformers and pre-trained language models",
            "char_start_offset": 6894,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1611
                },
                {
                    "start": 1614,
                    "end": 1778
                },
                {
                    "start": 1781,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2139
                },
                {
                    "start": 2140,
                    "end": 2308
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 112,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55126953125
        },
        {
            "corpus_id": "265329916",
            "title": "Large Language Models and Information Retrieval",
            "text": "Language models (LMs) are fundamental in predicting the generative likelihood of word sequences, effectively estimating the probability of subsequent words by considering the contextual information from preceding words. Initially designed for text generation, recent studies have uncovered their potential to reformulate a diverse array of natural language processing (NLP) problems into a text-to-text format. This transformation has positioned LMs as the go-to solution for a wide range of text-related challenges. \n\nThe evolution of LMs can be delineated into four primary stages. Initially, statistical learning techniques formed the basis, termed statistical language models, employing the Markov assumption to predict subsequent words based on preceding words. Following this, neural networks, particularly recurrent neural networks (RNNs), were introduced, allowing the calculation of the likelihood of text sequences and the development of neural language models. Advancements led to contextualized word representations through models like ELMo and BERT, initiating the era of pre-trained language models (PLMs). Scaling up these models in terms of size and data significantly improved their performance on downstream tasks, giving rise to large language models (LLMs) [6]. Existing LLMs can be broadly categorized based on their architectures into encoder-decoder and decoderonly models. Encoder-decoder models, such as T5, transform input text into vectors using an encoder and utilize them to generate output texts. On the other hand, decoder-only models, exemplified by GPT, rely on the Transformer decoder architecture, using a self-attention mechanism to generate sequences of words from left to right. With the advent of neural networks, a significant shift occurred in language modeling. Neural Language Models emerged, leveraging technologies such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks. These frameworks enabled models to capture contextual dependencies and represent words in a continuous vector space, vastly improving language understanding and generation. \n\nSubsequently, the paradigm evolved into Pre-trained Language Models (PLMs), marking a revolutionary breakthrough. PLMs pre-train models on extensive text corpora, learning rich language representations in an unsupervised manner.",
            "score": 0.36775270709714725,
            "section_title": "Large Language Model",
            "char_start_offset": 9469,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2117
                },
                {
                    "start": 2120,
                    "end": 2233
                },
                {
                    "start": 2234,
                    "end": 2348
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7080078125
        },
        {
            "corpus_id": "276574932",
            "title": "LawPal : A Retrieval Augmented Generation Based System for Enhanced Legal Accessibility in India",
            "text": "The advancement of artificial intelligence in the legal domain has led to the development of various tools that assist in legal research, document retrieval, and automated legal reasoning. Several studies have explored the use of Natural Language Processing (NLP) [3], machine learning models, and vector-based search mechanisms to enhance the efficiency of legal chatbots. The primary focus of this literature review is on retrieval-augmented generation (RAG) models, FAISS-based document retrieval, deep learning for legal applications, and the use of large language models (LLMs) in legal AI. \n\nRecent research on Retrieval-Augmented Generation (RAG) [4] for legal AI has demonstrated its potential in enhancing legal text retrieval and summarization. S. S. Manathunga, Y. and A. Illangasekara [5] proposed a RAG-based model that improves legal text summarization by dynamically fetching relevant documents before generating responses. Similarly, Lee and Ryu [6] explored the application of RAG in case law retrieval, demonstrating its superiority over traditional keyword-based search engines. The introduction of RAG has significantly improved response accuracy by grounding AIgenerated text in authoritative legal documents, reducing hallucinations in AI-driven legal assistance. \n\nThe efficiency of FAISS (Facebook AI Similarity Search) in legal document retrieval has also been widely studied. Zhao et al. [7] implemented FAISS to enhance large-scale legal question answering systems, achieving significant improvements in retrieval speed and relevance. N. Goyal and D. Chen [8] demonstrated that FAISS-based vector search mechanisms outperform conventional database searches in legal information retrieval, reducing query response time while maintaining high accuracy. The integration of FAISS with transformer-based models, as seen in the work of Hsieh and Wu, further enhances semantic retrieval, ensuring that chatbot responses align with actual legal texts. \n\nTransformer-based models such as BERT and GPT-based architecture have also contributed to the evolution of AIdriven legal research. Devlin et al. introduced BERT (Bidirectional Encoder Representations from Transformers), which significantly improved the understanding of legal language. RoBERTa, an optimized version of BERT, was later developed by Liu et al. [9] to enhance contextual understanding and document similarity matching in legal queries.",
            "score": 0.3676818953213737,
            "section_title": "II. LITERATURE SURVEY",
            "char_start_offset": 1290,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1285
                },
                {
                    "start": 1288,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1970
                },
                {
                    "start": 1973,
                    "end": 2104
                },
                {
                    "start": 2105,
                    "end": 2259
                },
                {
                    "start": 2260,
                    "end": 2423
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 267,
                    "matchedPaperCorpusId": "7678100"
                },
                {
                    "start": 962,
                    "end": 965,
                    "matchedPaperCorpusId": "265608091"
                },
                {
                    "start": 1414,
                    "end": 1417,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5986328125
        },
        {
            "corpus_id": "233296248",
            "title": "An Analysis of a BERT Deep Learning Strategy on a Technology Assisted Review Task",
            "text": "However, for different CLEF-TAR metrics such as the AP (Average Precision) and the NCG (Normalised Cumulative Gain) after certain documents, the authors reported that their method did not perform well [13]. Essentially, different people have implemented different strategies and they all used different methods, so itis hard to determine which IR strategy is the absolute best to use for the CLEF eHealth Task 2. \n\nNonetheless, pretrained DL architectures incorporating BERT proposed by Devlin et al. [14] have advanced performance in several NLP tasks with the MS MARCO and MultiNLI datasets. In fact, pretrained neural language models like BERT are recognised as the basis of SOTA NLP methods [15]. Specifically, BERT in a default set up has managed to exceed existent retrieval algorithms with the Robust04 dataset, on tasks that have not seen IR effectiveness advances over classic IR baselines for a long-time in accordance to Camara and Hauff [16]. BERT uses the Transformer mechanism which has a necessary encoder for reading text input to generate a LM (Language Model) and an optional decoder for producing task prediction. Furthermore, BERT has been investigated for some NLP and IR tasks in the context of Evidence retrieval and claim verification by Soleimani et al. [17], Rethinking query expansion for re-ranking by Padaki et al. [21], Retrieval heuristics by Camara et al. [16], Augmenting it with Graph Embedding (VGCN-BERT) for Text Classification by Lu et al. [18] and Alleviating Legal News Monitoring by Sanchez et al. [19]. Also, Akkalyoncu et al. introduced the Birch framework [20] a system that integrates neural networks (BERT) to document retrieval using the open-source Anserini [21] toolkit for IR to present end-to-end search over large document collections. Birch adopts an architecture that incorporates the Anserini IR toolkit for initial retrieval, and then inference using an applied BERT model. Birch implements simple ranking models which have achieved SOTA effectiveness on standard TREC newswire and social media test collections.",
            "score": 0.3676583313908902,
            "section_title": "Deep Learning for Information Retrieval Tasks",
            "char_start_offset": 12556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2068
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "225224083"
                },
                {
                    "start": 949,
                    "end": 953,
                    "matchedPaperCorpusId": "215746212"
                },
                {
                    "start": 1279,
                    "end": 1283,
                    "matchedPaperCorpusId": "203837038"
                },
                {
                    "start": 1344,
                    "end": 1348,
                    "matchedPaperCorpusId": "1340183"
                },
                {
                    "start": 1388,
                    "end": 1392,
                    "matchedPaperCorpusId": "215746212"
                },
                {
                    "start": 1478,
                    "end": 1482,
                    "matchedPaperCorpusId": "215744864"
                },
                {
                    "start": 1539,
                    "end": 1543,
                    "matchedPaperCorpusId": "215745844"
                },
                {
                    "start": 1706,
                    "end": 1710,
                    "matchedPaperCorpusId": "1340183"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2081298828125
        },
        {
            "corpus_id": "2434362",
            "title": "A Unified Model for Word Sense Representation and Disambiguation",
            "text": "Distributed representations for words were proposed in (Rumelhart et al., 1986) and have been successfully used in language models (Bengio et al., 2006;Mnih and Hinton, 2008) and many natural language processing tasks, such as word representation learning (Mikolov, 2012), named entity recognition (Turian et al., 2010), disambiguation , parsing and tagging (Socher et al., 2011;Socher et al., 2013). They are very useful in NLP tasks because they can be used as inputs to learning algorithms or as extra word features in NLP systems. Hence, many NLP applications, such as keyword extraction (Li-u et al., 2010;Liu et al., 2011b;Liu et al., 2012), social tag suggestion (Liu et al., 2011a) and text classification (Baker and McCallum, 1998), may also potentially benefit from distributed word representation. The main advantage is that the representations of similar words are close in vector space, which makes generalization to novel patterns easier and model estimation more robust. Word representations are hard to train due to the computational complexity. Recently, (Mikolov et al., 2013) proposed two particular models, Skipgram and CBOW, to learn word representations in large amounts of text data. The training objective of the CBOW model is to combine the representations of the surrounding words to predict the word in the middle, while the Skip-gram model's is to learn word representations that are good at predicting its context in the same sentence (Mikolov et al., 2013). Our paper uses the model architecture of Skip-gram.\n\nMost of the previous vector-space models use one representation per word. This is problematic because many words have multiple senses. The multi-prototype approach has been widely studied. (Reisinger and Mooney, 2010) proposed the multi-prototype vector-space model. (Huang et al., 2012) used the multi-prototype models to learn the vector for different senses of a word. All of these models use the clustering of contexts as a word sense and can not be directly used in word sense disambiguation.\n\nAfter our paper was submitted, we perceive the following recent advances: (Tian et al., 2014",
            "score": 0.36756420729934725,
            "section_title": "Word Representations",
            "char_start_offset": 25044,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 152,
                    "matchedPaperCorpusId": "53821397"
                },
                {
                    "start": 152,
                    "end": 173,
                    "matchedPaperCorpusId": "10097073"
                },
                {
                    "start": 298,
                    "end": 319,
                    "matchedPaperCorpusId": "629094"
                },
                {
                    "start": 358,
                    "end": 379,
                    "matchedPaperCorpusId": "18690358"
                },
                {
                    "start": 379,
                    "end": 399,
                    "matchedPaperCorpusId": "14687186"
                },
                {
                    "start": 611,
                    "end": 629,
                    "matchedPaperCorpusId": "8051179"
                },
                {
                    "start": 629,
                    "end": 646,
                    "matchedPaperCorpusId": "3397758"
                },
                {
                    "start": 670,
                    "end": 689,
                    "matchedPaperCorpusId": "11857586"
                },
                {
                    "start": 714,
                    "end": 740,
                    "matchedPaperCorpusId": "6146974"
                },
                {
                    "start": 1072,
                    "end": 1094,
                    "matchedPaperCorpusId": "5959482"
                },
                {
                    "start": 1464,
                    "end": 1486,
                    "matchedPaperCorpusId": "5959482"
                },
                {
                    "start": 1730,
                    "end": 1758,
                    "matchedPaperCorpusId": "2156506"
                },
                {
                    "start": 1808,
                    "end": 1828,
                    "matchedPaperCorpusId": "372093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.432861328125
        },
        {
            "corpus_id": "269983565",
            "title": "ECLIPSE: Semantic Entropy-LCS for Cross-Lingual Industrial Log Parsing",
            "text": "With the rapid advances in language modeling [2,4,13,31,33,37,38,41,45,47], and particularly the emergence of LLMs with Transformer-based architectures such as GPT-3.5,GPT-4 [28] and PaLM [1], Its excellent language understanding, generation, generalization, and reasoning capabilities greatly promote the integration of Natural Language Processor (NLP) and AIOps tasks.The integration of LLM with external databases and APIs further enhances its functionality [3,13,19,38], so that domain-specific knowledge can be more effectively integrated and continuously updated, especially when applied to the semantic analysis of logs, which greatly improves the accuracy of log parsing and anomaly detection [12,30,46].In addition, Retrieval Augmented Generation technology enables LLM to have the ability to access external knowledge sources, even when faced with more complex and knowledgeintensive tasks, generating answers that are more factual, specific, and diverse [2,21,45].In our work, Using LLM to reorder the representation vector of log keywords to highlight the most relevant results, and then with the powerful background knowledge of LLM, the most credible template matching length is selected for the result sequence, which effectively reduces the number of templates that need to be matched in the current input log and realizes the dual purpose of information retrieval enhancer and filter.Provide refined inputs for more accurate log parsing algorithms.",
            "score": 0.3675031796103757,
            "section_title": "LLM in log semantic parsing",
            "char_start_offset": 6903,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 370
                },
                {
                    "start": 370,
                    "end": 712
                },
                {
                    "start": 712,
                    "end": 975
                },
                {
                    "start": 975,
                    "end": 1401
                },
                {
                    "start": 1401,
                    "end": 1465
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 50,
                    "matchedPaperCorpusId": "202718969"
                },
                {
                    "start": 56,
                    "end": 59,
                    "matchedPaperCorpusId": "258833055"
                },
                {
                    "start": 65,
                    "end": 68,
                    "matchedPaperCorpusId": "220047192"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.313720703125
        },
        {
            "corpus_id": "269982020",
            "title": "Lessons from the Trenches on Reproducible Evaluation of Language Models",
            "text": "Recent work has explored the potential of various novel architectural designs to enable fully-subquadratic complexity in input sequence length while still achieving transformerlevel quality or better (See Table 2 for a number of references).However, tracking progress towards this goal requires a reliable set of evaluations that can 1) be used to compare fairly against baselines and 2) provide useful signal even at small scales of experimentation.\n\nAs shown in Section 5.2 and elsewhere in the literature, evaluating models on different prompts or differently-framed evaluation setups for the same evaluation \"task\" can render comparisons not meaningful.This is especially important in the case of small language models trained on novel architectures, as \"weaker\" models may be hypothetically less robust to evaluation noise or differences in evaluation setup.\n\nlm-eval has been used as a tool by many recent architecture releases to evaluate the performance of their proposed architecture against common baselines.We survey a number of recent releases, and note, for a number of commonly used benchmarks, whether researchers report their architecture's performance on that benchmark, and if they specifically state the usage of lm-eval to evaluate these tasks where applicable.\n\nThe selection of tasks we check are the following:\n\nWikitext-103 (\"Wiki\") (Merity et al., 2016): Wikitext-103 is a 103 million word language modeling dataset sourced from Wikipedia by Merity et al. (2016) to serve as a language modeling benchmark.It contains a training, validation, and test split, with a typical setup being to train a (small) model from scratch on the dataset and evaluate its test set perplexity (PPL).\n\nLong Range Arena (\"LRA\") (Tay et al., 2021): LRA is a sequence modeling dataset consisting of a suite of various tasks meant to test the long-range modeling abilities of models.While solving the more challenging longer-context tasks in LRA drove earlier work on long-context sequence models (Gu et al., 2022), subsequent work has shown that LRA may not correlate with desired downstream tasks for pretrained models (Alam et al., 2024).",
            "score": 0.36735038754546695,
            "section_title": "B.1 Case Study: Benchmarking Novel LM Architectures",
            "char_start_offset": 52857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 241,
                    "end": 450
                },
                {
                    "start": 452,
                    "end": 657
                },
                {
                    "start": 657,
                    "end": 863
                },
                {
                    "start": 865,
                    "end": 1018
                },
                {
                    "start": 1018,
                    "end": 1281
                },
                {
                    "start": 1283,
                    "end": 1333
                },
                {
                    "start": 1335,
                    "end": 1530
                },
                {
                    "start": 1530,
                    "end": 1705
                },
                {
                    "start": 1707,
                    "end": 1884
                },
                {
                    "start": 1884,
                    "end": 2142
                }
            ],
            "ref_mentions": [
                {
                    "start": 1732,
                    "end": 1750,
                    "matchedPaperCorpusId": "260440449"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.319580078125
        },
        {
            "corpus_id": "271244634",
            "title": "Exploring Advanced Large Language Models with LLMsuite",
            "text": "This tutorial paper has explored advancements and challenges in the development of Large Language Models (LLMs) like ChatGPT and Gemini.These models face limitations such as outdated knowledge, difficulty in complex computations, and generating incorrect information.To address these, several innovative techniques and frameworks have been introduced.Retrieval Augmented Generation (RAG) connects LLMs to current external databases, enhancing their accuracy and relevance.Program-Aided Language Models (PAL) use external code interpreters for precise computations, broadening LLM capabilities.LangChain, an open-source framework, simplifies the integration of LLMs with external data sources, enabling the development of domain-specific applications efficiently.It supports diverse applications such as chatbots and content generation without needing retraining or fine-tuning.Fine-tuning strategies like instruction fine-tuning, multitask fine-tuning, and parameter-efficient methods such as Low-Rank Adaptation (LoRA) and prompt tuning are discussed to mitigate catastrophic forgetting and improve performance.Reinforcement Learning from Human Feedback (RLHF) and Reinforced Self-Training (ReST) align LLMs with human preferences.RLHF uses human evaluations for iterative fine-tuning, while ReST combines reinforcement learning with self-training for efficiency and reduced computational costs.These methods refine LLM outputs to better meet user expectations.The paper also reviews transformer architectures that have revolutionized NLP, highlighting recent advancements for performance and efficiency improvements.Techniques for scaling model training beyond a single GPU, such as PyTorch's Distributed Data Parallel (DDP) and Fully Sharded Data Parallel (FSDP), along with the ZeRO stages for memory optimization, are discussed.Practical applications, such as customer service bots, demonstrate the benefits of integrating LLMs with real-time data and advanced reasoning strategies.These integrations enable LLMs to provide accurate, contextually relevant responses, enhancing user interactions.In summary, this tutorial presents a comprehensive examination of state-of-the-art advancements and challenges in LLM development.It introduces innovative techniques and frameworks to enhance LLM performance, reliability, and applicability.The overview of transformer architectures, fine-tuning strategies, and integration frameworks offers valuable insights for future research and practical applications in NLP.",
            "score": 0.3673355263477617,
            "section_title": "Conclusion",
            "char_start_offset": 54910,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 267
                },
                {
                    "start": 267,
                    "end": 351
                },
                {
                    "start": 351,
                    "end": 472
                },
                {
                    "start": 472,
                    "end": 593
                },
                {
                    "start": 593,
                    "end": 762
                },
                {
                    "start": 762,
                    "end": 877
                },
                {
                    "start": 877,
                    "end": 1112
                },
                {
                    "start": 1112,
                    "end": 1232
                },
                {
                    "start": 1232,
                    "end": 1396
                },
                {
                    "start": 1396,
                    "end": 1462
                },
                {
                    "start": 1462,
                    "end": 1618
                },
                {
                    "start": 1618,
                    "end": 1833
                },
                {
                    "start": 1833,
                    "end": 1987
                },
                {
                    "start": 1987,
                    "end": 2100
                },
                {
                    "start": 2100,
                    "end": 2230
                },
                {
                    "start": 2230,
                    "end": 2340
                },
                {
                    "start": 2340,
                    "end": 2513
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5341796875
        },
        {
            "corpus_id": "277780258",
            "title": "ExpertRAG: Efficient RAG with Mixture of Experts - Optimizing Context Retrieval for Adaptive LLM Responses",
            "text": "We have presented ExpertRAG, a new conceptual framework that marries the Mixture-of-Experts architecture with Retrieval-Augmented Generation. Through this integration, ExpertRAG brings together two powerful ideas in modern NLP: (1) sparse expert models for efficient scaling of language model capacity, and (2) dynamic retrieval of external knowledge for enhanced factual accuracy and up-to-date information. We detailed how ExpertRAG is constructedintroducing a retrieval gating mechanism that decides when to consult external memory and an expert routing mechanism that distributes the query and context across specialized sub-networks. Our theoretical analysis provided insights into why this approach is compelling: we showed that selective retrieval can yield significant efficiency gains without sacrificing performance, and we quantified how MoE routing keeps computation scalable even as we increase model parameters. We also formulated the balance between retrieved and parametric knowledge in a probabilistic mixture form, giving a principled view of ExpertRAG's decision-making. Comparatively, ExpertRAG addresses limitations of prior models. It improves upon standard RAG by avoiding needless retrieval and leveraging more parametric knowledge when appropriate, and it extends MoE models by providing a way to handle queries beyond the model's stored knowledge. In our discussion, we positioned ExpertRAG as a middle ground between oneshot retrieval models and agentic multi-step systems, aiming to capture much of the benefit of the latter within the efficiency of the former. The proposed experiments will evaluate these claims rigorously, and we expect to confirm improvements in both accuracy and speed against strong baselines on knowledge-intensive benchmarks. \n\nComparatively, ExpertRAG addresses limitations of prior models. It improves upon standard RAG by avoiding needless retrieval and leveraging more parametric knowledge when appropriate, and it extends MoE models by providing a way to handle queries beyond the model's stored knowledge. In our discussion, we positioned ExpertRAG as a middle ground between one-shot retrieval models and agentic multi-step systems, aiming to capture much of the benefit of the latter within the efficiency of the former. The proposed experiments will evaluate these claims rigorously, and we expect to confirm improvements in both accuracy and speed against strong baselines on knowledge-intensive benchmarks. \n\nThis work opens several future research directions and challenges to be addressed:",
            "score": 0.3673091203261256,
            "section_title": "Conclusion",
            "char_start_offset": 79816,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1778
                },
                {
                    "start": 1781,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2281
                },
                {
                    "start": 2282,
                    "end": 2470
                },
                {
                    "start": 2473,
                    "end": 2555
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7255859375
        }
    ],
    "quotes": {
        "cost": 0.24326699999999998,
        "quotes": [
            {
                "idx": 0,
                "key": "[218869575 | Lewis et al. | 2020 | Citations: 6476]",
                "snippets": "We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 671,
                        "end": 1224,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[252735056 | Siriwardhana et al. | 2022 | Citations: 179]",
                "snippets": "Recently, Retrieval Augmented Architectures (Lewis et al., 2020b;Guu et al., 2020) have drawn a lot of attention due to their explainable, scalable, and adaptable nature. Unlike other open-domain QA architectures, RAG (Lewis et al., 2020b) combines the information retrieval stage and answer generation stage in a differentiable manner. It uses a combination of parametric and non-parametric memory, where the parametric memory consists of a pre-trained seq2seq BART (Lewis et al., 2019) generator, and the non-parametric memory consists of dense vector representations of Wikipedia articles indexed with the FAISS library (Johnson et al., 2017). RAG first encodes a question into a dense representation, retrieves the relevant passages from an indexed Wikipedia knowledge base, and then feeds them into the generator. The loss function can finetune both the generator and the question encoder at the same time. Lewis et al. (Lewis et al., 2020b) highlight RAG's ability to perform well in Wikipedia-based general question-answering datasets like Natural Questions (Kwiatkowski et al., 2019)...(Guu et al., 2020) is a similar Retrieval Augmented model to RAG. REALM introduced a novel masked language pre-training step that involves an end-to-end trainable retriever.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Retrieval Augmented Architecture",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1091,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 170
                            },
                            {
                                "start": 171,
                                "end": 336
                            },
                            {
                                "start": 337,
                                "end": 646
                            },
                            {
                                "start": 647,
                                "end": 818
                            },
                            {
                                "start": 819,
                                "end": 911
                            },
                            {
                                "start": 912,
                                "end": 1092
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recently, Retrieval Augmented Architectures (Lewis et al., 2020b;Guu et al., 2020) have drawn a lot of attention due to their explainable, scalable, and adaptable nature. Unlike other open-domain QA architectures, RAG (Lewis et al., 2020b) combines the information retrieval stage and answer generation stage in a differentiable manner. It uses a combination of parametric and non-parametric memory, where the parametric memory consists of a pre-trained seq2seq BART (Lewis et al., 2019) generator, and the non-parametric memory consists of dense vector representations of Wikipedia articles indexed with the FAISS library (Johnson et al., 2017). RAG first encodes a question into a dense representation, retrieves the relevant passages from an indexed Wikipedia knowledge base, and then feeds them into the generator. The loss function can finetune both the generator and the question encoder at the same time. Lewis et al. (Lewis et al., 2020b) highlight RAG's ability to perform well in Wikipedia-based general question-answering datasets like Natural Questions (Kwiatkowski et al., 2019)"
                    },
                    {
                        "section_title": "Retrieval Augmented Architecture",
                        "pdf_hash": "",
                        "start": 2024,
                        "end": 2198,
                        "sentence_offsets": [
                            {
                                "start": 2024,
                                "end": 2089
                            },
                            {
                                "start": 2090,
                                "end": 2197
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "(Guu et al., 2020) is a similar Retrieval Augmented model to RAG. REALM introduced a novel masked language pre-training step that involves an end-to-end trainable retriever."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[252735160 | Chen et al. | 2022 | Citations: 159]",
                "snippets": "Retrieval Augmented Models Retrieval augmented models are hybrid models containing both parameterized sequence models and a nonparametric memory, infusing world knowledge into existing language models. Among them, KNN-LM (Khandelwal et al., 2019) was first proposed to retrieve instances from a text training corpus to help language modeling. Later, RETRO (Borgeaud et al., 2021) was proposed to scale up the text corpus to trillions of tokens, enabling the model to achieve similar perplexity to GPT-3 (Brown et al., 2020)) with 25x fewer model parameters. Another family of models, such as REALM (Guu et al., 2020), RAG (Lewis et al., 2020), and FiD (Izacard et al., 2020), integrate Wikipedia passages as a datastore to benefit downstream knowledge intensive tasks (e.g. Question Answering). REALM is an encoder-only model trained with masked lan-guage modeling, while RAG and FiD adopt an encoder-decoder model with a generative language modeling objective.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218869575 | Lewis et al. | 2020 | Citations: 6476]": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                    "[207870430 | Khandelwal et al. | 2019 | Citations: 842]": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.",
                    "[211204736 | Guu et al. | 2020 | Citations: 2119]": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.",
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                    "[220302360 | Izacard et al. | 2020 | Citations: 1181]": "Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 963,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "207870430",
                            "218971783",
                            "211204736",
                            "218869575",
                            "220302360"
                        ],
                        "quote": "Retrieval Augmented Models Retrieval augmented models are hybrid models containing both parameterized sequence models and a nonparametric memory, infusing world knowledge into existing language models. Among them, KNN-LM (Khandelwal et al., 2019) was first proposed to retrieve instances from a text training corpus to help language modeling. Later, RETRO (Borgeaud et al., 2021) was proposed to scale up the text corpus to trillions of tokens, enabling the model to achieve similar perplexity to GPT-3 (Brown et al., 2020)) with 25x fewer model parameters. Another family of models, such as REALM (Guu et al., 2020), RAG (Lewis et al., 2020), and FiD (Izacard et al., 2020), integrate Wikipedia passages as a datastore to benefit downstream knowledge intensive tasks (e.g. Question Answering). REALM is an encoder-only model trained with masked lan-guage modeling, while RAG and FiD adopt an encoder-decoder model with a generative language modeling objective."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[256389797 | Shi et al. | 2023 | Citations: 641]",
                "snippets": "Retrieval-augmented Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language modeling (Min et al., 2022;(Borgeaud et al., 2021)(Khandelwal et al., 2019) and open-domain question answering (Lewis et al., 2020;Izacard et al., 2022b;Hu et al., 2022). Specifically, using the input as query, (1) a retriever first retrieves a set of documents (i.e., sequences of tokens) from a corpus and then (2) a language model incorporates the retrieved documents as additional information to make a final prediction. This style of retrieval can be added to both encoderdecoder (Yu, 2022)Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2019)(Borgeaud et al., 2021)Shi et al., 2022;(Rubin et al., 2021). For example, Atlas (Izacard et al., 2022b) finetunes an encoder-decoder model jointly with the retriever by modeling documents as latent variables, while RETRO (Borgeaud et al., 2021) changes the decoderonly architecture to incorporate retrieved texts and pretrains the language model from scratch. Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs. Another line of retrieval-augmented LMs such as kNN-LM (Khandelwal et al., 2019)Zhong et al., 2022) retrieves a set of tokens and interpolates between the LM's next token distribution and kNN distributions computed from the retrieved tokens at inference.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207870430 | Khandelwal et al. | 2019 | Citations: 842]": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.",
                    "[244954723 | Borgeaud et al. | 2021 | Citations: 1100]": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",
                    "[245218561 | Rubin et al. | 2021 | Citations: 709]": "In-context learning is a recent paradigm in natural language understanding, where a large pre-trained language model (LM) observes a test instance and a few training examples as its input, and directly decodes the output without any update to its parameters. However, performance has been shown to strongly depend on the selected training examples (termed prompts). In this work, we propose an efficient method for retrieving prompts for in-context learning using annotated data and an LM. Given an input-output pair, we estimate the probability of the output given the input and a candidate training example as the prompt, and label training examples as positive or negative based on this probability. We then train an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time. We evaluate our approach on three sequence-to-sequence tasks where language utterances are mapped to meaning representations, and find that it substantially outperforms prior work and multiple baselines across the board.",
                    "[250391000 | Yu | 2022 | Citations: 42]": "Retrieval-augmented generation (RAG) methods have been receiving increasing attention from the NLP community and achieved state-of-the-art performance on many NLP downstream tasks. Compared with conventional pre-trained generation models, RAG methods have remarkable advantages such as easy knowledge acquisition, strong scalability, and low training cost. Although existing RAG models have been applied to various knowledge-intensive NLP tasks, such as open-domain QA and dialogue systems, most of the work has focused on retrieving unstructured text documents from Wikipedia. In this paper, I first elaborate on the current obstacles to retrieving knowledge from a single-source homogeneous corpus. Then, I demonstrate evidence from both existing literature and my experiments, and provide multiple solutions on retrieval-augmented generation methods across heterogeneous knowledge."
                },
                "metadata": [
                    {
                        "section_title": "Background and Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1509,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 380
                            },
                            {
                                "start": 381,
                                "end": 634
                            },
                            {
                                "start": 635,
                                "end": 836
                            },
                            {
                                "start": 837,
                                "end": 1135
                            },
                            {
                                "start": 1136,
                                "end": 1254
                            },
                            {
                                "start": 1255,
                                "end": 1509
                            }
                        ],
                        "ref_mentions": [
                            "244954723",
                            "207870430",
                            "250391000",
                            "207870430",
                            "244954723",
                            "245218561",
                            "244954723",
                            "207870430"
                        ],
                        "quote": "Retrieval-augmented Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language modeling (Min et al., 2022;(Borgeaud et al., 2021)(Khandelwal et al., 2019) and open-domain question answering (Lewis et al., 2020;Izacard et al., 2022b;Hu et al., 2022). Specifically, using the input as query, (1) a retriever first retrieves a set of documents (i.e., sequences of tokens) from a corpus and then (2) a language model incorporates the retrieved documents as additional information to make a final prediction. This style of retrieval can be added to both encoderdecoder (Yu, 2022)Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2019)(Borgeaud et al., 2021)Shi et al., 2022;(Rubin et al., 2021). For example, Atlas (Izacard et al., 2022b) finetunes an encoder-decoder model jointly with the retriever by modeling documents as latent variables, while RETRO (Borgeaud et al., 2021) changes the decoderonly architecture to incorporate retrieved texts and pretrains the language model from scratch. Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs. Another line of retrieval-augmented LMs such as kNN-LM (Khandelwal et al., 2019)Zhong et al., 2022) retrieves a set of tokens and interpolates between the LM's next token distribution and kNN distributions computed from the retrieved tokens at inference."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[259360590 | Lyu et al. | 2023 | Citations: 18]",
                "snippets": "Retrieval-augmented (RAG) models have recently been proposed [12,14]8]. A typical retrieval-augmented model consists of two parts, a retriever f ret and a generator f gen. Given a retrieval corpus D ret = {d 1, \u2022 \u2022 \u2022, d M}, the retriever f ret retrieves K data points for an input x i as f ret (x i, D ret) = {d \u03b11, d \u03b12,..., d \u03b1 K}. Here, \u03b1 k denotes the rank of each data point in the retrieval corpus assigned by the retriever.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249375466 | Yuan et al. | 2022 | Citations: 95]": "Training foundation models, such as GPT-3 and PaLM, can be extremely expensive, often involving tens of thousands of GPUs running continuously for months. These models are typically trained in specialized clusters featuring fast, homogeneous interconnects and using carefully designed software systems that support both data parallelism and model/pipeline parallelism. Such dedicated clusters can be costly and difficult to obtain. Can we instead leverage the much greater amount of decentralized, heterogeneous, and lower-bandwidth interconnected compute? Previous works examining the heterogeneous, decentralized setting focus on relatively small models that can be trained in a purely data parallel manner. State-of-the-art schemes for model parallel foundation model training, such as Megatron, only consider the homogeneous data center setting. In this paper, we present the first study of training large foundation models with model parallelism in a decentralized regime over a heterogeneous network. Our key technical contribution is a scheduling algorithm that allocates different computational\"tasklets\"in the training of foundation models to a group of decentralized GPU devices connected by a slow heterogeneous network. We provide a formal cost model and further propose an efficient evolutionary algorithm to find the optimal allocation strategy. We conduct extensive experiments that represent different scenarios for learning over geo-distributed devices simulated using real-world network measurements. In the most extreme case, across 8 different cities spanning 3 continents, our approach is 4.8X faster than prior state-of-the-art training systems (Megatron)."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 904,
                        "end": 1226,
                        "sentence_offsets": [
                            {
                                "start": 838,
                                "end": 939
                            },
                            {
                                "start": 940,
                                "end": 1110
                            },
                            {
                                "start": 1113,
                                "end": 1140
                            },
                            {
                                "start": 1141,
                                "end": 1238
                            }
                        ],
                        "ref_mentions": [
                            "249375466"
                        ],
                        "quote": "Retrieval-augmented (RAG) models have recently been proposed [12,14]8]. A typical retrieval-augmented model consists of two parts, a retriever f ret and a generator f gen. Given a retrieval corpus D ret = {d 1, \u2022 \u2022 \u2022, d M}, the retriever f ret retrieves K data points for an input x i as f ret (x i, D ret) = {d \u03b11, d \u03b12,"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1125,
                        "end": 1231,
                        "sentence_offsets": [
                            {
                                "start": 1113,
                                "end": 1140
                            },
                            {
                                "start": 1141,
                                "end": 1238
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ", d \u03b1 K}. Here, \u03b1 k denotes the rank of each data point in the retrieval corpus assigned by the retriever."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[260900354 | Huang et al. | 2023 | Citations: 35]",
                "snippets": "Retrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2022)(Lewis et al., 2020) and decoderonly (Khandelwal et al., 2019)Borgeaud et al., 2022;(Shi et al., 2022) architectures. For decoder-only LMs, the computational cost typically increases quadratically with the input length, as well as with the number of retrieval passages. In contrast, for encoder-decoder LMs with a Fusion-in-Decoder architecture, the computation cost grows linearly with the number of retrieved passages, as they only perform self-attention over one passage at a time (Izacard et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218869575 | Lewis et al. | 2020 | Citations: 6476]": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                    "[207870430 | Khandelwal et al. | 2019 | Citations: 842]": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.",
                    "[220302360 | Izacard et al. | 2020 | Citations: 1181]": "Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.",
                    "[249152130 | Shi et al. | 2022 | Citations: 32]": "Retrieval-augmented language models (LMs) use non-parametric memory to substantially outperform their non-retrieval counterparts on perplexity-based evaluations, but it is an open question whether they achieve similar gains in few- and zero-shot end-task accuracy. We extensively study one such model, the k-nearest neighbor LM (kNN-LM), showing that the gains marginally transfer. The main challenge is to achieve coverage of the verbalizer tokens that define the different end-task class labels. To address this challenge, we also introduce kNN-Prompt, a simple and effective kNN-LM with automatically expanded fuzzy verbalizers (e.g. to expand \u201cterrible\u201d to also include \u201csilly\u201d and other task-specific synonyms for sentiment classification). Across nine diverse end-tasks, using kNN-Prompt with GPT-2 large yields significant performance boosts over strong zeroshot baselines (13.4% absolute improvement over the base LM on average). We also show that other advantages of non-parametric augmentation hold for end tasks; kNN-Prompt is effective for domain adaptation with no further training, and gains increase with the size of the retrieval model.",
                    "[251371732 | Izacard et al. | 2022 | Citations: 783]": "Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters."
                },
                "metadata": [
                    {
                        "section_title": "Background and Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 910,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 141
                            },
                            {
                                "start": 142,
                                "end": 320
                            },
                            {
                                "start": 321,
                                "end": 519
                            },
                            {
                                "start": 520,
                                "end": 671
                            },
                            {
                                "start": 672,
                                "end": 910
                            }
                        ],
                        "ref_mentions": [
                            "251371732",
                            "218869575",
                            "207870430",
                            "249152130",
                            "220302360"
                        ],
                        "quote": "Retrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2022)(Lewis et al., 2020) and decoderonly (Khandelwal et al., 2019)Borgeaud et al., 2022;(Shi et al., 2022) architectures. For decoder-only LMs, the computational cost typically increases quadratically with the input length, as well as with the number of retrieval passages. In contrast, for encoder-decoder LMs with a Fusion-in-Decoder architecture, the computation cost grows linearly with the number of retrieved passages, as they only perform self-attention over one passage at a time (Izacard et al., 2020)."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[261030382 | Tang et al. | 2023 | Citations: 26]",
                "snippets": "Retrieval-augmented language models (R-LMs) utilize retrieval-based techniques to improve the performance of LMs and can be divided into two main categories: block R-LMs and token R-LMs. Block R-LMs [12], [13], [48] are similar to one-shot or few-shot learning [49], where one or a few examples are retrieved from a database instead of being randomly selected. Token R-LMs [15], [24], [50] retrieve tokens from database and then combine the retrieved results into the LM. Compared with block R-LMs, token R-LMs can update retrieval results at the same time of generating new tokens, hence our approach uses the architecture of token R-LM. However, token R-LMs suffer from high storage costs and require hyper-parameters selection to combine the inference results from the database and language model.\n\nVarious approaches have been proposed to address the limitations of token R-LMs. For example, kNN-Adapter [51] uses a trained network to determine the combination weights. To reduce the search cost, RetoMaton [52] uses the automaton states to save search time, while AdaptRet [53] uses a trained network to decide whether to use the retrieval module. GNN-LM [50] selects similar texts and builds a contextual graph to incorporate into the language model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207870430 | Khandelwal et al. | 2019 | Citations: 842]": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.",
                    "[211204736 | Guu et al. | 2020 | Citations: 2119]": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.",
                    "[222125236 | Khandelwal et al. | 2020 | Citations: 286]": "We introduce $k$-nearest-neighbor machine translation ($k$NN-MT), which predicts tokens with a nearest neighbor classifier over a large datastore of cached examples, using representations from a neural translation model for similarity search. This approach requires no additional training and scales to give the decoder direct access to billions of examples at test time, resulting in a highly expressive model that consistently improves performance across many settings. Simply adding nearest neighbor search improves a state-of-the-art German-English translation model by 1.5 BLEU. $k$NN-MT allows a single model to be adapted to diverse domains by using a domain-specific datastore, improving results by an average of 9.2 BLEU over zero-shot transfer, and achieving new state-of-the-art results---without training on these domains. A massively multilingual model can also be specialized for particular language pairs, with improvements of 3 BLEU for translating from English into German and Chinese. Qualitatively, $k$NN-MT is easily interpretable; it combines source and target context to retrieve highly relevant examples.",
                    "[237452184 | He et al. | 2021 | Citations: 104]": "Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time, significantly increasing the inference overhead and thus limiting the deployment of non-parametric NLMs in practical applications. In this paper, we take the recently proposed k-nearest neighbors language model as an example, exploring methods to improve its efficiency along various dimensions. Experiments on the standard WikiText-103 benchmark and domain-adaptation datasets show that our methods are able to achieve up to a 6x speed-up in inference speed while retaining comparable performance. The empirical analysis we present may provide guidelines for future research seeking to develop or deploy more efficient non-parametric NLMs.",
                    "[239016943 | Meng et al. | 2021 | Citations: 39]": "Inspired by the notion that ``{\\it to copy is easier than to memorize}``, in this work, we introduce GNN-LM, which extends the vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. \\footnote{The code can be found at https://github.com/ShannonAI/GNN-LM",
                    "[246431219 | Alon et al. | 2022 | Citations: 63]": "Retrieval-based language models (R-LM) model the probability of natural language text by combining a standard language model (LM) with examples retrieved from an external datastore at test time. While effective, a major bottleneck of using these models in practice is the computationally costly datastore search, which can be performed as frequently as every time step. In this paper, we present RetoMaton - retrieval automaton - which approximates the datastore search, based on (1) saving pointers between consecutive datastore entries, and (2) clustering of entries into\"states\". This effectively results in a weighted finite automaton built on top of the datastore, instead of representing the datastore as a flat list. The creation of the automaton is unsupervised, and a RetoMaton can be constructed from any text collection: either the original training corpus or from another domain. Traversing this automaton at inference time, in parallel to the LM inference, reduces its perplexity by up to 1.85, or alternatively saves up to 83% of the nearest neighbor searches over $k$NN-LM (Khandelwal et al., 2020) without hurting perplexity. Our code and trained models are available at https://github.com/neulab/retomaton .",
                    "[247450969 | Lu et al. | 2022 | Citations: 147]": "Code completion, which aims to predict the following code token(s) according to the code context, can improve the productivity of software development. Recent work has proved that statistical language modeling with transformers can greatly improve the performance in the code completion task via learning from large-scale source code datasets. However, current approaches focus only on code context within the file or project, i.e. internal context. Our distinction is utilizing \u201dexternal\u201d context, inspired by human behaviors of copying from the related code snippets when writing code. Specifically, we propose a retrieval-augmented code completion framework, leveraging both lexical copying and referring to code with similar semantics by retrieval. We adopt a stage-wise training approach that combines a source code retriever and an auto-regressive language model for programming language. We evaluate our approach in the code completion task in Python and Java programming languages, achieving a state-of-the-art performance on CodeXGLUE benchmark."
                },
                "metadata": [
                    {
                        "section_title": "B. Retrieval-augment Language Model",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1256,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 186
                            },
                            {
                                "start": 187,
                                "end": 360
                            },
                            {
                                "start": 361,
                                "end": 471
                            },
                            {
                                "start": 472,
                                "end": 638
                            },
                            {
                                "start": 639,
                                "end": 800
                            },
                            {
                                "start": 803,
                                "end": 883
                            },
                            {
                                "start": 884,
                                "end": 974
                            },
                            {
                                "start": 975,
                                "end": 1153
                            },
                            {
                                "start": 1154,
                                "end": 1257
                            }
                        ],
                        "ref_mentions": [
                            "211204736",
                            "247450969",
                            "207870430",
                            "222125236",
                            "239016943",
                            "246431219",
                            "237452184",
                            "239016943"
                        ],
                        "quote": "Retrieval-augmented language models (R-LMs) utilize retrieval-based techniques to improve the performance of LMs and can be divided into two main categories: block R-LMs and token R-LMs. Block R-LMs [12], [13], [48] are similar to one-shot or few-shot learning [49], where one or a few examples are retrieved from a database instead of being randomly selected. Token R-LMs [15], [24], [50] retrieve tokens from database and then combine the retrieved results into the LM. Compared with block R-LMs, token R-LMs can update retrieval results at the same time of generating new tokens, hence our approach uses the architecture of token R-LM. However, token R-LMs suffer from high storage costs and require hyper-parameters selection to combine the inference results from the database and language model.\n\nVarious approaches have been proposed to address the limitations of token R-LMs. For example, kNN-Adapter [51] uses a trained network to determine the combination weights. To reduce the search cost, RetoMaton [52] uses the automaton states to save search time, while AdaptRet [53] uses a trained network to decide whether to use the retrieval module. GNN-LM [50] selects similar texts and builds a contextual graph to incorporate into the language model."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[265308533 | Munikoti et al. | 2023 | Citations: 8]",
                "snippets": "The retrieval augmented language models (RALM) primarily address the grounding and scalability challenges in standard language models (LM). RALM aims to address these limitations by combining a LM with an external knowledge base. In this framework, the LM generates text conditioned not only on the input query but also on relevant knowledge retrieved from the knowledge base. The retrieved knowledge is usually the text chunks or passages from documents that provide factual grounding to contextualize the model's pre-Preprint version dictions. In other words, this approach decentralizes model knowledge into parameters and external knowledge sources, thereby addressing the challenges of scalability and adaptability.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 210,
                        "end": 930,
                        "sentence_offsets": [
                            {
                                "start": 210,
                                "end": 349
                            },
                            {
                                "start": 350,
                                "end": 439
                            },
                            {
                                "start": 440,
                                "end": 586
                            },
                            {
                                "start": 587,
                                "end": 755
                            },
                            {
                                "start": 756,
                                "end": 930
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The retrieval augmented language models (RALM) primarily address the grounding and scalability challenges in standard language models (LM). RALM aims to address these limitations by combining a LM with an external knowledge base. In this framework, the LM generates text conditioned not only on the input query but also on relevant knowledge retrieved from the knowledge base. The retrieved knowledge is usually the text chunks or passages from documents that provide factual grounding to contextualize the model's pre-Preprint version dictions. In other words, this approach decentralizes model knowledge into parameters and external knowledge sources, thereby addressing the challenges of scalability and adaptability."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[265594594 | Abdelazim et al. | 2023 | Citations: 8]",
                "snippets": "Retrieval Augmented Generation (RAG), introduced by Facebook Researchers in 2020 [1], is a pivotal AI framework facilitating information retrieval for Generative AI models, thereby enhancing their accuracy and capabilities. RAG empowers Large Language Models (LLMs) by granting them access to external knowledge sources, augmenting the content generation process. This dual functionality entails retrieval, wherein RAG meticulously selects pertinent information from provided sources and generation, whereby LLMs craft contextually relevant responses based on user input.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "I. INTRODUCTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 571,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 223
                            },
                            {
                                "start": 224,
                                "end": 363
                            },
                            {
                                "start": 364,
                                "end": 571
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Retrieval Augmented Generation (RAG), introduced by Facebook Researchers in 2020 [1], is a pivotal AI framework facilitating information retrieval for Generative AI models, thereby enhancing their accuracy and capabilities. RAG empowers Large Language Models (LLMs) by granting them access to external knowledge sources, augmenting the content generation process. This dual functionality entails retrieval, wherein RAG meticulously selects pertinent information from provided sources and generation, whereby LLMs craft contextually relevant responses based on user input."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[266163023 | Khosla et al. | 2023 | Citations: 2]",
                "snippets": "Retrieval Augmented Language Model (REALM) [27] is the first method to jointly train a knowledge retriever and a knowledge-augmented language encoder in an unsupervised manner. Retrieval augmented generation (RAG) [6] fine-tunes a pre-trained retriever (e.g., DPR [28]) and a pre-trained sequence-to-sequence model (e.g., BART [29]). RAG achieves superior performance on various knowledge-intensive tasks, including question answering, question generation and fact verification. Retrieval Augmented Translation (RAT) (Hoang et al., 2022) improves neural machine translation by treating the external knowledge base as a dictionary...Retrieval Enhanced Transformers (RETRO) [5] augments a language model with an external knowledge base consisting of 2 trillion tokens, and achieves performance comparable to GPT-3 (Brown et al., 2020) and Jurassic-1 [33], which has 25x more parameters.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                    "[252815975 | Hoang et al. | 2022 | Citations: 9]": "We explore zero-shot adaptation, where a general-domain model has access to customer or domain specific parallel data at inference time, but not during training. We build on the idea of Retrieval Augmented Translation (RAT) where top-k in-domain fuzzy matches are found for the source sentence, and target-language translations of those fuzzy-matched sentences are provided to the translation model at inference time. We propose a novel architecture to control interactions between a source sentence and the top-k fuzzy target-language matches, and compare it to architectures from prior work. We conduct experiments in two language pairs (En-De and En-Fr) by training models on WMT data and testing them with five and seven multi-domain datasets, respectively. Our approach consistently outperforms the alternative architectures, improving BLEU across language pair, domain, and number k of fuzzy matches."
                },
                "metadata": [
                    {
                        "section_title": "Natural Language Processing",
                        "pdf_hash": "",
                        "start": 308,
                        "end": 921,
                        "sentence_offsets": [
                            {
                                "start": 308,
                                "end": 484
                            },
                            {
                                "start": 485,
                                "end": 641
                            },
                            {
                                "start": 642,
                                "end": 786
                            },
                            {
                                "start": 787,
                                "end": 922
                            }
                        ],
                        "ref_mentions": [
                            "252815975"
                        ],
                        "quote": "Retrieval Augmented Language Model (REALM) [27] is the first method to jointly train a knowledge retriever and a knowledge-augmented language encoder in an unsupervised manner. Retrieval augmented generation (RAG) [6] fine-tunes a pre-trained retriever (e.g., DPR [28]) and a pre-trained sequence-to-sequence model (e.g., BART [29]). RAG achieves superior performance on various knowledge-intensive tasks, including question answering, question generation and fact verification. Retrieval Augmented Translation (RAT) (Hoang et al., 2022) improves neural machine translation by treating the external knowledge base as a dictionary"
                    },
                    {
                        "section_title": "Natural Language Processing",
                        "pdf_hash": "",
                        "start": 1449,
                        "end": 1686,
                        "sentence_offsets": [
                            {
                                "start": 1449,
                                "end": 1685
                            }
                        ],
                        "ref_mentions": [
                            "218971783"
                        ],
                        "quote": "Retrieval Enhanced Transformers (RETRO) [5] augments a language model with an external knowledge base consisting of 2 trillion tokens, and achieves performance comparable to GPT-3 (Brown et al., 2020) and Jurassic-1 [33], which has 25x more parameters."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[267061013 | Elgedawy et al. | 2024 | Citations: 6]",
                "snippets": "Retrieval augmented generation (RAG) refers to a novel technique where a model first retrieves relevant information from a large corpus or dataset and then generates responses or outputs based on the retrieved information. This approach combines the benefits of both retrieval-based and generation-based methods, allowing for more contextually relevant and informative responses in conversational systems. Recent research works are exploring the trade-offs between this method and traditional fine-tuning approaches for language models, with some favoring RAG for certain contexts [7]23].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 1939,
                        "end": 2527,
                        "sentence_offsets": [
                            {
                                "start": 1939,
                                "end": 2161
                            },
                            {
                                "start": 2162,
                                "end": 2344
                            },
                            {
                                "start": 2345,
                                "end": 2527
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Retrieval augmented generation (RAG) refers to a novel technique where a model first retrieves relevant information from a large corpus or dataset and then generates responses or outputs based on the retrieved information. This approach combines the benefits of both retrieval-based and generation-based methods, allowing for more contextually relevant and informative responses in conversational systems. Recent research works are exploring the trade-offs between this method and traditional fine-tuning approaches for language models, with some favoring RAG for certain contexts [7]23]."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[268031947 | Cao et al. | 2024 | Citations: 8]",
                "snippets": "kNN-LM (Khandelwal et al., 2020) is a retrieval-augmented LM that interpolates the next-token distribution of the base LM with a k-nearest neighbors (kNN) model.\n\nRETRO (Borgeaud et al., 2022) is a retrieval-augmented LM incorporated with a pre-trained document retriever, a document encoder and a cross-attention mechanism.\n\nCoG (Lan et al., 2023) is another retrieval-augmented LM that adopts a two-stage search pipeline. It first retrieves semantically-relevant documents, and then considers all n-grams within them as candidate phrases.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207870430 | Khandelwal et al. | 2019 | Citations: 842]": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.",
                    "[244954723 | Borgeaud et al. | 2021 | Citations: 1100]": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",
                    "[259298789 | Lan et al. | 2023 | Citations: 30]": "The dominant text generation models compose the output by sequentially selecting words from a fixed vocabulary. In this paper, we formulate text generation as progressively copying text segments (e.g., words or phrases) from an existing text collection. We compute the contextualized representations of meaningful text segments and index them using efficient vector search toolkits. The task of text generation is then decomposed into a series of copy-and-paste operations: at each time step, we seek suitable text spans from the text collection rather than selecting from a standalone vocabulary. Experiments on the standard language modeling benchmark (WikiText-103) show that our approach achieves better generation quality according to both automatic and human evaluations. Besides, its inference efficiency is comparable to token-level autoregressive models thanks to the reduction of decoding steps. We also show that our approach allows for effective domain adaptation by simply switching to domain-specific text collection without extra training. Finally, we observe that our approach attains additional performance gains by simply scaling up to larger text collections, again without further training.\\footnote{Our source codes are publicly available at \\url{https://github.com/gmftbyGMFTBY/Copyisallyouneed}.}"
                },
                "metadata": [
                    {
                        "section_title": "BASELINES",
                        "pdf_hash": "",
                        "start": 253,
                        "end": 793,
                        "sentence_offsets": [
                            {
                                "start": 162,
                                "end": 271
                            },
                            {
                                "start": 272,
                                "end": 331
                            },
                            {
                                "start": 334,
                                "end": 495
                            },
                            {
                                "start": 498,
                                "end": 661
                            },
                            {
                                "start": 664,
                                "end": 763
                            },
                            {
                                "start": 764,
                                "end": 880
                            }
                        ],
                        "ref_mentions": [
                            "160025533",
                            "207870430",
                            "244954723",
                            "259298789"
                        ],
                        "quote": "kNN-LM (Khandelwal et al., 2020) is a retrieval-augmented LM that interpolates the next-token distribution of the base LM with a k-nearest neighbors (kNN) model.\n\nRETRO (Borgeaud et al., 2022) is a retrieval-augmented LM incorporated with a pre-trained document retriever, a document encoder and a cross-attention mechanism.\n\nCoG (Lan et al., 2023) is another retrieval-augmented LM that adopts a two-stage search pipeline. It first retrieves semantically-relevant documents, and then considers all n-grams within them as candidate phrases."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[268248911 | Asai et al. | 2024 | Citations: 61]",
                "snippets": "The concept of retrieval augmentation has been extensively explored across various machine learning domains (Tian et al., 2019). In NLP, earlier efforts have been applied to specific tasks such as QA and machine translation. Chen et al. (2017) introduce DrQA, which combines a term-based information retrieval (IR) system with a neural QA model to answer knowledge-intensive questions. While IR and such task LMs were initially studied separately, several work explores more organic combinations of retrieval and LM by pre-training the two components jointly or sequentially, including REALM (Guu et al., 2020), RAG (Lewis et al., 2020a), RETRO (Borgeaud et al., 2022), etc.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[3618568 | Chen et al. | 2017 | Citations: 2019]": "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.",
                    "[57759353 | Tian et al. | 2019 | Citations: 147]": "Human perception of 3D shapes goes beyond reconstructing them as a set of points or a composition of geometric primitives: we also effortlessly understand higher-level shape structure such as the repetition and reflective symmetry of object parts. In contrast, recent advances in 3D shape sensing focus more on low-level geometry but less on these higher-level relationships. In this paper, we propose 3D shape programs, integrating bottom-up recognition systems with top-down, symbolic program structure to capture both low-level geometry and high-level structural priors for 3D shapes. Because there are no annotations of shape programs for real shapes, we develop neural modules that not only learn to infer 3D shape programs from raw, unannotated shapes, but also to execute these programs for shape reconstruction. After initial bootstrapping, our end-to-end differentiable model learns 3D shape programs by reconstructing shapes in a self-supervised manner. Experiments demonstrate that our model accurately infers and executes 3D shape programs for highly complex shapes from various categories. It can also be integrated with an image-to-shape module to infer 3D shape programs directly from an RGB image, leading to 3D shape reconstructions that are both more accurate and more physically plausible."
                },
                "metadata": [
                    {
                        "section_title": "How Can Retrieval-Augmented LMs",
                        "pdf_hash": "",
                        "start": 528,
                        "end": 1202,
                        "sentence_offsets": [
                            {
                                "start": 298,
                                "end": 531
                            },
                            {
                                "start": 532,
                                "end": 648
                            },
                            {
                                "start": 651,
                                "end": 687
                            },
                            {
                                "start": 690,
                                "end": 818
                            },
                            {
                                "start": 819,
                                "end": 914
                            },
                            {
                                "start": 915,
                                "end": 1074
                            },
                            {
                                "start": 1075,
                                "end": 1363
                            }
                        ],
                        "ref_mentions": [
                            "57759353",
                            "3618568"
                        ],
                        "quote": "The concept of retrieval augmentation has been extensively explored across various machine learning domains (Tian et al., 2019). In NLP, earlier efforts have been applied to specific tasks such as QA and machine translation. Chen et al. (2017) introduce DrQA, which combines a term-based information retrieval (IR) system with a neural QA model to answer knowledge-intensive questions. While IR and such task LMs were initially studied separately, several work explores more organic combinations of retrieval and LM by pre-training the two components jointly or sequentially, including REALM (Guu et al., 2020), RAG (Lewis et al., 2020a), RETRO (Borgeaud et al., 2022), etc."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[268856642 | Guo et al. | 2024 | Citations: 11]",
                "snippets": "Recently, a series of retrieval-augmented language models [14,24,47] have been proposed to augment language models with external knowledge [9,17,53]. Retrieval-augmented techniques can generally be divided into two types. The first type is at the input layer [14,20,42], where the retrieved information is text chunks. The second type is at the output layer [7,24,47], where the retrieved information is tokens. By combining the retrieved tokens with the tokens generated by the original model, the accuracy of the retrieval-augmented model's generation for each token can be improved.\n\nThe first type of method can provide the model with more external knowledge, making it adept at handling tasks in the NLP field such as knowledge-based question answering [27,45,49]. The second type of method can refer to the retrieved information to correct the generated tokens, making it more suited for handling strictly structured generative tasks, such as code completion [7,10,11].\n\nTo better understand the mechanism, we take kNN-LM [24] as an example for a detailed explanation. Given a context sequence   = ( 1 , . . .,   \u22121 ), the language models (LMs) estimate   (  |  ), i.e., the probability distribution over the next token   . kNN-LM is designed to augment a pre-trained language model with a set of nearest neighbours retrieved from an external text collection, which can be the training set.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Recently, a series of retrieval-augmented language models [14,24,47] have been proposed to augment language models with external knowledge [9,17,53]. Retrieval-augmented techniques can generally be divided into two types. The first type is at the input layer [14,20,42], where the retrieved information is text chunks. The second type is at the output layer [7,24,47], where the retrieved information is tokens. By combining the retrieved tokens with the tokens generated by the original model, the accuracy of the retrieval-augmented model's generation for each token can be improved.\n\nThe first type of method can provide the model with more external knowledge, making it adept at handling tasks in the NLP field such as knowledge-based question answering [27,45,49]. The second type of method can refer to the retrieved information to correct the generated tokens, making it more suited for handling strictly structured generative tasks, such as code completion [7,10,11].\n\nTo better understand the mechanism, we take kNN-LM [24] as an example for a detailed explanation. Given a context sequence   = ( 1 , . . .,   \u22121 ), the language models (LMs) estimate   (  |  ), i.e., the probability distribution over the next token   . kNN-LM is designed to augment a pre-trained language model with a set of nearest neighbours retrieved from an external text collection, which can be the training set.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[269188036 | Huang et al. | 2024 | Citations: 51]",
                "snippets": "Meanwhile, Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how interactions between retrievers, language models, and augmentations influence model architectures and applications.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1003,
                        "end": 1212,
                        "sentence_offsets": [
                            {
                                "start": 1003,
                                "end": 1212
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Meanwhile, Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how interactions between retrievers, language models, and augmentations influence model architectures and applications."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[269605025 | Zhang et al. | 2024 | Citations: 0]",
                "snippets": "In the literature, there are two major research aspects in this field: \n\n(1) Datastore Indexing (Khandelwal et al., 2019)10,(Wei et al., 2022)[48] and (2) Document Retrieval [35,27]. For Datastore Indexing, these approaches utilize pre-trained models to generate static embeddings for documents, which are viewed as mounted external memory, and they leverage various semantic similarities to enhance indexing. For Document Retrieval, the system initially retrieves a collection of relevant documents based on the semantic relevance between the user query and the documents. Then, the LLMs concatenate these highly related documents in an unordered manner to the prompt input [4], which makes LLMs better at answering factual questions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207870430 | Khandelwal et al. | 2019 | Citations: 842]": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.",
                    "[249674500 | Wei et al. | 2022 | Citations: 2516]": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 514,
                        "end": 1212,
                        "sentence_offsets": [
                            {
                                "start": 514,
                                "end": 584
                            },
                            {
                                "start": 587,
                                "end": 659
                            },
                            {
                                "start": 660,
                                "end": 886
                            },
                            {
                                "start": 887,
                                "end": 1050
                            },
                            {
                                "start": 1051,
                                "end": 1212
                            }
                        ],
                        "ref_mentions": [
                            "207870430",
                            "249674500"
                        ],
                        "quote": "In the literature, there are two major research aspects in this field: \n\n(1) Datastore Indexing (Khandelwal et al., 2019)10,(Wei et al., 2022)[48] and (2) Document Retrieval [35,27]. For Datastore Indexing, these approaches utilize pre-trained models to generate static embeddings for documents, which are viewed as mounted external memory, and they leverage various semantic similarities to enhance indexing. For Document Retrieval, the system initially retrieves a collection of relevant documents based on the semantic relevance between the user query and the documents. Then, the LLMs concatenate these highly related documents in an unordered manner to the prompt input [4], which makes LLMs better at answering factual questions."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[269983737 | Jiao et al. | 2024 | Citations: 1]",
                "snippets": "Retrieval-Augmented Language Models Augmenting language models with relevant information obtained from various external knowledge bases has been shown to significantly improve the performance of various NLP tasks, including language modeling (Guu et al., 2020)(Borgeaud et al., 2021)Shi et al., 2023;Lin et al., 2023) and open domain question answering (Izacard et al., 2022;Zhang et al., 2024).RAG mainly adopts the \"retrieve then read\" paradigm.Specifically, the input question is first used as the query, then the retrieval module retrieves relevant documents from the external knowledge base, and finally the retrieved documents and questions are merged into a complete input to generate final output.For example, RETRO (Borgeaud et al., 2021) modifies the autoregressive LM to focus on relevant documents through chunked cross-attention, thereby introducing new parameters to the model.REPLUG (Shi et al., 2023) assumes black-box access to LM and optimizes it by fine-tuning the retriever.RAFT (Zhang et al., 2024) proposes a fine-tuned data that additionally contains relevant documents and answers with reasoning chains to train language models for domain-specific open-book settings.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[211204736 | Guu et al. | 2020 | Citations: 2119]": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.",
                    "[244954723 | Borgeaud et al. | 2021 | Citations: 1100]": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1190,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 394
                            },
                            {
                                "start": 394,
                                "end": 446
                            },
                            {
                                "start": 446,
                                "end": 704
                            },
                            {
                                "start": 704,
                                "end": 890
                            },
                            {
                                "start": 890,
                                "end": 993
                            },
                            {
                                "start": 993,
                                "end": 1190
                            }
                        ],
                        "ref_mentions": [
                            "211204736",
                            "244954723",
                            "244954723"
                        ],
                        "quote": "Retrieval-Augmented Language Models Augmenting language models with relevant information obtained from various external knowledge bases has been shown to significantly improve the performance of various NLP tasks, including language modeling (Guu et al., 2020)(Borgeaud et al., 2021)Shi et al., 2023;Lin et al., 2023) and open domain question answering (Izacard et al., 2022;Zhang et al., 2024).RAG mainly adopts the \"retrieve then read\" paradigm.Specifically, the input question is first used as the query, then the retrieval module retrieves relevant documents from the external knowledge base, and finally the retrieved documents and questions are merged into a complete input to generate final output.For example, RETRO (Borgeaud et al., 2021) modifies the autoregressive LM to focus on relevant documents through chunked cross-attention, thereby introducing new parameters to the model.REPLUG (Shi et al., 2023) assumes black-box access to LM and optimizes it by fine-tuning the retriever.RAFT (Zhang et al., 2024) proposes a fine-tuned data that additionally contains relevant documents and answers with reasoning chains to train language models for domain-specific open-book settings."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[270521566 | Goel et al. | 2024 | Citations: 2]",
                "snippets": "In response to these challenges, the field has witnessed the emergence of Retrieval-Augmented Generation (RAG) models, evolving into what are now known as Retrieval-Augmented Language Models (RALMs). These new paradigms enhance LLMs by integrating dynamic external knowledge through sophisticated retrieval mechanisms, effectively addressing the inherent limitations of static knowledge bases (Lewis et al., 2020). Inspired by the success of open-domain question answering systems, RALMs utilize indexed text databases to enhance model responses by presenting retrieved information alongside input queries [1](Ram et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218869575 | Lewis et al. | 2020 | Citations: 6476]": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                    "[256459451 | Ram et al. | 2023 | Citations: 605]": "Abstract Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1"
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 881,
                        "end": 1477,
                        "sentence_offsets": [
                            {
                                "start": 881,
                                "end": 1080
                            },
                            {
                                "start": 1081,
                                "end": 1278
                            },
                            {
                                "start": 1279,
                                "end": 1477
                            }
                        ],
                        "ref_mentions": [
                            "218869575",
                            "256459451"
                        ],
                        "quote": "In response to these challenges, the field has witnessed the emergence of Retrieval-Augmented Generation (RAG) models, evolving into what are now known as Retrieval-Augmented Language Models (RALMs). These new paradigms enhance LLMs by integrating dynamic external knowledge through sophisticated retrieval mechanisms, effectively addressing the inherent limitations of static knowledge bases (Lewis et al., 2020). Inspired by the success of open-domain question answering systems, RALMs utilize indexed text databases to enhance model responses by presenting retrieved information alongside input queries [1](Ram et al., 2023)."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[270764768 | Rao et al. | 2024 | Citations: 5]",
                "snippets": "Retrieval augmentation has become an important technique for improving natural language processing models.One of the first works in this area was kNN-LM by (Khandelwal et al., 2019) who showed how interpolating over nearest neighbors from any text collection could improve generalization.This was followed by RETRO (Borgeaud et al., 2021), which scaled up the retrieval corpus to trillions of tokens.Another line of work has focused on integrating Wikipedia passages directly into models like REALM (Guu et al., 2020), RAG (Lewis et al., 2020), and FiD (Izacard et al., 2020).By retrieving and conditioning on relevant Wikipedia passages, these models can better perform knowledge-intensive downstream tasks like question answering.Overall, retrieval augmentation has proven to be a highly effective way of injecting knowledge into language models to improve their capabilities.The techniques have progressed from simple corpus retrieval to integrated and scalable architectures that retrieve from large knowledge bases like Wikipedia.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218869575 | Lewis et al. | 2020 | Citations: 6476]": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                    "[207870430 | Khandelwal et al. | 2019 | Citations: 842]": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.",
                    "[220302360 | Izacard et al. | 2020 | Citations: 1181]": "Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.",
                    "[244954723 | Borgeaud et al. | 2021 | Citations: 1100]": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."
                },
                "metadata": [
                    {
                        "section_title": "Retrieval Augmented Generation in NLP",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1056,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 106
                            },
                            {
                                "start": 106,
                                "end": 306
                            },
                            {
                                "start": 306,
                                "end": 418
                            },
                            {
                                "start": 418,
                                "end": 597
                            },
                            {
                                "start": 597,
                                "end": 753
                            },
                            {
                                "start": 753,
                                "end": 899
                            },
                            {
                                "start": 899,
                                "end": 1056
                            }
                        ],
                        "ref_mentions": [
                            "207870430",
                            "244954723",
                            "218869575",
                            "220302360"
                        ],
                        "quote": "Retrieval augmentation has become an important technique for improving natural language processing models.One of the first works in this area was kNN-LM by (Khandelwal et al., 2019) who showed how interpolating over nearest neighbors from any text collection could improve generalization.This was followed by RETRO (Borgeaud et al., 2021), which scaled up the retrieval corpus to trillions of tokens.Another line of work has focused on integrating Wikipedia passages directly into models like REALM (Guu et al., 2020), RAG (Lewis et al., 2020), and FiD (Izacard et al., 2020).By retrieving and conditioning on relevant Wikipedia passages, these models can better perform knowledge-intensive downstream tasks like question answering.Overall, retrieval augmentation has proven to be a highly effective way of injecting knowledge into language models to improve their capabilities.The techniques have progressed from simple corpus retrieval to integrated and scalable architectures that retrieve from large knowledge bases like Wikipedia."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[271570928 | Chen et al. | 2024 | Citations: 2]",
                "snippets": "Retrieval-Augmented Generation (RAG) enhances the generative capabilities of language models by incorporating retrieved knowledge for in-context learning [22], [26]. While general language models excel in producing responses for general queries, they are prone to generating hallucinations when tasked with domain-specific knowledge usage. RAG addresses this issue by retrieving established knowledge corpora and providing this information as context to the language model [22]. NaiveRAG represents the most basic architecture within this framework, in which the system retrieves the top-k documents that are most relevant to the query and integrate them into the prompt, thereby grounding the responses in more relevant information [25]. \n\nExpanding on NaiveRAG, advanced RAG incorporates additional modules or structures to improve retrieval precision. Reranking is a notable example, where a reranker is employed to refine the initial ranked list (e.g., Re2G [37] and bgereranker [38], both are based on BERT [39]). Furthermore, studies have indicated that excessive noise and lengthy context can have a negative impact on inference performance. To address this, prompt compression methods such as Selective Context [40] and LLMLingua [41] have been developed. These methods emphasize key information while reducing noise and context length, as discussed in [22].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Retrieval-Augmented Generation (RAG) enhances the generative capabilities of language models by incorporating retrieved knowledge for in-context learning [22], [26]. While general language models excel in producing responses for general queries, they are prone to generating hallucinations when tasked with domain-specific knowledge usage. RAG addresses this issue by retrieving established knowledge corpora and providing this information as context to the language model [22]. NaiveRAG represents the most basic architecture within this framework, in which the system retrieves the top-k documents that are most relevant to the query and integrate them into the prompt, thereby grounding the responses in more relevant information [25]. \n\nExpanding on NaiveRAG, advanced RAG incorporates additional modules or structures to improve retrieval precision. Reranking is a notable example, where a reranker is employed to refine the initial ranked list (e.g., Re2G [37] and bgereranker [38], both are based on BERT [39]). Furthermore, studies have indicated that excessive noise and lengthy context can have a negative impact on inference performance. To address this, prompt compression methods such as Selective Context [40] and LLMLingua [41] have been developed. These methods emphasize key information while reducing noise and context length, as discussed in [22].",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[271571401 | Gao et al. | 2024 | Citations: 20]",
                "snippets": "Hu et al., [30] discuss Retrieval-Augmented Language Models (RALMs) form three key components, including retrievers, language models, augmentations, and how their interactions lead to different model structures and applications.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "II. RELATED WORK",
                        "pdf_hash": "",
                        "start": 2090,
                        "end": 2318,
                        "sentence_offsets": [
                            {
                                "start": 2090,
                                "end": 2318
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Hu et al., [30] discuss Retrieval-Augmented Language Models (RALMs) form three key components, including retrievers, language models, augmentations, and how their interactions lead to different model structures and applications."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[273962778 | Liu et al. | 2024 | Citations: 2]",
                "snippets": "Retrieval-augmented Language Model: Currently, retrieval-augmented language models have proven effective in answering questions by leveraging external information through the integration of novel retrievers and LLMs (Zhu et al., 2021). However, the architectural gap between retrieval and generation continues to hinder unified optimization across the entire retrieval-augmented generation system [2]. To address the isolation between retrieval and generation, a novel architecture called RA-DIT was introduced [18]. By aligning retriever scoring with LSR scoring [27], it has been shown to deliver state-of-the-art performance across various tasks. However, it still employs dense retrievers like DRAGON+ [17] in the retrieval stage, which fails to eliminate the problem at its source and introduces inefficiencies throughout the process.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[230433817 | Zhu et al. | 2021 | Citations: 257]": "Open-domain Question Answering (OpenQA) is an important task in Natural Language Processing (NLP), which aims to answer a question in the form of natural language based on large-scale unstructured documents. Recently, there has been a surge in the amount of research literature on OpenQA, particularly on techniques that integrate with neural Machine Reading Comprehension (MRC). While these research works have advanced performance to new heights on benchmark datasets, they have been rarely covered in existing surveys on QA systems. In this work, we review the latest research trends in OpenQA, with particular attention to systems that incorporate neural MRC techniques. Specifically, we begin with revisiting the origin and development of OpenQA systems. We then introduce modern OpenQA architecture named\"Retriever-Reader\"and analyze the various systems that follow this architecture as well as the specific techniques adopted in each of the components. We then discuss key challenges to developing OpenQA systems and offer an analysis of benchmarks that are commonly used. We hope our work would enable researchers to be informed of the recent advancement and also the open challenges in OpenQA research, so as to stimulate further progress in this field."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 953,
                        "end": 1778,
                        "sentence_offsets": [
                            {
                                "start": 953,
                                "end": 1174
                            },
                            {
                                "start": 1175,
                                "end": 1340
                            },
                            {
                                "start": 1341,
                                "end": 1455
                            },
                            {
                                "start": 1456,
                                "end": 1588
                            },
                            {
                                "start": 1589,
                                "end": 1778
                            }
                        ],
                        "ref_mentions": [
                            "230433817"
                        ],
                        "quote": "Retrieval-augmented Language Model: Currently, retrieval-augmented language models have proven effective in answering questions by leveraging external information through the integration of novel retrievers and LLMs (Zhu et al., 2021). However, the architectural gap between retrieval and generation continues to hinder unified optimization across the entire retrieval-augmented generation system [2]. To address the isolation between retrieval and generation, a novel architecture called RA-DIT was introduced [18]. By aligning retriever scoring with LSR scoring [27], it has been shown to deliver state-of-the-art performance across various tasks. However, it still employs dense retrievers like DRAGON+ [17] in the retrieval stage, which fails to eliminate the problem at its source and introduces inefficiencies throughout the process."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[274131235 | Shu et al. | 2024 | Citations: 8]",
                "snippets": "We implement a RAG LLM low resource language translator by combining retrieval-based techniques with LLMs to ensure accurate and context-aware translations. The overall architecture is shown in Figure 1. The system utilizes dictionary entries, which are indexed through two complementary approaches: keyword-to-document mappings and vector embeddings. Key-to-document mappings in systems refer to a process where keywords are linked directly to the documents or data entries that contain or are relevant to those keywords. The keyword retriever will retrieve corresponding documents according to the key-to-document mapping, if the keyword is inside our storage. The vector embedding indexing process organizes raw linguistic data into retrievable units by associating words with dictionary definitions and using text-embedding-ada-002 model to encode the text into high-dimensional vectors that capture semantic relationships beyond mere surface forms.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Methodology",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 953,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 156
                            },
                            {
                                "start": 157,
                                "end": 203
                            },
                            {
                                "start": 204,
                                "end": 351
                            },
                            {
                                "start": 352,
                                "end": 522
                            },
                            {
                                "start": 523,
                                "end": 662
                            },
                            {
                                "start": 663,
                                "end": 953
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We implement a RAG LLM low resource language translator by combining retrieval-based techniques with LLMs to ensure accurate and context-aware translations. The overall architecture is shown in Figure 1. The system utilizes dictionary entries, which are indexed through two complementary approaches: keyword-to-document mappings and vector embeddings. Key-to-document mappings in systems refer to a process where keywords are linked directly to the documents or data entries that contain or are relevant to those keywords. The keyword retriever will retrieve corresponding documents according to the key-to-document mapping, if the keyword is inside our storage. The vector embedding indexing process organizes raw linguistic data into retrievable units by associating words with dictionary definitions and using text-embedding-ada-002 model to encode the text into high-dimensional vectors that capture semantic relationships beyond mere surface forms."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[274423377 | He et al. | 2024 | Citations: 1]",
                "snippets": "While RAG systems with a static retrieval component, which leverages a fixed corpus of data, are effective for tasks within well-defined knowledge domains, Internet search augmented generation [4,(Komeili et al., 2021)6,7,(Duffy et al., 2023)[9] offers distinct advantages.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[236034557 | Komeili et al. | 2021 | Citations: 289]": "The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020b).",
                    "[258190866 | Duffy et al. | 2023 | Citations: 13]": "Key-value SSDs (KVSSDs) represent a major shift in the storage stack design, with numerous potential benefits. Despite this, their lack of native features critical to operation in real world scenarios hinders their adoption, and these benefits go unrealized. Moreover, simply adapting existing key-value stores to run on KVSSDs proves underwhelming, as KVSSDs operate at lower raw device performance when compared to modern block SSDs.\n This paper introduces Dotori. Dotori is a KVSSD based key-value store that provides much needed functionality in a KVSSD through an upper layer in the host, and takes advantage of the unique KVSSD interface to enable further gains in functionality and performance. At the core of Dotori is a novel B+tree design that is only practical when the underlying storage device is a KVSSD.\n We test Dotori with an enterprise grade KVSSD against state-of-the-art block SSD based key-value stores through a range of micro-benchmarks and real world workloads. Despite low KVSSD raw device performance, Dotori achieves superior performance to these block-device based key-value stores while also showing significant gains in other important metrics."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 817,
                        "end": 1051,
                        "sentence_offsets": [
                            {
                                "start": 817,
                                "end": 1051
                            }
                        ],
                        "ref_mentions": [
                            "236034557",
                            "258190866"
                        ],
                        "quote": "While RAG systems with a static retrieval component, which leverages a fixed corpus of data, are effective for tasks within well-defined knowledge domains, Internet search augmented generation [4,(Komeili et al., 2021)6,7,(Duffy et al., 2023)[9] offers distinct advantages."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[274982275 | Zayyad et al. | 2024 | Citations: 0]",
                "snippets": "RAG is a sophisticated framework designed to enhance language models by coupling them with external retrieval systems, addressing limitations inherent in static, solely parameter-based language models. RAG integrates a dual-component architecture where a retriever dynamically searches a structured external corpus for relevant information based on the input query, and a generator LLM uses the retrieved content as context to generate accurate and contextually enriched responses [Gao et al., 2023, Mialon et al., 2023].\n\nRAGs can employ two main types of retrieval mechanisms: dense and sparse [Mialon et al., 2023]. Sparse retrievers rely on bag-of-words representations, excelling at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities, enhancing the model's comprehension of related concepts. By appending retrieved documents directly to the model's context, these retrievers allow the language model to ground its responses in a broader context, thereby increasing accuracy and factual consistency across complex tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "RAG is a sophisticated framework designed to enhance language models by coupling them with external retrieval systems, addressing limitations inherent in static, solely parameter-based language models. RAG integrates a dual-component architecture where a retriever dynamically searches a structured external corpus for relevant information based on the input query, and a generator LLM uses the retrieved content as context to generate accurate and contextually enriched responses [Gao et al., 2023, Mialon et al., 2023].\n\nRAGs can employ two main types of retrieval mechanisms: dense and sparse [Mialon et al., 2023]. Sparse retrievers rely on bag-of-words representations, excelling at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities, enhancing the model's comprehension of related concepts. By appending retrieved documents directly to the model's context, these retrievers allow the language model to ground its responses in a broader context, thereby increasing accuracy and factual consistency across complex tasks.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[275358357 | Yang et al. | 2025 | Citations: 2]",
                "snippets": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) overcomes the limitations of Large Language Models (LLMs), which rely on static, pre-trained datasets that can become outdated and lack domain-specific information. This restricts LLMs' ability to generate accurate and up-to-date responses. RAG integrates Information Retrieval (IR) systems with LLMs, enabling them to query external knowledge sources and access real-time, domain-relevant data. In a typical RAG framework, a retriever processes user queries and retrieves relevant documents based on semantic similarity. These documents are then combined with the original query and passed to the LLM to generate a more accurate and comprehensive response.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218869575 | Lewis et al. | 2020 | Citations: 6476]": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
                },
                "metadata": [
                    {
                        "section_title": "C. Retrieval-Augmented Generation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 698,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 205
                            },
                            {
                                "start": 206,
                                "end": 281
                            },
                            {
                                "start": 282,
                                "end": 436
                            },
                            {
                                "start": 437,
                                "end": 562
                            },
                            {
                                "start": 563,
                                "end": 698
                            }
                        ],
                        "ref_mentions": [
                            "218869575"
                        ],
                        "quote": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) overcomes the limitations of Large Language Models (LLMs), which rely on static, pre-trained datasets that can become outdated and lack domain-specific information. This restricts LLMs' ability to generate accurate and up-to-date responses. RAG integrates Information Retrieval (IR) systems with LLMs, enabling them to query external knowledge sources and access real-time, domain-relevant data. In a typical RAG framework, a retriever processes user queries and retrieves relevant documents based on semantic similarity. These documents are then combined with the original query and passed to the LLM to generate a more accurate and comprehensive response."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[275820707 | Yang et al. | 2025 | Citations: 4]",
                "snippets": "The k-Nearest Neighbor Language Model (kNN-LM) (Khandelwal et al., 2019) retrieve the k most similar training contexts for test context according to the distance in the embedding space of the pre-trained language model. In fact, the k training contexts correspond to k training targets. By normalizing and aggregating k training targets, kNN-LM can get a target distribution from k nearest neighbors, and the pre-trained language model can generate another target distribution directly according to current input. kNN-LM can merge the two distributions above by weighted sum to get the final target distribution. Different from kNN-LM, Retrieval-Augmented Language Model (REALM) (Guu et al., 2020), whose workflow can be summarized as the retriever-and-reader, has two components that are both trained. One is a neural knowledge retriever, which retrieves similar text with input using a dense inner product model. The other is the knowledge-augmented encoder, which predicts the final results based on input and the retrieved text in the last step. Actually, the prediction cannot generate texts using the encoder but extract a contiguous sequence from the retrieved text as the result. A similar workflow has been proposed and developed [8]49] before REALM occurs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207870430 | Khandelwal et al. | 2019 | Citations: 842]": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.",
                    "[211204736 | Guu et al. | 2020 | Citations: 2119]": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."
                },
                "metadata": [
                    {
                        "section_title": "Retrieval-Augmented Generation",
                        "pdf_hash": "",
                        "start": 992,
                        "end": 2223,
                        "sentence_offsets": [
                            {
                                "start": 992,
                                "end": 1190
                            },
                            {
                                "start": 1191,
                                "end": 1257
                            },
                            {
                                "start": 1258,
                                "end": 1484
                            },
                            {
                                "start": 1485,
                                "end": 1583
                            },
                            {
                                "start": 1584,
                                "end": 1759
                            },
                            {
                                "start": 1760,
                                "end": 1871
                            },
                            {
                                "start": 1872,
                                "end": 2006
                            },
                            {
                                "start": 2007,
                                "end": 2144
                            },
                            {
                                "start": 2145,
                                "end": 2223
                            }
                        ],
                        "ref_mentions": [
                            "207870430",
                            "211204736"
                        ],
                        "quote": "The k-Nearest Neighbor Language Model (kNN-LM) (Khandelwal et al., 2019) retrieve the k most similar training contexts for test context according to the distance in the embedding space of the pre-trained language model. In fact, the k training contexts correspond to k training targets. By normalizing and aggregating k training targets, kNN-LM can get a target distribution from k nearest neighbors, and the pre-trained language model can generate another target distribution directly according to current input. kNN-LM can merge the two distributions above by weighted sum to get the final target distribution. Different from kNN-LM, Retrieval-Augmented Language Model (REALM) (Guu et al., 2020), whose workflow can be summarized as the retriever-and-reader, has two components that are both trained. One is a neural knowledge retriever, which retrieves similar text with input using a dense inner product model. The other is the knowledge-augmented encoder, which predicts the final results based on input and the retrieved text in the last step. Actually, the prediction cannot generate texts using the encoder but extract a contiguous sequence from the retrieved text as the result. A similar workflow has been proposed and developed [8]49] before REALM occurs."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[277043297 | Cheng et al. | 2025 | Citations: 6]",
                "snippets": "We discuss the key characteristics of RAG, such as its ability to augment generative models with dynamic external knowledge, and the challenges associated with aligning retrieved information with generative objectives. We also present a taxonomy that categorizes RAG methods, ranging from basic retrieval-augmented approaches to more advanced models incorporating multi-modal data and reasoning capabilities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 603,
                        "end": 1011,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We discuss the key characteristics of RAG, such as its ability to augment generative models with dynamic external knowledge, and the challenges associated with aligning retrieved information with generative objectives. We also present a taxonomy that categorizes RAG methods, ranging from basic retrieval-augmented approaches to more advanced models incorporating multi-modal data and reasoning capabilities."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[277313225 | Wang et al. | 2025 | Citations: 0]",
                "snippets": "Recent efforts to improve RAG focus on two fronts: 1) enhancing retrieval efficiency through adaptive and modular frameworks (Gan et al., 2024;Ravuru et al., 2024;Zhang et al., 2024a); and 2) better structuring external knowledge, with graphbased RAGs emerging as a dominant approach (Edge et al., 2024;Guo et al., 2024;Potts, 2024).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 561,
                        "end": 894,
                        "sentence_offsets": [
                            {
                                "start": 561,
                                "end": 894
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recent efforts to improve RAG focus on two fronts: 1) enhancing retrieval efficiency through adaptive and modular frameworks (Gan et al., 2024;Ravuru et al., 2024;Zhang et al., 2024a); and 2) better structuring external knowledge, with graphbased RAGs emerging as a dominant approach (Edge et al., 2024;Guo et al., 2024;Potts, 2024)."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[277780258 | Gumaan | 2025 | Citations: 0]",
                "snippets": "Retrieval-Augmented Generation (RAG) was introduced to address this by equipping models with access to external nonparametric memory (e.g. a text corpus or database), allowing them to fetch relevant information on-the-fly [2]. RAG combines a parametric neural generator with a retrieval module, producing outputs that are more specific and factual than those of parametric-only models (Brown et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 334,
                        "end": 723,
                        "sentence_offsets": [
                            {
                                "start": 334,
                                "end": 560
                            },
                            {
                                "start": 561,
                                "end": 723
                            }
                        ],
                        "ref_mentions": [
                            "218971783"
                        ],
                        "quote": "Retrieval-Augmented Generation (RAG) was introduced to address this by equipping models with access to external nonparametric memory (e.g. a text corpus or database), allowing them to fetch relevant information on-the-fly [2]. RAG combines a parametric neural generator with a retrieval module, producing outputs that are more specific and factual than those of parametric-only models (Brown et al., 2020)."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[278714952 | Lee et al. | 2025 | Citations: 0]",
                "snippets": "Retrieval-Augmented Generation (RAG) augments large language models by incorporating external documents into the generation process. Rather than relying solely on the model's internal parameters, RAG retrieves relevant passages from a knowledge base and feeds them, along with the user query, into the model to guide its response (Zhou et al., 2024). \n\nA standard RAG system operates in two phases: \n\n\u2022 Retrieval. A retriever model selects the top-k most relevant documents for a given query. \n\n\u2022 Generation. A language model generates a response conditioned on both the query and the retrieved documents. \n\nThe appeal of RAG lies in its ability to dynamically access up-to-date or domain-specific information, which is especially useful in fast-changing or specialized fields. However, it also introduces new failure modes, particularly when the retrieval quality is imperfect (Barnett et al., 2024).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Retrieval-Augmented Generation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 901,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 132
                            },
                            {
                                "start": 133,
                                "end": 350
                            },
                            {
                                "start": 353,
                                "end": 398
                            },
                            {
                                "start": 401,
                                "end": 413
                            },
                            {
                                "start": 414,
                                "end": 492
                            },
                            {
                                "start": 495,
                                "end": 508
                            },
                            {
                                "start": 509,
                                "end": 605
                            },
                            {
                                "start": 608,
                                "end": 777
                            },
                            {
                                "start": 778,
                                "end": 901
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Retrieval-Augmented Generation (RAG) augments large language models by incorporating external documents into the generation process. Rather than relying solely on the model's internal parameters, RAG retrieves relevant passages from a knowledge base and feeds them, along with the user query, into the model to guide its response (Zhou et al., 2024). \n\nA standard RAG system operates in two phases: \n\n\u2022 Retrieval. A retriever model selects the top-k most relevant documents for a given query. \n\n\u2022 Generation. A language model generates a response conditioned on both the query and the retrieved documents. \n\nThe appeal of RAG lies in its ability to dynamically access up-to-date or domain-specific information, which is especially useful in fast-changing or specialized fields. However, it also introduces new failure modes, particularly when the retrieval quality is imperfect (Barnett et al., 2024)."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.032889,
        "cot": "Planning the taxonomy of retrieval-augmented language model architectures in NLP requires categorizing the different approaches based on their key characteristics, architectural designs, and use cases. \n\nLooking at the query, I need to create a detailed taxonomy with dimensions that cover:\n1. Introduction/Background to RAG models - To establish what retrieval-augmentation is and why it's important\n2. Core architectural categories - To organize the models by their fundamental design approaches\n3. Key model examples and their specific designs - To detail the major models mentioned (kNN-LM, RETRO, REALM, RAG, etc.)\n4. Retrieval mechanisms - To explain how these models retrieve information\n5. Integration mechanisms - To explain how retrieved information is incorporated into language models\n6. Use cases and applications - To highlight where these models excel\n\nFor the Introduction dimension, a synthesis format is appropriate to provide context about what retrieval-augmented language models are and why they're important.\n\nFor the Core architectural categories dimension, a synthesis format would work best to explain the fundamental approaches to retrieval augmentation.\n\nFor the Key model examples dimension, a list format makes sense as we want to enumerate and describe specific models like kNN-LM, RETRO, REALM, RAG, etc.\n\nFor the Retrieval mechanisms dimension, a synthesis format would work well to explain the different ways models retrieve information.\n\nFor the Integration mechanisms dimension, a synthesis format would be best to explain how retrieved information is incorporated into language models.\n\nFor the Use cases and applications dimension, a list format would be appropriate to clearly outline different application scenarios.\n\nLooking at the quotes provided, I have enough information to support all these dimensions with relevant evidence from the literature.",
        "plan": {
            "Introduction to Retrieval-Augmented Language Models (synthesis)": [
                7,
                8,
                10,
                14,
                17,
                20,
                24,
                25,
                27,
                29
            ],
            "Core Architectural Categories of RAG Models (synthesis)": [
                2,
                3,
                5,
                6,
                13,
                16,
                18,
                19,
                28,
                30
            ],
            "Major Retrieval-Augmented Models and Their Designs (list)": [
                0,
                1,
                2,
                3,
                4,
                5,
                9,
                11,
                12,
                13,
                16,
                18,
                21,
                26
            ],
            "Retrieval Mechanisms in RAG Systems (synthesis)": [
                0,
                1,
                3,
                4,
                5,
                13,
                15,
                22,
                23,
                24,
                30
            ],
            "Integration of Retrieved Information (synthesis)": [
                0,
                1,
                3,
                5,
                6,
                13,
                16,
                22,
                24
            ],
            "Use Cases and Applications (list)": [
                1,
                2,
                3,
                6,
                7,
                13,
                16,
                18,
                19,
                25
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Retrieval-Augmented Language Models",
                "tldr": "Retrieval-Augmented Language Models (RALMs) enhance traditional language models by incorporating external knowledge sources to overcome limitations in factual accuracy and knowledge scalability. They combine parametric memory (the model's parameters) with non-parametric memory (external knowledge bases) to provide more accurate, up-to-date, and contextually relevant responses. (12 sources)",
                "text": "\nRetrieval-Augmented Language Models (RALMs) represent a significant advancement in natural language processing, addressing fundamental limitations of traditional language models. These models were developed to overcome the constraints of static, parameter-based knowledge stored within large language models (LLMs), which can become outdated and lack domain-specific information <Paper corpusId=\"270521566\" paperTitle=\"(Goel et al., 2024)\" isShortName></Paper> <Paper corpusId=\"275358357\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>. The core innovation of RALMs lies in their ability to decouple knowledge storage, distributing it between the model's parameters and external knowledge sources <Paper corpusId=\"265308533\" paperTitle=\"(Munikoti et al., 2023)\" isShortName></Paper>.\n\nThe concept of Retrieval-Augmented Generation (RAG), a key implementation of RALMs, was formally introduced by Facebook researchers in 2020 <Paper corpusId=\"265594594\" paperTitle=\"(Abdelazim et al., 2023)\" isShortName></Paper> <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>. This approach combines the benefits of both retrieval-based and generation-based methods to produce more contextually relevant and informative responses <Paper corpusId=\"267061013\" paperTitle=\"(Elgedawy et al., 2024)\" isShortName></Paper>. In contrast to traditional LLMs that rely solely on their internal parameters, RAG models can access and utilize external knowledge bases on-demand <Paper corpusId=\"277780258\" paperTitle=\"(Gumaan, 2025)\" isShortName></Paper>.\n\nA typical RALM architecture consists of three key components: retrievers, language models, and augmentation mechanisms <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269188036\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>. The retriever processes user queries and searches through external knowledge sources for relevant information. This retrieved information is then integrated with the original query and passed to the language model, which generates responses grounded in both its parametric knowledge and the retrieved context <Paper corpusId=\"275358357\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>.\n\nRALMs can implement different types of retrieval mechanisms, primarily categorized as dense and sparse retrievers <Paper corpusId=\"274982275\" paperTitle=\"(Zayyad et al., 2024)\" isShortName></Paper>. Sparse retrievers excel at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities. Both approaches allow the language model to ground its responses in broader contexts, enhancing accuracy and factual consistency.\n\nThe integration of external knowledge can be implemented through various approaches, from the sophisticated architecture modifications in early RAG models to simpler \"In-Context RALM\" methods that prepend retrieved documents to the input without modifying the underlying LLM architecture <Paper corpusId=\"256459451\" paperTitle=\"(Ram et al., 2023)\" isShortName></Paper>. This flexibility has contributed to the rapid adoption and evolution of retrieval-augmented approaches across diverse applications <Paper corpusId=\"277043297\" paperTitle=\"(Cheng et al., 2025)\" isShortName></Paper>.\n\nBy augmenting LLMs with dynamic access to external knowledge, RALMs offer several advantages: they provide more specific and factual responses than parametric-only models <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>, they can be updated without retraining the entire model, and they offer natural source attribution by referencing the retrieved documents <Paper corpusId=\"256459451\" paperTitle=\"(Ram et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Goel et al., 2024)",
                        "snippets": [
                            "In response to these challenges, the field has witnessed the emergence of Retrieval-Augmented Generation (RAG) models, evolving into what are now known as Retrieval-Augmented Language Models (RALMs). These new paradigms enhance LLMs by integrating dynamic external knowledge through sophisticated retrieval mechanisms, effectively addressing the inherent limitations of static knowledge bases (Lewis et al., 2020). Inspired by the success of open-domain question answering systems, RALMs utilize indexed text databases to enhance model responses by presenting retrieved information alongside input queries [1](Ram et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 270521566,
                            "title": "HIRO: Hierarchical Information Retrieval Optimization",
                            "authors": [
                                {
                                    "authorId": "2306783450",
                                    "name": "Krish Goel"
                                },
                                {
                                    "authorId": "2306782235",
                                    "name": "Mahek Chandak"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.83447265625
                    },
                    {
                        "id": "(Yang et al., 2025)",
                        "snippets": [
                            "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) overcomes the limitations of Large Language Models (LLMs), which rely on static, pre-trained datasets that can become outdated and lack domain-specific information. This restricts LLMs' ability to generate accurate and up-to-date responses. RAG integrates Information Retrieval (IR) systems with LLMs, enabling them to query external knowledge sources and access real-time, domain-relevant data. In a typical RAG framework, a retriever processes user queries and retrieves relevant documents based on semantic similarity. These documents are then combined with the original query and passed to the LLM to generate a more accurate and comprehensive response."
                        ],
                        "paper": {
                            "corpus_id": 275358357,
                            "title": "Knowledge Retrieval Based on Generative AI",
                            "authors": [
                                {
                                    "authorId": "2191368257",
                                    "name": "Te-Lun Yang"
                                },
                                {
                                    "authorId": "2253878746",
                                    "name": "Jyi-Shane Liu"
                                },
                                {
                                    "authorId": "40130996",
                                    "name": "Yuen-Hsien Tseng"
                                },
                                {
                                    "authorId": "2262396644",
                                    "name": "Jyh-Shing Roger Jang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.77734375
                    },
                    {
                        "id": "(Munikoti et al., 2023)",
                        "snippets": [
                            "The retrieval augmented language models (RALM) primarily address the grounding and scalability challenges in standard language models (LM). RALM aims to address these limitations by combining a LM with an external knowledge base. In this framework, the LM generates text conditioned not only on the input query but also on relevant knowledge retrieved from the knowledge base. The retrieved knowledge is usually the text chunks or passages from documents that provide factual grounding to contextualize the model's pre-Preprint version dictions. In other words, this approach decentralizes model knowledge into parameters and external knowledge sources, thereby addressing the challenges of scalability and adaptability."
                        ],
                        "paper": {
                            "corpus_id": 265308533,
                            "title": "ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science",
                            "authors": [
                                {
                                    "authorId": "2258957941",
                                    "name": "Sai Munikoti"
                                },
                                {
                                    "authorId": "145536102",
                                    "name": "Anurag Acharya"
                                },
                                {
                                    "authorId": "2054838317",
                                    "name": "S. Wagle"
                                },
                                {
                                    "authorId": "24029613",
                                    "name": "Sameera Horawalavithana"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.82080078125
                    },
                    {
                        "id": "(Abdelazim et al., 2023)",
                        "snippets": [
                            "Retrieval Augmented Generation (RAG), introduced by Facebook Researchers in 2020 [1], is a pivotal AI framework facilitating information retrieval for Generative AI models, thereby enhancing their accuracy and capabilities. RAG empowers Large Language Models (LLMs) by granting them access to external knowledge sources, augmenting the content generation process. This dual functionality entails retrieval, wherein RAG meticulously selects pertinent information from provided sources and generation, whereby LLMs craft contextually relevant responses based on user input."
                        ],
                        "paper": {
                            "corpus_id": 265594594,
                            "title": "Semantic Embeddings for Arabic Retrieval Augmented Generation (ARAG)",
                            "authors": [
                                {
                                    "authorId": "2269250335",
                                    "name": "Hazem Abdelazim"
                                },
                                {
                                    "authorId": "2148715555",
                                    "name": "Mohamed Tharwat"
                                },
                                {
                                    "authorId": "2269771285",
                                    "name": "Ammar Mohamed"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Journal of Advanced Computer Science and Applications",
                            "n_citations": 8
                        },
                        "score": 0.720703125
                    },
                    {
                        "id": "(Lewis et al., 2020)",
                        "snippets": [
                            "We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token."
                        ],
                        "paper": {
                            "corpus_id": 218869575,
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "authors": [
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3439053",
                                    "name": "Ethan Perez"
                                },
                                {
                                    "authorId": "1716179427",
                                    "name": "Aleksandara Piktus"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "103131985",
                                    "name": "Heinrich Kuttler"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "2620211",
                                    "name": "Tim Rockt\u00e4schel"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "1743722",
                                    "name": "Douwe Kiela"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6476
                        },
                        "score": 0.82568359375
                    },
                    {
                        "id": "(Elgedawy et al., 2024)",
                        "snippets": [
                            "Retrieval augmented generation (RAG) refers to a novel technique where a model first retrieves relevant information from a large corpus or dataset and then generates responses or outputs based on the retrieved information. This approach combines the benefits of both retrieval-based and generation-based methods, allowing for more contextually relevant and informative responses in conversational systems. Recent research works are exploring the trade-offs between this method and traditional fine-tuning approaches for language models, with some favoring RAG for certain contexts [7]23]."
                        ],
                        "paper": {
                            "corpus_id": 267061013,
                            "title": "Dynamic Q&A of Clinical Documents with Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2280063225",
                                    "name": "Ran Elgedawy"
                                },
                                {
                                    "authorId": "2274464131",
                                    "name": "Ioana Danciu"
                                },
                                {
                                    "authorId": "1387927897",
                                    "name": "Maria Mahbub"
                                },
                                {
                                    "authorId": "2149506151",
                                    "name": "Sudarshan Srinivasan"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 6
                        },
                        "score": 0.728515625
                    },
                    {
                        "id": "(Gumaan, 2025)",
                        "snippets": [
                            "Retrieval-Augmented Generation (RAG) was introduced to address this by equipping models with access to external nonparametric memory (e.g. a text corpus or database), allowing them to fetch relevant information on-the-fly [2]. RAG combines a parametric neural generator with a retrieval module, producing outputs that are more specific and factual than those of parametric-only models (Brown et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 277780258,
                            "title": "ExpertRAG: Efficient RAG with Mixture of Experts - Optimizing Context Retrieval for Adaptive LLM Responses",
                            "authors": [
                                {
                                    "authorId": "2354181125",
                                    "name": "Esmail Gumaan"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.83447265625
                    },
                    {
                        "id": "(Gao et al., 2024)",
                        "snippets": [
                            "Hu et al., [30] discuss Retrieval-Augmented Language Models (RALMs) form three key components, including retrievers, language models, augmentations, and how their interactions lead to different model structures and applications."
                        ],
                        "paper": {
                            "corpus_id": 271571401,
                            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
                            "authors": [
                                {
                                    "authorId": "2280046531",
                                    "name": "Yunfan Gao"
                                },
                                {
                                    "authorId": "2275320371",
                                    "name": "Yun Xiong"
                                },
                                {
                                    "authorId": "2291409458",
                                    "name": "Meng Wang"
                                },
                                {
                                    "authorId": "2256769434",
                                    "name": "Haofen Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 20
                        },
                        "score": 0.7841796875
                    },
                    {
                        "id": "(Huang et al., 2024)",
                        "snippets": [
                            "Meanwhile, Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how interactions between retrievers, language models, and augmentations influence model architectures and applications."
                        ],
                        "paper": {
                            "corpus_id": 269188036,
                            "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2260272949",
                                    "name": "Yizheng Huang"
                                },
                                {
                                    "authorId": "2259653248",
                                    "name": "Jimmy X. Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 51
                        },
                        "score": 0.77880859375
                    },
                    {
                        "id": "(Zayyad et al., 2024)",
                        "snippets": [
                            "RAG is a sophisticated framework designed to enhance language models by coupling them with external retrieval systems, addressing limitations inherent in static, solely parameter-based language models. RAG integrates a dual-component architecture where a retriever dynamically searches a structured external corpus for relevant information based on the input query, and a generator LLM uses the retrieved content as context to generate accurate and contextually enriched responses [Gao et al., 2023, Mialon et al., 2023].\n\nRAGs can employ two main types of retrieval mechanisms: dense and sparse [Mialon et al., 2023]. Sparse retrievers rely on bag-of-words representations, excelling at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities, enhancing the model's comprehension of related concepts. By appending retrieved documents directly to the model's context, these retrievers allow the language model to ground its responses in a broader context, thereby increasing accuracy and factual consistency across complex tasks."
                        ],
                        "paper": {
                            "corpus_id": 274982275,
                            "title": "Formal Language Knowledge Corpus for Retrieval Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2336913948",
                                    "name": "Majd Zayyad"
                                },
                                {
                                    "authorId": "2727584",
                                    "name": "Yossi Adi"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.90283203125
                    },
                    {
                        "id": "(Ram et al., 2023)",
                        "snippets": [
                            "Abstract Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1"
                        ],
                        "paper": {
                            "corpus_id": 256459451,
                            "title": "In-Context Retrieval-Augmented Language Models",
                            "authors": [
                                {
                                    "authorId": "73775461",
                                    "name": "Ori Ram"
                                },
                                {
                                    "authorId": "152754428",
                                    "name": "Yoav Levine"
                                },
                                {
                                    "authorId": "1491822146",
                                    "name": "Itay Dalmedigos"
                                },
                                {
                                    "authorId": "51918041",
                                    "name": "Dor Muhlgay"
                                },
                                {
                                    "authorId": "3140335",
                                    "name": "A. Shashua"
                                },
                                {
                                    "authorId": "2066411743",
                                    "name": "Kevin Leyton-Brown"
                                },
                                {
                                    "authorId": "1701353",
                                    "name": "Y. Shoham"
                                }
                            ],
                            "year": 2023,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 605
                        },
                        "score": 0
                    },
                    {
                        "id": "(Cheng et al., 2025)",
                        "snippets": [
                            "We discuss the key characteristics of RAG, such as its ability to augment generative models with dynamic external knowledge, and the challenges associated with aligning retrieved information with generative objectives. We also present a taxonomy that categorizes RAG methods, ranging from basic retrieval-augmented approaches to more advanced models incorporating multi-modal data and reasoning capabilities."
                        ],
                        "paper": {
                            "corpus_id": 277043297,
                            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "1491233507",
                                    "name": "Mingyue Cheng"
                                },
                                {
                                    "authorId": "2208917508",
                                    "name": "Yucong Luo"
                                },
                                {
                                    "authorId": "2322501286",
                                    "name": "Ouyang Jie"
                                },
                                {
                                    "authorId": "2332691115",
                                    "name": "Qi Liu"
                                },
                                {
                                    "authorId": "2312648865",
                                    "name": "Huijie Liu"
                                },
                                {
                                    "authorId": "2291070758",
                                    "name": "Li Li"
                                },
                                {
                                    "authorId": "2322429208",
                                    "name": "Shuo Yu"
                                },
                                {
                                    "authorId": "2351226328",
                                    "name": "Bohou Zhang"
                                },
                                {
                                    "authorId": "2350426005",
                                    "name": "Jiawei Cao"
                                },
                                {
                                    "authorId": "2350427710",
                                    "name": "Jie Ma"
                                },
                                {
                                    "authorId": "2322524150",
                                    "name": "Daoyu Wang"
                                },
                                {
                                    "authorId": "2258714945",
                                    "name": "Enhong Chen"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.9296875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Core Architectural Categories of RAG Models",
                "tldr": "Retrieval-Augmented Language Models (RALMs) can be categorized based on their underlying architecture (encoder-decoder vs. decoder-only) and retrieval mechanisms (block-level vs. token-level). These architectural choices significantly impact computational efficiency, knowledge integration methods, and suitability for specific tasks. (13 sources)",
                "text": "\nRetrieval-augmented language models can be systematically categorized based on several key architectural dimensions that define how they retrieve and integrate external knowledge. These categories help organize the diverse landscape of RAG approaches and understand their relative strengths and limitations.\n\n## Model Architecture Base Types\n\nThe first major distinction is between encoder-decoder and decoder-only architectures:\n\n- **Encoder-Decoder Models**: Systems like REALM, RAG, and FiD use a sequence-to-sequence architecture that first encodes input texts and then generates outputs. These models often employ a Fusion-in-Decoder approach that enables efficient processing of multiple retrieved passages, as the computational cost grows linearly with the number of passages rather than quadratically <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"220302360\" paperTitle=\"(Izacard et al., 2020)\" isShortName></Paper>. Examples include the original RAG model <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper> and Atlas <Paper corpusId=\"251371732\" paperTitle=\"(Izacard et al., 2022)\" isShortName></Paper>.\n\n- **Decoder-Only Models**: Models like kNN-LM and RETRO use autoregressive decoder-only architectures, similar to GPT models. These approaches typically face higher computational costs as context length increases, since the computation scales quadratically with input length and retrieved passages <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper> <Paper corpusId=\"244954723\" paperTitle=\"(Borgeaud et al., 2021)\" isShortName></Paper>.\n\n## Retrieval Granularity\n\nAnother critical distinction is based on what is retrieved and how it's integrated:\n\n- **Block Retrieval Models (Input-Layer Augmentation)**: These models retrieve entire text chunks or passages that are incorporated into the context before generation. They excel at knowledge-intensive tasks like question answering by providing broader context <Paper corpusId=\"268856642\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"261030382\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>. Examples include REALM <Paper corpusId=\"211204736\" paperTitle=\"(Guu et al., 2020)\" isShortName></Paper>, RAG <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>, and RETRO <Paper corpusId=\"244954723\" paperTitle=\"(Borgeaud et al., 2021)\" isShortName></Paper>.\n\n- **Token Retrieval Models (Output-Layer Augmentation)**: These models retrieve at the token level and combine retrieved tokens with the model's native next-token predictions. They're particularly effective for structured generation tasks like code completion <Paper corpusId=\"268856642\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"261030382\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>. kNN-LM is the prototypical example, interpolating between the language model's predictions and a k-nearest neighbors distribution from retrieved tokens <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper>.\n\n## Integration Methods\n\nRAG models also differ in how they integrate retrieved information:\n\n- **Architectural Modification**: Some models like RETRO modify the model architecture, adding components such as chunked cross-attention mechanisms to integrate retrieved information <Paper corpusId=\"244954723\" paperTitle=\"(Borgeaud et al., 2021)\" isShortName></Paper>. This typically requires training or fine-tuning the model.\n\n- **Non-parametric Augmentation**: Models like kNN-LM use non-parametric methods such as interpolation between the base model's predictions and the retrieval-based predictions <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper>. This approach often doesn't require model retraining.\n\n- **In-Context Learning**: Some approaches like REPLUG treat the language model as a black box and simply prepend retrieved information to the input context <Paper corpusId=\"256389797\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"245218561\" paperTitle=\"(Rubin et al., 2021)\" isShortName></Paper>.\n\nRecent advancements in RAG architectures have focused on improving retrieval efficiency through adaptive frameworks and better structuring of external knowledge, with graph-based RAG systems emerging as a promising approach <Paper corpusId=\"277313225\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>. The standard RAG workflow has crystallized into a two-phase process: retrieval of relevant documents followed by generation conditioned on both the query and retrieved information <Paper corpusId=\"278714952\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Huang et al., 2023)",
                        "snippets": [
                            "Retrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2022)(Lewis et al., 2020) and decoderonly (Khandelwal et al., 2019)Borgeaud et al., 2022;(Shi et al., 2022) architectures. For decoder-only LMs, the computational cost typically increases quadratically with the input length, as well as with the number of retrieval passages. In contrast, for encoder-decoder LMs with a Fusion-in-Decoder architecture, the computation cost grows linearly with the number of retrieved passages, as they only perform self-attention over one passage at a time (Izacard et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 260900354,
                            "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models",
                            "authors": [
                                {
                                    "authorId": "1490651934",
                                    "name": "Jie Huang"
                                },
                                {
                                    "authorId": "2056440915",
                                    "name": "Wei Ping"
                                },
                                {
                                    "authorId": "145011005",
                                    "name": "Peng Xu"
                                },
                                {
                                    "authorId": "1911755",
                                    "name": "M. Shoeybi"
                                },
                                {
                                    "authorId": "143922493",
                                    "name": "K. Chang"
                                },
                                {
                                    "authorId": "2301680",
                                    "name": "Bryan Catanzaro"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 35
                        },
                        "score": 0.767578125
                    },
                    {
                        "id": "(Izacard et al., 2020)",
                        "snippets": [
                            "Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages."
                        ],
                        "paper": {
                            "corpus_id": 220302360,
                            "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "1410231361",
                                    "name": "Gautier Izacard"
                                },
                                {
                                    "authorId": "3024698",
                                    "name": "Edouard Grave"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 1181
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lewis et al., 2020)",
                        "snippets": [
                            "We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token."
                        ],
                        "paper": {
                            "corpus_id": 218869575,
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "authors": [
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3439053",
                                    "name": "Ethan Perez"
                                },
                                {
                                    "authorId": "1716179427",
                                    "name": "Aleksandara Piktus"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "103131985",
                                    "name": "Heinrich Kuttler"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "2620211",
                                    "name": "Tim Rockt\u00e4schel"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "1743722",
                                    "name": "Douwe Kiela"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6476
                        },
                        "score": 0.82568359375
                    },
                    {
                        "id": "(Izacard et al., 2022)",
                        "snippets": [
                            "Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters."
                        ],
                        "paper": {
                            "corpus_id": 251371732,
                            "title": "Few-shot Learning with Retrieval Augmented Language Models",
                            "authors": [
                                {
                                    "authorId": "1410231361",
                                    "name": "Gautier Izacard"
                                },
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3376175",
                                    "name": "M. Lomeli"
                                },
                                {
                                    "authorId": "26360550",
                                    "name": "Lucas Hosseini"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "32246932",
                                    "name": "Timo Schick"
                                },
                                {
                                    "authorId": "2129456957",
                                    "name": "Jane A. Yu"
                                },
                                {
                                    "authorId": "2319608",
                                    "name": "Armand Joulin"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "3024698",
                                    "name": "Edouard Grave"
                                }
                            ],
                            "year": 2022,
                            "venue": "Journal of machine learning research",
                            "n_citations": 783
                        },
                        "score": 0
                    },
                    {
                        "id": "(Khandelwal et al., 2019)",
                        "snippets": [
                            "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."
                        ],
                        "paper": {
                            "corpus_id": 207870430,
                            "title": "Generalization through Memorization: Nearest Neighbor Language Models",
                            "authors": [
                                {
                                    "authorId": "3030219",
                                    "name": "Urvashi Khandelwal"
                                },
                                {
                                    "authorId": "39455775",
                                    "name": "Omer Levy"
                                },
                                {
                                    "authorId": "1746807",
                                    "name": "Dan Jurafsky"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 842
                        },
                        "score": 0
                    },
                    {
                        "id": "(Borgeaud et al., 2021)",
                        "snippets": [
                            "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."
                        ],
                        "paper": {
                            "corpus_id": 244954723,
                            "title": "Improving language models by retrieving from trillions of tokens",
                            "authors": [
                                {
                                    "authorId": "148016269",
                                    "name": "Sebastian Borgeaud"
                                },
                                {
                                    "authorId": "1697879",
                                    "name": "A. Mensch"
                                },
                                {
                                    "authorId": "46616544",
                                    "name": "Jordan Hoffmann"
                                },
                                {
                                    "authorId": "2072572294",
                                    "name": "Trevor Cai"
                                },
                                {
                                    "authorId": "2143538252",
                                    "name": "Eliza Rutherford"
                                },
                                {
                                    "authorId": "2143434227",
                                    "name": "Katie Millican"
                                },
                                {
                                    "authorId": "47568983",
                                    "name": "George van den Driessche"
                                },
                                {
                                    "authorId": "143783339",
                                    "name": "Jean-Baptiste Lespiau"
                                },
                                {
                                    "authorId": "2143374656",
                                    "name": "Bogdan Damoc"
                                },
                                {
                                    "authorId": "31993415",
                                    "name": "Aidan Clark"
                                },
                                {
                                    "authorId": "40550616",
                                    "name": "Diego de Las Casas"
                                },
                                {
                                    "authorId": "40895205",
                                    "name": "Aurelia Guy"
                                },
                                {
                                    "authorId": "10698483",
                                    "name": "Jacob Menick"
                                },
                                {
                                    "authorId": "81387328",
                                    "name": "Roman Ring"
                                },
                                {
                                    "authorId": "4629007",
                                    "name": "T. Hennigan"
                                },
                                {
                                    "authorId": "2148653469",
                                    "name": "Saffron Huang"
                                },
                                {
                                    "authorId": "108173905",
                                    "name": "Lorenzo Maggiore"
                                },
                                {
                                    "authorId": "2115601070",
                                    "name": "Chris Jones"
                                },
                                {
                                    "authorId": "51042571",
                                    "name": "Albin Cassirer"
                                },
                                {
                                    "authorId": "2065040422",
                                    "name": "Andy Brock"
                                },
                                {
                                    "authorId": "35550664",
                                    "name": "Michela Paganini"
                                },
                                {
                                    "authorId": "2060655766",
                                    "name": "G. Irving"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "2217144",
                                    "name": "Simon Osindero"
                                },
                                {
                                    "authorId": "34838386",
                                    "name": "K. Simonyan"
                                },
                                {
                                    "authorId": "34269227",
                                    "name": "Jack W. Rae"
                                },
                                {
                                    "authorId": "152585800",
                                    "name": "Erich Elsen"
                                },
                                {
                                    "authorId": "2175946",
                                    "name": "L. Sifre"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1100
                        },
                        "score": 0
                    },
                    {
                        "id": "(Guo et al., 2024)",
                        "snippets": [
                            "Recently, a series of retrieval-augmented language models [14,24,47] have been proposed to augment language models with external knowledge [9,17,53]. Retrieval-augmented techniques can generally be divided into two types. The first type is at the input layer [14,20,42], where the retrieved information is text chunks. The second type is at the output layer [7,24,47], where the retrieved information is tokens. By combining the retrieved tokens with the tokens generated by the original model, the accuracy of the retrieval-augmented model's generation for each token can be improved.\n\nThe first type of method can provide the model with more external knowledge, making it adept at handling tasks in the NLP field such as knowledge-based question answering [27,45,49]. The second type of method can refer to the retrieved information to correct the generated tokens, making it more suited for handling strictly structured generative tasks, such as code completion [7,10,11].\n\nTo better understand the mechanism, we take kNN-LM [24] as an example for a detailed explanation. Given a context sequence   = ( 1 , . . .,   \u22121 ), the language models (LMs) estimate   (  |  ), i.e., the probability distribution over the next token   . kNN-LM is designed to augment a pre-trained language model with a set of nearest neighbours retrieved from an external text collection, which can be the training set."
                        ],
                        "paper": {
                            "corpus_id": 268856642,
                            "title": "FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion",
                            "authors": [
                                {
                                    "authorId": "2290464625",
                                    "name": "Qi Guo"
                                },
                                {
                                    "authorId": "2118890600",
                                    "name": "Xiaohong Li"
                                },
                                {
                                    "authorId": "2288741802",
                                    "name": "Xiaofei Xie"
                                },
                                {
                                    "authorId": "2290359321",
                                    "name": "Shangqing Liu"
                                },
                                {
                                    "authorId": "2109915677",
                                    "name": "Ze Tang"
                                },
                                {
                                    "authorId": "1758019",
                                    "name": "Ruitao Feng"
                                },
                                {
                                    "authorId": "2294667814",
                                    "name": "Junjie Wang"
                                },
                                {
                                    "authorId": "2248015856",
                                    "name": "Jidong Ge"
                                },
                                {
                                    "authorId": "2279752248",
                                    "name": "Lei Bu"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Symposium on Software Testing and Analysis",
                            "n_citations": 11
                        },
                        "score": 0.81396484375
                    },
                    {
                        "id": "(Tang et al., 2023)",
                        "snippets": [
                            "Retrieval-augmented language models (R-LMs) utilize retrieval-based techniques to improve the performance of LMs and can be divided into two main categories: block R-LMs and token R-LMs. Block R-LMs [12], [13], [48] are similar to one-shot or few-shot learning [49], where one or a few examples are retrieved from a database instead of being randomly selected. Token R-LMs [15], [24], [50] retrieve tokens from database and then combine the retrieved results into the LM. Compared with block R-LMs, token R-LMs can update retrieval results at the same time of generating new tokens, hence our approach uses the architecture of token R-LM. However, token R-LMs suffer from high storage costs and require hyper-parameters selection to combine the inference results from the database and language model.\n\nVarious approaches have been proposed to address the limitations of token R-LMs. For example, kNN-Adapter [51] uses a trained network to determine the combination weights. To reduce the search cost, RetoMaton [52] uses the automaton states to save search time, while AdaptRet [53] uses a trained network to decide whether to use the retrieval module. GNN-LM [50] selects similar texts and builds a contextual graph to incorporate into the language model."
                        ],
                        "paper": {
                            "corpus_id": 261030382,
                            "title": "Domain Adaptive Code Completion via Language Models and Decoupled Domain Databases",
                            "authors": [
                                {
                                    "authorId": "2109915677",
                                    "name": "Ze Tang"
                                },
                                {
                                    "authorId": "2669512",
                                    "name": "Jidong Ge"
                                },
                                {
                                    "authorId": "13877308",
                                    "name": "Shangqing Liu"
                                },
                                {
                                    "authorId": "3274600",
                                    "name": "Tingwei Zhu"
                                },
                                {
                                    "authorId": "2118717147",
                                    "name": "Tongtong Xu"
                                },
                                {
                                    "authorId": "1482584966",
                                    "name": "LiGuo Huang"
                                },
                                {
                                    "authorId": "2075400450",
                                    "name": "Bin Luo"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Automated Software Engineering",
                            "n_citations": 26
                        },
                        "score": 0.7783203125
                    },
                    {
                        "id": "(Guu et al., 2020)",
                        "snippets": [
                            "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."
                        ],
                        "paper": {
                            "corpus_id": 211204736,
                            "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
                            "authors": [
                                {
                                    "authorId": "2091768",
                                    "name": "Kelvin Guu"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "9941702",
                                    "name": "Zora Tung"
                                },
                                {
                                    "authorId": "2616463",
                                    "name": "Panupong Pasupat"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 2119
                        },
                        "score": 0
                    },
                    {
                        "id": "(Shi et al., 2023)",
                        "snippets": [
                            "Retrieval-augmented Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language modeling (Min et al., 2022;(Borgeaud et al., 2021)(Khandelwal et al., 2019) and open-domain question answering (Lewis et al., 2020;Izacard et al., 2022b;Hu et al., 2022). Specifically, using the input as query, (1) a retriever first retrieves a set of documents (i.e., sequences of tokens) from a corpus and then (2) a language model incorporates the retrieved documents as additional information to make a final prediction. This style of retrieval can be added to both encoderdecoder (Yu, 2022)Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2019)(Borgeaud et al., 2021)Shi et al., 2022;(Rubin et al., 2021). For example, Atlas (Izacard et al., 2022b) finetunes an encoder-decoder model jointly with the retriever by modeling documents as latent variables, while RETRO (Borgeaud et al., 2021) changes the decoderonly architecture to incorporate retrieved texts and pretrains the language model from scratch. Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs. Another line of retrieval-augmented LMs such as kNN-LM (Khandelwal et al., 2019)Zhong et al., 2022) retrieves a set of tokens and interpolates between the LM's next token distribution and kNN distributions computed from the retrieved tokens at inference."
                        ],
                        "paper": {
                            "corpus_id": 256389797,
                            "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                            "authors": [
                                {
                                    "authorId": "3040379",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "48872685",
                                    "name": "Sewon Min"
                                },
                                {
                                    "authorId": "19168196",
                                    "name": "Michihiro Yasunaga"
                                },
                                {
                                    "authorId": "4418074",
                                    "name": "Minjoon Seo"
                                },
                                {
                                    "authorId": "2191899140",
                                    "name": "Rich James"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "2072801764",
                                    "name": "Wen-tau Yih"
                                }
                            ],
                            "year": 2023,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 641
                        },
                        "score": 0.7978515625
                    },
                    {
                        "id": "(Rubin et al., 2021)",
                        "snippets": [
                            "In-context learning is a recent paradigm in natural language understanding, where a large pre-trained language model (LM) observes a test instance and a few training examples as its input, and directly decodes the output without any update to its parameters. However, performance has been shown to strongly depend on the selected training examples (termed prompts). In this work, we propose an efficient method for retrieving prompts for in-context learning using annotated data and an LM. Given an input-output pair, we estimate the probability of the output given the input and a candidate training example as the prompt, and label training examples as positive or negative based on this probability. We then train an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time. We evaluate our approach on three sequence-to-sequence tasks where language utterances are mapped to meaning representations, and find that it substantially outperforms prior work and multiple baselines across the board."
                        ],
                        "paper": {
                            "corpus_id": 245218561,
                            "title": "Learning To Retrieve Prompts for In-Context Learning",
                            "authors": [
                                {
                                    "authorId": "2001128224",
                                    "name": "Ohad Rubin"
                                },
                                {
                                    "authorId": "47426264",
                                    "name": "Jonathan Herzig"
                                },
                                {
                                    "authorId": "1750652",
                                    "name": "Jonathan Berant"
                                }
                            ],
                            "year": 2021,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 709
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2025)",
                        "snippets": [
                            "Recent efforts to improve RAG focus on two fronts: 1) enhancing retrieval efficiency through adaptive and modular frameworks (Gan et al., 2024;Ravuru et al., 2024;Zhang et al., 2024a); and 2) better structuring external knowledge, with graphbased RAGs emerging as a dominant approach (Edge et al., 2024;Guo et al., 2024;Potts, 2024)."
                        ],
                        "paper": {
                            "corpus_id": 277313225,
                            "title": "CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2352004975",
                                    "name": "Nengbo Wang"
                                },
                                {
                                    "authorId": "2346134041",
                                    "name": "Xiaotian Han"
                                },
                                {
                                    "authorId": "2352427030",
                                    "name": "Jagdip Singh"
                                },
                                {
                                    "authorId": "2352917796",
                                    "name": "Jing Ma"
                                },
                                {
                                    "authorId": "2346129602",
                                    "name": "Vipin Chaudhary"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.775390625
                    },
                    {
                        "id": "(Lee et al., 2025)",
                        "snippets": [
                            "Retrieval-Augmented Generation (RAG) augments large language models by incorporating external documents into the generation process. Rather than relying solely on the model's internal parameters, RAG retrieves relevant passages from a knowledge base and feeds them, along with the user query, into the model to guide its response (Zhou et al., 2024). \n\nA standard RAG system operates in two phases: \n\n\u2022 Retrieval. A retriever model selects the top-k most relevant documents for a given query. \n\n\u2022 Generation. A language model generates a response conditioned on both the query and the retrieved documents. \n\nThe appeal of RAG lies in its ability to dynamically access up-to-date or domain-specific information, which is especially useful in fast-changing or specialized fields. However, it also introduces new failure modes, particularly when the retrieval quality is imperfect (Barnett et al., 2024)."
                        ],
                        "paper": {
                            "corpus_id": 278714952,
                            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2362089035",
                                    "name": "Zhan Peng Lee"
                                },
                                {
                                    "authorId": "2362188632",
                                    "name": "Andre Lin"
                                },
                                {
                                    "authorId": "2363425126",
                                    "name": "Calvin Tan"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.76171875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Major Retrieval-Augmented Models and Their Designs",
                "tldr": "Key retrieval-augmented language models include kNN-LM, REALM, RAG, RETRO, and FiD, each employing distinct architectural approaches to integrate external knowledge. These models differ in their retrieval granularity, integration methods, and underlying architectures, with each design offering specific advantages for different knowledge-intensive NLP tasks. (14 sources)",
                "text": "\n## kNN-LM (Khandelwal et al., 2019)\n- **Core Mechanism**: Extends pre-trained language models by interpolating their predictions with a k-nearest neighbors distribution\n- **Architecture Type**: Decoder-only with token-level retrieval (output-layer augmentation)\n- **Retrieval Approach**: Retrieves tokens based on similarity in the embedding space of the pre-trained model\n- **Integration Method**: Non-parametric interpolation between the model's predictions and the kNN distribution\n- **Key Advantage**: Effective for rare patterns and factual knowledge without requiring model retraining\n- **Applications**: Language modeling, domain adaptation by simply varying the nearest neighbor datastore\n\n <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper> <Paper corpusId=\"256389797\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268031947\" paperTitle=\"(Cao et al., 2024)\" isShortName></Paper>\n\n## REALM (Guu et al., 2020)\n- **Core Mechanism**: Jointly pre-trained knowledge retriever and knowledge-augmented encoder\n- **Architecture Type**: Encoder-only with block-level retrieval\n- **Retrieval Approach**: Neural retriever trained with masked language modeling signal\n- **Integration Method**: Architectural modification with differentiable access to documents\n- **Key Advantage**: Joint unsupervised training of retriever and model for cohesive performance\n- **Applications**: Open-domain question answering, outperforming previous methods by 4-16% absolute accuracy\n\n <Paper corpusId=\"211204736\" paperTitle=\"(Guu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"252735056\" paperTitle=\"(Siriwardhana et al., 2022)\" isShortName></Paper> <Paper corpusId=\"252735160\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper>\n\n## RAG (Lewis et al., 2020)\n- **Core Mechanism**: Combines parametric memory (seq2seq model) with non-parametric memory (dense vector index)\n- **Architecture Type**: Encoder-decoder with block-level retrieval (input-layer augmentation)\n- **Retrieval Approach**: Pre-trained neural retriever (e.g., DPR) accessing Wikipedia knowledge base\n- **Integration Method**: Two variants: one conditioning on the same passages across generation, another using different passages per token\n- **Key Advantage**: Differentiable end-to-end training of both retriever and generator\n- **Applications**: Knowledge-intensive tasks like open-domain QA, question generation, fact verification\n\n <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper> <Paper corpusId=\"252735056\" paperTitle=\"(Siriwardhana et al., 2022)\" isShortName></Paper> <Paper corpusId=\"259360590\" paperTitle=\"(Lyu et al., 2023)\" isShortName></Paper>\n\n## RETRO (Borgeaud et al., 2021)\n- **Core Mechanism**: Enhances autoregressive language models with document chunks retrieved from a massive corpus\n- **Architecture Type**: Decoder-only with block-level retrieval\n- **Retrieval Approach**: BERT-based retriever accessing a 2 trillion token database\n- **Integration Method**: Chunked cross-attention mechanism to integrate retrieved context\n- **Key Advantage**: Achieves performance comparable to much larger models (GPT-3, Jurassic-1) with 25x fewer parameters\n- **Applications**: General language modeling, downstream knowledge-intensive tasks like question answering\n\n <Paper corpusId=\"244954723\" paperTitle=\"(Borgeaud et al., 2021)\" isShortName></Paper> <Paper corpusId=\"256389797\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"266163023\" paperTitle=\"(Khosla et al., 2023)\" isShortName></Paper>\n\n## FiD (Fusion-in-Decoder) (Izacard et al., 2020)\n- **Core Mechanism**: Encodes multiple retrieved passages independently and fuses them in the decoder\n- **Architecture Type**: Encoder-decoder with block-level retrieval\n- **Retrieval Approach**: Retrieves multiple text passages potentially containing evidence\n- **Integration Method**: Processes each passage separately in the encoder, then fuses all encoded passages in the decoder\n- **Key Advantage**: Computational cost scales linearly (not quadratically) with the number of passages\n- **Applications**: Open-domain question answering, achieving state-of-the-art results on benchmarks like Natural Questions and TriviaQA\n\n <Paper corpusId=\"220302360\" paperTitle=\"(Izacard et al., 2020)\" isShortName></Paper> <Paper corpusId=\"252735160\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper> <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>\n\n## Atlas (Izacard et al., 2022)\n- **Core Mechanism**: Carefully designed and pre-trained retrieval-augmented language model\n- **Architecture Type**: Encoder-decoder with Fusion-in-Decoder approach\n- **Retrieval Approach**: Jointly trained retriever that models documents as latent variables\n- **Integration Method**: Fusion of encoded passages in the decoder\n- **Key Advantage**: Few-shot learning capabilities for knowledge-intensive tasks\n- **Applications**: Knowledge-intensive tasks with minimal examples, reaching 42% accuracy on Natural Questions with only 64 examples\n\n <Paper corpusId=\"251371732\" paperTitle=\"(Izacard et al., 2022)\" isShortName></Paper> <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>\n\n## REPLUG\n- **Core Mechanism**: Assumes black-box access to language models\n- **Architecture Type**: Model-agnostic approach (in-context learning)\n- **Retrieval Approach**: Optimized retriever through fine-tuning\n- **Integration Method**: Prepends retrieved information to input context\n- **Key Advantage**: Compatible with black-box LLMs without modifying model architecture\n- **Applications**: Adaptable to various language models without requiring access to their parameters\n\n <Paper corpusId=\"256389797\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"269983737\" paperTitle=\"(Jiao et al., 2024)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Khandelwal et al., 2019)",
                        "snippets": [
                            "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."
                        ],
                        "paper": {
                            "corpus_id": 207870430,
                            "title": "Generalization through Memorization: Nearest Neighbor Language Models",
                            "authors": [
                                {
                                    "authorId": "3030219",
                                    "name": "Urvashi Khandelwal"
                                },
                                {
                                    "authorId": "39455775",
                                    "name": "Omer Levy"
                                },
                                {
                                    "authorId": "1746807",
                                    "name": "Dan Jurafsky"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 842
                        },
                        "score": 0
                    },
                    {
                        "id": "(Shi et al., 2023)",
                        "snippets": [
                            "Retrieval-augmented Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language modeling (Min et al., 2022;(Borgeaud et al., 2021)(Khandelwal et al., 2019) and open-domain question answering (Lewis et al., 2020;Izacard et al., 2022b;Hu et al., 2022). Specifically, using the input as query, (1) a retriever first retrieves a set of documents (i.e., sequences of tokens) from a corpus and then (2) a language model incorporates the retrieved documents as additional information to make a final prediction. This style of retrieval can be added to both encoderdecoder (Yu, 2022)Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2019)(Borgeaud et al., 2021)Shi et al., 2022;(Rubin et al., 2021). For example, Atlas (Izacard et al., 2022b) finetunes an encoder-decoder model jointly with the retriever by modeling documents as latent variables, while RETRO (Borgeaud et al., 2021) changes the decoderonly architecture to incorporate retrieved texts and pretrains the language model from scratch. Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs. Another line of retrieval-augmented LMs such as kNN-LM (Khandelwal et al., 2019)Zhong et al., 2022) retrieves a set of tokens and interpolates between the LM's next token distribution and kNN distributions computed from the retrieved tokens at inference."
                        ],
                        "paper": {
                            "corpus_id": 256389797,
                            "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                            "authors": [
                                {
                                    "authorId": "3040379",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "48872685",
                                    "name": "Sewon Min"
                                },
                                {
                                    "authorId": "19168196",
                                    "name": "Michihiro Yasunaga"
                                },
                                {
                                    "authorId": "4418074",
                                    "name": "Minjoon Seo"
                                },
                                {
                                    "authorId": "2191899140",
                                    "name": "Rich James"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "2072801764",
                                    "name": "Wen-tau Yih"
                                }
                            ],
                            "year": 2023,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 641
                        },
                        "score": 0.7978515625
                    },
                    {
                        "id": "(Cao et al., 2024)",
                        "snippets": [
                            "kNN-LM (Khandelwal et al., 2020) is a retrieval-augmented LM that interpolates the next-token distribution of the base LM with a k-nearest neighbors (kNN) model.\n\nRETRO (Borgeaud et al., 2022) is a retrieval-augmented LM incorporated with a pre-trained document retriever, a document encoder and a cross-attention mechanism.\n\nCoG (Lan et al., 2023) is another retrieval-augmented LM that adopts a two-stage search pipeline. It first retrieves semantically-relevant documents, and then considers all n-grams within them as candidate phrases."
                        ],
                        "paper": {
                            "corpus_id": 268031947,
                            "title": "Retrieval is Accurate Generation",
                            "authors": [
                                {
                                    "authorId": "2209367631",
                                    "name": "Bowen Cao"
                                },
                                {
                                    "authorId": "2266753374",
                                    "name": "Deng Cai"
                                },
                                {
                                    "authorId": "2279792419",
                                    "name": "Leyang Cui"
                                },
                                {
                                    "authorId": null,
                                    "name": "Xuxin Cheng"
                                },
                                {
                                    "authorId": "2237804371",
                                    "name": "Wei Bi"
                                },
                                {
                                    "authorId": "2260859476",
                                    "name": "Yuexian Zou"
                                },
                                {
                                    "authorId": "2257446263",
                                    "name": "Shuming Shi"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 8
                        },
                        "score": 0.763671875
                    },
                    {
                        "id": "(Guu et al., 2020)",
                        "snippets": [
                            "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."
                        ],
                        "paper": {
                            "corpus_id": 211204736,
                            "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
                            "authors": [
                                {
                                    "authorId": "2091768",
                                    "name": "Kelvin Guu"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "9941702",
                                    "name": "Zora Tung"
                                },
                                {
                                    "authorId": "2616463",
                                    "name": "Panupong Pasupat"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 2119
                        },
                        "score": 0
                    },
                    {
                        "id": "(Siriwardhana et al., 2022)",
                        "snippets": [
                            "Recently, Retrieval Augmented Architectures (Lewis et al., 2020b;Guu et al., 2020) have drawn a lot of attention due to their explainable, scalable, and adaptable nature. Unlike other open-domain QA architectures, RAG (Lewis et al., 2020b) combines the information retrieval stage and answer generation stage in a differentiable manner. It uses a combination of parametric and non-parametric memory, where the parametric memory consists of a pre-trained seq2seq BART (Lewis et al., 2019) generator, and the non-parametric memory consists of dense vector representations of Wikipedia articles indexed with the FAISS library (Johnson et al., 2017). RAG first encodes a question into a dense representation, retrieves the relevant passages from an indexed Wikipedia knowledge base, and then feeds them into the generator. The loss function can finetune both the generator and the question encoder at the same time. Lewis et al. (Lewis et al., 2020b) highlight RAG's ability to perform well in Wikipedia-based general question-answering datasets like Natural Questions (Kwiatkowski et al., 2019)",
                            "(Guu et al., 2020) is a similar Retrieval Augmented model to RAG. REALM introduced a novel masked language pre-training step that involves an end-to-end trainable retriever."
                        ],
                        "paper": {
                            "corpus_id": 252735056,
                            "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "51516859",
                                    "name": "Shamane Siriwardhana"
                                },
                                {
                                    "authorId": "52001535",
                                    "name": "Rivindu Weerasekera"
                                },
                                {
                                    "authorId": "2114425044",
                                    "name": "Elliott Wen"
                                },
                                {
                                    "authorId": "1992921690",
                                    "name": "Tharindu Kaluarachchi"
                                },
                                {
                                    "authorId": "1814487",
                                    "name": "R. Rana"
                                },
                                {
                                    "authorId": "1486464114",
                                    "name": "Suranga Nanayakkara"
                                }
                            ],
                            "year": 2022,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 179
                        },
                        "score": 0.77880859375
                    },
                    {
                        "id": "(Chen et al., 2022)",
                        "snippets": [
                            "Retrieval Augmented Models Retrieval augmented models are hybrid models containing both parameterized sequence models and a nonparametric memory, infusing world knowledge into existing language models. Among them, KNN-LM (Khandelwal et al., 2019) was first proposed to retrieve instances from a text training corpus to help language modeling. Later, RETRO (Borgeaud et al., 2021) was proposed to scale up the text corpus to trillions of tokens, enabling the model to achieve similar perplexity to GPT-3 (Brown et al., 2020)) with 25x fewer model parameters. Another family of models, such as REALM (Guu et al., 2020), RAG (Lewis et al., 2020), and FiD (Izacard et al., 2020), integrate Wikipedia passages as a datastore to benefit downstream knowledge intensive tasks (e.g. Question Answering). REALM is an encoder-only model trained with masked lan-guage modeling, while RAG and FiD adopt an encoder-decoder model with a generative language modeling objective."
                        ],
                        "paper": {
                            "corpus_id": 252735160,
                            "title": "MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text",
                            "authors": [
                                {
                                    "authorId": "2928777",
                                    "name": "Wenhu Chen"
                                },
                                {
                                    "authorId": "2804000",
                                    "name": "Hexiang Hu"
                                },
                                {
                                    "authorId": "2145309103",
                                    "name": "Xi Chen"
                                },
                                {
                                    "authorId": "2986975",
                                    "name": "Pat Verga"
                                },
                                {
                                    "authorId": "50056360",
                                    "name": "William W. Cohen"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 159
                        },
                        "score": 0.76220703125
                    },
                    {
                        "id": "(Lewis et al., 2020)",
                        "snippets": [
                            "We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token."
                        ],
                        "paper": {
                            "corpus_id": 218869575,
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "authors": [
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3439053",
                                    "name": "Ethan Perez"
                                },
                                {
                                    "authorId": "1716179427",
                                    "name": "Aleksandara Piktus"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "103131985",
                                    "name": "Heinrich Kuttler"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "2620211",
                                    "name": "Tim Rockt\u00e4schel"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "1743722",
                                    "name": "Douwe Kiela"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6476
                        },
                        "score": 0.82568359375
                    },
                    {
                        "id": "(Lyu et al., 2023)",
                        "snippets": [
                            "Retrieval-augmented (RAG) models have recently been proposed [12,14]8]. A typical retrieval-augmented model consists of two parts, a retriever f ret and a generator f gen. Given a retrieval corpus D ret = {d 1, \u2022 \u2022 \u2022, d M}, the retriever f ret retrieves K data points for an input x i as f ret (x i, D ret) = {d \u03b11, d \u03b12,",
                            ", d \u03b1 K}. Here, \u03b1 k denotes the rank of each data point in the retrieval corpus assigned by the retriever."
                        ],
                        "paper": {
                            "corpus_id": 259360590,
                            "title": "Improving Retrieval-Augmented Large Language Models via Data Importance Learning",
                            "authors": [
                                {
                                    "authorId": "35308280",
                                    "name": "Xiaozhong Lyu"
                                },
                                {
                                    "authorId": "1393463989",
                                    "name": "Stefan Grafberger"
                                },
                                {
                                    "authorId": "2086970867",
                                    "name": "Samantha Biegel"
                                },
                                {
                                    "authorId": "1409866591",
                                    "name": "Shaopeng Wei"
                                },
                                {
                                    "authorId": "2057073725",
                                    "name": "Meng Cao"
                                },
                                {
                                    "authorId": "2180399",
                                    "name": "Sebastian Schelter"
                                },
                                {
                                    "authorId": "1776014",
                                    "name": "Ce Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 18
                        },
                        "score": 0.763671875
                    },
                    {
                        "id": "(Borgeaud et al., 2021)",
                        "snippets": [
                            "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."
                        ],
                        "paper": {
                            "corpus_id": 244954723,
                            "title": "Improving language models by retrieving from trillions of tokens",
                            "authors": [
                                {
                                    "authorId": "148016269",
                                    "name": "Sebastian Borgeaud"
                                },
                                {
                                    "authorId": "1697879",
                                    "name": "A. Mensch"
                                },
                                {
                                    "authorId": "46616544",
                                    "name": "Jordan Hoffmann"
                                },
                                {
                                    "authorId": "2072572294",
                                    "name": "Trevor Cai"
                                },
                                {
                                    "authorId": "2143538252",
                                    "name": "Eliza Rutherford"
                                },
                                {
                                    "authorId": "2143434227",
                                    "name": "Katie Millican"
                                },
                                {
                                    "authorId": "47568983",
                                    "name": "George van den Driessche"
                                },
                                {
                                    "authorId": "143783339",
                                    "name": "Jean-Baptiste Lespiau"
                                },
                                {
                                    "authorId": "2143374656",
                                    "name": "Bogdan Damoc"
                                },
                                {
                                    "authorId": "31993415",
                                    "name": "Aidan Clark"
                                },
                                {
                                    "authorId": "40550616",
                                    "name": "Diego de Las Casas"
                                },
                                {
                                    "authorId": "40895205",
                                    "name": "Aurelia Guy"
                                },
                                {
                                    "authorId": "10698483",
                                    "name": "Jacob Menick"
                                },
                                {
                                    "authorId": "81387328",
                                    "name": "Roman Ring"
                                },
                                {
                                    "authorId": "4629007",
                                    "name": "T. Hennigan"
                                },
                                {
                                    "authorId": "2148653469",
                                    "name": "Saffron Huang"
                                },
                                {
                                    "authorId": "108173905",
                                    "name": "Lorenzo Maggiore"
                                },
                                {
                                    "authorId": "2115601070",
                                    "name": "Chris Jones"
                                },
                                {
                                    "authorId": "51042571",
                                    "name": "Albin Cassirer"
                                },
                                {
                                    "authorId": "2065040422",
                                    "name": "Andy Brock"
                                },
                                {
                                    "authorId": "35550664",
                                    "name": "Michela Paganini"
                                },
                                {
                                    "authorId": "2060655766",
                                    "name": "G. Irving"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "2217144",
                                    "name": "Simon Osindero"
                                },
                                {
                                    "authorId": "34838386",
                                    "name": "K. Simonyan"
                                },
                                {
                                    "authorId": "34269227",
                                    "name": "Jack W. Rae"
                                },
                                {
                                    "authorId": "152585800",
                                    "name": "Erich Elsen"
                                },
                                {
                                    "authorId": "2175946",
                                    "name": "L. Sifre"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1100
                        },
                        "score": 0
                    },
                    {
                        "id": "(Khosla et al., 2023)",
                        "snippets": [
                            "Retrieval Augmented Language Model (REALM) [27] is the first method to jointly train a knowledge retriever and a knowledge-augmented language encoder in an unsupervised manner. Retrieval augmented generation (RAG) [6] fine-tunes a pre-trained retriever (e.g., DPR [28]) and a pre-trained sequence-to-sequence model (e.g., BART [29]). RAG achieves superior performance on various knowledge-intensive tasks, including question answering, question generation and fact verification. Retrieval Augmented Translation (RAT) (Hoang et al., 2022) improves neural machine translation by treating the external knowledge base as a dictionary",
                            "Retrieval Enhanced Transformers (RETRO) [5] augments a language model with an external knowledge base consisting of 2 trillion tokens, and achieves performance comparable to GPT-3 (Brown et al., 2020) and Jurassic-1 [33], which has 25x more parameters."
                        ],
                        "paper": {
                            "corpus_id": 266163023,
                            "title": "Survey on Memory-Augmented Neural Networks: Cognitive Insights to AI Applications",
                            "authors": [
                                {
                                    "authorId": "2056070459",
                                    "name": "Savya Khosla"
                                },
                                {
                                    "authorId": "2273562657",
                                    "name": "Zhen Zhu"
                                },
                                {
                                    "authorId": "2273661082",
                                    "name": "Yifie He"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.81494140625
                    },
                    {
                        "id": "(Izacard et al., 2020)",
                        "snippets": [
                            "Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages."
                        ],
                        "paper": {
                            "corpus_id": 220302360,
                            "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "1410231361",
                                    "name": "Gautier Izacard"
                                },
                                {
                                    "authorId": "3024698",
                                    "name": "Edouard Grave"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 1181
                        },
                        "score": 0
                    },
                    {
                        "id": "(Huang et al., 2023)",
                        "snippets": [
                            "Retrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2022)(Lewis et al., 2020) and decoderonly (Khandelwal et al., 2019)Borgeaud et al., 2022;(Shi et al., 2022) architectures. For decoder-only LMs, the computational cost typically increases quadratically with the input length, as well as with the number of retrieval passages. In contrast, for encoder-decoder LMs with a Fusion-in-Decoder architecture, the computation cost grows linearly with the number of retrieved passages, as they only perform self-attention over one passage at a time (Izacard et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 260900354,
                            "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models",
                            "authors": [
                                {
                                    "authorId": "1490651934",
                                    "name": "Jie Huang"
                                },
                                {
                                    "authorId": "2056440915",
                                    "name": "Wei Ping"
                                },
                                {
                                    "authorId": "145011005",
                                    "name": "Peng Xu"
                                },
                                {
                                    "authorId": "1911755",
                                    "name": "M. Shoeybi"
                                },
                                {
                                    "authorId": "143922493",
                                    "name": "K. Chang"
                                },
                                {
                                    "authorId": "2301680",
                                    "name": "Bryan Catanzaro"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 35
                        },
                        "score": 0.767578125
                    },
                    {
                        "id": "(Izacard et al., 2022)",
                        "snippets": [
                            "Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters."
                        ],
                        "paper": {
                            "corpus_id": 251371732,
                            "title": "Few-shot Learning with Retrieval Augmented Language Models",
                            "authors": [
                                {
                                    "authorId": "1410231361",
                                    "name": "Gautier Izacard"
                                },
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3376175",
                                    "name": "M. Lomeli"
                                },
                                {
                                    "authorId": "26360550",
                                    "name": "Lucas Hosseini"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "32246932",
                                    "name": "Timo Schick"
                                },
                                {
                                    "authorId": "2129456957",
                                    "name": "Jane A. Yu"
                                },
                                {
                                    "authorId": "2319608",
                                    "name": "Armand Joulin"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "3024698",
                                    "name": "Edouard Grave"
                                }
                            ],
                            "year": 2022,
                            "venue": "Journal of machine learning research",
                            "n_citations": 783
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jiao et al., 2024)",
                        "snippets": [
                            "Retrieval-Augmented Language Models Augmenting language models with relevant information obtained from various external knowledge bases has been shown to significantly improve the performance of various NLP tasks, including language modeling (Guu et al., 2020)(Borgeaud et al., 2021)Shi et al., 2023;Lin et al., 2023) and open domain question answering (Izacard et al., 2022;Zhang et al., 2024).RAG mainly adopts the \"retrieve then read\" paradigm.Specifically, the input question is first used as the query, then the retrieval module retrieves relevant documents from the external knowledge base, and finally the retrieved documents and questions are merged into a complete input to generate final output.For example, RETRO (Borgeaud et al., 2021) modifies the autoregressive LM to focus on relevant documents through chunked cross-attention, thereby introducing new parameters to the model.REPLUG (Shi et al., 2023) assumes black-box access to LM and optimizes it by fine-tuning the retriever.RAFT (Zhang et al., 2024) proposes a fine-tuned data that additionally contains relevant documents and answers with reasoning chains to train language models for domain-specific open-book settings."
                        ],
                        "paper": {
                            "corpus_id": 269983737,
                            "title": "DuetRAG: Collaborative Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2302798653",
                                    "name": "Dian Jiao"
                                },
                                {
                                    "authorId": "2303434387",
                                    "name": "Li Cai"
                                },
                                {
                                    "authorId": "2303044665",
                                    "name": "Jingsheng Huang"
                                },
                                {
                                    "authorId": "2108125912",
                                    "name": "Wenqiao Zhang"
                                },
                                {
                                    "authorId": "2118071462",
                                    "name": "Siliang Tang"
                                },
                                {
                                    "authorId": "2253660817",
                                    "name": "Yueting Zhuang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.77490234375
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Retrieval Mechanisms in RAG Systems",
                "tldr": "Retrieval mechanisms in RAG systems can be categorized into dense and sparse approaches, each offering different strengths for knowledge retrieval. These retrieval systems work by either matching keywords directly (sparse retrieval) or capturing semantic relationships through neural embeddings (dense retrieval), with each approach suited to different types of knowledge-intensive tasks. (14 sources)",
                "text": "\nRetrieval is the cornerstone of Retrieval-Augmented Language Models, serving as the bridge between user queries and external knowledge sources. The effectiveness of a RAG model depends significantly on its ability to identify and retrieve relevant information from its knowledge base.\n\n## Types of Retrieval Mechanisms\n\nRetrieval mechanisms in RAG systems generally fall into two main categories:\n\n- **Sparse Retrievers**: These approaches rely on bag-of-words representations and focus on term frequency matching. They excel at finding documents with high lexical overlap with the query, making them particularly effective when exact terminology matches are important <Paper corpusId=\"274982275\" paperTitle=\"(Zayyad et al., 2024)\" isShortName></Paper>. A common implementation is keyword-to-document mapping, where specific keywords directly link to relevant documents or entries containing those terms <Paper corpusId=\"274131235\" paperTitle=\"(Shu et al., 2024)\" isShortName></Paper>.\n\n- **Dense Retrievers**: These systems utilize neural network embeddings to capture semantic similarities between queries and documents. Unlike sparse retrievers, they can identify conceptual relationships even when exact terminology differs <Paper corpusId=\"274982275\" paperTitle=\"(Zayyad et al., 2024)\" isShortName></Paper>. Dense retrievers convert text into high-dimensional vectors that represent semantic meaning, enabling more nuanced matching beyond surface-level word overlap <Paper corpusId=\"274131235\" paperTitle=\"(Shu et al., 2024)\" isShortName></Paper>.\n\n## Retrieval Implementation Approaches\n\nThe implementation of retrieval mechanisms in RAG systems varies across different architectures:\n\n- **Pre-trained Neural Retrievers**: Many RAG models, including the original RAG architecture, utilize pre-trained neural retrievers such as Dense Passage Retriever (DPR) to access external knowledge bases like Wikipedia <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>. These retrievers are specifically designed to understand the semantic relationship between queries and documents.\n\n- **BERT-based Retrievers**: Models like RETRO employ BERT-based retrievers that access massive token databases (up to 2 trillion tokens in RETRO's case) <Paper corpusId=\"244954723\" paperTitle=\"(Borgeaud et al., 2021)\" isShortName></Paper> <Paper corpusId=\"256389797\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>.\n\n- **Differentiable Retrieval**: Some approaches, like REALM, implement differentiable access to documents, allowing joint training of both the retriever and the language model through a masked language modeling signal <Paper corpusId=\"252735056\" paperTitle=\"(Siriwardhana et al., 2022)\" isShortName></Paper>.\n\n- **k-Nearest Neighbors**: kNN-LM utilizes a k-nearest neighbors approach that retrieves tokens based on similarity in the embedding space of the pre-trained model <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper> <Paper corpusId=\"256389797\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. This method extends pre-trained language models by interpolating their predictions with a k-nearest neighbors distribution <Paper corpusId=\"268856642\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.\n\n## Retrieval Workflow\n\nThe standard retrieval workflow in RAG systems typically follows a consistent pattern:\n\n1. **Query Processing**: The user query is encoded into a dense representation or processed for keyword matching <Paper corpusId=\"259360590\" paperTitle=\"(Lyu et al., 2023)\" isShortName></Paper>.\n\n2. **Corpus Search**: The retriever searches through an indexed knowledge base (such as Wikipedia articles) to find relevant documents or passages <Paper corpusId=\"252735056\" paperTitle=\"(Siriwardhana et al., 2022)\" isShortName></Paper>.\n\n3. **Ranking**: The retrieved candidates are ranked based on their relevance to the query, with the top-K most relevant documents selected for further processing <Paper corpusId=\"278714952\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>.\n\n4. **Integration**: The retrieved information is then fed into the generator along with the original query to produce contextually enhanced responses <Paper corpusId=\"278714952\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>.\n\nAn important advancement in retrieval mechanisms is the ability to dynamically access up-to-date information. While traditional RAG systems use static retrieval components that leverage fixed corpora of data, some newer approaches incorporate internet search capabilities to access current information <Paper corpusId=\"236034557\" paperTitle=\"(Komeili et al., 2021)\" isShortName></Paper> <Paper corpusId=\"274423377\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>.\n\nThe computational efficiency of retrieval operations varies based on the model architecture. For encoder-decoder models with a Fusion-in-Decoder architecture, the computational cost grows linearly with the number of retrieved passages, as they only perform self-attention over one passage at a time <Paper corpusId=\"220302360\" paperTitle=\"(Izacard et al., 2020)\" isShortName></Paper> <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>. In contrast, for decoder-only LMs, the computational cost typically increases quadratically with both the input length and the number of retrieval passages <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\nThe choice of retrieval mechanism significantly impacts a RAG model's performance on different tasks, with input-layer augmentation (block retrieval) being more suitable for knowledge-intensive tasks like question answering, while output-layer augmentation (token retrieval) works better for structured generation tasks like code completion <Paper corpusId=\"268856642\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Zayyad et al., 2024)",
                        "snippets": [
                            "RAG is a sophisticated framework designed to enhance language models by coupling them with external retrieval systems, addressing limitations inherent in static, solely parameter-based language models. RAG integrates a dual-component architecture where a retriever dynamically searches a structured external corpus for relevant information based on the input query, and a generator LLM uses the retrieved content as context to generate accurate and contextually enriched responses [Gao et al., 2023, Mialon et al., 2023].\n\nRAGs can employ two main types of retrieval mechanisms: dense and sparse [Mialon et al., 2023]. Sparse retrievers rely on bag-of-words representations, excelling at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities, enhancing the model's comprehension of related concepts. By appending retrieved documents directly to the model's context, these retrievers allow the language model to ground its responses in a broader context, thereby increasing accuracy and factual consistency across complex tasks."
                        ],
                        "paper": {
                            "corpus_id": 274982275,
                            "title": "Formal Language Knowledge Corpus for Retrieval Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2336913948",
                                    "name": "Majd Zayyad"
                                },
                                {
                                    "authorId": "2727584",
                                    "name": "Yossi Adi"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.90283203125
                    },
                    {
                        "id": "(Shu et al., 2024)",
                        "snippets": [
                            "We implement a RAG LLM low resource language translator by combining retrieval-based techniques with LLMs to ensure accurate and context-aware translations. The overall architecture is shown in Figure 1. The system utilizes dictionary entries, which are indexed through two complementary approaches: keyword-to-document mappings and vector embeddings. Key-to-document mappings in systems refer to a process where keywords are linked directly to the documents or data entries that contain or are relevant to those keywords. The keyword retriever will retrieve corresponding documents according to the key-to-document mapping, if the keyword is inside our storage. The vector embedding indexing process organizes raw linguistic data into retrievable units by associating words with dictionary definitions and using text-embedding-ada-002 model to encode the text into high-dimensional vectors that capture semantic relationships beyond mere surface forms."
                        ],
                        "paper": {
                            "corpus_id": 274131235,
                            "title": "Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation",
                            "authors": [
                                {
                                    "authorId": "2220096705",
                                    "name": "Peng Shu"
                                },
                                {
                                    "authorId": "2325897685",
                                    "name": "Junhao Chen"
                                },
                                {
                                    "authorId": "2145977326",
                                    "name": "Zheng Liu"
                                },
                                {
                                    "authorId": "2273568534",
                                    "name": "Hui Wang"
                                },
                                {
                                    "authorId": "2263593041",
                                    "name": "Zihao Wu"
                                },
                                {
                                    "authorId": "2215167446",
                                    "name": "Tianyang Zhong"
                                },
                                {
                                    "authorId": "2257102397",
                                    "name": "Yiwei Li"
                                },
                                {
                                    "authorId": "2276747984",
                                    "name": "Huaqin Zhao"
                                },
                                {
                                    "authorId": "2273631049",
                                    "name": "Hanqi Jiang"
                                },
                                {
                                    "authorId": "2221032216",
                                    "name": "Yi Pan"
                                },
                                {
                                    "authorId": "2325891087",
                                    "name": "Yifan Zhou"
                                },
                                {
                                    "authorId": "2331328795",
                                    "name": "Constance Owl"
                                },
                                {
                                    "authorId": "2249626607",
                                    "name": "Xiaoming Zhai"
                                },
                                {
                                    "authorId": "2238404369",
                                    "name": "Ninghao Liu"
                                },
                                {
                                    "authorId": "2331321061",
                                    "name": "Claudio Saunt"
                                },
                                {
                                    "authorId": "2254792886",
                                    "name": "Tianming Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.7470703125
                    },
                    {
                        "id": "(Lewis et al., 2020)",
                        "snippets": [
                            "We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token."
                        ],
                        "paper": {
                            "corpus_id": 218869575,
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "authors": [
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3439053",
                                    "name": "Ethan Perez"
                                },
                                {
                                    "authorId": "1716179427",
                                    "name": "Aleksandara Piktus"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "103131985",
                                    "name": "Heinrich Kuttler"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "2620211",
                                    "name": "Tim Rockt\u00e4schel"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "1743722",
                                    "name": "Douwe Kiela"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6476
                        },
                        "score": 0.82568359375
                    },
                    {
                        "id": "(Borgeaud et al., 2021)",
                        "snippets": [
                            "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."
                        ],
                        "paper": {
                            "corpus_id": 244954723,
                            "title": "Improving language models by retrieving from trillions of tokens",
                            "authors": [
                                {
                                    "authorId": "148016269",
                                    "name": "Sebastian Borgeaud"
                                },
                                {
                                    "authorId": "1697879",
                                    "name": "A. Mensch"
                                },
                                {
                                    "authorId": "46616544",
                                    "name": "Jordan Hoffmann"
                                },
                                {
                                    "authorId": "2072572294",
                                    "name": "Trevor Cai"
                                },
                                {
                                    "authorId": "2143538252",
                                    "name": "Eliza Rutherford"
                                },
                                {
                                    "authorId": "2143434227",
                                    "name": "Katie Millican"
                                },
                                {
                                    "authorId": "47568983",
                                    "name": "George van den Driessche"
                                },
                                {
                                    "authorId": "143783339",
                                    "name": "Jean-Baptiste Lespiau"
                                },
                                {
                                    "authorId": "2143374656",
                                    "name": "Bogdan Damoc"
                                },
                                {
                                    "authorId": "31993415",
                                    "name": "Aidan Clark"
                                },
                                {
                                    "authorId": "40550616",
                                    "name": "Diego de Las Casas"
                                },
                                {
                                    "authorId": "40895205",
                                    "name": "Aurelia Guy"
                                },
                                {
                                    "authorId": "10698483",
                                    "name": "Jacob Menick"
                                },
                                {
                                    "authorId": "81387328",
                                    "name": "Roman Ring"
                                },
                                {
                                    "authorId": "4629007",
                                    "name": "T. Hennigan"
                                },
                                {
                                    "authorId": "2148653469",
                                    "name": "Saffron Huang"
                                },
                                {
                                    "authorId": "108173905",
                                    "name": "Lorenzo Maggiore"
                                },
                                {
                                    "authorId": "2115601070",
                                    "name": "Chris Jones"
                                },
                                {
                                    "authorId": "51042571",
                                    "name": "Albin Cassirer"
                                },
                                {
                                    "authorId": "2065040422",
                                    "name": "Andy Brock"
                                },
                                {
                                    "authorId": "35550664",
                                    "name": "Michela Paganini"
                                },
                                {
                                    "authorId": "2060655766",
                                    "name": "G. Irving"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "2217144",
                                    "name": "Simon Osindero"
                                },
                                {
                                    "authorId": "34838386",
                                    "name": "K. Simonyan"
                                },
                                {
                                    "authorId": "34269227",
                                    "name": "Jack W. Rae"
                                },
                                {
                                    "authorId": "152585800",
                                    "name": "Erich Elsen"
                                },
                                {
                                    "authorId": "2175946",
                                    "name": "L. Sifre"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1100
                        },
                        "score": 0
                    },
                    {
                        "id": "(Shi et al., 2023)",
                        "snippets": [
                            "Retrieval-augmented Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language modeling (Min et al., 2022;(Borgeaud et al., 2021)(Khandelwal et al., 2019) and open-domain question answering (Lewis et al., 2020;Izacard et al., 2022b;Hu et al., 2022). Specifically, using the input as query, (1) a retriever first retrieves a set of documents (i.e., sequences of tokens) from a corpus and then (2) a language model incorporates the retrieved documents as additional information to make a final prediction. This style of retrieval can be added to both encoderdecoder (Yu, 2022)Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2019)(Borgeaud et al., 2021)Shi et al., 2022;(Rubin et al., 2021). For example, Atlas (Izacard et al., 2022b) finetunes an encoder-decoder model jointly with the retriever by modeling documents as latent variables, while RETRO (Borgeaud et al., 2021) changes the decoderonly architecture to incorporate retrieved texts and pretrains the language model from scratch. Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs. Another line of retrieval-augmented LMs such as kNN-LM (Khandelwal et al., 2019)Zhong et al., 2022) retrieves a set of tokens and interpolates between the LM's next token distribution and kNN distributions computed from the retrieved tokens at inference."
                        ],
                        "paper": {
                            "corpus_id": 256389797,
                            "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                            "authors": [
                                {
                                    "authorId": "3040379",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "48872685",
                                    "name": "Sewon Min"
                                },
                                {
                                    "authorId": "19168196",
                                    "name": "Michihiro Yasunaga"
                                },
                                {
                                    "authorId": "4418074",
                                    "name": "Minjoon Seo"
                                },
                                {
                                    "authorId": "2191899140",
                                    "name": "Rich James"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "2072801764",
                                    "name": "Wen-tau Yih"
                                }
                            ],
                            "year": 2023,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 641
                        },
                        "score": 0.7978515625
                    },
                    {
                        "id": "(Siriwardhana et al., 2022)",
                        "snippets": [
                            "Recently, Retrieval Augmented Architectures (Lewis et al., 2020b;Guu et al., 2020) have drawn a lot of attention due to their explainable, scalable, and adaptable nature. Unlike other open-domain QA architectures, RAG (Lewis et al., 2020b) combines the information retrieval stage and answer generation stage in a differentiable manner. It uses a combination of parametric and non-parametric memory, where the parametric memory consists of a pre-trained seq2seq BART (Lewis et al., 2019) generator, and the non-parametric memory consists of dense vector representations of Wikipedia articles indexed with the FAISS library (Johnson et al., 2017). RAG first encodes a question into a dense representation, retrieves the relevant passages from an indexed Wikipedia knowledge base, and then feeds them into the generator. The loss function can finetune both the generator and the question encoder at the same time. Lewis et al. (Lewis et al., 2020b) highlight RAG's ability to perform well in Wikipedia-based general question-answering datasets like Natural Questions (Kwiatkowski et al., 2019)",
                            "(Guu et al., 2020) is a similar Retrieval Augmented model to RAG. REALM introduced a novel masked language pre-training step that involves an end-to-end trainable retriever."
                        ],
                        "paper": {
                            "corpus_id": 252735056,
                            "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "51516859",
                                    "name": "Shamane Siriwardhana"
                                },
                                {
                                    "authorId": "52001535",
                                    "name": "Rivindu Weerasekera"
                                },
                                {
                                    "authorId": "2114425044",
                                    "name": "Elliott Wen"
                                },
                                {
                                    "authorId": "1992921690",
                                    "name": "Tharindu Kaluarachchi"
                                },
                                {
                                    "authorId": "1814487",
                                    "name": "R. Rana"
                                },
                                {
                                    "authorId": "1486464114",
                                    "name": "Suranga Nanayakkara"
                                }
                            ],
                            "year": 2022,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 179
                        },
                        "score": 0.77880859375
                    },
                    {
                        "id": "(Khandelwal et al., 2019)",
                        "snippets": [
                            "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."
                        ],
                        "paper": {
                            "corpus_id": 207870430,
                            "title": "Generalization through Memorization: Nearest Neighbor Language Models",
                            "authors": [
                                {
                                    "authorId": "3030219",
                                    "name": "Urvashi Khandelwal"
                                },
                                {
                                    "authorId": "39455775",
                                    "name": "Omer Levy"
                                },
                                {
                                    "authorId": "1746807",
                                    "name": "Dan Jurafsky"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 842
                        },
                        "score": 0
                    },
                    {
                        "id": "(Guo et al., 2024)",
                        "snippets": [
                            "Recently, a series of retrieval-augmented language models [14,24,47] have been proposed to augment language models with external knowledge [9,17,53]. Retrieval-augmented techniques can generally be divided into two types. The first type is at the input layer [14,20,42], where the retrieved information is text chunks. The second type is at the output layer [7,24,47], where the retrieved information is tokens. By combining the retrieved tokens with the tokens generated by the original model, the accuracy of the retrieval-augmented model's generation for each token can be improved.\n\nThe first type of method can provide the model with more external knowledge, making it adept at handling tasks in the NLP field such as knowledge-based question answering [27,45,49]. The second type of method can refer to the retrieved information to correct the generated tokens, making it more suited for handling strictly structured generative tasks, such as code completion [7,10,11].\n\nTo better understand the mechanism, we take kNN-LM [24] as an example for a detailed explanation. Given a context sequence   = ( 1 , . . .,   \u22121 ), the language models (LMs) estimate   (  |  ), i.e., the probability distribution over the next token   . kNN-LM is designed to augment a pre-trained language model with a set of nearest neighbours retrieved from an external text collection, which can be the training set."
                        ],
                        "paper": {
                            "corpus_id": 268856642,
                            "title": "FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion",
                            "authors": [
                                {
                                    "authorId": "2290464625",
                                    "name": "Qi Guo"
                                },
                                {
                                    "authorId": "2118890600",
                                    "name": "Xiaohong Li"
                                },
                                {
                                    "authorId": "2288741802",
                                    "name": "Xiaofei Xie"
                                },
                                {
                                    "authorId": "2290359321",
                                    "name": "Shangqing Liu"
                                },
                                {
                                    "authorId": "2109915677",
                                    "name": "Ze Tang"
                                },
                                {
                                    "authorId": "1758019",
                                    "name": "Ruitao Feng"
                                },
                                {
                                    "authorId": "2294667814",
                                    "name": "Junjie Wang"
                                },
                                {
                                    "authorId": "2248015856",
                                    "name": "Jidong Ge"
                                },
                                {
                                    "authorId": "2279752248",
                                    "name": "Lei Bu"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Symposium on Software Testing and Analysis",
                            "n_citations": 11
                        },
                        "score": 0.81396484375
                    },
                    {
                        "id": "(Lyu et al., 2023)",
                        "snippets": [
                            "Retrieval-augmented (RAG) models have recently been proposed [12,14]8]. A typical retrieval-augmented model consists of two parts, a retriever f ret and a generator f gen. Given a retrieval corpus D ret = {d 1, \u2022 \u2022 \u2022, d M}, the retriever f ret retrieves K data points for an input x i as f ret (x i, D ret) = {d \u03b11, d \u03b12,",
                            ", d \u03b1 K}. Here, \u03b1 k denotes the rank of each data point in the retrieval corpus assigned by the retriever."
                        ],
                        "paper": {
                            "corpus_id": 259360590,
                            "title": "Improving Retrieval-Augmented Large Language Models via Data Importance Learning",
                            "authors": [
                                {
                                    "authorId": "35308280",
                                    "name": "Xiaozhong Lyu"
                                },
                                {
                                    "authorId": "1393463989",
                                    "name": "Stefan Grafberger"
                                },
                                {
                                    "authorId": "2086970867",
                                    "name": "Samantha Biegel"
                                },
                                {
                                    "authorId": "1409866591",
                                    "name": "Shaopeng Wei"
                                },
                                {
                                    "authorId": "2057073725",
                                    "name": "Meng Cao"
                                },
                                {
                                    "authorId": "2180399",
                                    "name": "Sebastian Schelter"
                                },
                                {
                                    "authorId": "1776014",
                                    "name": "Ce Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 18
                        },
                        "score": 0.763671875
                    },
                    {
                        "id": "(Lee et al., 2025)",
                        "snippets": [
                            "Retrieval-Augmented Generation (RAG) augments large language models by incorporating external documents into the generation process. Rather than relying solely on the model's internal parameters, RAG retrieves relevant passages from a knowledge base and feeds them, along with the user query, into the model to guide its response (Zhou et al., 2024). \n\nA standard RAG system operates in two phases: \n\n\u2022 Retrieval. A retriever model selects the top-k most relevant documents for a given query. \n\n\u2022 Generation. A language model generates a response conditioned on both the query and the retrieved documents. \n\nThe appeal of RAG lies in its ability to dynamically access up-to-date or domain-specific information, which is especially useful in fast-changing or specialized fields. However, it also introduces new failure modes, particularly when the retrieval quality is imperfect (Barnett et al., 2024)."
                        ],
                        "paper": {
                            "corpus_id": 278714952,
                            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2362089035",
                                    "name": "Zhan Peng Lee"
                                },
                                {
                                    "authorId": "2362188632",
                                    "name": "Andre Lin"
                                },
                                {
                                    "authorId": "2363425126",
                                    "name": "Calvin Tan"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.76171875
                    },
                    {
                        "id": "(Komeili et al., 2021)",
                        "snippets": [
                            "The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020b)."
                        ],
                        "paper": {
                            "corpus_id": 236034557,
                            "title": "Internet-Augmented Dialogue Generation",
                            "authors": [
                                {
                                    "authorId": "100653935",
                                    "name": "M. Komeili"
                                },
                                {
                                    "authorId": "35752280",
                                    "name": "Kurt Shuster"
                                },
                                {
                                    "authorId": "145183709",
                                    "name": "J. Weston"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 289
                        },
                        "score": 0
                    },
                    {
                        "id": "(He et al., 2024)",
                        "snippets": [
                            "While RAG systems with a static retrieval component, which leverages a fixed corpus of data, are effective for tasks within well-defined knowledge domains, Internet search augmented generation [4,(Komeili et al., 2021)6,7,(Duffy et al., 2023)[9] offers distinct advantages."
                        ],
                        "paper": {
                            "corpus_id": 274423377,
                            "title": "Zero-Indexing Internet Search Augmented Generation for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2333982900",
                                    "name": "Guangxin He"
                                },
                                {
                                    "authorId": "83732784",
                                    "name": "Zonghong Dai"
                                },
                                {
                                    "authorId": "2290357307",
                                    "name": "Jiangcheng Zhu"
                                },
                                {
                                    "authorId": "2333537636",
                                    "name": "Binqiang Zhao"
                                },
                                {
                                    "authorId": "2313032000",
                                    "name": "Chenyue Li"
                                },
                                {
                                    "authorId": "2333318691",
                                    "name": "You Peng"
                                },
                                {
                                    "authorId": "2333373998",
                                    "name": "Chen Wang"
                                },
                                {
                                    "authorId": "2312922261",
                                    "name": "Binhang Yuan"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.7958984375
                    },
                    {
                        "id": "(Izacard et al., 2020)",
                        "snippets": [
                            "Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages."
                        ],
                        "paper": {
                            "corpus_id": 220302360,
                            "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "1410231361",
                                    "name": "Gautier Izacard"
                                },
                                {
                                    "authorId": "3024698",
                                    "name": "Edouard Grave"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 1181
                        },
                        "score": 0
                    },
                    {
                        "id": "(Huang et al., 2023)",
                        "snippets": [
                            "Retrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2022)(Lewis et al., 2020) and decoderonly (Khandelwal et al., 2019)Borgeaud et al., 2022;(Shi et al., 2022) architectures. For decoder-only LMs, the computational cost typically increases quadratically with the input length, as well as with the number of retrieval passages. In contrast, for encoder-decoder LMs with a Fusion-in-Decoder architecture, the computation cost grows linearly with the number of retrieved passages, as they only perform self-attention over one passage at a time (Izacard et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 260900354,
                            "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models",
                            "authors": [
                                {
                                    "authorId": "1490651934",
                                    "name": "Jie Huang"
                                },
                                {
                                    "authorId": "2056440915",
                                    "name": "Wei Ping"
                                },
                                {
                                    "authorId": "145011005",
                                    "name": "Peng Xu"
                                },
                                {
                                    "authorId": "1911755",
                                    "name": "M. Shoeybi"
                                },
                                {
                                    "authorId": "143922493",
                                    "name": "K. Chang"
                                },
                                {
                                    "authorId": "2301680",
                                    "name": "Bryan Catanzaro"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 35
                        },
                        "score": 0.767578125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Integration of Retrieved Information",
                "tldr": "Retrieval-augmented language models employ various methods to integrate retrieved information with model parameters, ranging from architectural modifications to non-parametric interpolation. The integration approach varies based on retrieval granularity (block vs. token level) and model architecture (encoder-decoder vs. decoder-only), with each method offering distinct computational efficiency and performance characteristics. (11 sources)",
                "text": "\nThe effectiveness of retrieval-augmented language models depends not only on what information is retrieved but also on how this information is integrated with the language model's generation process. Different integration approaches have emerged, each with distinct characteristics and applications.\n\n## Block-Level Integration Methods\n\nFor models that retrieve entire text passages (block-level retrieval), several integration approaches have been developed:\n\n- **Differentiable End-to-End Integration**: Models like RAG combine parametric memory (a pre-trained seq2seq model) with non-parametric memory (a dense vector index) in a differentiable manner. RAG offers two integration variants: one that conditions on the same retrieved passages across the entire generated sequence, and another that can use different passages per token <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>.\n\n- **Chunked Cross-Attention**: RETRO introduces a specialized chunked cross-attention mechanism that allows the model to focus on relevant retrieved document chunks. This architectural modification enables the model to predict tokens based on significantly more data than what is typically consumed during training <Paper corpusId=\"244954723\" paperTitle=\"(Borgeaud et al., 2021)\" isShortName></Paper>.\n\n- **Fusion-in-Decoder Architecture**: This approach, used in encoder-decoder models, processes each retrieved passage separately in the encoder and then fuses all encoded passages in the decoder. A key advantage is computational efficiency\u2014the cost scales linearly with the number of passages rather than quadratically <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"220302360\" paperTitle=\"(Izacard et al., 2020)\" isShortName></Paper>.\n\n- **In-Context Integration**: Some models like REPLUG assume black-box access to language models and simply prepend retrieved information to the input context. This approach requires no architectural modifications to the underlying model, making it highly versatile for deployment with various language models <Paper corpusId=\"269983737\" paperTitle=\"(Jiao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"256389797\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>.\n\n## Token-Level Integration Methods\n\nFor models that retrieve at the token level, different integration techniques are used:\n\n- **Distribution Interpolation**: kNN-LM exemplifies this approach by extending pre-trained language models through interpolating their predictions with a k-nearest neighbors distribution. This non-parametric method combines the model's next token predictions with a distribution derived from retrieved tokens <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper>.\n\n- **Adaptive Interpolation**: More advanced approaches train additional networks to determine the optimal interpolation weights between the language model's predictions and retrieved token distributions <Paper corpusId=\"261030382\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>.\n\n## Integration Based on Model Architecture\n\nThe choice of integration method is often influenced by the underlying model architecture:\n\n- **Encoder-Decoder Models**: These architectures typically employ block-level retrieval with methods like Fusion-in-Decoder, which offers better computational efficiency for processing multiple retrieved passages <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\n- **Decoder-Only Models**: These models often face higher computational costs when integrating retrieved information because the computation typically scales quadratically with both input length and the number of retrieved passages <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\n## Task-Specific Integration Considerations\n\nThe choice of integration approach also depends on the target application:\n\n- **Knowledge-Intensive Tasks**: Input-layer augmentation (block retrieval) is generally more suitable for tasks like question answering, where broader context from retrieved passages provides valuable background information <Paper corpusId=\"268856642\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.\n\n- **Structured Generation Tasks**: Output-layer augmentation (token retrieval) tends to work better for strictly structured generation tasks such as code completion, where retrieved tokens can help correct the generated output <Paper corpusId=\"268856642\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247450969\" paperTitle=\"(Lu et al., 2022)\" isShortName></Paper>.\n\nThe integration of retrieved information in RAG systems represents a crucial design choice that impacts both performance and efficiency. By appending retrieved documents to the model's context, these systems allow language models to ground their responses in broader contexts, thereby increasing accuracy and factual consistency across complex tasks <Paper corpusId=\"274982275\" paperTitle=\"(Zayyad et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Lewis et al., 2020)",
                        "snippets": [
                            "We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token."
                        ],
                        "paper": {
                            "corpus_id": 218869575,
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "authors": [
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3439053",
                                    "name": "Ethan Perez"
                                },
                                {
                                    "authorId": "1716179427",
                                    "name": "Aleksandara Piktus"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "103131985",
                                    "name": "Heinrich Kuttler"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "2620211",
                                    "name": "Tim Rockt\u00e4schel"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "1743722",
                                    "name": "Douwe Kiela"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6476
                        },
                        "score": 0.82568359375
                    },
                    {
                        "id": "(Borgeaud et al., 2021)",
                        "snippets": [
                            "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."
                        ],
                        "paper": {
                            "corpus_id": 244954723,
                            "title": "Improving language models by retrieving from trillions of tokens",
                            "authors": [
                                {
                                    "authorId": "148016269",
                                    "name": "Sebastian Borgeaud"
                                },
                                {
                                    "authorId": "1697879",
                                    "name": "A. Mensch"
                                },
                                {
                                    "authorId": "46616544",
                                    "name": "Jordan Hoffmann"
                                },
                                {
                                    "authorId": "2072572294",
                                    "name": "Trevor Cai"
                                },
                                {
                                    "authorId": "2143538252",
                                    "name": "Eliza Rutherford"
                                },
                                {
                                    "authorId": "2143434227",
                                    "name": "Katie Millican"
                                },
                                {
                                    "authorId": "47568983",
                                    "name": "George van den Driessche"
                                },
                                {
                                    "authorId": "143783339",
                                    "name": "Jean-Baptiste Lespiau"
                                },
                                {
                                    "authorId": "2143374656",
                                    "name": "Bogdan Damoc"
                                },
                                {
                                    "authorId": "31993415",
                                    "name": "Aidan Clark"
                                },
                                {
                                    "authorId": "40550616",
                                    "name": "Diego de Las Casas"
                                },
                                {
                                    "authorId": "40895205",
                                    "name": "Aurelia Guy"
                                },
                                {
                                    "authorId": "10698483",
                                    "name": "Jacob Menick"
                                },
                                {
                                    "authorId": "81387328",
                                    "name": "Roman Ring"
                                },
                                {
                                    "authorId": "4629007",
                                    "name": "T. Hennigan"
                                },
                                {
                                    "authorId": "2148653469",
                                    "name": "Saffron Huang"
                                },
                                {
                                    "authorId": "108173905",
                                    "name": "Lorenzo Maggiore"
                                },
                                {
                                    "authorId": "2115601070",
                                    "name": "Chris Jones"
                                },
                                {
                                    "authorId": "51042571",
                                    "name": "Albin Cassirer"
                                },
                                {
                                    "authorId": "2065040422",
                                    "name": "Andy Brock"
                                },
                                {
                                    "authorId": "35550664",
                                    "name": "Michela Paganini"
                                },
                                {
                                    "authorId": "2060655766",
                                    "name": "G. Irving"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "2217144",
                                    "name": "Simon Osindero"
                                },
                                {
                                    "authorId": "34838386",
                                    "name": "K. Simonyan"
                                },
                                {
                                    "authorId": "34269227",
                                    "name": "Jack W. Rae"
                                },
                                {
                                    "authorId": "152585800",
                                    "name": "Erich Elsen"
                                },
                                {
                                    "authorId": "2175946",
                                    "name": "L. Sifre"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1100
                        },
                        "score": 0
                    },
                    {
                        "id": "(Huang et al., 2023)",
                        "snippets": [
                            "Retrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2022)(Lewis et al., 2020) and decoderonly (Khandelwal et al., 2019)Borgeaud et al., 2022;(Shi et al., 2022) architectures. For decoder-only LMs, the computational cost typically increases quadratically with the input length, as well as with the number of retrieval passages. In contrast, for encoder-decoder LMs with a Fusion-in-Decoder architecture, the computation cost grows linearly with the number of retrieved passages, as they only perform self-attention over one passage at a time (Izacard et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 260900354,
                            "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models",
                            "authors": [
                                {
                                    "authorId": "1490651934",
                                    "name": "Jie Huang"
                                },
                                {
                                    "authorId": "2056440915",
                                    "name": "Wei Ping"
                                },
                                {
                                    "authorId": "145011005",
                                    "name": "Peng Xu"
                                },
                                {
                                    "authorId": "1911755",
                                    "name": "M. Shoeybi"
                                },
                                {
                                    "authorId": "143922493",
                                    "name": "K. Chang"
                                },
                                {
                                    "authorId": "2301680",
                                    "name": "Bryan Catanzaro"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 35
                        },
                        "score": 0.767578125
                    },
                    {
                        "id": "(Izacard et al., 2020)",
                        "snippets": [
                            "Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages."
                        ],
                        "paper": {
                            "corpus_id": 220302360,
                            "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "1410231361",
                                    "name": "Gautier Izacard"
                                },
                                {
                                    "authorId": "3024698",
                                    "name": "Edouard Grave"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 1181
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jiao et al., 2024)",
                        "snippets": [
                            "Retrieval-Augmented Language Models Augmenting language models with relevant information obtained from various external knowledge bases has been shown to significantly improve the performance of various NLP tasks, including language modeling (Guu et al., 2020)(Borgeaud et al., 2021)Shi et al., 2023;Lin et al., 2023) and open domain question answering (Izacard et al., 2022;Zhang et al., 2024).RAG mainly adopts the \"retrieve then read\" paradigm.Specifically, the input question is first used as the query, then the retrieval module retrieves relevant documents from the external knowledge base, and finally the retrieved documents and questions are merged into a complete input to generate final output.For example, RETRO (Borgeaud et al., 2021) modifies the autoregressive LM to focus on relevant documents through chunked cross-attention, thereby introducing new parameters to the model.REPLUG (Shi et al., 2023) assumes black-box access to LM and optimizes it by fine-tuning the retriever.RAFT (Zhang et al., 2024) proposes a fine-tuned data that additionally contains relevant documents and answers with reasoning chains to train language models for domain-specific open-book settings."
                        ],
                        "paper": {
                            "corpus_id": 269983737,
                            "title": "DuetRAG: Collaborative Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2302798653",
                                    "name": "Dian Jiao"
                                },
                                {
                                    "authorId": "2303434387",
                                    "name": "Li Cai"
                                },
                                {
                                    "authorId": "2303044665",
                                    "name": "Jingsheng Huang"
                                },
                                {
                                    "authorId": "2108125912",
                                    "name": "Wenqiao Zhang"
                                },
                                {
                                    "authorId": "2118071462",
                                    "name": "Siliang Tang"
                                },
                                {
                                    "authorId": "2253660817",
                                    "name": "Yueting Zhuang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.77490234375
                    },
                    {
                        "id": "(Shi et al., 2023)",
                        "snippets": [
                            "Retrieval-augmented Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language modeling (Min et al., 2022;(Borgeaud et al., 2021)(Khandelwal et al., 2019) and open-domain question answering (Lewis et al., 2020;Izacard et al., 2022b;Hu et al., 2022). Specifically, using the input as query, (1) a retriever first retrieves a set of documents (i.e., sequences of tokens) from a corpus and then (2) a language model incorporates the retrieved documents as additional information to make a final prediction. This style of retrieval can be added to both encoderdecoder (Yu, 2022)Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2019)(Borgeaud et al., 2021)Shi et al., 2022;(Rubin et al., 2021). For example, Atlas (Izacard et al., 2022b) finetunes an encoder-decoder model jointly with the retriever by modeling documents as latent variables, while RETRO (Borgeaud et al., 2021) changes the decoderonly architecture to incorporate retrieved texts and pretrains the language model from scratch. Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs. Another line of retrieval-augmented LMs such as kNN-LM (Khandelwal et al., 2019)Zhong et al., 2022) retrieves a set of tokens and interpolates between the LM's next token distribution and kNN distributions computed from the retrieved tokens at inference."
                        ],
                        "paper": {
                            "corpus_id": 256389797,
                            "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                            "authors": [
                                {
                                    "authorId": "3040379",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "48872685",
                                    "name": "Sewon Min"
                                },
                                {
                                    "authorId": "19168196",
                                    "name": "Michihiro Yasunaga"
                                },
                                {
                                    "authorId": "4418074",
                                    "name": "Minjoon Seo"
                                },
                                {
                                    "authorId": "2191899140",
                                    "name": "Rich James"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "2072801764",
                                    "name": "Wen-tau Yih"
                                }
                            ],
                            "year": 2023,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 641
                        },
                        "score": 0.7978515625
                    },
                    {
                        "id": "(Khandelwal et al., 2019)",
                        "snippets": [
                            "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."
                        ],
                        "paper": {
                            "corpus_id": 207870430,
                            "title": "Generalization through Memorization: Nearest Neighbor Language Models",
                            "authors": [
                                {
                                    "authorId": "3030219",
                                    "name": "Urvashi Khandelwal"
                                },
                                {
                                    "authorId": "39455775",
                                    "name": "Omer Levy"
                                },
                                {
                                    "authorId": "1746807",
                                    "name": "Dan Jurafsky"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 842
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tang et al., 2023)",
                        "snippets": [
                            "Retrieval-augmented language models (R-LMs) utilize retrieval-based techniques to improve the performance of LMs and can be divided into two main categories: block R-LMs and token R-LMs. Block R-LMs [12], [13], [48] are similar to one-shot or few-shot learning [49], where one or a few examples are retrieved from a database instead of being randomly selected. Token R-LMs [15], [24], [50] retrieve tokens from database and then combine the retrieved results into the LM. Compared with block R-LMs, token R-LMs can update retrieval results at the same time of generating new tokens, hence our approach uses the architecture of token R-LM. However, token R-LMs suffer from high storage costs and require hyper-parameters selection to combine the inference results from the database and language model.\n\nVarious approaches have been proposed to address the limitations of token R-LMs. For example, kNN-Adapter [51] uses a trained network to determine the combination weights. To reduce the search cost, RetoMaton [52] uses the automaton states to save search time, while AdaptRet [53] uses a trained network to decide whether to use the retrieval module. GNN-LM [50] selects similar texts and builds a contextual graph to incorporate into the language model."
                        ],
                        "paper": {
                            "corpus_id": 261030382,
                            "title": "Domain Adaptive Code Completion via Language Models and Decoupled Domain Databases",
                            "authors": [
                                {
                                    "authorId": "2109915677",
                                    "name": "Ze Tang"
                                },
                                {
                                    "authorId": "2669512",
                                    "name": "Jidong Ge"
                                },
                                {
                                    "authorId": "13877308",
                                    "name": "Shangqing Liu"
                                },
                                {
                                    "authorId": "3274600",
                                    "name": "Tingwei Zhu"
                                },
                                {
                                    "authorId": "2118717147",
                                    "name": "Tongtong Xu"
                                },
                                {
                                    "authorId": "1482584966",
                                    "name": "LiGuo Huang"
                                },
                                {
                                    "authorId": "2075400450",
                                    "name": "Bin Luo"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Automated Software Engineering",
                            "n_citations": 26
                        },
                        "score": 0.7783203125
                    },
                    {
                        "id": "(Guo et al., 2024)",
                        "snippets": [
                            "Recently, a series of retrieval-augmented language models [14,24,47] have been proposed to augment language models with external knowledge [9,17,53]. Retrieval-augmented techniques can generally be divided into two types. The first type is at the input layer [14,20,42], where the retrieved information is text chunks. The second type is at the output layer [7,24,47], where the retrieved information is tokens. By combining the retrieved tokens with the tokens generated by the original model, the accuracy of the retrieval-augmented model's generation for each token can be improved.\n\nThe first type of method can provide the model with more external knowledge, making it adept at handling tasks in the NLP field such as knowledge-based question answering [27,45,49]. The second type of method can refer to the retrieved information to correct the generated tokens, making it more suited for handling strictly structured generative tasks, such as code completion [7,10,11].\n\nTo better understand the mechanism, we take kNN-LM [24] as an example for a detailed explanation. Given a context sequence   = ( 1 , . . .,   \u22121 ), the language models (LMs) estimate   (  |  ), i.e., the probability distribution over the next token   . kNN-LM is designed to augment a pre-trained language model with a set of nearest neighbours retrieved from an external text collection, which can be the training set."
                        ],
                        "paper": {
                            "corpus_id": 268856642,
                            "title": "FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion",
                            "authors": [
                                {
                                    "authorId": "2290464625",
                                    "name": "Qi Guo"
                                },
                                {
                                    "authorId": "2118890600",
                                    "name": "Xiaohong Li"
                                },
                                {
                                    "authorId": "2288741802",
                                    "name": "Xiaofei Xie"
                                },
                                {
                                    "authorId": "2290359321",
                                    "name": "Shangqing Liu"
                                },
                                {
                                    "authorId": "2109915677",
                                    "name": "Ze Tang"
                                },
                                {
                                    "authorId": "1758019",
                                    "name": "Ruitao Feng"
                                },
                                {
                                    "authorId": "2294667814",
                                    "name": "Junjie Wang"
                                },
                                {
                                    "authorId": "2248015856",
                                    "name": "Jidong Ge"
                                },
                                {
                                    "authorId": "2279752248",
                                    "name": "Lei Bu"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Symposium on Software Testing and Analysis",
                            "n_citations": 11
                        },
                        "score": 0.81396484375
                    },
                    {
                        "id": "(Lu et al., 2022)",
                        "snippets": [
                            "Code completion, which aims to predict the following code token(s) according to the code context, can improve the productivity of software development. Recent work has proved that statistical language modeling with transformers can greatly improve the performance in the code completion task via learning from large-scale source code datasets. However, current approaches focus only on code context within the file or project, i.e. internal context. Our distinction is utilizing \"external\" context, inspired by human behaviors of copying from the related code snippets when writing code. Specifically, we propose a retrieval-augmented code completion framework, leveraging both lexical copying and referring to code with similar semantics by retrieval. We adopt a stage-wise training approach that combines a source code retriever and an auto-regressive language model for programming language. We evaluate our approach in the code completion task in Python and Java programming languages, achieving a state-of-the-art performance on CodeXGLUE benchmark."
                        ],
                        "paper": {
                            "corpus_id": 247450969,
                            "title": "ReACC: A Retrieval-Augmented Code Completion Framework",
                            "authors": [
                                {
                                    "authorId": "2115338656",
                                    "name": "Shuai Lu"
                                },
                                {
                                    "authorId": "46429989",
                                    "name": "Nan Duan"
                                },
                                {
                                    "authorId": "5534572",
                                    "name": "Hojae Han"
                                },
                                {
                                    "authorId": "2278834796",
                                    "name": "Daya Guo"
                                },
                                {
                                    "authorId": "1716415",
                                    "name": "Seung-won Hwang"
                                },
                                {
                                    "authorId": "2061625488",
                                    "name": "Alexey Svyatkovskiy"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 147
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zayyad et al., 2024)",
                        "snippets": [
                            "RAG is a sophisticated framework designed to enhance language models by coupling them with external retrieval systems, addressing limitations inherent in static, solely parameter-based language models. RAG integrates a dual-component architecture where a retriever dynamically searches a structured external corpus for relevant information based on the input query, and a generator LLM uses the retrieved content as context to generate accurate and contextually enriched responses [Gao et al., 2023, Mialon et al., 2023].\n\nRAGs can employ two main types of retrieval mechanisms: dense and sparse [Mialon et al., 2023]. Sparse retrievers rely on bag-of-words representations, excelling at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities, enhancing the model's comprehension of related concepts. By appending retrieved documents directly to the model's context, these retrievers allow the language model to ground its responses in a broader context, thereby increasing accuracy and factual consistency across complex tasks."
                        ],
                        "paper": {
                            "corpus_id": 274982275,
                            "title": "Formal Language Knowledge Corpus for Retrieval Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2336913948",
                                    "name": "Majd Zayyad"
                                },
                                {
                                    "authorId": "2727584",
                                    "name": "Yossi Adi"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.90283203125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Use Cases and Applications",
                "tldr": "Retrieval-augmented language models excel in knowledge-intensive tasks like open-domain question answering, fact verification, and domain-specific applications. Different architectural approaches are suited to particular use cases, with block-level retrieval models performing better on knowledge-intensive tasks while token-level models excel at structured generation tasks like code completion. (12 sources)",
                "text": "\n## Open-Domain Question Answering\n- **Primary Application**: Retrieving factual knowledge to answer questions without domain constraints\n- **Notable Models**: REALM, RAG, FiD, and Atlas have all demonstrated significant improvements in this domain\n- **Key Advantages**: RAG set state-of-the-art results on three open-domain QA tasks, outperforming both parametric-only models and task-specific architectures <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>\n- **Performance Metrics**: REALM outperformed previous methods by 4-16% absolute accuracy on open-domain QA benchmarks <Paper corpusId=\"211204736\" paperTitle=\"(Guu et al., 2020)\" isShortName></Paper>\n- **Scaling Benefits**: FiD showed that performance significantly improves when increasing the number of retrieved passages, demonstrating the effective aggregation of evidence <Paper corpusId=\"220302360\" paperTitle=\"(Izacard et al., 2020)\" isShortName></Paper>\n\n## Language Modeling\n- **Application**: Improving general text prediction and generation capabilities\n- **Key Models**: kNN-LM and RETRO have shown substantial improvements in language modeling perplexity\n- **Performance**: kNN-LM achieved a 2.9 point improvement in perplexity on Wikitext-103 without additional training <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper>\n- **Scaling Advantages**: RETRO, with its 2 trillion token database, achieved performance comparable to much larger models like GPT-3 and Jurassic-1 despite using 25x fewer parameters <Paper corpusId=\"244954723\" paperTitle=\"(Borgeaud et al., 2021)\" isShortName></Paper>\n\n## Domain Adaptation\n- **Application**: Adapting language models to specific domains without retraining\n- **Approach**: kNN-LM demonstrated effective domain adaptation simply by varying the nearest neighbor datastore <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper>\n- **Benefits**: Allows models to adapt to new domains with minimal computational resources\n- **Implementation**: Atlas showed few-shot learning capabilities for knowledge-intensive tasks, reaching 42% accuracy on Natural Questions with only 64 examples <Paper corpusId=\"252735160\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper>\n\n## Code Completion\n- **Application**: Generating programming code with higher accuracy\n- **Best Architecture**: Token-level retrieval models (output-layer augmentation) perform better for structured generation tasks like code completion <Paper corpusId=\"268856642\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>\n- **Implementation Example**: Retrieval-augmented code completion frameworks leverage both lexical copying and semantic similarity to improve code generation <Paper corpusId=\"247450969\" paperTitle=\"(Lu et al., 2022)\" isShortName></Paper>\n- **Advantage**: Allows models to reference similar code patterns when generating new code, similar to how human programmers work\n\n## Fact Verification and Knowledge-Intensive Tasks\n- **Application**: Verifying factual claims and answering knowledge-intensive questions\n- **Architecture Preference**: Block-level retrieval models (input-layer augmentation) excel at these tasks by providing broader context <Paper corpusId=\"268856642\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>\n- **Key Models**: RAG models have been applied to fact verification tasks with strong results <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>\n- **Advantage**: Provides natural source attribution by referencing the retrieved documents\n\n## Real-Time Knowledge Access\n- **Application**: Accessing up-to-date information beyond the model's training cutoff\n- **Implementation**: Advanced RAG systems can incorporate internet search capabilities to access current information <Paper corpusId=\"265308533\" paperTitle=\"(Munikoti et al., 2023)\" isShortName></Paper>\n- **Use Case**: Particularly valuable for queries about recent events or rapidly changing information\n- **Advantage**: Overcomes the limitation of static knowledge in traditional language models <Paper corpusId=\"275358357\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>\n\n## Specialized Domain Knowledge\n- **Application**: Accessing specialized information from heterogeneous knowledge sources\n- **Advanced Approaches**: Recent work has expanded beyond retrieving unstructured text from Wikipedia to incorporate heterogeneous knowledge sources <Paper corpusId=\"250391000\" paperTitle=\"(Yu, 2022)\" isShortName></Paper>\n- **Implementation**: Graph-based RAG systems provide a promising approach for structured knowledge retrieval\n- **Use Case**: Medical, legal, financial, and other domain-specific applications requiring specialized knowledge\n\n## Conversational Systems\n- **Application**: Enhancing dialogue systems with factual grounding\n- **Implementation**: Retrieval-augmented models provide more specific and factual responses in conversational contexts\n- **Key Advantage**: Reduces hallucinations and improves factual consistency in dialogue responses <Paper corpusId=\"271570928\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>\n- **Models**: Various RAG approaches have been adapted for conversational contexts to enhance response quality",
                "citations": [
                    {
                        "id": "(Lewis et al., 2020)",
                        "snippets": [
                            "We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token."
                        ],
                        "paper": {
                            "corpus_id": 218869575,
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "authors": [
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3439053",
                                    "name": "Ethan Perez"
                                },
                                {
                                    "authorId": "1716179427",
                                    "name": "Aleksandara Piktus"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "103131985",
                                    "name": "Heinrich Kuttler"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "2620211",
                                    "name": "Tim Rockt\u00e4schel"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "1743722",
                                    "name": "Douwe Kiela"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6476
                        },
                        "score": 0.82568359375
                    },
                    {
                        "id": "(Guu et al., 2020)",
                        "snippets": [
                            "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."
                        ],
                        "paper": {
                            "corpus_id": 211204736,
                            "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
                            "authors": [
                                {
                                    "authorId": "2091768",
                                    "name": "Kelvin Guu"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "9941702",
                                    "name": "Zora Tung"
                                },
                                {
                                    "authorId": "2616463",
                                    "name": "Panupong Pasupat"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 2119
                        },
                        "score": 0
                    },
                    {
                        "id": "(Izacard et al., 2020)",
                        "snippets": [
                            "Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages."
                        ],
                        "paper": {
                            "corpus_id": 220302360,
                            "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "1410231361",
                                    "name": "Gautier Izacard"
                                },
                                {
                                    "authorId": "3024698",
                                    "name": "Edouard Grave"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 1181
                        },
                        "score": 0
                    },
                    {
                        "id": "(Khandelwal et al., 2019)",
                        "snippets": [
                            "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."
                        ],
                        "paper": {
                            "corpus_id": 207870430,
                            "title": "Generalization through Memorization: Nearest Neighbor Language Models",
                            "authors": [
                                {
                                    "authorId": "3030219",
                                    "name": "Urvashi Khandelwal"
                                },
                                {
                                    "authorId": "39455775",
                                    "name": "Omer Levy"
                                },
                                {
                                    "authorId": "1746807",
                                    "name": "Dan Jurafsky"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 842
                        },
                        "score": 0
                    },
                    {
                        "id": "(Borgeaud et al., 2021)",
                        "snippets": [
                            "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."
                        ],
                        "paper": {
                            "corpus_id": 244954723,
                            "title": "Improving language models by retrieving from trillions of tokens",
                            "authors": [
                                {
                                    "authorId": "148016269",
                                    "name": "Sebastian Borgeaud"
                                },
                                {
                                    "authorId": "1697879",
                                    "name": "A. Mensch"
                                },
                                {
                                    "authorId": "46616544",
                                    "name": "Jordan Hoffmann"
                                },
                                {
                                    "authorId": "2072572294",
                                    "name": "Trevor Cai"
                                },
                                {
                                    "authorId": "2143538252",
                                    "name": "Eliza Rutherford"
                                },
                                {
                                    "authorId": "2143434227",
                                    "name": "Katie Millican"
                                },
                                {
                                    "authorId": "47568983",
                                    "name": "George van den Driessche"
                                },
                                {
                                    "authorId": "143783339",
                                    "name": "Jean-Baptiste Lespiau"
                                },
                                {
                                    "authorId": "2143374656",
                                    "name": "Bogdan Damoc"
                                },
                                {
                                    "authorId": "31993415",
                                    "name": "Aidan Clark"
                                },
                                {
                                    "authorId": "40550616",
                                    "name": "Diego de Las Casas"
                                },
                                {
                                    "authorId": "40895205",
                                    "name": "Aurelia Guy"
                                },
                                {
                                    "authorId": "10698483",
                                    "name": "Jacob Menick"
                                },
                                {
                                    "authorId": "81387328",
                                    "name": "Roman Ring"
                                },
                                {
                                    "authorId": "4629007",
                                    "name": "T. Hennigan"
                                },
                                {
                                    "authorId": "2148653469",
                                    "name": "Saffron Huang"
                                },
                                {
                                    "authorId": "108173905",
                                    "name": "Lorenzo Maggiore"
                                },
                                {
                                    "authorId": "2115601070",
                                    "name": "Chris Jones"
                                },
                                {
                                    "authorId": "51042571",
                                    "name": "Albin Cassirer"
                                },
                                {
                                    "authorId": "2065040422",
                                    "name": "Andy Brock"
                                },
                                {
                                    "authorId": "35550664",
                                    "name": "Michela Paganini"
                                },
                                {
                                    "authorId": "2060655766",
                                    "name": "G. Irving"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "2217144",
                                    "name": "Simon Osindero"
                                },
                                {
                                    "authorId": "34838386",
                                    "name": "K. Simonyan"
                                },
                                {
                                    "authorId": "34269227",
                                    "name": "Jack W. Rae"
                                },
                                {
                                    "authorId": "152585800",
                                    "name": "Erich Elsen"
                                },
                                {
                                    "authorId": "2175946",
                                    "name": "L. Sifre"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1100
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2022)",
                        "snippets": [
                            "Retrieval Augmented Models Retrieval augmented models are hybrid models containing both parameterized sequence models and a nonparametric memory, infusing world knowledge into existing language models. Among them, KNN-LM (Khandelwal et al., 2019) was first proposed to retrieve instances from a text training corpus to help language modeling. Later, RETRO (Borgeaud et al., 2021) was proposed to scale up the text corpus to trillions of tokens, enabling the model to achieve similar perplexity to GPT-3 (Brown et al., 2020)) with 25x fewer model parameters. Another family of models, such as REALM (Guu et al., 2020), RAG (Lewis et al., 2020), and FiD (Izacard et al., 2020), integrate Wikipedia passages as a datastore to benefit downstream knowledge intensive tasks (e.g. Question Answering). REALM is an encoder-only model trained with masked lan-guage modeling, while RAG and FiD adopt an encoder-decoder model with a generative language modeling objective."
                        ],
                        "paper": {
                            "corpus_id": 252735160,
                            "title": "MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text",
                            "authors": [
                                {
                                    "authorId": "2928777",
                                    "name": "Wenhu Chen"
                                },
                                {
                                    "authorId": "2804000",
                                    "name": "Hexiang Hu"
                                },
                                {
                                    "authorId": "2145309103",
                                    "name": "Xi Chen"
                                },
                                {
                                    "authorId": "2986975",
                                    "name": "Pat Verga"
                                },
                                {
                                    "authorId": "50056360",
                                    "name": "William W. Cohen"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 159
                        },
                        "score": 0.76220703125
                    },
                    {
                        "id": "(Guo et al., 2024)",
                        "snippets": [
                            "Recently, a series of retrieval-augmented language models [14,24,47] have been proposed to augment language models with external knowledge [9,17,53]. Retrieval-augmented techniques can generally be divided into two types. The first type is at the input layer [14,20,42], where the retrieved information is text chunks. The second type is at the output layer [7,24,47], where the retrieved information is tokens. By combining the retrieved tokens with the tokens generated by the original model, the accuracy of the retrieval-augmented model's generation for each token can be improved.\n\nThe first type of method can provide the model with more external knowledge, making it adept at handling tasks in the NLP field such as knowledge-based question answering [27,45,49]. The second type of method can refer to the retrieved information to correct the generated tokens, making it more suited for handling strictly structured generative tasks, such as code completion [7,10,11].\n\nTo better understand the mechanism, we take kNN-LM [24] as an example for a detailed explanation. Given a context sequence   = ( 1 , . . .,   \u22121 ), the language models (LMs) estimate   (  |  ), i.e., the probability distribution over the next token   . kNN-LM is designed to augment a pre-trained language model with a set of nearest neighbours retrieved from an external text collection, which can be the training set."
                        ],
                        "paper": {
                            "corpus_id": 268856642,
                            "title": "FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion",
                            "authors": [
                                {
                                    "authorId": "2290464625",
                                    "name": "Qi Guo"
                                },
                                {
                                    "authorId": "2118890600",
                                    "name": "Xiaohong Li"
                                },
                                {
                                    "authorId": "2288741802",
                                    "name": "Xiaofei Xie"
                                },
                                {
                                    "authorId": "2290359321",
                                    "name": "Shangqing Liu"
                                },
                                {
                                    "authorId": "2109915677",
                                    "name": "Ze Tang"
                                },
                                {
                                    "authorId": "1758019",
                                    "name": "Ruitao Feng"
                                },
                                {
                                    "authorId": "2294667814",
                                    "name": "Junjie Wang"
                                },
                                {
                                    "authorId": "2248015856",
                                    "name": "Jidong Ge"
                                },
                                {
                                    "authorId": "2279752248",
                                    "name": "Lei Bu"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Symposium on Software Testing and Analysis",
                            "n_citations": 11
                        },
                        "score": 0.81396484375
                    },
                    {
                        "id": "(Lu et al., 2022)",
                        "snippets": [
                            "Code completion, which aims to predict the following code token(s) according to the code context, can improve the productivity of software development. Recent work has proved that statistical language modeling with transformers can greatly improve the performance in the code completion task via learning from large-scale source code datasets. However, current approaches focus only on code context within the file or project, i.e. internal context. Our distinction is utilizing \"external\" context, inspired by human behaviors of copying from the related code snippets when writing code. Specifically, we propose a retrieval-augmented code completion framework, leveraging both lexical copying and referring to code with similar semantics by retrieval. We adopt a stage-wise training approach that combines a source code retriever and an auto-regressive language model for programming language. We evaluate our approach in the code completion task in Python and Java programming languages, achieving a state-of-the-art performance on CodeXGLUE benchmark."
                        ],
                        "paper": {
                            "corpus_id": 247450969,
                            "title": "ReACC: A Retrieval-Augmented Code Completion Framework",
                            "authors": [
                                {
                                    "authorId": "2115338656",
                                    "name": "Shuai Lu"
                                },
                                {
                                    "authorId": "46429989",
                                    "name": "Nan Duan"
                                },
                                {
                                    "authorId": "5534572",
                                    "name": "Hojae Han"
                                },
                                {
                                    "authorId": "2278834796",
                                    "name": "Daya Guo"
                                },
                                {
                                    "authorId": "1716415",
                                    "name": "Seung-won Hwang"
                                },
                                {
                                    "authorId": "2061625488",
                                    "name": "Alexey Svyatkovskiy"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 147
                        },
                        "score": 0
                    },
                    {
                        "id": "(Munikoti et al., 2023)",
                        "snippets": [
                            "The retrieval augmented language models (RALM) primarily address the grounding and scalability challenges in standard language models (LM). RALM aims to address these limitations by combining a LM with an external knowledge base. In this framework, the LM generates text conditioned not only on the input query but also on relevant knowledge retrieved from the knowledge base. The retrieved knowledge is usually the text chunks or passages from documents that provide factual grounding to contextualize the model's pre-Preprint version dictions. In other words, this approach decentralizes model knowledge into parameters and external knowledge sources, thereby addressing the challenges of scalability and adaptability."
                        ],
                        "paper": {
                            "corpus_id": 265308533,
                            "title": "ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science",
                            "authors": [
                                {
                                    "authorId": "2258957941",
                                    "name": "Sai Munikoti"
                                },
                                {
                                    "authorId": "145536102",
                                    "name": "Anurag Acharya"
                                },
                                {
                                    "authorId": "2054838317",
                                    "name": "S. Wagle"
                                },
                                {
                                    "authorId": "24029613",
                                    "name": "Sameera Horawalavithana"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.82080078125
                    },
                    {
                        "id": "(Yang et al., 2025)",
                        "snippets": [
                            "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) overcomes the limitations of Large Language Models (LLMs), which rely on static, pre-trained datasets that can become outdated and lack domain-specific information. This restricts LLMs' ability to generate accurate and up-to-date responses. RAG integrates Information Retrieval (IR) systems with LLMs, enabling them to query external knowledge sources and access real-time, domain-relevant data. In a typical RAG framework, a retriever processes user queries and retrieves relevant documents based on semantic similarity. These documents are then combined with the original query and passed to the LLM to generate a more accurate and comprehensive response."
                        ],
                        "paper": {
                            "corpus_id": 275358357,
                            "title": "Knowledge Retrieval Based on Generative AI",
                            "authors": [
                                {
                                    "authorId": "2191368257",
                                    "name": "Te-Lun Yang"
                                },
                                {
                                    "authorId": "2253878746",
                                    "name": "Jyi-Shane Liu"
                                },
                                {
                                    "authorId": "40130996",
                                    "name": "Yuen-Hsien Tseng"
                                },
                                {
                                    "authorId": "2262396644",
                                    "name": "Jyh-Shing Roger Jang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.77734375
                    },
                    {
                        "id": "(Yu, 2022)",
                        "snippets": [
                            "Retrieval-augmented generation (RAG) methods have been receiving increasing attention from the NLP community and achieved state-of-the-art performance on many NLP downstream tasks. Compared with conventional pre-trained generation models, RAG methods have remarkable advantages such as easy knowledge acquisition, strong scalability, and low training cost. Although existing RAG models have been applied to various knowledge-intensive NLP tasks, such as open-domain QA and dialogue systems, most of the work has focused on retrieving unstructured text documents from Wikipedia. In this paper, I first elaborate on the current obstacles to retrieving knowledge from a single-source homogeneous corpus. Then, I demonstrate evidence from both existing literature and my experiments, and provide multiple solutions on retrieval-augmented generation methods across heterogeneous knowledge."
                        ],
                        "paper": {
                            "corpus_id": 250391000,
                            "title": "Retrieval-augmented Generation across Heterogeneous Knowledge",
                            "authors": [
                                {
                                    "authorId": "38767143",
                                    "name": "W. Yu"
                                }
                            ],
                            "year": 2022,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 42
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "Retrieval-Augmented Generation (RAG) enhances the generative capabilities of language models by incorporating retrieved knowledge for in-context learning [22], [26]. While general language models excel in producing responses for general queries, they are prone to generating hallucinations when tasked with domain-specific knowledge usage. RAG addresses this issue by retrieving established knowledge corpora and providing this information as context to the language model [22]. NaiveRAG represents the most basic architecture within this framework, in which the system retrieves the top-k documents that are most relevant to the query and integrate them into the prompt, thereby grounding the responses in more relevant information [25]. \n\nExpanding on NaiveRAG, advanced RAG incorporates additional modules or structures to improve retrieval precision. Reranking is a notable example, where a reranker is employed to refine the initial ranked list (e.g., Re2G [37] and bgereranker [38], both are based on BERT [39]). Furthermore, studies have indicated that excessive noise and lengthy context can have a negative impact on inference performance. To address this, prompt compression methods such as Selective Context [40] and LLMLingua [41] have been developed. These methods emphasize key information while reducing noise and context length, as discussed in [22]."
                        ],
                        "paper": {
                            "corpus_id": 271570928,
                            "title": "Knowledge Pyramid Construction for Multi-Level Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2314169880",
                                    "name": "Rubing Chen"
                                },
                                {
                                    "authorId": "2108162240",
                                    "name": "Xulu Zhang"
                                },
                                {
                                    "authorId": "2313746412",
                                    "name": "Jiaxin Wu"
                                },
                                {
                                    "authorId": "2291324376",
                                    "name": "Wenqi Fan"
                                },
                                {
                                    "authorId": "2115493866",
                                    "name": "Xiao Wei"
                                },
                                {
                                    "authorId": "2293397899",
                                    "name": "Qing Li"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.77001953125
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.314358
    }
}