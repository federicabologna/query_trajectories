{
    "query": "analyze training dynamics of language models",
    "user_id": "lib_user",
    "task_id": "047e0897-b997-4650-b3e5-92a20d2e7bfb",
    "timestamp": "2025-06-23T23:47:32.989272",
    "n_retrieval": 256,
    "n_retrieved": 262,
    "n_candidates": 35,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.522171,
    "decomposed_query": {
        "rewritten_query": "Analysis of training dynamics of language models.",
        "keyword_query": "training dynamics language models",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009339,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2022,
            "reference_count": 106,
            "citation_count": 197,
            "influential_citation_count": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2205.10770",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.10770, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2551387",
                    "name": "Kushal Tirumala"
                },
                {
                    "authorId": "153608000",
                    "name": "Aram H. Markosyan"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "2201435",
                    "name": "Armen Aghajanyan"
                }
            ],
            "abstract": "Despite their wide adoption, the underlying training and memorization dynamics of very large language models is not well understood. We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process. We measure the effects of dataset size, learning rate, and model size on memorization, finding that larger language models memorize training data faster across all settings. Surprisingly, we show that larger models can memorize a larger portion of the data before over-fitting and tend to forget less throughout the training process. We also analyze the memorization dynamics of different parts of speech and find that models memorize nouns and numbers first; we hypothesize and provide empirical evidence that nouns and numbers act as a unique identifier for memorizing individual training examples. Together, these findings present another piece of the broader puzzle of trying to understand what actually improves as models get bigger.",
            "corpus_id": 248986465,
            "sentences": [
                {
                    "corpus_id": "248986465",
                    "title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models",
                    "text": "Memorization in Language Models: Unintended memorization is a known challenge for language models [14,85], which makes them open to extraction attacks [15,89] and membership inference attacks [41,64], although there has been work on mitigating these vulnerabilities [51,88]. Recent work has argued that memorization is not exclusively harmful, and can be crucial for certain types of generalization (e.g., on QA tasks) [11,46,87], while also allowing the models to encode significant amounts of world or factual knowledge [4,35,71]. There is also a growing body of work analyzing fundamental properties of memorization in language models [16,47,60,95]. Most related to our work Carlini et al. [16] analyzes memorization of fully trained language models and observes a dependence on model scale, training data duplication, and prompting context length. While we also study scaling behavior, our focus instead is on the memorization dynamics throughout training. \n\nLanguage Model Training Dynamics: Previous work has extensively analyzed training dynamics to understand how neural models acquire information over training [1,30,34,66,74]. Saphra and Lopez [80] were the first to analyze training dynamics for language modeling, focusing on the evolution of internal representations over pre-training. This inspired a line of work analyzing how neural language models learn linguistic structure/world knowledge [20,21,53], individual words [17], and cross-lingual structure [10] over pre-training. This analysis has been extended to many downstream tasks, including text summarization [33], machine/speech translation [81,86,92], and various NLP tasks [36,61]. \n\nForgetting in Language Models: There has also been work studying memory degradation (forgetting) in language models. Catastrophic forgetting or catastrophic interference, first reported in [59,77], studies how neural networks tend to forget the information from previous trained tasks or training batches, when trained on new data. This provides a key challenge for continual learning (or life-long learning) [19], where the goal is to gradually learn from a single pass over a, typically very large, stream of data.",
                    "score": 0.4788464259136644,
                    "section_title": "Background and Related Work",
                    "char_start_offset": 1743,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 274
                        },
                        {
                            "start": 275,
                            "end": 532
                        },
                        {
                            "start": 533,
                            "end": 652
                        },
                        {
                            "start": 653,
                            "end": 851
                        },
                        {
                            "start": 852,
                            "end": 960
                        },
                        {
                            "start": 963,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1298
                        },
                        {
                            "start": 1299,
                            "end": 1494
                        },
                        {
                            "start": 1495,
                            "end": 1657
                        },
                        {
                            "start": 1660,
                            "end": 1776
                        },
                        {
                            "start": 1777,
                            "end": 1991
                        },
                        {
                            "start": 1992,
                            "end": 2176
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 98,
                            "end": 102,
                            "matchedPaperCorpusId": "170076423"
                        },
                        {
                            "start": 102,
                            "end": 105,
                            "matchedPaperCorpusId": "159041003"
                        },
                        {
                            "start": 151,
                            "end": 155,
                            "matchedPaperCorpusId": "229156229"
                        },
                        {
                            "start": 155,
                            "end": 158,
                            "matchedPaperCorpusId": "220658693"
                        },
                        {
                            "start": 1120,
                            "end": 1123,
                            "matchedPaperCorpusId": "108298098"
                        },
                        {
                            "start": 1129,
                            "end": 1132,
                            "matchedPaperCorpusId": "49271358"
                        },
                        {
                            "start": 1437,
                            "end": 1441,
                            "matchedPaperCorpusId": "238408423"
                        },
                        {
                            "start": 1615,
                            "end": 1619,
                            "matchedPaperCorpusId": "250390453"
                        },
                        {
                            "start": 1619,
                            "end": 1622,
                            "matchedPaperCorpusId": "237239585"
                        },
                        {
                            "start": 1622,
                            "end": 1625,
                            "matchedPaperCorpusId": "224818197"
                        },
                        {
                            "start": 1649,
                            "end": 1653,
                            "matchedPaperCorpusId": "227905362"
                        },
                        {
                            "start": 1653,
                            "end": 1656,
                            "matchedPaperCorpusId": "216914339"
                        },
                        {
                            "start": 1849,
                            "end": 1853,
                            "matchedPaperCorpusId": "61019113"
                        },
                        {
                            "start": 1853,
                            "end": 1856,
                            "matchedPaperCorpusId": "18556305"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9990234375
                },
                {
                    "corpus_id": "248986465",
                    "title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models",
                    "text": "The rate and extent to which a model memorizes its training data are key statistics that provide evidence about how it is likely to generalize to new test instances. Classical frameworks, such as bias-variance tradeoff [31], argued for fitting a training set without full memorization. However, recent work has established a more symbiotic relationship between memorization and generalization in deep learning [13,26,28]. This paper empirically studies memorization in causal and masked language modeling, across model sizes and throughout the training process. \n\nMuch of the recent performance gains for language models have come from scale, with the most recent models reaching up to 10 11 parameters [22,73,83]. Larger models are also known to memorize more training data [16], which is a crucial component of their improved generalization. However, perhaps surprisingly, relatively little work has been done in understanding the impact of scale on the dynamics of language model memorization over training. Existing work focuses on analyzing memorization post-training [16,47,88,95]. In this work, we study the memorization and forgetting dynamics in language models, with a focus on better measuring how they change as we scale up model size. Our primary contributions include: \n\n1. We measure the dependence of memorization dynamics over training on model size (and other factors such as dataset size, overfitting, and learning rate). We find that larger language models memorize training data faster ( \u00a7 4). 2. We design controlled experiments that allow us to characterize the forgetting curves in language models (i.e., how language models naturally forget memories throughout training).",
                    "score": 0.47389060568496133,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 165
                        },
                        {
                            "start": 166,
                            "end": 285
                        },
                        {
                            "start": 286,
                            "end": 421
                        },
                        {
                            "start": 422,
                            "end": 561
                        },
                        {
                            "start": 564,
                            "end": 714
                        },
                        {
                            "start": 715,
                            "end": 843
                        },
                        {
                            "start": 844,
                            "end": 1010
                        },
                        {
                            "start": 1011,
                            "end": 1087
                        },
                        {
                            "start": 1088,
                            "end": 1247
                        },
                        {
                            "start": 1248,
                            "end": 1282
                        },
                        {
                            "start": 1285,
                            "end": 1440
                        },
                        {
                            "start": 1441,
                            "end": 1696
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 219,
                            "end": 223,
                            "matchedPaperCorpusId": "58068920"
                        },
                        {
                            "start": 410,
                            "end": 414,
                            "matchedPaperCorpusId": "228376138"
                        },
                        {
                            "start": 417,
                            "end": 420,
                            "matchedPaperCorpusId": "225366984"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9990234375
                },
                {
                    "corpus_id": "248986465",
                    "title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models",
                    "text": "To investigate the dependence of our observations on the particular language modeling task, we repeat this analysis for the masked language modeling task on WIKITEXT103 with mask probability 0.15. \n\nUnlike in causal language modeling, Figure 2 shows that T (N, \u03c4 ) is not monotonically decreasing in N for lower values of \u03c4 , and is monotonically decreasing in N for higher values of \u03c4 , where the phase transition4 between these two regimes occurs between \u03c4 = 0.6 and \u03c4 = 0.7. Smaller models memorize the training data quicker initially and slower in the long run (e.g., right plot of Figure 11). Language model training is heavily dependent on the dataset size [44], and therefore we expect M (f ) to be similarly impacted. In Figure 3, we analyze training set memorization on the much bigger ROBERTA dataset for both masked and causal language modeling. With large datasets such as ROBERTA dataset, it becomes infeasible to perform multiple epochs and evaluate memorization on the entire training set, especially when training larger models. Consequently, we focus on smaller values of \u03c4 and investigate the number of gradient descent updates it takes to reach memorization thresholds, i.e., T update (N, \u03c4 ). In Figure 3 we observe a similar trend as Figure 1, where T update (N, \u03c4 ) is monotonically decreasing with N for various \u03c4 , in both masked and causal language modeling. Unlike with WIKITEXT103, masked language modeling does not have a phase transition for \u03c4 .",
                    "score": 0.4121202598171314,
                    "section_title": "Dependence on Language Modeling Task and Dataset Size",
                    "char_start_offset": 9814,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 196
                        },
                        {
                            "start": 199,
                            "end": 477
                        },
                        {
                            "start": 478,
                            "end": 597
                        },
                        {
                            "start": 598,
                            "end": 725
                        },
                        {
                            "start": 726,
                            "end": 856
                        },
                        {
                            "start": 857,
                            "end": 1044
                        },
                        {
                            "start": 1045,
                            "end": 1212
                        },
                        {
                            "start": 1213,
                            "end": 1383
                        },
                        {
                            "start": 1384,
                            "end": 1474
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                }
            ],
            "relevance_judgement": 0.9990234375,
            "relevance_judgment_input_expanded": "# Title: Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Kushal Tirumala, Aram H. Markosyan, Luke Zettlemoyer, Armen Aghajanyan\n## Abstract\nDespite their wide adoption, the underlying training and memorization dynamics of very large language models is not well understood. We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process. We measure the effects of dataset size, learning rate, and model size on memorization, finding that larger language models memorize training data faster across all settings. Surprisingly, we show that larger models can memorize a larger portion of the data before over-fitting and tend to forget less throughout the training process. We also analyze the memorization dynamics of different parts of speech and find that models memorize nouns and numbers first; we hypothesize and provide empirical evidence that nouns and numbers act as a unique identifier for memorizing individual training examples. Together, these findings present another piece of the broader puzzle of trying to understand what actually improves as models get bigger.\n## Introduction\nThe rate and extent to which a model memorizes its training data are key statistics that provide evidence about how it is likely to generalize to new test instances. Classical frameworks, such as bias-variance tradeoff [31], argued for fitting a training set without full memorization. However, recent work has established a more symbiotic relationship between memorization and generalization in deep learning [13,26,28]. This paper empirically studies memorization in causal and masked language modeling, across model sizes and throughout the training process. \n\nMuch of the recent performance gains for language models have come from scale, with the most recent models reaching up to 10 11 parameters [22,73,83]. Larger models are also known to memorize more training data [16], which is a crucial component of their improved generalization. However, perhaps surprisingly, relatively little work has been done in understanding the impact of scale on the dynamics of language model memorization over training. Existing work focuses on analyzing memorization post-training [16,47,88,95]. In this work, we study the memorization and forgetting dynamics in language models, with a focus on better measuring how they change as we scale up model size. Our primary contributions include: \n\n1. We measure the dependence of memorization dynamics over training on model size (and other factors such as dataset size, overfitting, and learning rate). We find that larger language models memorize training data faster ( \u00a7 4). 2. We design controlled experiments that allow us to characterize the forgetting curves in language models (i.e., how language models naturally forget memories throughout training).\n\n## Background and Related Work\nMemorization in Language Models: Unintended memorization is a known challenge for language models [14,85], which makes them open to extraction attacks [15,89] and membership inference attacks [41,64], although there has been work on mitigating these vulnerabilities [51,88]. Recent work has argued that memorization is not exclusively harmful, and can be crucial for certain types of generalization (e.g., on QA tasks) [11,46,87], while also allowing the models to encode significant amounts of world or factual knowledge [4,35,71]. There is also a growing body of work analyzing fundamental properties of memorization in language models [16,47,60,95]. Most related to our work Carlini et al. [16] analyzes memorization of fully trained language models and observes a dependence on model scale, training data duplication, and prompting context length. While we also study scaling behavior, our focus instead is on the memorization dynamics throughout training. \n\nLanguage Model Training Dynamics: Previous work has extensively analyzed training dynamics to understand how neural models acquire information over training [1,30,34,66,74]. Saphra and Lopez [80] were the first to analyze training dynamics for language modeling, focusing on the evolution of internal representations over pre-training. This inspired a line of work analyzing how neural language models learn linguistic structure/world knowledge [20,21,53], individual words [17], and cross-lingual structure [10] over pre-training. This analysis has been extended to many downstream tasks, including text summarization [33], machine/speech translation [81,86,92], and various NLP tasks [36,61]. \n\nForgetting in Language Models: There has also been work studying memory degradation (forgetting) in language models. Catastrophic forgetting or catastrophic interference, first reported in [59,77], studies how neural networks tend to forget the information from previous trained tasks or training batches, when trained on new data. This provides a key challenge for continual learning (or life-long learning) [19], where the goal is to gradually learn from a single pass over a, typically very large, stream of data.\n\n## Dependence on Language Modeling Task and Dataset Size\nTo investigate the dependence of our observations on the particular language modeling task, we repeat this analysis for the masked language modeling task on WIKITEXT103 with mask probability 0.15. \n\nUnlike in causal language modeling, Figure 2 shows that T (N, \u03c4 ) is not monotonically decreasing in N for lower values of \u03c4 , and is monotonically decreasing in N for higher values of \u03c4 , where the phase transition4 between these two regimes occurs between \u03c4 = 0.6 and \u03c4 = 0.7. Smaller models memorize the training data quicker initially and slower in the long run (e.g., right plot of Figure 11). Language model training is heavily dependent on the dataset size [44], and therefore we expect M (f ) to be similarly impacted. In Figure 3, we analyze training set memorization on the much bigger ROBERTA dataset for both masked and causal language modeling. With large datasets such as ROBERTA dataset, it becomes infeasible to perform multiple epochs and evaluate memorization on the entire training set, especially when training larger models. Consequently, we focus on smaller values of \u03c4 and investigate the number of gradient descent updates it takes to reach memorization thresholds, i.e., T update (N, \u03c4 ). In Figure 3 we observe a similar trend as Figure 1, where T update (N, \u03c4 ) is monotonically decreasing with N for various \u03c4 , in both masked and causal language modeling. Unlike with WIKITEXT103, masked language modeling does not have a phase transition for \u03c4 .",
            "reference_string": "[248986465 | Tirumala et al. | 2022 | Citations: 197]"
        },
        {
            "title": "Training Trajectories of Language Models Across Scales",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 49,
            "citation_count": 64,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2212.09803",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.09803, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "67284811",
                    "name": "Mengzhou Xia"
                },
                {
                    "authorId": "2347956",
                    "name": "Mikel Artetxe"
                },
                {
                    "authorId": "2384711",
                    "name": "Chunting Zhou"
                },
                {
                    "authorId": "143724481",
                    "name": "Xi Victoria Lin"
                },
                {
                    "authorId": "10721120",
                    "name": "Ramakanth Pasunuru"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "1389924486",
                    "name": "Ves Stoyanov"
                }
            ],
            "abstract": "Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al., 2022)\u2014from 125M to 175B parameters\u2014on next-token prediction, sequence-level generation and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior (Nakkiran et al., 2020); 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; and 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation.",
            "corpus_id": 254877112,
            "sentences": [
                {
                    "corpus_id": "254877112",
                    "title": "Training Trajectories of Language Models Across Scales",
                    "text": "Scaling up language models has been shown to improve language modeling perplexity (Kaplan et al., 2020;Hernandez et al., 2022) as well as zero-or few-shot end task accuracies (Brown et al., 2020;Rae et al., 2021;Chowdhery et al., 2022;Zhang et al., 2022). However, relatively little is understood about why or how this happens. How do the training dynamics differ as models get larger? What do language models of different sizes learn during pre-training in terms of both generating texts and solving end tasks?\n\nWe attempt to make progress to answer these questions by studying the training trajectories of differently-sized OPT models (Zhang et al., 2022) through analyzing their intermediate checkpoints.\n\nIn contrast to prior work, which studies the trajectories of small models with up to 300M parameters (Liu et al., 2021;Choshen et al., 2022;Blevins et al., 2022) or focuses on the language modeling objective alone (Kaplan et al., 2020;Hernandez et al., 2021Hernandez et al., , 2022, we are the first to comprehensively study the training trajectories of large-scale autoregressive language models with up to 175B parameters across a wide range of settings.\n\nRepeatedly across training and different model scales, we analyze three aspects of model performance: (i) next-token prediction on subsets of tokens (ii) sequence-level generation and (iii) downstream task performance. We use perplexity, which is closely tied to language model evaluation, as the major metric throughout the study.\n\nFor next-token prediction ( \u00a73), we study the trajectory by categorizing each token's prediction as stagnated, upward or downward according to its perplexity trend as training progresses. We find each category comprising a significant number of tokens: while a significant number of tokens' perplexity stagnate, a subset of tokens with an increasing perplexity in smaller models exhibit a doubledescent trend (Nakkiran et al., 2020) where perplexity increases and then decreases in larger models. These behaviors primarily emerge at a similar validation perplexity across model scales",
                    "score": 0.44957114632138095,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                }
            ],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: Training Trajectories of Language Models Across Scales\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, Ves Stoyanov\n## Abstract\nScaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al., 2022)\u2014from 125M to 175B parameters\u2014on next-token prediction, sequence-level generation and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior (Nakkiran et al., 2020); 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; and 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation.\n## Introduction\nScaling up language models has been shown to improve language modeling perplexity (Kaplan et al., 2020;Hernandez et al., 2022) as well as zero-or few-shot end task accuracies (Brown et al., 2020;Rae et al., 2021;Chowdhery et al., 2022;Zhang et al., 2022). However, relatively little is understood about why or how this happens. How do the training dynamics differ as models get larger? What do language models of different sizes learn during pre-training in terms of both generating texts and solving end tasks?\n\nWe attempt to make progress to answer these questions by studying the training trajectories of differently-sized OPT models (Zhang et al., 2022) through analyzing their intermediate checkpoints.\n\nIn contrast to prior work, which studies the trajectories of small models with up to 300M parameters (Liu et al., 2021;Choshen et al., 2022;Blevins et al., 2022) or focuses on the language modeling objective alone (Kaplan et al., 2020;Hernandez et al., 2021Hernandez et al., , 2022, we are the first to comprehensively study the training trajectories of large-scale autoregressive language models with up to 175B parameters across a wide range of settings.\n\nRepeatedly across training and different model scales, we analyze three aspects of model performance: (i) next-token prediction on subsets of tokens (ii) sequence-level generation and (iii) downstream task performance. We use perplexity, which is closely tied to language model evaluation, as the major metric throughout the study.\n\nFor next-token prediction ( \u00a73), we study the trajectory by categorizing each token's prediction as stagnated, upward or downward according to its perplexity trend as training progresses. We find each category comprising a significant number of tokens: while a significant number of tokens' perplexity stagnate, a subset of tokens with an increasing perplexity in smaller models exhibit a doubledescent trend (Nakkiran et al., 2020) where perplexity increases and then decreases in larger models. These behaviors primarily emerge at a similar validation perplexity across model scales",
            "reference_string": "[254877112 | Xia et al. | 2022 | Citations: 64]"
        },
        {
            "title": "Training Dynamics of Contextual N-Grams in Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 35,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.00863, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2243240285",
                    "name": "Lucia Quirke"
                },
                {
                    "authorId": "2047549746",
                    "name": "Lovis Heindrich"
                },
                {
                    "authorId": "2056771333",
                    "name": "Wes Gurnee"
                },
                {
                    "authorId": "2051128902",
                    "name": "Neel Nanda"
                }
            ],
            "abstract": "Prior work has shown the existence of contextual neurons in language models, including a neuron that activates on German text. We show that this neuron exists within a broader contextual n-gram circuit: we find late layer neurons which recognize and continue n-grams common in German text, but which only activate if the German neuron is active. We investigate the formation of this circuit throughout training and find that it is an example of what we call a second-order circuit. In particular, both the constituent n-gram circuits and the German detection circuit which culminates in the German neuron form with independent functions early in training - the German detection circuit partially through modeling German unigram statistics, and the n-grams by boosting appropriate completions. Only after both circuits have already formed do they fit together into a second-order circuit. Contrary to the hypotheses presented in prior work, we find that the contextual n-gram circuit forms gradually rather than in a sudden phase transition. We further present a range of anomalous observations such as a simultaneous phase transition in many tasks coinciding with the learning rate warm-up, and evidence that many context neurons form simultaneously early in training but are later unlearned.",
            "corpus_id": 264935245,
            "sentences": [
                {
                    "corpus_id": "264935245",
                    "title": "Training Dynamics of Contextual N-Grams in Language Models",
                    "text": "Training dynamics Several works (Olsson et al., 2022;Nanda et al., 2023) discovered circuits in language models that seem causally relevant to emergent capabilities and can be ablated to reduce the capabilities. Michaud et al. (2023) postulated that many capabilities in language models emerge rapidly in phase transitions and proposed a model in which any capabilities which appear to instead form gradually can be decomposed into discrete, additive capabilities which emerge in independent phase transitions. Finally, several works (Varma et al., 2023;Nanda et al., 2023) observed the phenomenon of grokking, where a model that has already achieved near-perfect performance on the training data transitions to a mechanism that generalizes. \n\nWhile only a minority of circuits-style analysis has focused on model development, a wealth of results in the field of developmental interpretability have characterized the training dynamics of language models using other units of analysis. Behavioral analysis has shown that many model capabilities, such as predicting parts of speech and individual tokens, are learned during independent phase transitions characterized by varying slopes and points of maximum curvature (Chiang et al., 2020;Wei et al., 2022). Raghu et al. (2017) proposed singular vector canonical correlation analysis, a tool which allows different model layers and networks to be compared, and showed that early network layers converge earlier in training than late layers. Saphra and Lopez (2020b,a) found that LSTM models learn hierarchical sequential features in natural language bottom up -shorter sequences are learned first and form the basis for the representation of longer sequences that contain the shorter ones. Finally, Saxe et al. (2019) found that the features are learned in an order determined by the magnitude of the corresponding singular value in the model. Dodge et al. (2020) found that differences in random parameter initializations and training data order contribute comparably to differences between learned representations of fine-tuning runs of the same model on identical data, and McCoy et al. (2020) found that identically trained BERT transformer models are consistent with this finding, varying widely in generalization performance.",
                    "score": 0.5338107389463258,
                    "section_title": "Related Work",
                    "char_start_offset": 17886,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 510
                        },
                        {
                            "start": 511,
                            "end": 741
                        },
                        {
                            "start": 744,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1255
                        },
                        {
                            "start": 1256,
                            "end": 1488
                        },
                        {
                            "start": 1489,
                            "end": 1737
                        },
                        {
                            "start": 1738,
                            "end": 1891
                        },
                        {
                            "start": 1892,
                            "end": 2279
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1216,
                            "end": 1237,
                            "matchedPaperCorpusId": "222140842"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                }
            ],
            "relevance_judgement": 0.99755859375,
            "relevance_judgment_input_expanded": "# Title: Training Dynamics of Contextual N-Grams in Language Models\n# Venue: arXiv.org\n# Authors: Lucia Quirke, Lovis Heindrich, Wes Gurnee, Neel Nanda\n## Abstract\nPrior work has shown the existence of contextual neurons in language models, including a neuron that activates on German text. We show that this neuron exists within a broader contextual n-gram circuit: we find late layer neurons which recognize and continue n-grams common in German text, but which only activate if the German neuron is active. We investigate the formation of this circuit throughout training and find that it is an example of what we call a second-order circuit. In particular, both the constituent n-gram circuits and the German detection circuit which culminates in the German neuron form with independent functions early in training - the German detection circuit partially through modeling German unigram statistics, and the n-grams by boosting appropriate completions. Only after both circuits have already formed do they fit together into a second-order circuit. Contrary to the hypotheses presented in prior work, we find that the contextual n-gram circuit forms gradually rather than in a sudden phase transition. We further present a range of anomalous observations such as a simultaneous phase transition in many tasks coinciding with the learning rate warm-up, and evidence that many context neurons form simultaneously early in training but are later unlearned.\n## Related Work\nTraining dynamics Several works (Olsson et al., 2022;Nanda et al., 2023) discovered circuits in language models that seem causally relevant to emergent capabilities and can be ablated to reduce the capabilities. Michaud et al. (2023) postulated that many capabilities in language models emerge rapidly in phase transitions and proposed a model in which any capabilities which appear to instead form gradually can be decomposed into discrete, additive capabilities which emerge in independent phase transitions. Finally, several works (Varma et al., 2023;Nanda et al., 2023) observed the phenomenon of grokking, where a model that has already achieved near-perfect performance on the training data transitions to a mechanism that generalizes. \n\nWhile only a minority of circuits-style analysis has focused on model development, a wealth of results in the field of developmental interpretability have characterized the training dynamics of language models using other units of analysis. Behavioral analysis has shown that many model capabilities, such as predicting parts of speech and individual tokens, are learned during independent phase transitions characterized by varying slopes and points of maximum curvature (Chiang et al., 2020;Wei et al., 2022). Raghu et al. (2017) proposed singular vector canonical correlation analysis, a tool which allows different model layers and networks to be compared, and showed that early network layers converge earlier in training than late layers. Saphra and Lopez (2020b,a) found that LSTM models learn hierarchical sequential features in natural language bottom up -shorter sequences are learned first and form the basis for the representation of longer sequences that contain the shorter ones. Finally, Saxe et al. (2019) found that the features are learned in an order determined by the magnitude of the corresponding singular value in the model. Dodge et al. (2020) found that differences in random parameter initializations and training data order contribute comparably to differences between learned representations of fine-tuning runs of the same model on identical data, and McCoy et al. (2020) found that identically trained BERT transformer models are consistent with this finding, varying widely in generalization performance.",
            "reference_string": "[264935245 | Quirke et al. | 2023 | Citations: 5]"
        },
        {
            "title": "How do language models learn facts? Dynamics, curricula and hallucinations",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 47,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.21676, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1729494470",
                    "name": "Nicolas Zucchet"
                },
                {
                    "authorId": "2320771936",
                    "name": "J\u00f6rg Bornschein"
                },
                {
                    "authorId": "2316336431",
                    "name": "Stephanie Chan"
                },
                {
                    "authorId": "2270676283",
                    "name": "Andrew K. Lampinen"
                },
                {
                    "authorId": "1996134",
                    "name": "Razvan Pascanu"
                },
                {
                    "authorId": "2289159449",
                    "name": "Soham De"
                }
            ],
            "abstract": "Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distributions lead to shorter plateaus. Finally, hallucinations emerge simultaneously with knowledge, and integrating new knowledge into the model through fine-tuning is challenging, as it quickly corrupts its existing parametric memories. Our results emphasize the importance of data distribution in knowledge acquisition and suggest novel data scheduling strategies to accelerate neural network training.",
            "corpus_id": 277349236,
            "sentences": [
                {
                    "corpus_id": "277349236",
                    "title": "How do language models learn facts? Dynamics, curricula and hallucinations",
                    "text": "Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distributions lead to shorter plateaus. Finally, hallucinations emerge simultaneously with knowledge, and integrating new knowledge into the model through fine-tuning is challenging, as it quickly corrupts its existing parametric memories. Our results emphasize the importance of data distribution in knowledge acquisition and suggest novel data scheduling strategies to accelerate neural network training.",
                    "score": 0.46254871793668956,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                },
                {
                    "corpus_id": "277349236",
                    "title": "How do language models learn facts? Dynamics, curricula and hallucinations",
                    "text": "On the learning dynamics of language models. This work reveals the existence of phase transitions, the formation of induction heads being the canonical example (Garg et al., 2022;Olsson et al., 2022;Reddy, 2024), in tasks as simple as factual recall. While the results presented above were obtained on an isolated task, we speculate that even with natural text and its multi-task nature, task-specific learning dynamics retain the same abrupt transitions and plateaus appear when the formation of a new circuit is the main bottleneck for solving a specific task. We found the time spent in the transition phase to depend more on the data distribution than the model size. Emergent abilities (Wei et al., 2022) can thus result from increased training time as models are scaled. On the data distribution side, our results have two consequences: First, they suggest the benefits of using synthetic data early in pre-training, since data used before the end of the plateau is not retained in the final model. Second, data schedulers, potentially even adaptive ones that reduce diversity when performance plateaus on a task, appear to be a promising direction to improve learning speed. Finally, our identification of changes in feedforward associative memories leading to rapid performance drops during early fine-tuning provides a simple explanation for the practically-observed ineffectiveness of fine-tuning for new knowledge (e.g., Jain et al. (2024); Ovadia et al. ( 2023)). \n\nOn the learning dynamics of neural networks more broadly. This analysis reveals that, in a simple but relevant setting, attention-based recall circuits emerge before the formation of associative memories in feedforward layers. We hypothesize that this occurs as established circuits amplify the correlation between the inputs and backpropagated errors received by feedforward layers. Prior to circuit formation, task learning remains possible (e.g., by artificially increasing name token values), but progress is slow, performance plateaus, and generalization likely suffers. Phenomena like grokking (Nanda et al., 2023a;Power et al., 2022) may indicate that learning dynamics initially find this shortcut, before abandoning it due to regularization.",
                    "score": 0.41621821062589864,
                    "section_title": "Discussion",
                    "char_start_offset": 27933,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 44
                        },
                        {
                            "start": 45,
                            "end": 250
                        },
                        {
                            "start": 251,
                            "end": 562
                        },
                        {
                            "start": 563,
                            "end": 671
                        },
                        {
                            "start": 672,
                            "end": 776
                        },
                        {
                            "start": 777,
                            "end": 1004
                        },
                        {
                            "start": 1005,
                            "end": 1181
                        },
                        {
                            "start": 1182,
                            "end": 1475
                        },
                        {
                            "start": 1478,
                            "end": 1535
                        },
                        {
                            "start": 1536,
                            "end": 1704
                        },
                        {
                            "start": 1705,
                            "end": 1861
                        },
                        {
                            "start": 1862,
                            "end": 2053
                        },
                        {
                            "start": 2054,
                            "end": 2228
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 160,
                            "end": 179,
                            "matchedPaperCorpusId": "251253368"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99462890625
                },
                {
                    "corpus_id": "277349236",
                    "title": "How do language models learn facts? Dynamics, curricula and hallucinations",
                    "text": "This study investigates the learning dynamics underlying factual recall and knowledge acquisition in language models. This presents two main methodological challenges: First, we need to isolate the model's knowledge from other abilities. Second, we want to evaluate the models over the course of learning, which requires computationally efficient measurement techniques. Following Allen-Zhu and Li (2023), we train language models on synthetic biographies that feature key properties of datasets used to train large language models. By carefully designing these synthetic biographies, we can attribute the model's ability to predict specific tokens solely to its acquired knowledge and efficiently measure its knowledge through its performance on these tokens. In this section, we first define knowledge and contrast it with memory, then describe our synthetic dataset and introduce the metrics we use to track knowledge during training, and finally describe training details.",
                    "score": 0.4146767728668682,
                    "section_title": "An experimental setup to track knowledge over the course of learning",
                    "char_start_offset": 2523,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 117
                        },
                        {
                            "start": 118,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 370
                        },
                        {
                            "start": 371,
                            "end": 532
                        },
                        {
                            "start": 533,
                            "end": 760
                        },
                        {
                            "start": 761,
                            "end": 976
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98779296875
                }
            ],
            "relevance_judgement": 0.99755859375,
            "relevance_judgment_input_expanded": "# Title: How do language models learn facts? Dynamics, curricula and hallucinations\n# Venue: arXiv.org\n# Authors: Nicolas Zucchet, J\u00f6rg Bornschein, Stephanie Chan, Andrew K. Lampinen, Razvan Pascanu, Soham De\n## Abstract\nLarge language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distributions lead to shorter plateaus. Finally, hallucinations emerge simultaneously with knowledge, and integrating new knowledge into the model through fine-tuning is challenging, as it quickly corrupts its existing parametric memories. Our results emphasize the importance of data distribution in knowledge acquisition and suggest novel data scheduling strategies to accelerate neural network training.\n## An experimental setup to track knowledge over the course of learning\nThis study investigates the learning dynamics underlying factual recall and knowledge acquisition in language models. This presents two main methodological challenges: First, we need to isolate the model's knowledge from other abilities. Second, we want to evaluate the models over the course of learning, which requires computationally efficient measurement techniques. Following Allen-Zhu and Li (2023), we train language models on synthetic biographies that feature key properties of datasets used to train large language models. By carefully designing these synthetic biographies, we can attribute the model's ability to predict specific tokens solely to its acquired knowledge and efficiently measure its knowledge through its performance on these tokens. In this section, we first define knowledge and contrast it with memory, then describe our synthetic dataset and introduce the metrics we use to track knowledge during training, and finally describe training details.\n\n## Discussion\nOn the learning dynamics of language models. This work reveals the existence of phase transitions, the formation of induction heads being the canonical example (Garg et al., 2022;Olsson et al., 2022;Reddy, 2024), in tasks as simple as factual recall. While the results presented above were obtained on an isolated task, we speculate that even with natural text and its multi-task nature, task-specific learning dynamics retain the same abrupt transitions and plateaus appear when the formation of a new circuit is the main bottleneck for solving a specific task. We found the time spent in the transition phase to depend more on the data distribution than the model size. Emergent abilities (Wei et al., 2022) can thus result from increased training time as models are scaled. On the data distribution side, our results have two consequences: First, they suggest the benefits of using synthetic data early in pre-training, since data used before the end of the plateau is not retained in the final model. Second, data schedulers, potentially even adaptive ones that reduce diversity when performance plateaus on a task, appear to be a promising direction to improve learning speed. Finally, our identification of changes in feedforward associative memories leading to rapid performance drops during early fine-tuning provides a simple explanation for the practically-observed ineffectiveness of fine-tuning for new knowledge (e.g., Jain et al. (2024); Ovadia et al. ( 2023)). \n\nOn the learning dynamics of neural networks more broadly. This analysis reveals that, in a simple but relevant setting, attention-based recall circuits emerge before the formation of associative memories in feedforward layers. We hypothesize that this occurs as established circuits amplify the correlation between the inputs and backpropagated errors received by feedforward layers. Prior to circuit formation, task learning remains possible (e.g., by artificially increasing name token values), but progress is slow, performance plateaus, and generalization likely suffers. Phenomena like grokking (Nanda et al., 2023a;Power et al., 2022) may indicate that learning dynamics initially find this shortcut, before abandoning it due to regularization.",
            "reference_string": "[277349236 | Zucchet et al. | 2025 | Citations: 7]"
        },
        {
            "title": "Training Dynamics of Transformers to Recognize Word Co-occurrence via Gradient Flow Analysis",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.09605, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118571035",
                    "name": "Hongru Yang"
                },
                {
                    "authorId": "1749353",
                    "name": "B. Kailkhura"
                },
                {
                    "authorId": "2269758970",
                    "name": "Zhangyang Wang"
                },
                {
                    "authorId": "2269703508",
                    "name": "Yingbin Liang"
                }
            ],
            "abstract": "Understanding the training dynamics of transformers is important to explain the impressive capabilities behind large language models. In this work, we study the dynamics of training a shallow transformer on a task of recognizing co-occurrence of two designated words. In the literature of studying training dynamics of transformers, several simplifications are commonly adopted such as weight reparameterization, attention linearization, special initialization, and lazy regime. In contrast, we analyze the gradient flow dynamics of simultaneously training three attention matrices and a linear MLP layer from random initialization, and provide a framework of analyzing such dynamics via a coupled dynamical system. We establish near minimum loss and characterize the attention model after training. We discover that gradient flow serves as an inherent mechanism that naturally divide the training process into two phases. In Phase 1, the linear MLP quickly aligns with the two target signals for correct classification, whereas the softmax attention remains almost unchanged. In Phase 2, the attention matrices and the MLP evolve jointly to enlarge the classification margin and reduce the loss to a near minimum value. Technically, we prove a novel property of the gradient flow, termed \\textit{automatic balancing of gradients}, which enables the loss values of different samples to decrease almost at the same rate and further facilitates the proof of near minimum training loss. We also conduct experiments to verify our theoretical results.",
            "corpus_id": 273346324,
            "sentences": [
                {
                    "corpus_id": "273346324",
                    "title": "Training Dynamics of Transformers to Recognize Word Co-occurrence via Gradient Flow Analysis",
                    "text": "Understanding the training dynamics of transformers is important to explain the impressive capabilities behind large language models. In this work, we study the dynamics of training a shallow transformer on a task of recognizing co-occurrence of two designated words. In the literature of studying training dynamics of transformers, several simplifications are commonly adopted such as weight reparameterization, attention linearization, special initialization, and lazy regime. In contrast, we analyze the gradient flow dynamics of simultaneously training three attention matrices and a linear MLP layer from random initialization, and provide a framework of analyzing such dynamics via a coupled dynamical system. We establish near minimum loss and characterize the attention model after training. We discover that gradient flow serves as an inherent mechanism that naturally divide the training process into two phases. In Phase 1, the linear MLP quickly aligns with the two target signals for correct classification, whereas the softmax attention remains almost unchanged. In Phase 2, the attention matrices and the MLP evolve jointly to enlarge the classification margin and reduce the loss to a near minimum value. Technically, we prove a novel property of the gradient flow, termed \\textit{automatic balancing of gradients}, which enables the loss values of different samples to decrease almost at the same rate and further facilitates the proof of near minimum training loss. We also conduct experiments to verify our theoretical results.",
                    "score": 0.4277459607418496,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99658203125
                }
            ],
            "relevance_judgement": 0.99658203125,
            "relevance_judgment_input_expanded": "# Title: Training Dynamics of Transformers to Recognize Word Co-occurrence via Gradient Flow Analysis\n# Venue: Neural Information Processing Systems\n# Authors: Hongru Yang, B. Kailkhura, Zhangyang Wang, Yingbin Liang\n## Abstract\nUnderstanding the training dynamics of transformers is important to explain the impressive capabilities behind large language models. In this work, we study the dynamics of training a shallow transformer on a task of recognizing co-occurrence of two designated words. In the literature of studying training dynamics of transformers, several simplifications are commonly adopted such as weight reparameterization, attention linearization, special initialization, and lazy regime. In contrast, we analyze the gradient flow dynamics of simultaneously training three attention matrices and a linear MLP layer from random initialization, and provide a framework of analyzing such dynamics via a coupled dynamical system. We establish near minimum loss and characterize the attention model after training. We discover that gradient flow serves as an inherent mechanism that naturally divide the training process into two phases. In Phase 1, the linear MLP quickly aligns with the two target signals for correct classification, whereas the softmax attention remains almost unchanged. In Phase 2, the attention matrices and the MLP evolve jointly to enlarge the classification margin and reduce the loss to a near minimum value. Technically, we prove a novel property of the gradient flow, termed \\textit{automatic balancing of gradients}, which enables the loss values of different samples to decrease almost at the same rate and further facilitates the proof of near minimum training loss. We also conduct experiments to verify our theoretical results.\n",
            "reference_string": "[273346324 | Yang et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Hop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 20,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2024.findings-emnlp.954?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2024.findings-emnlp.954, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237161748",
                    "name": "Shreyas Vathul Subramanian"
                },
                {
                    "authorId": "7194698",
                    "name": "Vignesh Ganapathiraman"
                },
                {
                    "authorId": "2258574389",
                    "name": "Corey Barrett"
                }
            ],
            "abstract": "Various types of learning rate (LR) schedulers are being used for training or fine tuning of Large Language Models today. In practice, several mid-flight changes are required in the LR schedule either manually, or with careful choices around warmup steps, peak LR, type of decay and restarts. To study this further, we consider the effect of switching the learning rate at a predetermined time during training, which we refer to as \u201cSkipLR\u201d. We model SGD as a stochastic gradient flow and show that when starting from the same initial parameters, switching the learning rate causes the loss curves to contract towards each other. We demonstrate this theoretically for some simple cases, and empirically on large language models. Our analysis provides insight into how learning rate schedules affect the training dynamics, and could inform the design of new schedules to accelerate convergence.",
            "corpus_id": 274060474,
            "sentences": [],
            "relevance_judgement": 0.99658203125,
            "relevance_judgment_input_expanded": "# Title: Hop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Shreyas Vathul Subramanian, Vignesh Ganapathiraman, Corey Barrett\n## Abstract\nVarious types of learning rate (LR) schedulers are being used for training or fine tuning of Large Language Models today. In practice, several mid-flight changes are required in the LR schedule either manually, or with careful choices around warmup steps, peak LR, type of decay and restarts. To study this further, we consider the effect of switching the learning rate at a predetermined time during training, which we refer to as \u201cSkipLR\u201d. We model SGD as a stochastic gradient flow and show that when starting from the same initial parameters, switching the learning rate causes the loss curves to contract towards each other. We demonstrate this theoretically for some simple cases, and empirically on large language models. Our analysis provides insight into how learning rate schedules affect the training dynamics, and could inform the design of new schedules to accelerate convergence.\n",
            "reference_string": "[274060474 | Subramanian et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models",
            "venue": "Workshop on Representation Learning for NLP",
            "year": 2024,
            "reference_count": 105,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.06663, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2087314342",
                    "name": "Kaiser Sun"
                },
                {
                    "authorId": "1478928280",
                    "name": "Mark Dredze"
                }
            ],
            "abstract": "The development of large language models leads to the formation of a pre-train-then-align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream tasks. In this work, we investigate the relationship between pre-training and fine-tuning by fine-tuning multiple intermediate pre-trained model checkpoints. Our results on 18 datasets suggest that i) continual pre-training improves the model in a latent way that unveils after fine-tuning; ii) with extra fine-tuning, the datasets that the model does not demonstrate capability gain much more than those that the model performs well during the pre-training stage; iii) although model benefits significantly through supervised fine-tuning, it may forget previously known domain knowledge and the tasks that are not seen during fine-tuning; iv) the model resembles high sensitivity to evaluation prompts after supervised fine-tuning, but this sensitivity can be alleviated by more pre-training.",
            "corpus_id": 271860164,
            "sentences": [
                {
                    "corpus_id": "271860164",
                    "title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models",
                    "text": "Recent studies identify phase transition of model training (Olsson et al., 2022;Wei et al., 2022), where new capabilities or behaviors suddenly emerge when certain thresholds of model complexity are reached. The aspects of complexity often include model size, amount of training by FLOPs or tokens, and model architecture. Several prior works studied the training dynamics of language models by analyzing the internals of train-fromscratch models (Tirumala et al., 2022;Chen et al., 2023;Tian et al., 2023;Chen et al., 2024;Chang et al., 2024). The results of these works suggest that the behaviors that are often overlooked after training could be valuable signals for model analysis. In addition to train-from-scratch models, Ren and Sutherland (2024) studied the fine-tuning dynamics of language models. This work focuses on the effect of pre-training dynamics on downstream fine-tuning results by fine-tuning intermediate pretraining checkpoints on various tasks. Due to the scarcity of publically accessible intermediate pretraining checkpoints, the effect of fine-tuning at different pre-training stages is largely unexplored. Concurrent work (Snell et al., 2024) also fine-tunes intermediate pre-training checkpoints and finds that supervised fine-tuning results can be used as a signal to predict when emergence occurs, while our findings point out a dichotomy of model behavior on different datasets, with the potential for data-efficient and budget-friendly training by understanding the stages of model training.",
                    "score": 0.605957493975886,
                    "section_title": "Related Work",
                    "char_start_offset": 25228,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 207
                        },
                        {
                            "start": 208,
                            "end": 322
                        },
                        {
                            "start": 323,
                            "end": 544
                        },
                        {
                            "start": 545,
                            "end": 685
                        },
                        {
                            "start": 686,
                            "end": 806
                        },
                        {
                            "start": 807,
                            "end": 967
                        },
                        {
                            "start": 968,
                            "end": 1132
                        },
                        {
                            "start": 1133,
                            "end": 1523
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 447,
                            "end": 470,
                            "matchedPaperCorpusId": "248986465"
                        },
                        {
                            "start": 488,
                            "end": 506,
                            "matchedPaperCorpusId": "258947127"
                        },
                        {
                            "start": 524,
                            "end": 543,
                            "matchedPaperCorpusId": "261277016"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99609375
                },
                {
                    "corpus_id": "271860164",
                    "title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models",
                    "text": "We explore the relationship between fine-tuning and pre-training LLMs through fine-tuning multiple pre-training checkpoints of large language models. Our results on 18 datasets and two models provide insights into LLM training. We identify the aspects that LLM learns and forgets during supervised fine-tuning; By analyzing pre-training history, we find that pre-training improves the model in a latent way that is only observable after finetuning. The model may excel at some tasks without fine-tuning. However, the model can rapidly learn datasets that it does not demonstrate capabilities during pre-training with a small amount of supervision. Overall, our study highlights the value of analyzing language model training dynamics. We encourage model developers to release pre-training checkpoints to facilitate research on LLM training.",
                    "score": 0.44740278391535426,
                    "section_title": "Conclusion",
                    "char_start_offset": 26766,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 149
                        },
                        {
                            "start": 150,
                            "end": 227
                        },
                        {
                            "start": 228,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 503
                        },
                        {
                            "start": 504,
                            "end": 647
                        },
                        {
                            "start": 648,
                            "end": 734
                        },
                        {
                            "start": 735,
                            "end": 840
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99072265625
                }
            ],
            "relevance_judgement": 0.99609375,
            "relevance_judgment_input_expanded": "# Title: Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models\n# Venue: Workshop on Representation Learning for NLP\n# Authors: Kaiser Sun, Mark Dredze\n## Abstract\nThe development of large language models leads to the formation of a pre-train-then-align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream tasks. In this work, we investigate the relationship between pre-training and fine-tuning by fine-tuning multiple intermediate pre-trained model checkpoints. Our results on 18 datasets suggest that i) continual pre-training improves the model in a latent way that unveils after fine-tuning; ii) with extra fine-tuning, the datasets that the model does not demonstrate capability gain much more than those that the model performs well during the pre-training stage; iii) although model benefits significantly through supervised fine-tuning, it may forget previously known domain knowledge and the tasks that are not seen during fine-tuning; iv) the model resembles high sensitivity to evaluation prompts after supervised fine-tuning, but this sensitivity can be alleviated by more pre-training.\n## Related Work\nRecent studies identify phase transition of model training (Olsson et al., 2022;Wei et al., 2022), where new capabilities or behaviors suddenly emerge when certain thresholds of model complexity are reached. The aspects of complexity often include model size, amount of training by FLOPs or tokens, and model architecture. Several prior works studied the training dynamics of language models by analyzing the internals of train-fromscratch models (Tirumala et al., 2022;Chen et al., 2023;Tian et al., 2023;Chen et al., 2024;Chang et al., 2024). The results of these works suggest that the behaviors that are often overlooked after training could be valuable signals for model analysis. In addition to train-from-scratch models, Ren and Sutherland (2024) studied the fine-tuning dynamics of language models. This work focuses on the effect of pre-training dynamics on downstream fine-tuning results by fine-tuning intermediate pretraining checkpoints on various tasks. Due to the scarcity of publically accessible intermediate pretraining checkpoints, the effect of fine-tuning at different pre-training stages is largely unexplored. Concurrent work (Snell et al., 2024) also fine-tunes intermediate pre-training checkpoints and finds that supervised fine-tuning results can be used as a signal to predict when emergence occurs, while our findings point out a dichotomy of model behavior on different datasets, with the potential for data-efficient and budget-friendly training by understanding the stages of model training.\n\n## Conclusion\nWe explore the relationship between fine-tuning and pre-training LLMs through fine-tuning multiple pre-training checkpoints of large language models. Our results on 18 datasets and two models provide insights into LLM training. We identify the aspects that LLM learns and forgets during supervised fine-tuning; By analyzing pre-training history, we find that pre-training improves the model in a latent way that is only observable after finetuning. The model may excel at some tasks without fine-tuning. However, the model can rapidly learn datasets that it does not demonstrate capabilities during pre-training with a small amount of supervision. Overall, our study highlights the value of analyzing language model training dynamics. We encourage model developers to release pre-training checkpoints to facilitate research on LLM training.",
            "reference_string": "[271860164 | Sun et al. | 2024 | Citations: 2]"
        },
        {
            "title": "PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs",
            "venue": "International Conference on Learning Representations",
            "year": 2025,
            "reference_count": 74,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.09543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1986356851",
                    "name": "Oskar van der Wal"
                },
                {
                    "authorId": "2325954375",
                    "name": "Pietro Lesci"
                },
                {
                    "authorId": "1416353805",
                    "name": "Max M\u00fcller-Eberstein"
                },
                {
                    "authorId": "2308101135",
                    "name": "Naomi Saphra"
                },
                {
                    "authorId": "2184031883",
                    "name": "Hailey Schoelkopf"
                },
                {
                    "authorId": "2254288138",
                    "name": "Willem Zuidema"
                },
                {
                    "authorId": "2273535086",
                    "name": "Stella Biderman"
                }
            ],
            "abstract": "The stability of language model pre-training and its effects on downstream performance are still understudied. Prior work shows that the training process can yield significantly different results in response to slight variations in initial conditions, e.g., the random seed. Crucially, the research community still lacks sufficient resources and tools to systematically investigate pre-training stability, particularly for decoder-only language models. We introduce the PolyPythias, a set of 45 new training runs for the Pythia model suite: 9 new seeds across 5 model sizes, from 14M to 410M parameters, resulting in about 7k new checkpoints that we release. Using these new 45 training runs, in addition to the 5 already available, we study the effects of different initial conditions determined by the seed -- i.e., parameters' initialisation and data order -- on (i) downstream performance, (ii) learned linguistic representations, and (iii) emergence of training phases. In addition to common scaling behaviours, our analyses generally reveal highly consistent training dynamics across both model sizes and initial conditions. Further, the new seeds for each model allow us to identify outlier training runs and delineate their characteristics. Our findings show the potential of using these methods to predict training stability.",
            "corpus_id": 276937763,
            "sentences": [
                {
                    "corpus_id": "276937763",
                    "title": "PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs",
                    "text": "Our experiments on downstream performance ( \u00a73), intermediate representations ( \u00a74), and model parameters ( \u00a75) allow us to examine the stability of training and find outlier runs using different methods across the model training pipeline. In this section, we analyse commonalities across the resulting metrics to identify broader characteristics of LM pre-training dynamics. \n\nLanguage modelling is largely stable. Generally, we observe LM pre-training dynamics to follow consistent trajectories. Across seeds and model sizes, downstream performance and representational efficiency consistently increase during pre-training, and training maps are linear (except for the outlier seeds). Furthermore, model scaling laws seem to hold across seeds, not only for downstream performance but also for information content and representational efficiency of model representations. \n\nSimilarly, both at the performance and representational level, we observe the effect of \"saturation\" (Michaelov and Bergen, 2023;Godey et al., 2024) in smaller models. \n\nLinguistic information is encoded in the initial learning phase (10 3 -10 4 steps). Across all experiments, metrics start moving away from the initial random baseline around step 10 3 (2B tokens circa) and reach their convergence level around step 10 4 (20B tokens circa). In this initial phase, representational shift peaks and linguistic information begins to be encoded into the models' latent representations. Through the lens of multiple metrics, we can analyse model behaviour in this phase in detail. Specifically, while the amount and the efficiency with which linguistic information is encoded in model representations have already increased substantially at step 10 3 , the model does not yet generate coherent outputs, as indicated by low performance on linguistic acceptability benchmarks like BLiMP (Gender Agreement). Simultaneously, self-consistency is low while inter-seed agreement is high, which we hypothesise to be an artefact of all models initially choosing an incorrect baseline answer. In terms of training maps, this phase corresponds to the 0 \u2192 1 transition, which occurs consistently in this initial training phase for all model sizes (except for the 410M outliers). \n\nMost improvements happen in the \"critical\" learning phase (10 4 -10 5 steps). In the range of 10 3 to 10 4 steps, most learning occurs, as measured by all of our metrics.",
                    "score": 0.48574325876560387,
                    "section_title": "DISCUSSION",
                    "char_start_offset": 28106,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 375
                        },
                        {
                            "start": 378,
                            "end": 415
                        },
                        {
                            "start": 416,
                            "end": 497
                        },
                        {
                            "start": 498,
                            "end": 686
                        },
                        {
                            "start": 687,
                            "end": 872
                        },
                        {
                            "start": 875,
                            "end": 1042
                        },
                        {
                            "start": 1045,
                            "end": 1128
                        },
                        {
                            "start": 1129,
                            "end": 1317
                        },
                        {
                            "start": 1318,
                            "end": 1458
                        },
                        {
                            "start": 1459,
                            "end": 1552
                        },
                        {
                            "start": 1553,
                            "end": 1876
                        },
                        {
                            "start": 1877,
                            "end": 2054
                        },
                        {
                            "start": 2055,
                            "end": 2238
                        },
                        {
                            "start": 2241,
                            "end": 2318
                        },
                        {
                            "start": 2319,
                            "end": 2411
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 976,
                            "end": 1004,
                            "matchedPaperCorpusId": "258865474"
                        },
                        {
                            "start": 1004,
                            "end": 1023,
                            "matchedPaperCorpusId": "269042847"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99609375
                },
                {
                    "corpus_id": "276937763",
                    "title": "PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs",
                    "text": "In this work, we define stability as the change in a metric of interest (e.g., validation loss) caused by changes in randomness factors and quantify it using the standard deviation of that metric (see Du and Nguyen, 2023 for other approaches to quantify stability). To provide a basis for analysing the stability of LMs to randomness factors (e.g., their training dynamics or final performance) without incurring the costs to train contemporary LMs, we introduce the PolyPythias: an extension of the Pythia model suite (Biderman et al., 2023b) trained on the Pile dataset (Gao et al., 2021), for which we release 9 new training runs for 5 model sizes, from 14M up to 410M parameters. These new 45 training runs-in addition to the 5 already available in the suite-cover approximately 7k checkpoints across pre-training, and enable us to analyse training stability of large-scale transformer-based LM with respect to model size, parameter initialisation, and data order as quantified by metrics along the entire model training pipeline: downstream performance and consistency of predictions ( \u00a73), shifts in linguistic representations ( \u00a74), and dynamics of the model parameters and training phases ( \u00a75). \n\nBy studying the PolyPythias, we find that: (i) language modelling is largely stable and follows predictable scaling laws with respect to downstream performance; (ii) across training, we identify consistent learning phases: an initial learning phase between steps 103 -10 4 and a critical learning phase between steps 10 4 -10 5 ; (iii) using training maps constructed from statistics of the model parameters, we identify the characteristics of stable training runs and the early signals of instability. \n\nIn the following sections, we describe the PolyPythias release ( \u00a72) and how we use the multiple training runs per model size to study the stability of models across various stages of the model training pipeline ( \u00a73- \u00a75). We conclude by combining the insights from the individual analyses ( \u00a76).",
                    "score": 0.39130519242555994,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 1590,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 265
                        },
                        {
                            "start": 266,
                            "end": 683
                        },
                        {
                            "start": 684,
                            "end": 1203
                        },
                        {
                            "start": 1206,
                            "end": 1708
                        },
                        {
                            "start": 1711,
                            "end": 1933
                        },
                        {
                            "start": 1934,
                            "end": 2007
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 201,
                            "end": 220,
                            "matchedPaperCorpusId": "256868814"
                        },
                        {
                            "start": 519,
                            "end": 543,
                            "matchedPaperCorpusId": "257921893"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9931640625
                }
            ],
            "relevance_judgement": 0.99609375,
            "relevance_judgment_input_expanded": "# Title: PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs\n# Venue: International Conference on Learning Representations\n# Authors: Oskar van der Wal, Pietro Lesci, Max M\u00fcller-Eberstein, Naomi Saphra, Hailey Schoelkopf, Willem Zuidema, Stella Biderman\n## Abstract\nThe stability of language model pre-training and its effects on downstream performance are still understudied. Prior work shows that the training process can yield significantly different results in response to slight variations in initial conditions, e.g., the random seed. Crucially, the research community still lacks sufficient resources and tools to systematically investigate pre-training stability, particularly for decoder-only language models. We introduce the PolyPythias, a set of 45 new training runs for the Pythia model suite: 9 new seeds across 5 model sizes, from 14M to 410M parameters, resulting in about 7k new checkpoints that we release. Using these new 45 training runs, in addition to the 5 already available, we study the effects of different initial conditions determined by the seed -- i.e., parameters' initialisation and data order -- on (i) downstream performance, (ii) learned linguistic representations, and (iii) emergence of training phases. In addition to common scaling behaviours, our analyses generally reveal highly consistent training dynamics across both model sizes and initial conditions. Further, the new seeds for each model allow us to identify outlier training runs and delineate their characteristics. Our findings show the potential of using these methods to predict training stability.\n## INTRODUCTION\nIn this work, we define stability as the change in a metric of interest (e.g., validation loss) caused by changes in randomness factors and quantify it using the standard deviation of that metric (see Du and Nguyen, 2023 for other approaches to quantify stability). To provide a basis for analysing the stability of LMs to randomness factors (e.g., their training dynamics or final performance) without incurring the costs to train contemporary LMs, we introduce the PolyPythias: an extension of the Pythia model suite (Biderman et al., 2023b) trained on the Pile dataset (Gao et al., 2021), for which we release 9 new training runs for 5 model sizes, from 14M up to 410M parameters. These new 45 training runs-in addition to the 5 already available in the suite-cover approximately 7k checkpoints across pre-training, and enable us to analyse training stability of large-scale transformer-based LM with respect to model size, parameter initialisation, and data order as quantified by metrics along the entire model training pipeline: downstream performance and consistency of predictions ( \u00a73), shifts in linguistic representations ( \u00a74), and dynamics of the model parameters and training phases ( \u00a75). \n\nBy studying the PolyPythias, we find that: (i) language modelling is largely stable and follows predictable scaling laws with respect to downstream performance; (ii) across training, we identify consistent learning phases: an initial learning phase between steps 103 -10 4 and a critical learning phase between steps 10 4 -10 5 ; (iii) using training maps constructed from statistics of the model parameters, we identify the characteristics of stable training runs and the early signals of instability. \n\nIn the following sections, we describe the PolyPythias release ( \u00a72) and how we use the multiple training runs per model size to study the stability of models across various stages of the model training pipeline ( \u00a73- \u00a75). We conclude by combining the insights from the individual analyses ( \u00a76).\n\n## DISCUSSION\nOur experiments on downstream performance ( \u00a73), intermediate representations ( \u00a74), and model parameters ( \u00a75) allow us to examine the stability of training and find outlier runs using different methods across the model training pipeline. In this section, we analyse commonalities across the resulting metrics to identify broader characteristics of LM pre-training dynamics. \n\nLanguage modelling is largely stable. Generally, we observe LM pre-training dynamics to follow consistent trajectories. Across seeds and model sizes, downstream performance and representational efficiency consistently increase during pre-training, and training maps are linear (except for the outlier seeds). Furthermore, model scaling laws seem to hold across seeds, not only for downstream performance but also for information content and representational efficiency of model representations. \n\nSimilarly, both at the performance and representational level, we observe the effect of \"saturation\" (Michaelov and Bergen, 2023;Godey et al., 2024) in smaller models. \n\nLinguistic information is encoded in the initial learning phase (10 3 -10 4 steps). Across all experiments, metrics start moving away from the initial random baseline around step 10 3 (2B tokens circa) and reach their convergence level around step 10 4 (20B tokens circa). In this initial phase, representational shift peaks and linguistic information begins to be encoded into the models' latent representations. Through the lens of multiple metrics, we can analyse model behaviour in this phase in detail. Specifically, while the amount and the efficiency with which linguistic information is encoded in model representations have already increased substantially at step 10 3 , the model does not yet generate coherent outputs, as indicated by low performance on linguistic acceptability benchmarks like BLiMP (Gender Agreement). Simultaneously, self-consistency is low while inter-seed agreement is high, which we hypothesise to be an artefact of all models initially choosing an incorrect baseline answer. In terms of training maps, this phase corresponds to the 0 \u2192 1 transition, which occurs consistently in this initial training phase for all model sizes (except for the 410M outliers). \n\nMost improvements happen in the \"critical\" learning phase (10 4 -10 5 steps). In the range of 10 3 to 10 4 steps, most learning occurs, as measured by all of our metrics.",
            "reference_string": "[276937763 | Wal et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Critical Data Size of Language Models from a Grokking Perspective",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 28,
            "citation_count": 18,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.10463, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2145238612",
                    "name": "Xuekai Zhu"
                },
                {
                    "authorId": "2280103402",
                    "name": "Yao Fu"
                },
                {
                    "authorId": "2218723159",
                    "name": "Bowen Zhou"
                },
                {
                    "authorId": "2280367391",
                    "name": "Zhouhan Lin"
                }
            ],
            "abstract": "We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data. Our results deepen the understanding of language model training, offering a novel perspective on the role of data in the learning mechanism of language models.",
            "corpus_id": 267061159,
            "sentences": [
                {
                    "corpus_id": "267061159",
                    "title": "Critical Data Size of Language Models from a Grokking Perspective",
                    "text": "We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data. Our results deepen the understanding of language model training, offering a novel perspective on the role of data in the learning mechanism of language models.",
                    "score": 0.39680604860107155,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99560546875
                }
            ],
            "relevance_judgement": 0.99560546875,
            "relevance_judgment_input_expanded": "# Title: Critical Data Size of Language Models from a Grokking Perspective\n# Venue: arXiv.org\n# Authors: Xuekai Zhu, Yao Fu, Bowen Zhou, Zhouhan Lin\n## Abstract\nWe explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data. Our results deepen the understanding of language model training, offering a novel perspective on the role of data in the learning mechanism of language models.\n",
            "reference_string": "[267061159 | Zhu et al. | 2024 | Citations: 18]"
        },
        {
            "title": "Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 62,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.16747, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303396796",
                    "name": "Akiyoshi Tomihari"
                },
                {
                    "authorId": "2303397982",
                    "name": "Issei Sato"
                }
            ],
            "abstract": "The two-stage fine-tuning (FT) method, linear probing (LP) then fine-tuning (LP-FT), outperforms linear probing and FT alone. This holds true for both in-distribution (ID) and out-of-distribution (OOD) data. One key reason for its success is the preservation of pre-trained features, achieved by obtaining a near-optimal linear head during LP. However, despite the widespread use of large language models, there has been limited exploration of more complex architectures such as Transformers. In this paper, we analyze the training dynamics of LP-FT for classification tasks on the basis of the neural tangent kernel (NTK) theory. Our analysis decomposes the NTK matrix into two components. This decomposition highlights the importance of the linear head norm alongside the prediction accuracy at the start of the FT stage. We also observe a significant increase in the linear head norm during LP, which stems from training with the cross-entropy (CE) loss. This increase in the linear head norm effectively reduces changes in learned features. Furthermore, we find that this increased norm can adversely affect model calibration, which can be corrected using temperature scaling. Additionally, we extend our analysis with the NTK to the low-rank adaptation (LoRA) method and validate its effectiveness. Our experiments using a Transformer-based model on multiple natural language processing datasets confirm our theoretical analysis. Our study demonstrates the effectiveness of LP-FT for fine-tuning language models. Code is available at https://github.com/tom4649/lp-ft_ntk.",
            "corpus_id": 270062468,
            "sentences": [
                {
                    "corpus_id": "270062468",
                    "title": "Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective",
                    "text": "In this paper, we explored the LP-FT training dynamics in complex classification models using the NTK to analyze feature changes. Our analysis identified classifier weight norms at the start of the FT stage as a key factor influencing FT dynamics. These norms balance the NTK matrix components and help reduce feature changes. Our findings support the existing feature distortion theory from an NTK perspective and emphasize the role of classifier weight norms alongside prediction accuracy. We also found that increases in classifier weight norms, characteristic of training with CE loss, may negatively impact model calibration, and this can be mitigated by temperature scaling. Additionally, the approximation effectiveness of LoRA is theoretically validated in terms of the similarity of the NTK matrix components. Empirical experiments with Transformer-based language models supported our theoretical insights, validating our understanding of the NTK, feature changes, and the benefits of temperature scaling. Overall, our study substantiates the efficacy of LP-FT as a robust method for adapting pre-trained complex models while preserving their well-trained features. \n\nLimitations The main limitation of our study is that it is based on the NTK regime, which might not fully capture the training dynamics. Additionally, we consider just one epoch of gradient descent in FT, which may not effectively represent the overall training. In our experiments, we specifically focused on validating the effectiveness of LP-FT on language models. Therefore, areas other than natural language processing are outside the scope of our experiments. \n\n[57] Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou, Xingyi Cheng, Heng Ji, Zhiyuan Liu, and Maosong Sun. Revisiting out-of-distribution robustness in nlp: Benchmark, analysis, and llms evaluations. arXiv preprint arXiv:2306.04618, 2023. \n\n[58] Yuchen Zeng and Kangwook Lee. The expressive power of low-rank adaptation. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=likXVjmh3E. \n\n[59] Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik.",
                    "score": 0.3924607378153187,
                    "section_title": "Conclusion",
                    "char_start_offset": 26226,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 129
                        },
                        {
                            "start": 130,
                            "end": 247
                        },
                        {
                            "start": 248,
                            "end": 326
                        },
                        {
                            "start": 327,
                            "end": 491
                        },
                        {
                            "start": 492,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 818
                        },
                        {
                            "start": 819,
                            "end": 1014
                        },
                        {
                            "start": 1015,
                            "end": 1174
                        },
                        {
                            "start": 1177,
                            "end": 1313
                        },
                        {
                            "start": 1314,
                            "end": 1439
                        },
                        {
                            "start": 1440,
                            "end": 1544
                        },
                        {
                            "start": 1545,
                            "end": 1642
                        },
                        {
                            "start": 1645,
                            "end": 1767
                        },
                        {
                            "start": 1768,
                            "end": 1860
                        },
                        {
                            "start": 1861,
                            "end": 1893
                        },
                        {
                            "start": 1894,
                            "end": 1899
                        },
                        {
                            "start": 1902,
                            "end": 1936
                        },
                        {
                            "start": 1937,
                            "end": 1981
                        },
                        {
                            "start": 1982,
                            "end": 2056
                        },
                        {
                            "start": 2057,
                            "end": 2084
                        },
                        {
                            "start": 2085,
                            "end": 2105
                        },
                        {
                            "start": 2108,
                            "end": 2193
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99560546875
                }
            ],
            "relevance_judgement": 0.99560546875,
            "relevance_judgment_input_expanded": "# Title: Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective\n# Venue: Neural Information Processing Systems\n# Authors: Akiyoshi Tomihari, Issei Sato\n## Abstract\nThe two-stage fine-tuning (FT) method, linear probing (LP) then fine-tuning (LP-FT), outperforms linear probing and FT alone. This holds true for both in-distribution (ID) and out-of-distribution (OOD) data. One key reason for its success is the preservation of pre-trained features, achieved by obtaining a near-optimal linear head during LP. However, despite the widespread use of large language models, there has been limited exploration of more complex architectures such as Transformers. In this paper, we analyze the training dynamics of LP-FT for classification tasks on the basis of the neural tangent kernel (NTK) theory. Our analysis decomposes the NTK matrix into two components. This decomposition highlights the importance of the linear head norm alongside the prediction accuracy at the start of the FT stage. We also observe a significant increase in the linear head norm during LP, which stems from training with the cross-entropy (CE) loss. This increase in the linear head norm effectively reduces changes in learned features. Furthermore, we find that this increased norm can adversely affect model calibration, which can be corrected using temperature scaling. Additionally, we extend our analysis with the NTK to the low-rank adaptation (LoRA) method and validate its effectiveness. Our experiments using a Transformer-based model on multiple natural language processing datasets confirm our theoretical analysis. Our study demonstrates the effectiveness of LP-FT for fine-tuning language models. Code is available at https://github.com/tom4649/lp-ft_ntk.\n## Conclusion\nIn this paper, we explored the LP-FT training dynamics in complex classification models using the NTK to analyze feature changes. Our analysis identified classifier weight norms at the start of the FT stage as a key factor influencing FT dynamics. These norms balance the NTK matrix components and help reduce feature changes. Our findings support the existing feature distortion theory from an NTK perspective and emphasize the role of classifier weight norms alongside prediction accuracy. We also found that increases in classifier weight norms, characteristic of training with CE loss, may negatively impact model calibration, and this can be mitigated by temperature scaling. Additionally, the approximation effectiveness of LoRA is theoretically validated in terms of the similarity of the NTK matrix components. Empirical experiments with Transformer-based language models supported our theoretical insights, validating our understanding of the NTK, feature changes, and the benefits of temperature scaling. Overall, our study substantiates the efficacy of LP-FT as a robust method for adapting pre-trained complex models while preserving their well-trained features. \n\nLimitations The main limitation of our study is that it is based on the NTK regime, which might not fully capture the training dynamics. Additionally, we consider just one epoch of gradient descent in FT, which may not effectively represent the overall training. In our experiments, we specifically focused on validating the effectiveness of LP-FT on language models. Therefore, areas other than natural language processing are outside the scope of our experiments. \n\n[57] Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou, Xingyi Cheng, Heng Ji, Zhiyuan Liu, and Maosong Sun. Revisiting out-of-distribution robustness in nlp: Benchmark, analysis, and llms evaluations. arXiv preprint arXiv:2306.04618, 2023. \n\n[58] Yuchen Zeng and Kangwook Lee. The expressive power of low-rank adaptation. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=likXVjmh3E. \n\n[59] Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik.",
            "reference_string": "[270062468 | Tomihari et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Learning Dynamics in Continual Pre-Training for Large Language Models",
            "venue": "",
            "year": 2025,
            "reference_count": 43,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.07796, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268303033",
                    "name": "Xingjin Wang"
                },
                {
                    "authorId": "2316486693",
                    "name": "Howe Tissue"
                },
                {
                    "authorId": "2316501479",
                    "name": "Lu Wang"
                },
                {
                    "authorId": "2107923812",
                    "name": "Linjing Li"
                },
                {
                    "authorId": "2263800857",
                    "name": "D. Zeng"
                }
            ],
            "abstract": "Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.",
            "corpus_id": 278534923,
            "sentences": [
                {
                    "corpus_id": "278534923",
                    "title": "Learning Dynamics in Continual Pre-Training for Large Language Models",
                    "text": "In conclusion, we explore the learning dynamics in continual pre-training of large language models. We focus on the evolution of performance across general and downstream domains, with domain performance assessed via validation loss. By observations and analyses, we propose a CPT scaling law that integrates distribution shift and learning rate annealing to predict the validation loss at any intermediate training step under common learning rate schedules. Our scaling law provides a comprehensive understanding of key CPT factors and helps optimize the hyper-parameters in CPT for different training goals. Further experiments demon-strate that the law can be also extended to more complicated scenarios such as out-of-domain datasets and models with unknown information. We believe that our CPT scaling law is promising to reshape the understanding of researchers for LLM continual pre-training and scaling laws.",
                    "score": 0.385754733396352,
                    "section_title": "Conclusion",
                    "char_start_offset": 25347,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 99
                        },
                        {
                            "start": 100,
                            "end": 233
                        },
                        {
                            "start": 234,
                            "end": 458
                        },
                        {
                            "start": 459,
                            "end": 609
                        },
                        {
                            "start": 610,
                            "end": 774
                        },
                        {
                            "start": 775,
                            "end": 916
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99560546875
                },
                {
                    "corpus_id": "278534923",
                    "title": "Learning Dynamics in Continual Pre-Training for Large Language Models",
                    "text": "In recent years, large language models (LLMs) exhibit versatile abilities and have garnered significant academic and industrial attention (Dubey et al., 2024;OpenAI, 2023). the LLMs' abilities in specific downstream domains (e.g. coding, finance, math, etc.) while mitigating the substantial costs associated with re-training (Chen et al., 2023a;C \u00b8agatay Y\u0131ld\u0131z et al., 2024;Ibrahim et al., 2024). \n\nCPT primarily focuses on the evolution of performance across general and downstream domains. It is widely observed and believed that improvements on downstream datasets may come at the expense of performance on general domain tasks, or the well-known catastrophic forgetting (French, 1999;Gupta et al., 2023). \n\nRecently, some scaling laws for transfer are proposed in continual pre-training scenarios. For example, Hernandez et al. (2021b) and Barnett (2024) discover a law to describe how effective data transferred scales with fine-tuning dataset size and model size. Que et al. (2024) and Gu et al. (2024) propose a law to find the optimal replay ratio to balance general and downstream performances. However, very few studies have attempted to quantitatively describe the learning dynamics, particularly many performance variations on general and downstream domains throughout the CPT process. We have two primary research questions (RQs): (1) Can we find such an accurate law containing as many variables that affect the final CPT performance as possible? (2) Can we trace the performance of LLMs throughout CPT process, rather than only final performance like previous works? Studying the first RQ helps researchers clarify the various factors that affect CPT performance and then optimize hyper-parameters through prediction; Studying the second RQ helps community understand the learning dynamics of LLMs at each step of the CPT process, providing deeper insights and theoretical guidance for subsequent CPT research. \n\nFollowing previous works (Gupta et al., 2023;Ibrahim et al., 2024;Que et al., 2024), we trace performance changes using validation loss of the corresponding domains. We find that the CPT loss curve is a transfer curve and can be described by decoupling the effects of distribution shift and LR annealing.",
                    "score": 0.39949118640737047,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 229
                        },
                        {
                            "start": 230,
                            "end": 398
                        },
                        {
                            "start": 401,
                            "end": 493
                        },
                        {
                            "start": 494,
                            "end": 710
                        },
                        {
                            "start": 713,
                            "end": 803
                        },
                        {
                            "start": 804,
                            "end": 971
                        },
                        {
                            "start": 972,
                            "end": 1105
                        },
                        {
                            "start": 1106,
                            "end": 1299
                        },
                        {
                            "start": 1300,
                            "end": 1462
                        },
                        {
                            "start": 1463,
                            "end": 1583
                        },
                        {
                            "start": 1584,
                            "end": 1927
                        },
                        {
                            "start": 1930,
                            "end": 2095
                        },
                        {
                            "start": 2096,
                            "end": 2234
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 326,
                            "end": 346,
                            "matchedPaperCorpusId": "258833488"
                        },
                        {
                            "start": 376,
                            "end": 397,
                            "matchedPaperCorpusId": "268379604"
                        },
                        {
                            "start": 676,
                            "end": 690,
                            "matchedPaperCorpusId": "2691726"
                        },
                        {
                            "start": 994,
                            "end": 1010,
                            "matchedPaperCorpusId": "271404362"
                        },
                        {
                            "start": 1975,
                            "end": 1996,
                            "matchedPaperCorpusId": "268379604"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.990234375
                },
                {
                    "corpus_id": "278534923",
                    "title": "Learning Dynamics in Continual Pre-Training for Large Language Models",
                    "text": "Continual Pre-Training (CPT) is a effective method to enhance the foundation large language models to specific downstream domains or tasks. Our work provide a scaling law to quantitatively describe the learning dynamics of CPT process, enabling to optimize the training hyper-parameters for balancing the general and downstream performance. While there will be important impacts resulting from the use of CPT in general, here we focus on the impact of using our scaling law to provide explanations for CPT process. There are many benefits to using our method, such as predict the loss curve dynamics and optimize hyper-parameters. This paper presents work whose goal is to advance the field of Large Language Models. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. language modeling performance, 2024. URL https: //arxiv.org/abs/2403.16952. \n\nC \u00b8agatay Y\u0131ld\u0131z, Ravichandran, N. K., Punia, P., Bethge, M., and Ermis, B. Investigating continual pretraining in large language models: Insights and implications, 2024. URL https://arxiv.org/abs/2402.17400.",
                    "score": 0.3845291391330515,
                    "section_title": "Impact Statement",
                    "char_start_offset": 26284,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 139
                        },
                        {
                            "start": 140,
                            "end": 340
                        },
                        {
                            "start": 341,
                            "end": 514
                        },
                        {
                            "start": 515,
                            "end": 630
                        },
                        {
                            "start": 631,
                            "end": 716
                        },
                        {
                            "start": 717,
                            "end": 834
                        },
                        {
                            "start": 835,
                            "end": 871
                        },
                        {
                            "start": 872,
                            "end": 910
                        },
                        {
                            "start": 913,
                            "end": 1083
                        },
                        {
                            "start": 1084,
                            "end": 1121
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.990234375
                }
            ],
            "relevance_judgement": 0.99560546875,
            "relevance_judgment_input_expanded": "# Title: Learning Dynamics in Continual Pre-Training for Large Language Models\n# Venue: \n# Authors: Xingjin Wang, Howe Tissue, Lu Wang, Linjing Li, D. Zeng\n## Abstract\nContinual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.\n## Introduction\nIn recent years, large language models (LLMs) exhibit versatile abilities and have garnered significant academic and industrial attention (Dubey et al., 2024;OpenAI, 2023). the LLMs' abilities in specific downstream domains (e.g. coding, finance, math, etc.) while mitigating the substantial costs associated with re-training (Chen et al., 2023a;C \u00b8agatay Y\u0131ld\u0131z et al., 2024;Ibrahim et al., 2024). \n\nCPT primarily focuses on the evolution of performance across general and downstream domains. It is widely observed and believed that improvements on downstream datasets may come at the expense of performance on general domain tasks, or the well-known catastrophic forgetting (French, 1999;Gupta et al., 2023). \n\nRecently, some scaling laws for transfer are proposed in continual pre-training scenarios. For example, Hernandez et al. (2021b) and Barnett (2024) discover a law to describe how effective data transferred scales with fine-tuning dataset size and model size. Que et al. (2024) and Gu et al. (2024) propose a law to find the optimal replay ratio to balance general and downstream performances. However, very few studies have attempted to quantitatively describe the learning dynamics, particularly many performance variations on general and downstream domains throughout the CPT process. We have two primary research questions (RQs): (1) Can we find such an accurate law containing as many variables that affect the final CPT performance as possible? (2) Can we trace the performance of LLMs throughout CPT process, rather than only final performance like previous works? Studying the first RQ helps researchers clarify the various factors that affect CPT performance and then optimize hyper-parameters through prediction; Studying the second RQ helps community understand the learning dynamics of LLMs at each step of the CPT process, providing deeper insights and theoretical guidance for subsequent CPT research. \n\nFollowing previous works (Gupta et al., 2023;Ibrahim et al., 2024;Que et al., 2024), we trace performance changes using validation loss of the corresponding domains. We find that the CPT loss curve is a transfer curve and can be described by decoupling the effects of distribution shift and LR annealing.\n\n## Conclusion\nIn conclusion, we explore the learning dynamics in continual pre-training of large language models. We focus on the evolution of performance across general and downstream domains, with domain performance assessed via validation loss. By observations and analyses, we propose a CPT scaling law that integrates distribution shift and learning rate annealing to predict the validation loss at any intermediate training step under common learning rate schedules. Our scaling law provides a comprehensive understanding of key CPT factors and helps optimize the hyper-parameters in CPT for different training goals. Further experiments demon-strate that the law can be also extended to more complicated scenarios such as out-of-domain datasets and models with unknown information. We believe that our CPT scaling law is promising to reshape the understanding of researchers for LLM continual pre-training and scaling laws.\n\n## Impact Statement\nContinual Pre-Training (CPT) is a effective method to enhance the foundation large language models to specific downstream domains or tasks. Our work provide a scaling law to quantitatively describe the learning dynamics of CPT process, enabling to optimize the training hyper-parameters for balancing the general and downstream performance. While there will be important impacts resulting from the use of CPT in general, here we focus on the impact of using our scaling law to provide explanations for CPT process. There are many benefits to using our method, such as predict the loss curve dynamics and optimize hyper-parameters. This paper presents work whose goal is to advance the field of Large Language Models. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. language modeling performance, 2024. URL https: //arxiv.org/abs/2403.16952. \n\nC \u00b8agatay Y\u0131ld\u0131z, Ravichandran, N. K., Punia, P., Bethge, M., and Ermis, B. Investigating continual pretraining in large language models: Insights and implications, 2024. URL https://arxiv.org/abs/2402.17400.",
            "reference_string": "[278534923 | Wang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 54,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.08739, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256990455",
                    "name": "Carlo Nicolini"
                },
                {
                    "authorId": "2256994086",
                    "name": "Jacopo Staiano"
                },
                {
                    "authorId": "2291065942",
                    "name": "Bruno Lepri"
                },
                {
                    "authorId": "2291066066",
                    "name": "Raffaele Marino"
                }
            ],
            "abstract": "A substantial gap persists in understanding the reasons behind the exceptional performance of the Transformer architecture in NLP. A particularly unexplored area involves the mechanistic description of how the distribution of parameters evolves over time during training. In this work we suggest that looking at the time evolution of the statistic distribution of model parameters, and specifically at bifurcation effects, can help understanding the model quality, potentially reducing training costs and evaluation efforts and empirically showing the reasons behind the effectiveness of weights sparsification.",
            "corpus_id": 268379408,
            "sentences": [
                {
                    "corpus_id": "268379408",
                    "title": "The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models",
                    "text": "In this study we have analyzed both the temporal and spatial dimensions of training a large language model.As discussed above, our work is the first one dealing with distribution of network weights as a whole, by means of computational methods borrowed from statistical mechanics.More specifically, this work shows that a bifurcation occurs in the dynamics of the weights during the training process.Such transitions are observed across various models of different sizes trained with distinct datasets.We have conducted a thorough and meticulous analysis of this aspect and concluded that this bifurcation marks a transition to a stationary state, indicating that further training is unlikely to significantly alter the weight values.Thus, training can be efficiently terminated upon reaching such a stationary state.Moreover, our study has offered a possible interpretation of the bifurcation phenomenon in terms of model perplexity.\n\nJust as in the early days of thermodynamics, when empirical observations drove technological advancements, we advocate for the development of Large Language Models (LLMs) to be grounded in the observation of their internal dynamics.The identification of stationary states in the weight dynamics exemplifies this philosophy, marking a step toward a more observational and theoretically informed approach to LLM development.\n\nIntriguingly, we finally note how recent works in the physics of complex networks point at diversity of information pathways as the main driver of sparsity in real networks Ghavasieh & De Domenico (2024): in this sense, we hypothesize that the poli-semanticity of natural language may act as the main driving force for network self-sparsification.\n\nThe presented results can have far-reaching implications as they demonstrate that keeping track of the collective behaviour of network weights could be a powerful indicator of training convergence, as opposed to the classical methods based on evaluation metrics which suffer from the confounding effects of nonlinearity, hence giving raise to false claims about \"emergent\" properties.As a future work, we would like to further investigate the training loss dynamics, to check whether the sudden changes in quantitative microscopic parameters align with the development of induction heads in the model, as shown by (Bietti et al., 2023).",
                    "score": 0.44235627110847087,
                    "section_title": "Conclusions",
                    "char_start_offset": 26866,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 107
                        },
                        {
                            "start": 107,
                            "end": 280
                        },
                        {
                            "start": 280,
                            "end": 400
                        },
                        {
                            "start": 400,
                            "end": 502
                        },
                        {
                            "start": 502,
                            "end": 734
                        },
                        {
                            "start": 734,
                            "end": 817
                        },
                        {
                            "start": 817,
                            "end": 934
                        },
                        {
                            "start": 936,
                            "end": 1168
                        },
                        {
                            "start": 1168,
                            "end": 1358
                        },
                        {
                            "start": 1360,
                            "end": 1707
                        },
                        {
                            "start": 1709,
                            "end": 2093
                        },
                        {
                            "start": 2093,
                            "end": 2345
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1533,
                            "end": 1563,
                            "matchedPaperCorpusId": "267188216"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9951171875
                },
                {
                    "corpus_id": "268379408",
                    "title": "The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models",
                    "text": "A substantial gap persists in understanding the reasons behind the exceptional performance of the Transformer architecture in NLP. A particularly unexplored area involves the mechanistic description of how the distribution of parameters evolves over time during training. In this work we suggest that looking at the time evolution of the statistic distribution of model parameters, and specifically at bifurcation effects, can help understanding the model quality, potentially reducing training costs and evaluation efforts and empirically showing the reasons behind the effectiveness of weights sparsification.",
                    "score": 0.3832814172581209,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98291015625
                }
            ],
            "relevance_judgement": 0.9951171875,
            "relevance_judgment_input_expanded": "# Title: The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models\n# Venue: arXiv.org\n# Authors: Carlo Nicolini, Jacopo Staiano, Bruno Lepri, Raffaele Marino\n## Abstract\nA substantial gap persists in understanding the reasons behind the exceptional performance of the Transformer architecture in NLP. A particularly unexplored area involves the mechanistic description of how the distribution of parameters evolves over time during training. In this work we suggest that looking at the time evolution of the statistic distribution of model parameters, and specifically at bifurcation effects, can help understanding the model quality, potentially reducing training costs and evaluation efforts and empirically showing the reasons behind the effectiveness of weights sparsification.\n## Conclusions\nIn this study we have analyzed both the temporal and spatial dimensions of training a large language model.As discussed above, our work is the first one dealing with distribution of network weights as a whole, by means of computational methods borrowed from statistical mechanics.More specifically, this work shows that a bifurcation occurs in the dynamics of the weights during the training process.Such transitions are observed across various models of different sizes trained with distinct datasets.We have conducted a thorough and meticulous analysis of this aspect and concluded that this bifurcation marks a transition to a stationary state, indicating that further training is unlikely to significantly alter the weight values.Thus, training can be efficiently terminated upon reaching such a stationary state.Moreover, our study has offered a possible interpretation of the bifurcation phenomenon in terms of model perplexity.\n\nJust as in the early days of thermodynamics, when empirical observations drove technological advancements, we advocate for the development of Large Language Models (LLMs) to be grounded in the observation of their internal dynamics.The identification of stationary states in the weight dynamics exemplifies this philosophy, marking a step toward a more observational and theoretically informed approach to LLM development.\n\nIntriguingly, we finally note how recent works in the physics of complex networks point at diversity of information pathways as the main driver of sparsity in real networks Ghavasieh & De Domenico (2024): in this sense, we hypothesize that the poli-semanticity of natural language may act as the main driving force for network self-sparsification.\n\nThe presented results can have far-reaching implications as they demonstrate that keeping track of the collective behaviour of network weights could be a powerful indicator of training convergence, as opposed to the classical methods based on evaluation metrics which suffer from the confounding effects of nonlinearity, hence giving raise to false claims about \"emergent\" properties.As a future work, we would like to further investigate the training loss dynamics, to check whether the sudden changes in quantitative microscopic parameters align with the development of induction heads in the model, as shown by (Bietti et al., 2023).",
            "reference_string": "[268379408 | Nicolini et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Training Dynamics of a 1.7B LLaMa Model: A Data-Efficient Approach",
            "venue": "",
            "year": 2024,
            "reference_count": 27,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.13335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2139317485",
                    "name": "Miles Q. Li"
                },
                {
                    "authorId": "2333234336",
                    "name": "Benjamin C. M. Fung"
                },
                {
                    "authorId": "2333311177",
                    "name": "Shih-Chia Huang"
                }
            ],
            "abstract": "Pretraining large language models is a complex endeavor influenced by multiple factors, including model architecture, data quality, training continuity, and hardware constraints. In this paper, we share insights gained from the experience of training DMaS-LLaMa-Lite, a fully open source, 1.7-billion-parameter, LLaMa-based model, on approximately 20 billion tokens of carefully curated data. We chronicle the full training trajectory, documenting how evolving validation loss levels and downstream benchmarks reflect transitions from incoherent text to fluent, contextually grounded output. Beyond pretraining, we extend our analysis to include a post-training phase focused on instruction tuning, where the model was refined to produce more contextually appropriate, user-aligned responses. We highlight practical considerations such as the importance of restoring optimizer states when resuming from checkpoints, and the impact of hardware changes on training stability and throughput. While qualitative evaluation provides an intuitive understanding of model improvements, our analysis extends to various performance benchmarks, demonstrating how high-quality data and thoughtful scaling enable competitive results with significantly fewer training tokens. By detailing these experiences and offering training logs, checkpoints, and sample outputs, we aim to guide future researchers and practitioners in refining their pretraining strategies. The training script is available on Github at https://github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code. The model checkpoints are available on Huggingface at https://huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb.",
            "corpus_id": 274823085,
            "sentences": [
                {
                    "corpus_id": "274823085",
                    "title": "Training Dynamics of a 1.7B LLaMa Model: A Data-Efficient Approach",
                    "text": "Large language models (LLMs) have achieved unprecedented performance across a variety of natural language understanding and generation tasks. These advancements have been driven by innovations in model architectures, pre-training methodologies, and the availability of large-scale, high-quality data (Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell et al., 2020;Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann et al., 2022;Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Roziere, Goyal, Hambro, Azhar et al., 2023a). However, despite significant progress, many aspects of LLM training remain underexplored. Practical challenges-ranging from data curation and training stability to qualitative and quantitative evaluation-play a critical role in determining model performance but are often overlooked in favor of final benchmark results. \n\nIn this paper, we present our experience pre-training and post-training DMaS-LLaMa-Lite, a 1.7-billion-parameter LLaMa-based model, on 20 billion tokens of carefully curated data. Unlike many existing studies that focus solely on the final outputs of large-scale training, we emphasize the training process itself and the insights gleaned from it. Specifically, we document the following key aspects of our work: \n\n1. Training Dynamics: We analyze how validation loss and downstream benchmarks (e.g., Hellaswag, ARC) evolve over 40,000+ training steps and correlate these metrics with improvements in text fluency, coherence, and factual accuracy.",
                    "score": 0.4348085577332137,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 141
                        },
                        {
                            "start": 142,
                            "end": 591
                        },
                        {
                            "start": 592,
                            "end": 681
                        },
                        {
                            "start": 682,
                            "end": 911
                        },
                        {
                            "start": 914,
                            "end": 1093
                        },
                        {
                            "start": 1094,
                            "end": 1261
                        },
                        {
                            "start": 1262,
                            "end": 1326
                        },
                        {
                            "start": 1329,
                            "end": 1561
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 300,
                            "end": 396,
                            "matchedPaperCorpusId": "218971783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9951171875
                },
                {
                    "corpus_id": "274823085",
                    "title": "Training Dynamics of a 1.7B LLaMa Model: A Data-Efficient Approach",
                    "text": "Our findings, along with the provided training logs, checkpoints, and sample outputs, aim to guide future researchers and practitioners in refining both pre-training and post-training strategies. By documenting the full trajectory from pre-training dynamics to post-training refinement, we emphasize a process-focused perspective where transparency and detailed insights foster reproducibility and more effective model development. This approach lays a foundation for future studies to explore additional best practices, methodological refinements, and novel techniques in both pre-training and instruction-tuning phases of large language models.",
                    "score": 0.4781339282833546,
                    "section_title": "Conclusion",
                    "char_start_offset": 36146,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 195
                        },
                        {
                            "start": 196,
                            "end": 431
                        },
                        {
                            "start": 432,
                            "end": 646
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98486328125
                }
            ],
            "relevance_judgement": 0.9951171875,
            "relevance_judgment_input_expanded": "# Title: Training Dynamics of a 1.7B LLaMa Model: A Data-Efficient Approach\n# Venue: \n# Authors: Miles Q. Li, Benjamin C. M. Fung, Shih-Chia Huang\n## Abstract\nPretraining large language models is a complex endeavor influenced by multiple factors, including model architecture, data quality, training continuity, and hardware constraints. In this paper, we share insights gained from the experience of training DMaS-LLaMa-Lite, a fully open source, 1.7-billion-parameter, LLaMa-based model, on approximately 20 billion tokens of carefully curated data. We chronicle the full training trajectory, documenting how evolving validation loss levels and downstream benchmarks reflect transitions from incoherent text to fluent, contextually grounded output. Beyond pretraining, we extend our analysis to include a post-training phase focused on instruction tuning, where the model was refined to produce more contextually appropriate, user-aligned responses. We highlight practical considerations such as the importance of restoring optimizer states when resuming from checkpoints, and the impact of hardware changes on training stability and throughput. While qualitative evaluation provides an intuitive understanding of model improvements, our analysis extends to various performance benchmarks, demonstrating how high-quality data and thoughtful scaling enable competitive results with significantly fewer training tokens. By detailing these experiences and offering training logs, checkpoints, and sample outputs, we aim to guide future researchers and practitioners in refining their pretraining strategies. The training script is available on Github at https://github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code. The model checkpoints are available on Huggingface at https://huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb.\n## Introduction\nLarge language models (LLMs) have achieved unprecedented performance across a variety of natural language understanding and generation tasks. These advancements have been driven by innovations in model architectures, pre-training methodologies, and the availability of large-scale, high-quality data (Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell et al., 2020;Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann et al., 2022;Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Roziere, Goyal, Hambro, Azhar et al., 2023a). However, despite significant progress, many aspects of LLM training remain underexplored. Practical challenges-ranging from data curation and training stability to qualitative and quantitative evaluation-play a critical role in determining model performance but are often overlooked in favor of final benchmark results. \n\nIn this paper, we present our experience pre-training and post-training DMaS-LLaMa-Lite, a 1.7-billion-parameter LLaMa-based model, on 20 billion tokens of carefully curated data. Unlike many existing studies that focus solely on the final outputs of large-scale training, we emphasize the training process itself and the insights gleaned from it. Specifically, we document the following key aspects of our work: \n\n1. Training Dynamics: We analyze how validation loss and downstream benchmarks (e.g., Hellaswag, ARC) evolve over 40,000+ training steps and correlate these metrics with improvements in text fluency, coherence, and factual accuracy.\n\n## Conclusion\nOur findings, along with the provided training logs, checkpoints, and sample outputs, aim to guide future researchers and practitioners in refining both pre-training and post-training strategies. By documenting the full trajectory from pre-training dynamics to post-training refinement, we emphasize a process-focused perspective where transparency and detailed insights foster reproducibility and more effective model development. This approach lays a foundation for future studies to explore additional best practices, methodological refinements, and novel techniques in both pre-training and instruction-tuning phases of large language models.",
            "reference_string": "[274823085 | Li et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Tracking the Feature Dynamics in LLM Training: A Mechanistic Study",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.17626, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2154895781",
                    "name": "Yangfan Xu"
                },
                {
                    "authorId": "2226917099",
                    "name": "Yi Wang"
                },
                {
                    "authorId": "2328190828",
                    "name": "Hao Wang"
                }
            ],
            "abstract": "Understanding training dynamics and feature evolution is crucial for the mechanistic interpretability of large language models (LLMs). Although sparse autoencoders (SAEs) have been used to identify features within LLMs, a clear picture of how these features evolve during training remains elusive. In this study, we (1) introduce SAE-Track, a novel method for efficiently obtaining a continual series of SAEs, providing the foundation for a mechanistic study that covers (2) the semantic evolution of features, (3) the underlying processes of feature formation, and (4) the directional drift of feature vectors. Our work provides new insights into the dynamics of features in LLMs, enhancing our understanding of training mechanisms and feature evolution. For reproducibility, our code is available at https://github.com/Superposition09m/SAE-Track.",
            "corpus_id": 274981612,
            "sentences": [
                {
                    "corpus_id": "274981612",
                    "title": "Tracking the Feature Dynamics in LLM Training: A Mechanistic Study",
                    "text": "Understanding training dynamics and feature evolution is crucial for the mechanistic interpretability of large language models (LLMs). Although sparse autoencoders (SAEs) have been used to identify features within LLMs, a clear picture of how these features evolve during training remains elusive. In this study, we: (1) introduce SAE-Track, a novel method to efficiently obtain a continual series of SAEs; (2) mechanistically investigate feature formation and develop a progress measure for it ; and (3) analyze and visualize feature drift during training. Our work provides new insights into the dynamics of features in LLMs, enhancing our understanding of training mechanisms and feature evolution.",
                    "score": 0.4244425015891494,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99462890625
                }
            ],
            "relevance_judgement": 0.99462890625,
            "relevance_judgment_input_expanded": "# Title: Tracking the Feature Dynamics in LLM Training: A Mechanistic Study\n# Venue: arXiv.org\n# Authors: Yangfan Xu, Yi Wang, Hao Wang\n## Abstract\nUnderstanding training dynamics and feature evolution is crucial for the mechanistic interpretability of large language models (LLMs). Although sparse autoencoders (SAEs) have been used to identify features within LLMs, a clear picture of how these features evolve during training remains elusive. In this study, we (1) introduce SAE-Track, a novel method for efficiently obtaining a continual series of SAEs, providing the foundation for a mechanistic study that covers (2) the semantic evolution of features, (3) the underlying processes of feature formation, and (4) the directional drift of feature vectors. Our work provides new insights into the dynamics of features in LLMs, enhancing our understanding of training mechanisms and feature evolution. For reproducibility, our code is available at https://github.com/Superposition09m/SAE-Track.\n",
            "reference_string": "[274981612 | Xu et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 58,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.15419, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2087001989",
                    "name": "Tyler A. Chang"
                },
                {
                    "authorId": "144035504",
                    "name": "Z. Tu"
                },
                {
                    "authorId": "24316216",
                    "name": "B. Bergen"
                }
            ],
            "abstract": "Abstract How do language models learn to make predictions during pre-training? To study this, we extract learning curves from five autoregressive English language model pre-training runs, for 1M unseen tokens in context. We observe that the language models generate short repetitive phrases before learning to generate longer and more coherent text. We also find that individual tokens often exhibit sudden increases or decreases in loss that are surprisingly consistent across pre-training runs. To better understand these fluctuations, we quantify the final surprisal, within-run variability, age of acquisition, forgettability, and cross-run variability of learning curves for individual tokens in context. More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be \u201cforgotten\u201d during pre-training. Higher n-gram probabilities further accentuate these effects. Independent of the target token, shorter and more frequent contexts correlate with marginally more stable and quickly acquired predictions. Based on our results, we argue for the existence of sequential learning dependencies between different model capabilities, and we characterize language model learning as early n-gram learning before gradual refinement of tail n-gram predictions.",
            "corpus_id": 261277016,
            "sentences": [
                {
                    "corpus_id": "261277016",
                    "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
                    "text": "In the previous sections, we report general patterns during language model pre-training ( \u00a74), define ways to characterize learning curves ( \u00a75), and isolate specific features that predict the speed and stability of learning for individual tokens in context ( \u00a76). Our results contribute to ongoing work studying language model pre-training dynamics, with implications for robust model deployment. \n\nSequential learning. Previous work has demonstrated that language models exhibit fine-grained learning patterns that are not captured by the corpus-level loss curve (related work in \u00a72). In particular, sudden increases and decreases in example loss ( \u00a75 and Xia et al., 2023) may be somewhat surprising given that the pre-training text is i.i.d. for all pre-training steps. By demonstrating that many of these sudden changes are consistent regardless of random initialization and data shuffling ( \u00a75.2), our work indicates that some instances of sudden learning and \"forgetting\" are not due to random chance or the specific examples observed in a given step. 10 Rather, they reflect some change in model processing that consistently occurs partially into pre-training (roughly step t \u0338 = 0). Because such a sudden change cannot be attributed to the specific examples observed (robust to random shuffling) or any change in the pre-training distribution at time t (the data is always i.i.d.), the primary remaining explanation is that the models' sudden \"learning\" at step t \u0338 = 0 is made possible by some systematic difference between models (and their optimizers) just before step t vs. at step 0. \n\nFramed from a potentially more interesting perspective, some types of language model \"learning\" appear to be dependent on previous learning and the linguistic abilities already present in the model. This aligns with previous work showing that language models acquire linguistic abilities in a systematic order during pre-training (Liu et al., 2021;Choshen et al., 2022), although not necessarily due to sequential dependencies. For example, Evanson et al. (2023) show that despite similar acquisition orders across models, different syntactic abilities are learned in parallel; performance for most individual abilities increases from the onset of pre-training. Our work provides evidence that there exist other capabilities or types of generalizations (e.g.",
                    "score": 0.4497071349042731,
                    "section_title": "Discussion",
                    "char_start_offset": 27189,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 264
                        },
                        {
                            "start": 265,
                            "end": 397
                        },
                        {
                            "start": 400,
                            "end": 420
                        },
                        {
                            "start": 421,
                            "end": 586
                        },
                        {
                            "start": 587,
                            "end": 773
                        },
                        {
                            "start": 774,
                            "end": 1061
                        },
                        {
                            "start": 1062,
                            "end": 1191
                        },
                        {
                            "start": 1192,
                            "end": 1597
                        },
                        {
                            "start": 1600,
                            "end": 1798
                        },
                        {
                            "start": 1799,
                            "end": 2027
                        },
                        {
                            "start": 2028,
                            "end": 2261
                        },
                        {
                            "start": 2262,
                            "end": 2358
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 658,
                            "end": 674,
                            "matchedPaperCorpusId": "254877112"
                        },
                        {
                            "start": 1930,
                            "end": 1948,
                            "matchedPaperCorpusId": "233289478"
                        },
                        {
                            "start": 2041,
                            "end": 2062,
                            "matchedPaperCorpusId": "259089351"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.994140625
                },
                {
                    "corpus_id": "261277016",
                    "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
                    "text": "Previous work has studied the pre-training dynamics of language models (Saphra and Lopez, 2019). Choshen et al. (2022) and Evanson et al. (2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020;Liu et al., 2021). Olsson et al. (2022) find that copy mechanisms (\"induction heads\" for in-context learning) appear at an inflection point during pre-training. These results establish when a variety of abilities emerge in language models. Our work studies more fine-grained learning trajectories by evaluating individual tokens in context. \n\nIndeed, previous work has studied how individual tokens are learned during pre-training. \n\nFor example, word learning is highly dependent on word frequency (Chang and Bergen, 2022b). Larger models memorize more examples during pre-training without overfitting (Tirumala et al., 2022), but the time step that a model sees an example does not affect memorization (Biderman et al., 2023). Most similar to our work, Xia et al. (2023) collect learning curves for individual tokens in context, finding that some examples exhibit a \"doubledescent\" trend where they first increase then decrease in surprisal. All of the studies above collect language model learning curves during pre-training, either for individual examples or targeted benchmark performance. Here, we introduce metrics to characterize such curves, we identify general learning patterns, and we isolate text features that are predictive of learning speed and stability.",
                    "score": 0.4279126903800746,
                    "section_title": "Related Work",
                    "char_start_offset": 3961,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 96
                        },
                        {
                            "start": 97,
                            "end": 291
                        },
                        {
                            "start": 292,
                            "end": 461
                        },
                        {
                            "start": 462,
                            "end": 603
                        },
                        {
                            "start": 604,
                            "end": 682
                        },
                        {
                            "start": 683,
                            "end": 783
                        },
                        {
                            "start": 786,
                            "end": 874
                        },
                        {
                            "start": 877,
                            "end": 968
                        },
                        {
                            "start": 969,
                            "end": 1171
                        },
                        {
                            "start": 1172,
                            "end": 1386
                        },
                        {
                            "start": 1387,
                            "end": 1537
                        },
                        {
                            "start": 1538,
                            "end": 1714
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 71,
                            "end": 95,
                            "matchedPaperCorpusId": "267903883"
                        },
                        {
                            "start": 123,
                            "end": 144,
                            "matchedPaperCorpusId": "259089351"
                        },
                        {
                            "start": 422,
                            "end": 443,
                            "matchedPaperCorpusId": "222140842"
                        },
                        {
                            "start": 443,
                            "end": 460,
                            "matchedPaperCorpusId": "233289478"
                        },
                        {
                            "start": 942,
                            "end": 967,
                            "matchedPaperCorpusId": "238408423"
                        },
                        {
                            "start": 1046,
                            "end": 1069,
                            "matchedPaperCorpusId": "248986465"
                        },
                        {
                            "start": 1147,
                            "end": 1170,
                            "matchedPaperCorpusId": "257921893"
                        },
                        {
                            "start": 1198,
                            "end": 1215,
                            "matchedPaperCorpusId": "254877112"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.994140625
                },
                {
                    "corpus_id": "261277016",
                    "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
                    "text": "Second, we only consider language models with 124M parameters trained on 5.1B tokens. Previous work has demonstrated that learning curves differ across model sizes (Xia et al., 2023); larger models are able to \"learn\" some examples (usually late in pre-training) for which smaller models reach nonoptimal local minima or even diverge. Larger models also exhibit less forgetting of pre-training examples (Tirumala et al., 2022), although it remains unclear whether similar mechanisms are responsible for evaluation example forgetting (i.e. surprisal increases for seen vs. unseen examples). Further research is necessary to determine the effects of model size on learning speed, variability, and forgetting; with a larger compute budget, the methods presented in our work can easily be applied to larger models. Nonetheless, previous work has documented similar behaviors for different model sizes when they achieve similar perplexities (Choshen et al., 2022;Xia et al., 2023), suggesting that pretraining dynamics in smaller models may be similar to the early dynamics of larger models. A particularly exciting direction future work is to characterize the examples (e.g. based on types of reasoning, world knowledge, or commonsense) that fluctuate at different points during pre-training across model sizes.",
                    "score": 0.39742057044778967,
                    "section_title": "Discussion",
                    "char_start_offset": 33481,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 85
                        },
                        {
                            "start": 86,
                            "end": 334
                        },
                        {
                            "start": 335,
                            "end": 538
                        },
                        {
                            "start": 539,
                            "end": 589
                        },
                        {
                            "start": 590,
                            "end": 810
                        },
                        {
                            "start": 811,
                            "end": 1086
                        },
                        {
                            "start": 1087,
                            "end": 1307
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 164,
                            "end": 182,
                            "matchedPaperCorpusId": "254877112"
                        },
                        {
                            "start": 403,
                            "end": 426,
                            "matchedPaperCorpusId": "248986465"
                        },
                        {
                            "start": 958,
                            "end": 975,
                            "matchedPaperCorpusId": "254877112"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9873046875
                }
            ],
            "relevance_judgement": 0.994140625,
            "relevance_judgment_input_expanded": "# Title: Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability\n# Venue: Transactions of the Association for Computational Linguistics\n# Authors: Tyler A. Chang, Z. Tu, B. Bergen\n## Abstract\nAbstract How do language models learn to make predictions during pre-training? To study this, we extract learning curves from five autoregressive English language model pre-training runs, for 1M unseen tokens in context. We observe that the language models generate short repetitive phrases before learning to generate longer and more coherent text. We also find that individual tokens often exhibit sudden increases or decreases in loss that are surprisingly consistent across pre-training runs. To better understand these fluctuations, we quantify the final surprisal, within-run variability, age of acquisition, forgettability, and cross-run variability of learning curves for individual tokens in context. More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be \u201cforgotten\u201d during pre-training. Higher n-gram probabilities further accentuate these effects. Independent of the target token, shorter and more frequent contexts correlate with marginally more stable and quickly acquired predictions. Based on our results, we argue for the existence of sequential learning dependencies between different model capabilities, and we characterize language model learning as early n-gram learning before gradual refinement of tail n-gram predictions.\n## Related Work\nPrevious work has studied the pre-training dynamics of language models (Saphra and Lopez, 2019). Choshen et al. (2022) and Evanson et al. (2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020;Liu et al., 2021). Olsson et al. (2022) find that copy mechanisms (\"induction heads\" for in-context learning) appear at an inflection point during pre-training. These results establish when a variety of abilities emerge in language models. Our work studies more fine-grained learning trajectories by evaluating individual tokens in context. \n\nIndeed, previous work has studied how individual tokens are learned during pre-training. \n\nFor example, word learning is highly dependent on word frequency (Chang and Bergen, 2022b). Larger models memorize more examples during pre-training without overfitting (Tirumala et al., 2022), but the time step that a model sees an example does not affect memorization (Biderman et al., 2023). Most similar to our work, Xia et al. (2023) collect learning curves for individual tokens in context, finding that some examples exhibit a \"doubledescent\" trend where they first increase then decrease in surprisal. All of the studies above collect language model learning curves during pre-training, either for individual examples or targeted benchmark performance. Here, we introduce metrics to characterize such curves, we identify general learning patterns, and we isolate text features that are predictive of learning speed and stability.\n\n## Discussion\nIn the previous sections, we report general patterns during language model pre-training ( \u00a74), define ways to characterize learning curves ( \u00a75), and isolate specific features that predict the speed and stability of learning for individual tokens in context ( \u00a76). Our results contribute to ongoing work studying language model pre-training dynamics, with implications for robust model deployment. \n\nSequential learning. Previous work has demonstrated that language models exhibit fine-grained learning patterns that are not captured by the corpus-level loss curve (related work in \u00a72). In particular, sudden increases and decreases in example loss ( \u00a75 and Xia et al., 2023) may be somewhat surprising given that the pre-training text is i.i.d. for all pre-training steps. By demonstrating that many of these sudden changes are consistent regardless of random initialization and data shuffling ( \u00a75.2), our work indicates that some instances of sudden learning and \"forgetting\" are not due to random chance or the specific examples observed in a given step. 10 Rather, they reflect some change in model processing that consistently occurs partially into pre-training (roughly step t \u0338 = 0). Because such a sudden change cannot be attributed to the specific examples observed (robust to random shuffling) or any change in the pre-training distribution at time t (the data is always i.i.d.), the primary remaining explanation is that the models' sudden \"learning\" at step t \u0338 = 0 is made possible by some systematic difference between models (and their optimizers) just before step t vs. at step 0. \n\nFramed from a potentially more interesting perspective, some types of language model \"learning\" appear to be dependent on previous learning and the linguistic abilities already present in the model. This aligns with previous work showing that language models acquire linguistic abilities in a systematic order during pre-training (Liu et al., 2021;Choshen et al., 2022), although not necessarily due to sequential dependencies. For example, Evanson et al. (2023) show that despite similar acquisition orders across models, different syntactic abilities are learned in parallel; performance for most individual abilities increases from the onset of pre-training. Our work provides evidence that there exist other capabilities or types of generalizations (e.g.\n...\nSecond, we only consider language models with 124M parameters trained on 5.1B tokens. Previous work has demonstrated that learning curves differ across model sizes (Xia et al., 2023); larger models are able to \"learn\" some examples (usually late in pre-training) for which smaller models reach nonoptimal local minima or even diverge. Larger models also exhibit less forgetting of pre-training examples (Tirumala et al., 2022), although it remains unclear whether similar mechanisms are responsible for evaluation example forgetting (i.e. surprisal increases for seen vs. unseen examples). Further research is necessary to determine the effects of model size on learning speed, variability, and forgetting; with a larger compute budget, the methods presented in our work can easily be applied to larger models. Nonetheless, previous work has documented similar behaviors for different model sizes when they achieve similar perplexities (Choshen et al., 2022;Xia et al., 2023), suggesting that pretraining dynamics in smaller models may be similar to the early dynamics of larger models. A particularly exciting direction future work is to characterize the examples (e.g. based on types of reasoning, world knowledge, or commonsense) that fluctuate at different points during pre-training across model sizes.",
            "reference_string": "[261277016 | Chang et al. | 2023 | Citations: 13]"
        },
        {
            "title": "Temporal Scaling Law for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.17785, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2249971338",
                    "name": "Yizhe Xiong"
                },
                {
                    "authorId": "2298904872",
                    "name": "Xiansheng Chen"
                },
                {
                    "authorId": "2299108794",
                    "name": "Xin Ye"
                },
                {
                    "authorId": "2298921971",
                    "name": "Hui Chen"
                },
                {
                    "authorId": "1818920",
                    "name": "Zijia Lin"
                },
                {
                    "authorId": "2298903058",
                    "name": "Haoran Lian"
                },
                {
                    "authorId": "2293626051",
                    "name": "Jianwei Niu"
                },
                {
                    "authorId": "2242661989",
                    "name": "Guiguang Ding"
                }
            ],
            "abstract": "Recently, Large Language Models (LLMs) have been widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed Scaling Laws, have discovered that the final test loss of LLMs scales as power-laws with model size, computational budget, and dataset size. However, the temporal change of the test loss of an LLM throughout its pre-training process remains unexplored, though it is valuable in many aspects, such as selecting better hyperparameters \\textit{directly} on the target LLM. In this paper, we propose the novel concept of Temporal Scaling Law, studying how the test loss of an LLM evolves as the training steps scale up. In contrast to modeling the test loss as a whole in a coarse-grained manner, we break it down and dive into the fine-grained test loss of each token position, and further develop a dynamic hyperbolic-law. Afterwards, we derive the much more precise temporal scaling law by studying the temporal patterns of the parameters in the dynamic hyperbolic-law. Results on both in-distribution (ID) and out-of-distribution (OOD) validation datasets demonstrate that our temporal scaling law accurately predicts the test loss of LLMs across training steps. Our temporal scaling law has broad practical applications. First, it enables direct and efficient hyperparameter selection on the target LLM, such as data mixture proportions. Secondly, viewing the LLM pre-training dynamics from the token position granularity provides some insights to enhance the understanding of LLM pre-training.",
            "corpus_id": 269449894,
            "sentences": [
                {
                    "corpus_id": "269449894",
                    "title": "Temporal Scaling Law for Large Language Models",
                    "text": "Our research introduces the novel concept of Temporal Scaling Law within the context of Large Language Models (LLMs), shifting the focus from static attributes of model scalability to the dynamic, temporal aspects of training. We analyze the loss patterns associated with each token and discovered that these patterns conform to a reciprocallaw across different scales and stages of training. Utilizing the reciprocal-law, we could precisely predict the pre-training trajectory for the test loss (or perplexity) of LLMs, marking a significant improvement over baseline methods. This capability is crucial for numerous possible applications during training, like tuning data mixture proportions, selecting hyperparameters, and determining the precise point for early stopping. With the temporal scaling law, we provide empirical evidence supporting that LLMs enjoy an equalized learning approach across different token positions. Our findings validate the existing training paradigms where no position-based re-weighting strategies are applied to different token positions. \n\nLimitations. Our temporal scaling law is derived on the mainstream structure of LLMs, the GPT-based decoder transformers. The applicabil-ity of our findings to other architectures, such as Mixed of Experts models, remains to be explored. Furthermore, our research primarily focuses on the pre-training stage. The dynamics in other scenarios, such as transfer learning, are not covered. We leave the further investigations to our future research.",
                    "score": 0.49181502987762865,
                    "section_title": "Conclusion",
                    "char_start_offset": 25281,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 226
                        },
                        {
                            "start": 227,
                            "end": 392
                        },
                        {
                            "start": 393,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 775
                        },
                        {
                            "start": 776,
                            "end": 928
                        },
                        {
                            "start": 929,
                            "end": 1072
                        },
                        {
                            "start": 1075,
                            "end": 1087
                        },
                        {
                            "start": 1088,
                            "end": 1196
                        },
                        {
                            "start": 1197,
                            "end": 1312
                        },
                        {
                            "start": 1313,
                            "end": 1383
                        },
                        {
                            "start": 1384,
                            "end": 1460
                        },
                        {
                            "start": 1461,
                            "end": 1520
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99365234375
                }
            ],
            "relevance_judgement": 0.99365234375,
            "relevance_judgment_input_expanded": "# Title: Temporal Scaling Law for Large Language Models\n# Venue: arXiv.org\n# Authors: Yizhe Xiong, Xiansheng Chen, Xin Ye, Hui Chen, Zijia Lin, Haoran Lian, Jianwei Niu, Guiguang Ding\n## Abstract\nRecently, Large Language Models (LLMs) have been widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed Scaling Laws, have discovered that the final test loss of LLMs scales as power-laws with model size, computational budget, and dataset size. However, the temporal change of the test loss of an LLM throughout its pre-training process remains unexplored, though it is valuable in many aspects, such as selecting better hyperparameters \\textit{directly} on the target LLM. In this paper, we propose the novel concept of Temporal Scaling Law, studying how the test loss of an LLM evolves as the training steps scale up. In contrast to modeling the test loss as a whole in a coarse-grained manner, we break it down and dive into the fine-grained test loss of each token position, and further develop a dynamic hyperbolic-law. Afterwards, we derive the much more precise temporal scaling law by studying the temporal patterns of the parameters in the dynamic hyperbolic-law. Results on both in-distribution (ID) and out-of-distribution (OOD) validation datasets demonstrate that our temporal scaling law accurately predicts the test loss of LLMs across training steps. Our temporal scaling law has broad practical applications. First, it enables direct and efficient hyperparameter selection on the target LLM, such as data mixture proportions. Secondly, viewing the LLM pre-training dynamics from the token position granularity provides some insights to enhance the understanding of LLM pre-training.\n## Conclusion\nOur research introduces the novel concept of Temporal Scaling Law within the context of Large Language Models (LLMs), shifting the focus from static attributes of model scalability to the dynamic, temporal aspects of training. We analyze the loss patterns associated with each token and discovered that these patterns conform to a reciprocallaw across different scales and stages of training. Utilizing the reciprocal-law, we could precisely predict the pre-training trajectory for the test loss (or perplexity) of LLMs, marking a significant improvement over baseline methods. This capability is crucial for numerous possible applications during training, like tuning data mixture proportions, selecting hyperparameters, and determining the precise point for early stopping. With the temporal scaling law, we provide empirical evidence supporting that LLMs enjoy an equalized learning approach across different token positions. Our findings validate the existing training paradigms where no position-based re-weighting strategies are applied to different token positions. \n\nLimitations. Our temporal scaling law is derived on the mainstream structure of LLMs, the GPT-based decoder transformers. The applicabil-ity of our findings to other architectures, such as Mixed of Experts models, remains to be explored. Furthermore, our research primarily focuses on the pre-training stage. The dynamics in other scenarios, such as transfer learning, are not covered. We leave the further investigations to our future research.",
            "reference_string": "[269449894 | Xiong et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 46,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.13728, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2214094238",
                    "name": "Anna M'esz'aros"
                },
                {
                    "authorId": "2299943014",
                    "name": "Szilvia Ujv'ary"
                },
                {
                    "authorId": "40634590",
                    "name": "Wieland Brendel"
                },
                {
                    "authorId": "1382657853",
                    "name": "Patrik Reizinger"
                },
                {
                    "authorId": "2322443584",
                    "name": "Ferenc Husz'ar"
                }
            ],
            "abstract": "LLMs show remarkable emergent abilities, such as inferring concepts from presumably out-of-distribution prompts, known as in-context learning. Though this success is often attributed to the Transformer architecture, our systematic understanding is limited. In complex real-world data sets, even defining what is out-of-distribution is not obvious. To better understand the OOD behaviour of autoregressive LLMs, we focus on formal languages, which are defined by the intersection of rules. We define a new scenario of OOD compositional generalization, termed rule extrapolation. Rule extrapolation describes OOD scenarios, where the prompt violates at least one rule. We evaluate rule extrapolation in formal languages with varying complexity in linear and recurrent architectures, the Transformer, and state space models to understand the architectures' influence on rule extrapolation. We also lay the first stones of a normative theory of rule extrapolation, inspired by the Solomonoff prior in algorithmic information theory.",
            "corpus_id": 272826680,
            "sentences": [
                {
                    "corpus_id": "272826680",
                    "title": "Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts",
                    "text": "Here, we argue informally that our normative algorithm provides a notion of a rational pre-training process, and thus helps explain the training dynamics of practical LMs, and is also capable of rule extrapolation. We support our arguments by showing the role of simplicity bias (towards low Kolmogorov complexity) in the dynamics of learning the a n b n language with Transformers. Explaining training dynamics. We analize the dynamics of learning rule extrapolation. We report results on the Transformer (training dynamics of Mamba and the LSTM are in Appx. A), trained on the a n b n language-where high rule extrapolation ability is achieved. Fig. 3 shows that first, the model learns the sequences obeying (R2), then it learns the language (R1) \u2229 (R2) as its subset. We argue that the order in which rules are learnt is governed by the relative simplicity of the rules, quantified by Kolmogorov complexity. Given a formal language with rules (R1) and (R2), let p 1 , p 2 and p 1,2 be distributions defined by LMs that generate sequences that satisfy (R1), (R2) and (R1) \u2229 (R2), respectively. If, e.g., K(p 2 ) \u226a K(p 1,2 ), our normative algorithm will first learn (R2), and then learn the (R1) \u2229 (R2) as its subset. In the a n b n language, (R2) (a's before b's), is, on average, simpler to generate than (R1) (#a=#b) and (R1) \u2229 (R2). Therefore, we expect our normative algorithm to first learn (R2), and then learn (R1) \u2229 (R2) as its subset. Remarkably, our Transformer employs the same strategy ( Fig. 3), verifying the presence of simplicity bias. This result is matches past observations that Transformers are biased towards low Kolmogorov complexity [Goldblum et al., 2023]. Towards explaining rule extrapolation.",
                    "score": 0.43178075737591326,
                    "section_title": "Towards explaining training dynamics and rule extrapolation",
                    "char_start_offset": 31665,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 214
                        },
                        {
                            "start": 215,
                            "end": 382
                        },
                        {
                            "start": 383,
                            "end": 412
                        },
                        {
                            "start": 413,
                            "end": 468
                        },
                        {
                            "start": 469,
                            "end": 559
                        },
                        {
                            "start": 560,
                            "end": 646
                        },
                        {
                            "start": 647,
                            "end": 771
                        },
                        {
                            "start": 772,
                            "end": 911
                        },
                        {
                            "start": 912,
                            "end": 1096
                        },
                        {
                            "start": 1097,
                            "end": 1220
                        },
                        {
                            "start": 1221,
                            "end": 1339
                        },
                        {
                            "start": 1340,
                            "end": 1447
                        },
                        {
                            "start": 1448,
                            "end": 1555
                        },
                        {
                            "start": 1556,
                            "end": 1684
                        },
                        {
                            "start": 1685,
                            "end": 1723
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99365234375
                }
            ],
            "relevance_judgement": 0.99365234375,
            "relevance_judgment_input_expanded": "# Title: Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts\n# Venue: arXiv.org\n# Authors: Anna M'esz'aros, Szilvia Ujv'ary, Wieland Brendel, Patrik Reizinger, Ferenc Husz'ar\n## Abstract\nLLMs show remarkable emergent abilities, such as inferring concepts from presumably out-of-distribution prompts, known as in-context learning. Though this success is often attributed to the Transformer architecture, our systematic understanding is limited. In complex real-world data sets, even defining what is out-of-distribution is not obvious. To better understand the OOD behaviour of autoregressive LLMs, we focus on formal languages, which are defined by the intersection of rules. We define a new scenario of OOD compositional generalization, termed rule extrapolation. Rule extrapolation describes OOD scenarios, where the prompt violates at least one rule. We evaluate rule extrapolation in formal languages with varying complexity in linear and recurrent architectures, the Transformer, and state space models to understand the architectures' influence on rule extrapolation. We also lay the first stones of a normative theory of rule extrapolation, inspired by the Solomonoff prior in algorithmic information theory.\n## Towards explaining training dynamics and rule extrapolation\nHere, we argue informally that our normative algorithm provides a notion of a rational pre-training process, and thus helps explain the training dynamics of practical LMs, and is also capable of rule extrapolation. We support our arguments by showing the role of simplicity bias (towards low Kolmogorov complexity) in the dynamics of learning the a n b n language with Transformers. Explaining training dynamics. We analize the dynamics of learning rule extrapolation. We report results on the Transformer (training dynamics of Mamba and the LSTM are in Appx. A), trained on the a n b n language-where high rule extrapolation ability is achieved. Fig. 3 shows that first, the model learns the sequences obeying (R2), then it learns the language (R1) \u2229 (R2) as its subset. We argue that the order in which rules are learnt is governed by the relative simplicity of the rules, quantified by Kolmogorov complexity. Given a formal language with rules (R1) and (R2), let p 1 , p 2 and p 1,2 be distributions defined by LMs that generate sequences that satisfy (R1), (R2) and (R1) \u2229 (R2), respectively. If, e.g., K(p 2 ) \u226a K(p 1,2 ), our normative algorithm will first learn (R2), and then learn the (R1) \u2229 (R2) as its subset. In the a n b n language, (R2) (a's before b's), is, on average, simpler to generate than (R1) (#a=#b) and (R1) \u2229 (R2). Therefore, we expect our normative algorithm to first learn (R2), and then learn (R1) \u2229 (R2) as its subset. Remarkably, our Transformer employs the same strategy ( Fig. 3), verifying the presence of simplicity bias. This result is matches past observations that Transformers are biased towards low Kolmogorov complexity [Goldblum et al., 2023]. Towards explaining rule extrapolation.",
            "reference_string": "[272826680 | M'esz'aros et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Tending Towards Stability: Convergence Challenges in Small Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 26,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.11451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2266941716",
                    "name": "Richard Diehl Martinez"
                },
                {
                    "authorId": "2325954375",
                    "name": "Pietro Lesci"
                },
                {
                    "authorId": "33490976",
                    "name": "P. Buttery"
                }
            ],
            "abstract": "Increasing the number of parameters in language models is a common strategy to enhance their performance. However, smaller language models remain valuable due to their lower operational costs. Despite their advantages, smaller models frequently underperform compared to their larger counterparts, even when provided with equivalent data and computational resources. Specifically, their performance tends to degrade in the late pretraining phase. This is anecdotally attributed to their reduced representational capacity. Yet, the exact causes of this performance degradation remain unclear. We use the Pythia model suite to analyse the training dynamics that underlie this phenomenon. Across different model sizes, we investigate the convergence of the Attention and MLP activations to their final state and examine how the effective rank of their parameters influences this process. We find that nearly all layers in larger models stabilise early in training - within the first 20% - whereas layers in smaller models exhibit slower and less stable convergence, especially when their parameters have lower effective rank. By linking the convergence of layers' activations to their parameters' effective rank, our analyses can guide future work to address inefficiencies in the learning dynamics of small models.",
            "corpus_id": 273351173,
            "sentences": [
                {
                    "corpus_id": "273351173",
                    "title": "Tending Towards Stability: Convergence Challenges in Small Language Models",
                    "text": "Increasing the number of parameters in language models is a common strategy to enhance their performance. However, smaller language models remain valuable due to their lower operational costs. Despite their advantages, smaller models frequently underperform compared to their larger counterparts, even when provided with equivalent data and computational resources. Specifically, their performance tends to degrade in the late pretraining phase. This is anecdotally attributed to their reduced representational capacity. Yet, the exact causes of this performance degradation remain unclear. We use the Pythia model suite to analyse the training dynamics that underlie this phenomenon. Across different model sizes, we investigate the convergence of the Attention and MLP activations to their final state and examine how the effective rank of their parameters influences this process. We find that nearly all layers in larger models stabilise early in training - within the first 20% - whereas layers in smaller models exhibit slower and less stable convergence, especially when their parameters have lower effective rank. By linking the convergence of layers' activations to their parameters' effective rank, our analyses can guide future work to address inefficiencies in the learning dynamics of small models.",
                    "score": 0.38115113924083144,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9931640625
                }
            ],
            "relevance_judgement": 0.9931640625,
            "relevance_judgment_input_expanded": "# Title: Tending Towards Stability: Convergence Challenges in Small Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Richard Diehl Martinez, Pietro Lesci, P. Buttery\n## Abstract\nIncreasing the number of parameters in language models is a common strategy to enhance their performance. However, smaller language models remain valuable due to their lower operational costs. Despite their advantages, smaller models frequently underperform compared to their larger counterparts, even when provided with equivalent data and computational resources. Specifically, their performance tends to degrade in the late pretraining phase. This is anecdotally attributed to their reduced representational capacity. Yet, the exact causes of this performance degradation remain unclear. We use the Pythia model suite to analyse the training dynamics that underlie this phenomenon. Across different model sizes, we investigate the convergence of the Attention and MLP activations to their final state and examine how the effective rank of their parameters influences this process. We find that nearly all layers in larger models stabilise early in training - within the first 20% - whereas layers in smaller models exhibit slower and less stable convergence, especially when their parameters have lower effective rank. By linking the convergence of layers' activations to their parameters' effective rank, our analyses can guide future work to address inefficiencies in the learning dynamics of small models.\n",
            "reference_string": "[273351173 | Martinez et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Emergent Structures and Training Dynamics in Large Language Models",
            "venue": "BIGSCIENCE",
            "year": 2022,
            "reference_count": 121,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.bigscience-1.11.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2022.bigscience-1.11, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2131107966",
                    "name": "Ryan Teehan"
                },
                {
                    "authorId": "2029314697",
                    "name": "Miruna Clinciu"
                },
                {
                    "authorId": "1799401599",
                    "name": "Oleg Serikov"
                },
                {
                    "authorId": "50812522",
                    "name": "Eliza Szczechla"
                },
                {
                    "authorId": "12046785",
                    "name": "Natasha Seelam"
                },
                {
                    "authorId": "8963527",
                    "name": "Shachar Mirkin"
                },
                {
                    "authorId": "2273789852",
                    "name": "Aaron Gokaslan"
                }
            ],
            "abstract": "Large language models have achieved success on a number of downstream tasks, particularly in a few and zero-shot manner. As a consequence, researchers have been investigating both the kind of information these networks learn and how such information can be encoded in the parameters of the model. We survey the literature on changes in the network during training, drawing from work outside of NLP when necessary, and on learned representations of linguistic features in large language models. We note in particular the lack of sufficient research on the emergence of functional units, subsections of the network where related functions are grouped or organised, within large language models and motivate future work that grounds the study of language models in an analysis of their changing internal structure during training time.",
            "corpus_id": 247656607,
            "sentences": [
                {
                    "corpus_id": "247656607",
                    "title": "Emergent Structures and Training Dynamics in Large Language Models",
                    "text": "Training dynamics is an emerging field of research, promising to improve our understanding of knowledge acquisition in neural networks and offering insights into the utility of pre-trained models and embedded representations for downstream tasks. Most studies of Transformers (e.g. RoBERTa (Zhuang et al., 2021)) and LSTMs (Hochreiter and Schmidhuber, 1997) agree that models acquire linguistic knowledge early in the learning process.\n\nLocal syntactic information, such as parts of speech, is learned earlier than information encoding long-distance dependencies (e.g. topic) Saphra, 2021). Exploration of AL-BERT (Lan et al., 2019) and LSTM-based networks reveals different learning patterns for function and content words with more fine-grained distinctions within these categories including part of speech and verb form (Saphra, 2021;Chiang et al., 2020).\n\nDifferences in learning trajectory were also observed between layers. In LSTMs, recurrent layers become more task-independent over the course of training, while embeddings become more taskspecific (Saphra, 2021). In Transformer-based architectures, i.e.: ALBERT and ELECTRA, Chiang et al. (2020) observe differences in performance patterns between the top and last layers. Similarly to other areas of research in NLP, most of the literature on training dynamics concentrate on English-language models. Another possible direction for future work is extending studies conducted on LSTMs to more widely used Transformers.",
                    "score": 0.4872818586341583,
                    "section_title": "Training Dynamics of Internal Representation Development",
                    "char_start_offset": 17316,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 290,
                            "end": 311,
                            "matchedPaperCorpusId": "237101158"
                        },
                        {
                            "start": 323,
                            "end": 357,
                            "matchedPaperCorpusId": "1915014"
                        },
                        {
                            "start": 837,
                            "end": 857,
                            "matchedPaperCorpusId": "222140842"
                        },
                        {
                            "start": 1135,
                            "end": 1155,
                            "matchedPaperCorpusId": "222140842"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99267578125
                }
            ],
            "relevance_judgement": 0.99267578125,
            "relevance_judgment_input_expanded": "# Title: Emergent Structures and Training Dynamics in Large Language Models\n# Venue: BIGSCIENCE\n# Authors: Ryan Teehan, Miruna Clinciu, Oleg Serikov, Eliza Szczechla, Natasha Seelam, Shachar Mirkin, Aaron Gokaslan\n## Abstract\nLarge language models have achieved success on a number of downstream tasks, particularly in a few and zero-shot manner. As a consequence, researchers have been investigating both the kind of information these networks learn and how such information can be encoded in the parameters of the model. We survey the literature on changes in the network during training, drawing from work outside of NLP when necessary, and on learned representations of linguistic features in large language models. We note in particular the lack of sufficient research on the emergence of functional units, subsections of the network where related functions are grouped or organised, within large language models and motivate future work that grounds the study of language models in an analysis of their changing internal structure during training time.\n## Training Dynamics of Internal Representation Development\nTraining dynamics is an emerging field of research, promising to improve our understanding of knowledge acquisition in neural networks and offering insights into the utility of pre-trained models and embedded representations for downstream tasks. Most studies of Transformers (e.g. RoBERTa (Zhuang et al., 2021)) and LSTMs (Hochreiter and Schmidhuber, 1997) agree that models acquire linguistic knowledge early in the learning process.\n\nLocal syntactic information, such as parts of speech, is learned earlier than information encoding long-distance dependencies (e.g. topic) Saphra, 2021). Exploration of AL-BERT (Lan et al., 2019) and LSTM-based networks reveals different learning patterns for function and content words with more fine-grained distinctions within these categories including part of speech and verb form (Saphra, 2021;Chiang et al., 2020).\n\nDifferences in learning trajectory were also observed between layers. In LSTMs, recurrent layers become more task-independent over the course of training, while embeddings become more taskspecific (Saphra, 2021). In Transformer-based architectures, i.e.: ALBERT and ELECTRA, Chiang et al. (2020) observe differences in performance patterns between the top and last layers. Similarly to other areas of research in NLP, most of the literature on training dynamics concentrate on English-language models. Another possible direction for future work is extending studies conducted on LSTMs to more widely used Transformers.",
            "reference_string": "[247656607 | Teehan et al. | 2022 | Citations: 11]"
        },
        {
            "title": "The Grammar-Learning Trajectories of Neural Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2021,
            "reference_count": 67,
            "citation_count": 29,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.acl-long.568.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.06096, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "41019330",
                    "name": "Leshem Choshen"
                },
                {
                    "authorId": "94064232",
                    "name": "Guy Hacohen"
                },
                {
                    "authorId": "1789171",
                    "name": "D. Weinshall"
                },
                {
                    "authorId": "2769805",
                    "name": "Omri Abend"
                }
            ],
            "abstract": "The learning trajectories of linguistic phenomena in humans provide insight into linguistic representation, beyond what can be gleaned from inspecting the behavior of an adult speaker. To apply a similar approach to analyze neural language models (NLM), it is first necessary to establish that different models are similar enough in the generalizations they make. In this paper, we show that NLMs with different initialization, architecture, and training data acquire linguistic phenomena in a similar order, despite their different end performance. These findings suggest that there is some mutual inductive bias that underlies these models\u2019 learning of linguistic phenomena. Taking inspiration from psycholinguistics, we argue that studying this inductive bias is an opportunity to study the linguistic representation implicit in NLMs.Leveraging these findings, we compare the relative performance on different phenomena at varying learning stages with simpler reference models. Results suggest that NLMs exhibit consistent \u201cdevelopmental\u201d stages. Moreover, we find the learning trajectory to be approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired.Initial analysis of these stages presents phenomena clusters (notably morphological ones), whose performance progresses in unison, suggesting a potential link between the generalizations behind them.",
            "corpus_id": 237491997,
            "sentences": [
                {
                    "corpus_id": "237491997",
                    "title": "The Grammar-Learning Trajectories of Neural Language Models",
                    "text": "We begin ( \u00a73) by establishing that NLMs learn grammatical phenomena in a consistent order. We evaluate NLMs at different time points along their training, showing that the performance on linguis- tic phenomena across initializations is highly correlated. We further find many similarities in the set of examples that they correctly classify.\n\nStill, models of different architectures learn at a different pace, and hence cannot be directly compared at identical time points. In \u00a73.3, we overcome this by re-scaling the timeline. We then show that despite architectural differences, NLMs present highly correlated performance trajectories. In \u00a73.4, we further demonstrate that even the choice of training data has minor influence on the results. Finally, in \u00a73.5 we show that the learning dynamics essentially follows a single dimension. Namely, where the average performance is similar, success on linguistic phenomena is also similar.\n\nWe proceed by analyzing the early stages of learning in \u00a74. We find that, at first, NLMs rely mostly on local cues and not on word order. They thus resemble bag-of-words models over a window of the preceding tokens. Later stages seem to drift further away from bag-of-words models toward n-gram models, and with time seem to be more sensitive to structural cues. We also find evidence that some latent features that the model learns may not be related to linguistic phenomena.\n\nFinally, in \u00a75 we take the first steps in categorizing linguistic phenomena by their learning trajectories. We identify links between their representations by finding phenomena that progress in unison. For example, we find that morphological phenomena are mostly learned at similar stages. Of particular interest are cases where performance decreases with time, which may suggest either overgeneralization or biases in the BLIMP challenges.",
                    "score": 0.3774206056549347,
                    "section_title": "Introduction",
                    "char_start_offset": 2063,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99267578125
                }
            ],
            "relevance_judgement": 0.99267578125,
            "relevance_judgment_input_expanded": "# Title: The Grammar-Learning Trajectories of Neural Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Leshem Choshen, Guy Hacohen, D. Weinshall, Omri Abend\n## Abstract\nThe learning trajectories of linguistic phenomena in humans provide insight into linguistic representation, beyond what can be gleaned from inspecting the behavior of an adult speaker. To apply a similar approach to analyze neural language models (NLM), it is first necessary to establish that different models are similar enough in the generalizations they make. In this paper, we show that NLMs with different initialization, architecture, and training data acquire linguistic phenomena in a similar order, despite their different end performance. These findings suggest that there is some mutual inductive bias that underlies these models\u2019 learning of linguistic phenomena. Taking inspiration from psycholinguistics, we argue that studying this inductive bias is an opportunity to study the linguistic representation implicit in NLMs.Leveraging these findings, we compare the relative performance on different phenomena at varying learning stages with simpler reference models. Results suggest that NLMs exhibit consistent \u201cdevelopmental\u201d stages. Moreover, we find the learning trajectory to be approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired.Initial analysis of these stages presents phenomena clusters (notably morphological ones), whose performance progresses in unison, suggesting a potential link between the generalizations behind them.\n## Introduction\nWe begin ( \u00a73) by establishing that NLMs learn grammatical phenomena in a consistent order. We evaluate NLMs at different time points along their training, showing that the performance on linguis- tic phenomena across initializations is highly correlated. We further find many similarities in the set of examples that they correctly classify.\n\nStill, models of different architectures learn at a different pace, and hence cannot be directly compared at identical time points. In \u00a73.3, we overcome this by re-scaling the timeline. We then show that despite architectural differences, NLMs present highly correlated performance trajectories. In \u00a73.4, we further demonstrate that even the choice of training data has minor influence on the results. Finally, in \u00a73.5 we show that the learning dynamics essentially follows a single dimension. Namely, where the average performance is similar, success on linguistic phenomena is also similar.\n\nWe proceed by analyzing the early stages of learning in \u00a74. We find that, at first, NLMs rely mostly on local cues and not on word order. They thus resemble bag-of-words models over a window of the preceding tokens. Later stages seem to drift further away from bag-of-words models toward n-gram models, and with time seem to be more sensitive to structural cues. We also find evidence that some latent features that the model learns may not be related to linguistic phenomena.\n\nFinally, in \u00a75 we take the first steps in categorizing linguistic phenomena by their learning trajectories. We identify links between their representations by finding phenomena that progress in unison. For example, we find that morphological phenomena are mostly learned at similar stages. Of particular interest are cases where performance decreases with time, which may suggest either overgeneralization or biases in the BLIMP challenges.",
            "reference_string": "[237491997 | Choshen et al. | 2021 | Citations: 29]"
        },
        {
            "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
            "venue": "International Conference on Machine Learning",
            "year": 2023,
            "reference_count": 101,
            "citation_count": 1306,
            "influential_citation_count": 189,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.01373",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.01373, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "103476203",
                    "name": "Stella Biderman"
                },
                {
                    "authorId": "2184031883",
                    "name": "Hailey Schoelkopf"
                },
                {
                    "authorId": "1404060481",
                    "name": "Quentin G. Anthony"
                },
                {
                    "authorId": "2070768742",
                    "name": "Herbie Bradley"
                },
                {
                    "authorId": "2212970046",
                    "name": "Kyle O'Brien"
                },
                {
                    "authorId": "2162462983",
                    "name": "Eric Hallahan"
                },
                {
                    "authorId": "2168771748",
                    "name": "Mohammad Aflah Khan"
                },
                {
                    "authorId": "2162467233",
                    "name": "Shivanshu Purohit"
                },
                {
                    "authorId": "2162462141",
                    "name": "USVSN Sai Prashanth"
                },
                {
                    "authorId": "34885007",
                    "name": "Edward Raff"
                },
                {
                    "authorId": "2213349418",
                    "name": "Aviya Skowron"
                },
                {
                    "authorId": "35566806",
                    "name": "Lintang Sutawika"
                },
                {
                    "authorId": "1986356851",
                    "name": "Oskar van der Wal"
                }
            ],
            "abstract": "How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\url{https://github.com/EleutherAI/pythia}.",
            "corpus_id": 257921893,
            "sentences": [
                {
                    "corpus_id": "257921893",
                    "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
                    "text": "How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\url{https://github.com/EleutherAI/pythia}.",
                    "score": 0.375186923640818,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99267578125
                }
            ],
            "relevance_judgement": 0.99267578125,
            "relevance_judgment_input_expanded": "# Title: Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling\n# Venue: International Conference on Machine Learning\n# Authors: Stella Biderman, Hailey Schoelkopf, Quentin G. Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal\n## Abstract\nHow do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\url{https://github.com/EleutherAI/pythia}.\n",
            "reference_string": "[257921893 | Biderman et al. | 2023 | Citations: 1306]"
        },
        {
            "title": "Rho-1: Not All Tokens Are What You Need",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 118,
            "citation_count": 75,
            "influential_citation_count": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.07965, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "31113759",
                    "name": "Zheng-Wen Lin"
                },
                {
                    "authorId": "1797090",
                    "name": "Zhibin Gou"
                },
                {
                    "authorId": "2254121650",
                    "name": "Yeyun Gong"
                },
                {
                    "authorId": "49544272",
                    "name": "Xiao Liu"
                },
                {
                    "authorId": "2237948786",
                    "name": "Yelong Shen"
                },
                {
                    "authorId": "2266367743",
                    "name": "Ruochen Xu"
                },
                {
                    "authorId": "2269773814",
                    "name": "Chen Lin"
                },
                {
                    "authorId": "2284727148",
                    "name": "Yujiu Yang"
                },
                {
                    "authorId": "2143968416",
                    "name": "Jian Jiao"
                },
                {
                    "authorId": "2269471632",
                    "name": "Nan Duan"
                },
                {
                    "authorId": "2249538838",
                    "name": "Weizhu Chen"
                }
            ],
            "abstract": "Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that\"9l training\". Our initial analysis examines token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called Rho-1. Unlike traditional LMs that learn to predict every next token in a corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring pretraining tokens using a reference model, and then training the language model with a focused loss on tokens with higher scores. When continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks. After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the pretraining tokens. Furthermore, when continual pretraining on 80B general tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both efficiency and performance of the language model pre-training.",
            "corpus_id": 269042762,
            "sentences": [
                {
                    "corpus_id": "269042762",
                    "title": "Rho-1: Not All Tokens Are What You Need",
                    "text": "Investigating the training dynamics of language models is essential for understanding their behavior throughout the training process. This research includes studying internal representations [Saphra and Lopez, 2018], the acquisition of linguistic knowledge [Choshen et al., 2021, Liu et al., 2021], and the phenomenon of grokking [Power et al., 2022]. The analysis by Xia et al. [2022] is the most related to ours, which examines token-level training trajectories in models of varying sizes. Our findings, however, diverge from those of Xia et al. [2022], who posit that tokens with little change in perplexity are \"already learned\". We identify a spectrum of token patterns, including \"easy tokens\" and \"hard tokens\" that resist convergence. Recognizing this, we propose a method of selective language modeling that targets the influential tokens, optimizing the learning process.",
                    "score": 0.5576720725638022,
                    "section_title": "B.3 Language Model Training Dynamics",
                    "char_start_offset": 26780,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 351
                        },
                        {
                            "start": 352,
                            "end": 491
                        },
                        {
                            "start": 492,
                            "end": 633
                        },
                        {
                            "start": 634,
                            "end": 742
                        },
                        {
                            "start": 743,
                            "end": 881
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99169921875
                },
                {
                    "corpus_id": "269042762",
                    "title": "Rho-1: Not All Tokens Are What You Need",
                    "text": "To explore how language models learn at the token level, we initially examined training dynamics, particularly how the token-level loss evolves during usual pretraining. In \u00a72.1, we evaluated the model's token perplexity at different checkpoints and categorized tokens into different types. Our findings reveal that significant loss reduction is limited to a select group of tokens. Many tokens are \"easy tokens\" that are already learned, and some are \"hard tokens\" that exhibit variable losses and resist convergence. These tokens can lead to numerous ineffective gradient updates. \n\nBased on these analyses, we introduce RHO-1 models trained with a novel Selective Language Modeling (SLM) objective3 . As shown in Figure 2 (Right), this approach inputs the full sequence into the model and selectively removes the loss of undesired tokens. The detailed pipeline is depicted in Figure 4: First, SLM trains a reference language model on high-quality corpora. This model establishes utility metrics to score tokens according to the desired distribution, naturally filtering out unclean and irrelevant tokens. Second, SLM uses the reference model to score each token in a corpus using its loss ( \u00a72.2). Finally, we train a language model only on those tokens that exhibit a high excess loss between the reference and the training model, selectively learning the tokens that best benefit downstream applications ( \u00a72.2). \n\nWe show through comprehensive experiments that SLM significantly enhances token efficiency during training and improves performance on downstream tasks. Furthermore, our findings indicate that SLM effectively identifies tokens relevant to the target distribution, resulting in improved perplexity scores on benchmarks for models trained with the selected tokens. \u00a73.2 shows the effectiveness of SLM on math continual pretraining: both 1B and 7B RHO-1 outperform CLM-trained baselines by over 16% on the GSM8k and MATH datasets. SLM reaches baseline accuracy up to 10x faster, as shown in Figure 1. Remarkably, RHO-1-7B matches the state-of-the-art performance of DeepSeekMath-7B using only 15B tokens, compared to the 500B tokens required by DeepSeekMath.",
                    "score": 0.37741877661636886,
                    "section_title": "body",
                    "char_start_offset": 1945,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 169
                        },
                        {
                            "start": 170,
                            "end": 290
                        },
                        {
                            "start": 291,
                            "end": 382
                        },
                        {
                            "start": 383,
                            "end": 518
                        },
                        {
                            "start": 519,
                            "end": 582
                        },
                        {
                            "start": 585,
                            "end": 703
                        },
                        {
                            "start": 704,
                            "end": 841
                        },
                        {
                            "start": 842,
                            "end": 958
                        },
                        {
                            "start": 959,
                            "end": 1107
                        },
                        {
                            "start": 1108,
                            "end": 1200
                        },
                        {
                            "start": 1201,
                            "end": 1417
                        },
                        {
                            "start": 1420,
                            "end": 1572
                        },
                        {
                            "start": 1573,
                            "end": 1782
                        },
                        {
                            "start": 1783,
                            "end": 1947
                        },
                        {
                            "start": 1948,
                            "end": 2017
                        },
                        {
                            "start": 2018,
                            "end": 2175
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98486328125
                }
            ],
            "relevance_judgement": 0.99169921875,
            "relevance_judgment_input_expanded": "# Title: Rho-1: Not All Tokens Are What You Need\n# Venue: arXiv.org\n# Authors: Zheng-Wen Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, Weizhu Chen\n## Abstract\nPrevious language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that\"9l training\". Our initial analysis examines token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called Rho-1. Unlike traditional LMs that learn to predict every next token in a corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring pretraining tokens using a reference model, and then training the language model with a focused loss on tokens with higher scores. When continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks. After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the pretraining tokens. Furthermore, when continual pretraining on 80B general tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both efficiency and performance of the language model pre-training.\n## body\nTo explore how language models learn at the token level, we initially examined training dynamics, particularly how the token-level loss evolves during usual pretraining. In \u00a72.1, we evaluated the model's token perplexity at different checkpoints and categorized tokens into different types. Our findings reveal that significant loss reduction is limited to a select group of tokens. Many tokens are \"easy tokens\" that are already learned, and some are \"hard tokens\" that exhibit variable losses and resist convergence. These tokens can lead to numerous ineffective gradient updates. \n\nBased on these analyses, we introduce RHO-1 models trained with a novel Selective Language Modeling (SLM) objective3 . As shown in Figure 2 (Right), this approach inputs the full sequence into the model and selectively removes the loss of undesired tokens. The detailed pipeline is depicted in Figure 4: First, SLM trains a reference language model on high-quality corpora. This model establishes utility metrics to score tokens according to the desired distribution, naturally filtering out unclean and irrelevant tokens. Second, SLM uses the reference model to score each token in a corpus using its loss ( \u00a72.2). Finally, we train a language model only on those tokens that exhibit a high excess loss between the reference and the training model, selectively learning the tokens that best benefit downstream applications ( \u00a72.2). \n\nWe show through comprehensive experiments that SLM significantly enhances token efficiency during training and improves performance on downstream tasks. Furthermore, our findings indicate that SLM effectively identifies tokens relevant to the target distribution, resulting in improved perplexity scores on benchmarks for models trained with the selected tokens. \u00a73.2 shows the effectiveness of SLM on math continual pretraining: both 1B and 7B RHO-1 outperform CLM-trained baselines by over 16% on the GSM8k and MATH datasets. SLM reaches baseline accuracy up to 10x faster, as shown in Figure 1. Remarkably, RHO-1-7B matches the state-of-the-art performance of DeepSeekMath-7B using only 15B tokens, compared to the 500B tokens required by DeepSeekMath.\n\n## B.3 Language Model Training Dynamics\nInvestigating the training dynamics of language models is essential for understanding their behavior throughout the training process. This research includes studying internal representations [Saphra and Lopez, 2018], the acquisition of linguistic knowledge [Choshen et al., 2021, Liu et al., 2021], and the phenomenon of grokking [Power et al., 2022]. The analysis by Xia et al. [2022] is the most related to ours, which examines token-level training trajectories in models of varying sizes. Our findings, however, diverge from those of Xia et al. [2022], who posit that tokens with little change in perplexity are \"already learned\". We identify a spectrum of token patterns, including \"easy tokens\" and \"hard tokens\" that resist convergence. Recognizing this, we propose a method of selective language modeling that targets the influential tokens, optimizing the learning process.",
            "reference_string": "[269042762 | Lin et al. | 2024 | Citations: 75]"
        },
        {
            "title": "The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.01204, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2345264293",
                    "name": "Chenghao Yang"
                },
                {
                    "authorId": "2294388803",
                    "name": "Junzhuo Li"
                },
                {
                    "authorId": "2290184043",
                    "name": "Xinyao Niu"
                },
                {
                    "authorId": "2279346001",
                    "name": "Xinrun Du"
                },
                {
                    "authorId": "2294382707",
                    "name": "Songyang Gao"
                },
                {
                    "authorId": "2281020035",
                    "name": "Haoran Zhang"
                },
                {
                    "authorId": "2294810112",
                    "name": "Zhaoliang Chen"
                },
                {
                    "authorId": "2239104064",
                    "name": "Xingwei Qu"
                },
                {
                    "authorId": "2032236274",
                    "name": "Ruibin Yuan"
                },
                {
                    "authorId": "2129449392",
                    "name": "Yizhi Li"
                },
                {
                    "authorId": "2294523552",
                    "name": "Jiaheng Liu"
                },
                {
                    "authorId": "2283188391",
                    "name": "Stephen W. Huang"
                },
                {
                    "authorId": "2282979169",
                    "name": "Shawn Yue"
                },
                {
                    "authorId": "2249847177",
                    "name": "Wenhu Chen"
                },
                {
                    "authorId": "2265967208",
                    "name": "Jie Fu"
                },
                {
                    "authorId": "2143853895",
                    "name": "Ge Zhang"
                }
            ],
            "abstract": "Uncovering early-stage metrics that reflect final model performance is one core principle for large-scale pretraining. The existing scaling law demonstrates the power-law correlation between pretraining loss and training flops, which serves as an important indicator of the current training state for large language models. However, this principle only focuses on the model's compression properties on the training data, resulting in an inconsistency with the ability improvements on the downstream tasks. Some follow-up works attempted to extend the scaling-law to more complex metrics (such as hyperparameters), but still lacked a comprehensive analysis of the dynamic differences among various capabilities during pretraining. To address the aforementioned limitations, this paper undertakes a comprehensive comparison of model capabilities at various pretraining intermediate checkpoints. Through this analysis, we confirm that specific downstream metrics exhibit similar training dynamics across models of different sizes, up to 67 billion parameters. In addition to our core findings, we've reproduced Amber and OpenLLaMA, releasing their intermediate checkpoints. This initiative offers valuable resources to the research community and facilitates the verification and exploration of LLM pretraining by open-source researchers. Besides, we provide empirical summaries, including performance comparisons of different models and capabilities, and tuition of key metrics for different training phases. Based on these findings, we provide a more user-friendly strategy for evaluating the optimization state, offering guidance for establishing a stable pretraining process.",
            "corpus_id": 268820276,
            "sentences": [
                {
                    "corpus_id": "268820276",
                    "title": "The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis",
                    "text": "This investigation into large language models (LLMs) enhances our understanding of model training complexities and scaling laws. Analyzing model dynamics across various tasks and training stages, we have gleaned insights to improve training and optimization strategies. \n\nOur study reveals the predictive power of task dynamics within a domain for unseen tasks, suggesting the benefit of adaptable training protocols. Drawing parallels between AI learning and human cognition, we see potential in applying cross-domain insights for better training outcomes.",
                    "score": 0.545375491425341,
                    "section_title": "Conclusion",
                    "char_start_offset": 21179,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 269
                        },
                        {
                            "start": 272,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 557
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99169921875
                },
                {
                    "corpus_id": "268820276",
                    "title": "The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis",
                    "text": "In this work, we first investigate the dynamics of several open-sourced large language models (i.e., Baichuan-7B (Yang et al., 2023), DeepSeek-7B (DeepSeek-AI et al., 2024), Amber-7B (Liu et al., 2023), OpenLLaMA-7B (Geng & Liu, 2023;Touvron et al., 2023;Computer, 2023), Yi-34B (AI et al., 2024) and DeepSeek-67B), and analyze their performance results across diverse tasks using the corresponding intermediate checkpoints based on the number of pre-trained tokens. Then, based on the theoretical framework of the Scaling Law (Kaplan et al., 2020), we analyze the performance patterns of different models across various downstream tasks, and provide findings to facilitate further research on the training dynamics of the LLMs, where the findings are shown as follows: \n\n\u2022 Findings on task dynamic prediction: Within the training process of LLMs, we observe that the dynamics of existing downstream tasks within a domain can predict the dynamics of unseen tasks. This suggests that a model's performance on known tasks can inform us about how it might perform on similar, yet unseen tasks within the same domain. (Section 4.1.1) \n\n\u2022 Findings on cross-domain promotion: Similar to the human cognitive process, the enhancement of different abilities across various domains progresses from basic to advanced levels following curriculum learning. The curriculum between crossdomain tasks can guide the training direction of models and the insights gained from one domain can potentially promote the learning process of other domains. (Section 4.1.2) \n\n\u2022 Findings on the effect of training strategies, model architecture, etc. : Based on the results of several 7b-scale models (Baichuan2-7b, DeepSeek-7b, OpenLLaMA-7b, and Amber-7b), we comprehensively analyze the effect of training strategies, dataset quality, and model architecture in training LLMs. For example, we observe that the training dataset, learning rate adjustments, batch size, and regularization techniques play a significant role in the learning efficiency in the early training stage (Section 4.2.1).",
                    "score": 0.46013654483073024,
                    "section_title": "Preprint",
                    "char_start_offset": 968,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 466
                        },
                        {
                            "start": 467,
                            "end": 769
                        },
                        {
                            "start": 772,
                            "end": 963
                        },
                        {
                            "start": 964,
                            "end": 1113
                        },
                        {
                            "start": 1114,
                            "end": 1129
                        },
                        {
                            "start": 1132,
                            "end": 1343
                        },
                        {
                            "start": 1344,
                            "end": 1530
                        },
                        {
                            "start": 1531,
                            "end": 1546
                        },
                        {
                            "start": 1549,
                            "end": 1849
                        },
                        {
                            "start": 1850,
                            "end": 2065
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99169921875
                },
                {
                    "corpus_id": "268820276",
                    "title": "The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis",
                    "text": "Uncovering early-stage metrics that reflect final model performance is one core principle for large-scale pretraining. The existing scaling law demonstrates the power-law correlation between pretraining loss and training flops, which serves as an important indicator of the current training state for large language models. However, this principle only focuses on the model's compression properties on the training data, resulting in an inconsistency with the ability improvements on the downstream tasks. Some follow-up works attempted to extend the scaling-law to more complex metrics (such as hyperparameters), but still lacked a comprehensive analysis of the dynamic differences among various capabilities during pretraining. To address the aforementioned limitations, this paper undertakes a comprehensive comparison of model capabilities at various pretraining intermediate checkpoints. Through this analysis, we confirm that specific downstream metrics exhibit similar training dynamics across models of different sizes, up to 67 billion parameters. In addition to our core findings, we've reproduced Amber and OpenLLaMA, releasing their intermediate checkpoints. This initiative offers valuable resources to the research community and facilitates the verification and exploration of LLM pretraining by open-source researchers. Besides, we provide empirical summaries, including performance comparisons of different models and capabilities, and tuition of key metrics for different training phases. Based on these findings, we provide a more user-friendly strategy for evaluating the optimization state, offering guidance for establishing a stable pretraining process.",
                    "score": 0.3904934077150004,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98828125
                }
            ],
            "relevance_judgement": 0.99169921875,
            "relevance_judgment_input_expanded": "# Title: The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis\n# Venue: arXiv.org\n# Authors: Chenghao Yang, Junzhuo Li, Xinyao Niu, Xinrun Du, Songyang Gao, Haoran Zhang, Zhaoliang Chen, Xingwei Qu, Ruibin Yuan, Yizhi Li, Jiaheng Liu, Stephen W. Huang, Shawn Yue, Wenhu Chen, Jie Fu, Ge Zhang\n## Abstract\nUncovering early-stage metrics that reflect final model performance is one core principle for large-scale pretraining. The existing scaling law demonstrates the power-law correlation between pretraining loss and training flops, which serves as an important indicator of the current training state for large language models. However, this principle only focuses on the model's compression properties on the training data, resulting in an inconsistency with the ability improvements on the downstream tasks. Some follow-up works attempted to extend the scaling-law to more complex metrics (such as hyperparameters), but still lacked a comprehensive analysis of the dynamic differences among various capabilities during pretraining. To address the aforementioned limitations, this paper undertakes a comprehensive comparison of model capabilities at various pretraining intermediate checkpoints. Through this analysis, we confirm that specific downstream metrics exhibit similar training dynamics across models of different sizes, up to 67 billion parameters. In addition to our core findings, we've reproduced Amber and OpenLLaMA, releasing their intermediate checkpoints. This initiative offers valuable resources to the research community and facilitates the verification and exploration of LLM pretraining by open-source researchers. Besides, we provide empirical summaries, including performance comparisons of different models and capabilities, and tuition of key metrics for different training phases. Based on these findings, we provide a more user-friendly strategy for evaluating the optimization state, offering guidance for establishing a stable pretraining process.\n## Preprint\nIn this work, we first investigate the dynamics of several open-sourced large language models (i.e., Baichuan-7B (Yang et al., 2023), DeepSeek-7B (DeepSeek-AI et al., 2024), Amber-7B (Liu et al., 2023), OpenLLaMA-7B (Geng & Liu, 2023;Touvron et al., 2023;Computer, 2023), Yi-34B (AI et al., 2024) and DeepSeek-67B), and analyze their performance results across diverse tasks using the corresponding intermediate checkpoints based on the number of pre-trained tokens. Then, based on the theoretical framework of the Scaling Law (Kaplan et al., 2020), we analyze the performance patterns of different models across various downstream tasks, and provide findings to facilitate further research on the training dynamics of the LLMs, where the findings are shown as follows: \n\n\u2022 Findings on task dynamic prediction: Within the training process of LLMs, we observe that the dynamics of existing downstream tasks within a domain can predict the dynamics of unseen tasks. This suggests that a model's performance on known tasks can inform us about how it might perform on similar, yet unseen tasks within the same domain. (Section 4.1.1) \n\n\u2022 Findings on cross-domain promotion: Similar to the human cognitive process, the enhancement of different abilities across various domains progresses from basic to advanced levels following curriculum learning. The curriculum between crossdomain tasks can guide the training direction of models and the insights gained from one domain can potentially promote the learning process of other domains. (Section 4.1.2) \n\n\u2022 Findings on the effect of training strategies, model architecture, etc. : Based on the results of several 7b-scale models (Baichuan2-7b, DeepSeek-7b, OpenLLaMA-7b, and Amber-7b), we comprehensively analyze the effect of training strategies, dataset quality, and model architecture in training LLMs. For example, we observe that the training dataset, learning rate adjustments, batch size, and regularization techniques play a significant role in the learning efficiency in the early training stage (Section 4.2.1).\n\n## Conclusion\nThis investigation into large language models (LLMs) enhances our understanding of model training complexities and scaling laws. Analyzing model dynamics across various tasks and training stages, we have gleaned insights to improve training and optimization strategies. \n\nOur study reveals the predictive power of task dynamics within a domain for unseen tasks, suggesting the benefit of adaptable training protocols. Drawing parallels between AI learning and human cognition, we see potential in applying cross-domain insights for better training outcomes.",
            "reference_string": "[268820276 | Yang et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 94,
            "citation_count": 74,
            "influential_citation_count": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.07311",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.07311, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "13336152",
                    "name": "Angelica Chen"
                },
                {
                    "authorId": "2240524527",
                    "name": "Ravid Schwartz-Ziv"
                },
                {
                    "authorId": "1979489",
                    "name": "Kyunghyun Cho"
                },
                {
                    "authorId": "2240527814",
                    "name": "Matthew L. Leavitt"
                },
                {
                    "authorId": "2362960",
                    "name": "Naomi Saphra"
                }
            ],
            "abstract": "Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by manipulating SAS during training, and demonstrate that SAS is necessary for the development of grammatical capabilities. We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality. These findings offer an interpretation of a real-world example of both simplicity bias and breakthrough training dynamics.",
            "corpus_id": 261822542,
            "sentences": [
                {
                    "corpus_id": "261822542",
                    "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
                    "text": "While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2023), or phase transitions (Olsson et al., 2022). The interpretability literature rarely illuminates how these capabilities emerge, in part because most analyses only examine the final trained model. Instead, we consider developmental analysis as a complementary explanatory lens. \n\nTo better understand the role of interpretable artifacts in model development, we analyze and manipulate these artifacts during training. We focus on a case study of Syntactic Attention Structure (SAS), a model behavior thought to relate to grammatical structure. By measuring and controlling the emergence of SAS, we deepen our understanding of the relationship between the internal structural traits and extrinsic capabilities of masked language models (MLMs). \n\nSAS occurs when a model learns specialized attention heads that focus on a word's syntactic neighbors. This behavior emerges naturally during conventional MLM pre-training (Clark et al., 2019;Voita et al., 2019;Manning et al., 2020). We observe an abrupt spike in SAS at a consistent point in training, and explore its impact on MLM capabilities by manipulating SAS during training. Our observations paint a picture of how interpretability artifacts may represent simplicity biases that compete with other learning strategies during MLM training. In summary, our main contributions are: \n\n\u2022 Monitoring latent syntactic structure (defined in Section 2.1) throughout training, we identify (Section 4.1) a precipitous loss drop composed of multiple phase transitions (defined in Section 2.3) relating to various linguistic abilities. At the onset of this stage (which we call the structure onset), SAS spikes.",
                    "score": 0.4524773919435431,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 144
                        },
                        {
                            "start": 145,
                            "end": 233
                        },
                        {
                            "start": 234,
                            "end": 381
                        },
                        {
                            "start": 382,
                            "end": 657
                        },
                        {
                            "start": 658,
                            "end": 807
                        },
                        {
                            "start": 808,
                            "end": 888
                        },
                        {
                            "start": 891,
                            "end": 1028
                        },
                        {
                            "start": 1029,
                            "end": 1154
                        },
                        {
                            "start": 1155,
                            "end": 1353
                        },
                        {
                            "start": 1356,
                            "end": 1458
                        },
                        {
                            "start": 1459,
                            "end": 1589
                        },
                        {
                            "start": 1590,
                            "end": 1738
                        },
                        {
                            "start": 1739,
                            "end": 1902
                        },
                        {
                            "start": 1903,
                            "end": 1942
                        },
                        {
                            "start": 1945,
                            "end": 2186
                        },
                        {
                            "start": 2187,
                            "end": 2262
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 588,
                            "end": 612,
                            "matchedPaperCorpusId": "253117181"
                        },
                        {
                            "start": 1567,
                            "end": 1588,
                            "matchedPaperCorpusId": "219315567"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99169921875
                }
            ],
            "relevance_judgement": 0.99169921875,
            "relevance_judgment_input_expanded": "# Title: Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs\n# Venue: International Conference on Learning Representations\n# Authors: Angelica Chen, Ravid Schwartz-Ziv, Kyunghyun Cho, Matthew L. Leavitt, Naomi Saphra\n## Abstract\nMost interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by manipulating SAS during training, and demonstrate that SAS is necessary for the development of grammatical capabilities. We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality. These findings offer an interpretation of a real-world example of both simplicity bias and breakthrough training dynamics.\n## INTRODUCTION\nWhile language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2023), or phase transitions (Olsson et al., 2022). The interpretability literature rarely illuminates how these capabilities emerge, in part because most analyses only examine the final trained model. Instead, we consider developmental analysis as a complementary explanatory lens. \n\nTo better understand the role of interpretable artifacts in model development, we analyze and manipulate these artifacts during training. We focus on a case study of Syntactic Attention Structure (SAS), a model behavior thought to relate to grammatical structure. By measuring and controlling the emergence of SAS, we deepen our understanding of the relationship between the internal structural traits and extrinsic capabilities of masked language models (MLMs). \n\nSAS occurs when a model learns specialized attention heads that focus on a word's syntactic neighbors. This behavior emerges naturally during conventional MLM pre-training (Clark et al., 2019;Voita et al., 2019;Manning et al., 2020). We observe an abrupt spike in SAS at a consistent point in training, and explore its impact on MLM capabilities by manipulating SAS during training. Our observations paint a picture of how interpretability artifacts may represent simplicity biases that compete with other learning strategies during MLM training. In summary, our main contributions are: \n\n\u2022 Monitoring latent syntactic structure (defined in Section 2.1) throughout training, we identify (Section 4.1) a precipitous loss drop composed of multiple phase transitions (defined in Section 2.3) relating to various linguistic abilities. At the onset of this stage (which we call the structure onset), SAS spikes.",
            "reference_string": "[261822542 | Chen et al. | 2023 | Citations: 74]"
        },
        {
            "title": "An Analysis for Reasoning Bias of Language Models with Small Initialization",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 57,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.04375, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2277847328",
                    "name": "Junjie Yao"
                },
                {
                    "authorId": "1953073920",
                    "name": "Zhongwang Zhang"
                },
                {
                    "authorId": "2136355451",
                    "name": "Z. Xu"
                }
            ],
            "abstract": "Transformer-based Large Language Models (LLMs) have revolutionized Natural Language Processing by demonstrating exceptional performance across diverse tasks. This study investigates the impact of the parameter initialization scale on the training behavior and task preferences of LLMs. We discover that smaller initialization scales encourage models to favor reasoning tasks, whereas larger initialization scales lead to a preference for memorization tasks. We validate this reasoning bias via real datasets and meticulously designed anchor functions. Further analysis of initial training dynamics suggests that specific model components, particularly the embedding space and self-attention mechanisms, play pivotal roles in shaping these learning biases. We provide a theoretical framework from the perspective of model training dynamics to explain these phenomena. Additionally, experiments on real-world language tasks corroborate our theoretical insights. This work enhances our understanding of how initialization strategies influence LLM performance on reasoning tasks and offers valuable guidelines for training models.",
            "corpus_id": 276235362,
            "sentences": [
                {
                    "corpus_id": "276235362",
                    "title": "An Analysis for Reasoning Bias of Language Models with Small Initialization",
                    "text": "Transformer-based Large Language Models (LLMs) have revolutionized Natural Language Processing by demonstrating exceptional performance across diverse tasks. This study investigates the impact of the parameter initialization scale on the training behavior and task preferences of LLMs. We discover that smaller initialization scales encourage models to favor reasoning tasks, whereas larger initialization scales lead to a preference for memorization tasks. We validate this reasoning bias via real datasets and meticulously designed anchor functions. Further analysis of initial training dynamics suggests that specific model components, particularly the embedding space and self-attention mechanisms, play pivotal roles in shaping these learning biases. We provide a theoretical framework from the perspective of model training dynamics to explain these phenomena. Additionally, experiments on real-world language tasks corroborate our theoretical insights. This work enhances our understanding of how initialization strategies influence LLM performance on reasoning tasks and offers valuable guidelines for training models.",
                    "score": 0.3873542107335443,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99072265625
                }
            ],
            "relevance_judgement": 0.99072265625,
            "relevance_judgment_input_expanded": "# Title: An Analysis for Reasoning Bias of Language Models with Small Initialization\n# Venue: arXiv.org\n# Authors: Junjie Yao, Zhongwang Zhang, Z. Xu\n## Abstract\nTransformer-based Large Language Models (LLMs) have revolutionized Natural Language Processing by demonstrating exceptional performance across diverse tasks. This study investigates the impact of the parameter initialization scale on the training behavior and task preferences of LLMs. We discover that smaller initialization scales encourage models to favor reasoning tasks, whereas larger initialization scales lead to a preference for memorization tasks. We validate this reasoning bias via real datasets and meticulously designed anchor functions. Further analysis of initial training dynamics suggests that specific model components, particularly the embedding space and self-attention mechanisms, play pivotal roles in shaping these learning biases. We provide a theoretical framework from the perspective of model training dynamics to explain these phenomena. Additionally, experiments on real-world language tasks corroborate our theoretical insights. This work enhances our understanding of how initialization strategies influence LLM performance on reasoning tasks and offers valuable guidelines for training models.\n",
            "reference_string": "[276235362 | Yao et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Scaling Law with Learning Rate Annealing",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 34,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.11029, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316486693",
                    "name": "Howe Tissue"
                },
                {
                    "authorId": "2316485233",
                    "name": "Venus Wang"
                },
                {
                    "authorId": "2316501479",
                    "name": "Lu Wang"
                }
            ],
            "abstract": "We find that the cross-entropy loss curves of neural language models empirically adhere to a scaling law with learning rate (LR) annealing over training steps: $$L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C\\cdot S_2,$$ where $L(s)$ is the validation loss at step $s$, $S_1$ is the area under the LR curve, $S_2$ is the LR annealing area, and $L_0$, $A$, $C$, $\\alpha$ are constant parameters. This formulation takes into account two factors: (1) power-law scaling over data size, and (2) the additional loss reduction during LR annealing. Therefore, this formulation can describe the full loss curve at each step, rather than the single loss point at the end of training. Applying the scaling law with LR annealing and fitting only one or two training curves, we can accurately predict the loss at any given step across any learning rate scheduler (LRS). This approach significantly reduces computational cost in formulating scaling laws while providing more accuracy and expressiveness for training dynamics. Extensive experiments demonstrate that our findings hold across a range of hyper-parameters and model architectures, and our equation can extend to scaling effect of model sizes. Moreover, our formulation provides accurate theoretical verification and explanation for empirical results observed in numerous previous studies, particularly those focusing on LR schedule and annealing. We believe that this work is promising to enhance the understanding of LLM training dynamics while greatly democratizing scaling laws, and it can guide researchers in refining training strategies (e.g. critical LRS) for further LLMs.",
            "corpus_id": 271909320,
            "sentences": [
                {
                    "corpus_id": "271909320",
                    "title": "Scaling Law with Learning Rate Annealing",
                    "text": "We apply our proposed formulation to validate and provide a theoretical explanation for numerous existing experimental findings regarding the training dynamics of language models. These key insights also guide researchers in selecting critical LRS before initiating model training. An interesting summary is that \n\nThe art of learning rate schedule lies in the delicate balancing act between forward area and annealing area.",
                    "score": 0.49062239613508524,
                    "section_title": "TAKEAWAYS: EXPERIMENTAL FINDINGS VERIFICATION AND EXPLANATION",
                    "char_start_offset": 20132,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 312
                        },
                        {
                            "start": 315,
                            "end": 424
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98828125
                }
            ],
            "relevance_judgement": 0.98828125,
            "relevance_judgment_input_expanded": "# Title: Scaling Law with Learning Rate Annealing\n# Venue: arXiv.org\n# Authors: Howe Tissue, Venus Wang, Lu Wang\n## Abstract\nWe find that the cross-entropy loss curves of neural language models empirically adhere to a scaling law with learning rate (LR) annealing over training steps: $$L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C\\cdot S_2,$$ where $L(s)$ is the validation loss at step $s$, $S_1$ is the area under the LR curve, $S_2$ is the LR annealing area, and $L_0$, $A$, $C$, $\\alpha$ are constant parameters. This formulation takes into account two factors: (1) power-law scaling over data size, and (2) the additional loss reduction during LR annealing. Therefore, this formulation can describe the full loss curve at each step, rather than the single loss point at the end of training. Applying the scaling law with LR annealing and fitting only one or two training curves, we can accurately predict the loss at any given step across any learning rate scheduler (LRS). This approach significantly reduces computational cost in formulating scaling laws while providing more accuracy and expressiveness for training dynamics. Extensive experiments demonstrate that our findings hold across a range of hyper-parameters and model architectures, and our equation can extend to scaling effect of model sizes. Moreover, our formulation provides accurate theoretical verification and explanation for empirical results observed in numerous previous studies, particularly those focusing on LR schedule and annealing. We believe that this work is promising to enhance the understanding of LLM training dynamics while greatly democratizing scaling laws, and it can guide researchers in refining training strategies (e.g. critical LRS) for further LLMs.\n## TAKEAWAYS: EXPERIMENTAL FINDINGS VERIFICATION AND EXPLANATION\nWe apply our proposed formulation to validate and provide a theoretical explanation for numerous existing experimental findings regarding the training dynamics of language models. These key insights also guide researchers in selecting critical LRS before initiating model training. An interesting summary is that \n\nThe art of learning rate schedule lies in the delicate balancing act between forward area and annealing area.",
            "reference_string": "[271909320 | Tissue et al. | 2024 | Citations: 9]"
        },
        {
            "title": "A Kernel-Based View of Language Model Fine-Tuning",
            "venue": "International Conference on Machine Learning",
            "year": 2022,
            "reference_count": 90,
            "citation_count": 69,
            "influential_citation_count": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.05643",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.05643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "49288855",
                    "name": "Sadhika Malladi"
                },
                {
                    "authorId": "2127066887",
                    "name": "Alexander Wettig"
                },
                {
                    "authorId": "2141026731",
                    "name": "Dingli Yu"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                },
                {
                    "authorId": "145563465",
                    "name": "Sanjeev Arora"
                }
            ],
            "abstract": "It has become standard to solve NLP tasks by fine-tuning pre-trained language models (LMs), especially in low-data settings. There is minimal theoretical understanding of empirical success, e.g., why fine-tuning a model with $10^8$ or more parameters on a couple dozen training points does not result in overfitting. We investigate whether the Neural Tangent Kernel (NTK) - which originated as a model to study the gradient descent dynamics of infinitely wide networks with suitable random initialization - describes fine-tuning of pre-trained LMs. This study was inspired by the decent performance of NTK for computer vision tasks (Wei et al., 2022). We extend the NTK formalism to Adam and use Tensor Programs (Yang, 2020) to characterize conditions under which the NTK lens may describe fine-tuning updates to pre-trained language models. Extensive experiments on 14 NLP tasks validate our theory and show that formulating the downstream task as a masked word prediction problem through prompting often induces kernel-based dynamics during fine-tuning. Finally, we use this kernel view to propose an explanation for the success of parameter-efficient subspace-based fine-tuning methods.",
            "corpus_id": 252815771,
            "sentences": [
                {
                    "corpus_id": "252815771",
                    "title": "A Kernel-Based View of Language Model Fine-Tuning",
                    "text": "We use NTKs to mathematically formalize the general intuition that fine-tuning pretrained language models to solve downstream tasks requires only a \"small change.\" We derive a new kernel to describe Adam training (Section 4) and we use it in Section 5 to show how prompt-based fine-tuning can exhibit kernel behavior. Extensive experiments in Section 6 on 14 NLU tasks demonstrate that including a meaningful prompt often causes FT to exhibit kernel behavior ( Figure 1) and that kernel dynamics describe prompt-based FT on tasks that the eNTK can solve (Section 6.2). We demonstrate one possible use of the kernel view to explain empirical phenomena by applying it to understand subspace-based fine-tuning methods (Section 7), and we note that the kernel has many mathematically useful properties that can aid design and study of alternate fine-tuning methods.\n\nOur work suggests that a kernel-based view of language model fine-tuning is plausible, but there are several limitations. First, our experiments are limited to few-shot classification tasks and a single masked language model with specific prompts. Extending to additional settings (e.g., increasing k) and models require significant computational cost because the eNTK is expensive to compute. The theoretical results also apply only to \"early-stage\" training with Adam, and it is not clear how well they can describe longer training schemes; concurrent work in Littwin & Yang (2023) suggests that the reduction of Adam to SignGD is crucial to observe kernel dynamics. Nevertheless, our work provides substantial empirical and theoretical evidence that fine-tuning can be analyzed in terms of kernel behavior.\n\nAs a future direction, one can use the kernel analog to study the inductive bias of FT, as was done for gradient descent from a random initialization in the past ( Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv. Yang, G., Hu, E. J., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., Ryder, N.,",
                    "score": 0.3781483152221908,
                    "section_title": "Conclusion",
                    "char_start_offset": 32551,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98779296875
                }
            ],
            "relevance_judgement": 0.98779296875,
            "relevance_judgment_input_expanded": "# Title: A Kernel-Based View of Language Model Fine-Tuning\n# Venue: International Conference on Machine Learning\n# Authors: Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, Sanjeev Arora\n## Abstract\nIt has become standard to solve NLP tasks by fine-tuning pre-trained language models (LMs), especially in low-data settings. There is minimal theoretical understanding of empirical success, e.g., why fine-tuning a model with $10^8$ or more parameters on a couple dozen training points does not result in overfitting. We investigate whether the Neural Tangent Kernel (NTK) - which originated as a model to study the gradient descent dynamics of infinitely wide networks with suitable random initialization - describes fine-tuning of pre-trained LMs. This study was inspired by the decent performance of NTK for computer vision tasks (Wei et al., 2022). We extend the NTK formalism to Adam and use Tensor Programs (Yang, 2020) to characterize conditions under which the NTK lens may describe fine-tuning updates to pre-trained language models. Extensive experiments on 14 NLP tasks validate our theory and show that formulating the downstream task as a masked word prediction problem through prompting often induces kernel-based dynamics during fine-tuning. Finally, we use this kernel view to propose an explanation for the success of parameter-efficient subspace-based fine-tuning methods.\n## Conclusion\nWe use NTKs to mathematically formalize the general intuition that fine-tuning pretrained language models to solve downstream tasks requires only a \"small change.\" We derive a new kernel to describe Adam training (Section 4) and we use it in Section 5 to show how prompt-based fine-tuning can exhibit kernel behavior. Extensive experiments in Section 6 on 14 NLU tasks demonstrate that including a meaningful prompt often causes FT to exhibit kernel behavior ( Figure 1) and that kernel dynamics describe prompt-based FT on tasks that the eNTK can solve (Section 6.2). We demonstrate one possible use of the kernel view to explain empirical phenomena by applying it to understand subspace-based fine-tuning methods (Section 7), and we note that the kernel has many mathematically useful properties that can aid design and study of alternate fine-tuning methods.\n\nOur work suggests that a kernel-based view of language model fine-tuning is plausible, but there are several limitations. First, our experiments are limited to few-shot classification tasks and a single masked language model with specific prompts. Extending to additional settings (e.g., increasing k) and models require significant computational cost because the eNTK is expensive to compute. The theoretical results also apply only to \"early-stage\" training with Adam, and it is not clear how well they can describe longer training schemes; concurrent work in Littwin & Yang (2023) suggests that the reduction of Adam to SignGD is crucial to observe kernel dynamics. Nevertheless, our work provides substantial empirical and theoretical evidence that fine-tuning can be analyzed in terms of kernel behavior.\n\nAs a future direction, one can use the kernel analog to study the inductive bias of FT, as was done for gradient descent from a random initialization in the past ( Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv. Yang, G., Hu, E. J., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., Ryder, N.,",
            "reference_string": "[252815771 | Malladi et al. | 2022 | Citations: 69]"
        },
        {
            "title": "Unveiling Key Aspects of Fine-Tuning in Sentence Embeddings: A Representation Rank Analysis",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.11297, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2053481633",
                    "name": "Euna Jung"
                },
                {
                    "authorId": "2157223778",
                    "name": "Jaeill Kim"
                },
                {
                    "authorId": "2244228799",
                    "name": "Jungmin Ko"
                },
                {
                    "authorId": "2302470951",
                    "name": "Jinwoo Park"
                },
                {
                    "authorId": "2292408696",
                    "name": "Wonjong Rhee"
                }
            ],
            "abstract": "The latest advancements in unsupervised learning of sentence embeddings predominantly involve employing contrastive learning-based (CL-based) fine-tuning over pre-trained language models. In this study, we analyze the latest sentence embedding methods by adopting representation rank as the primary tool of analysis. We first define Phase 1 and Phase 2 of fine-tuning based on when representation rank peaks. Utilizing these phases, we conduct a thorough analysis and obtain essential findings across key aspects, including alignment and uniformity, linguistic abilities, and correlation between performance and rank. For instance, we find that the dynamics of the key aspects can undergo significant changes as fine-tuning transitions from Phase 1 to Phase 2. Based on these findings, we experiment with a rank reduction (RR) strategy that facilitates rapid and stable fine-tuning of the latest CL-based methods. Through empirical investigations, we showcase the efficacy of RR in enhancing the performance and stability of five state-of-the-art sentence embedding methods. The code is available at (https://github.com/SNU-DRL/SentenceEmbedding_Rank).",
            "corpus_id": 269921716,
            "sentences": [
                {
                    "corpus_id": "269921716",
                    "title": "Unveiling Key Aspects of Fine-Tuning in Sentence Embeddings: A Representation Rank Analysis",
                    "text": "The purpose of fine-tuning a language model is to leverage the language model's pre-learned linguistic abilities.However, due to the use of cosine similarity as the measure of sentence embedding, fine-tuning with contrastive loss becomes imperative.For the case of BERT, it becomes essential to enhance uniformity through fine-tuning despite the possibility of negatively affecting BERT's linguistic abilities.To explore linguistic aspects, we followed the methodology of Conneau et al. (2018) and utilized SentEval toolkit \u2021 to investigate training dynamics of ten different linguistic abilities.Based on the observed trends in training dynamics, we grouped the ten probing tasks into three categories, as presented in Figure 4.\n\nThe first group (Figure 4a) consists of three tasks that exhibit deterioration in Phase 1 followed by recovery in Phase 2. They are Length (number of tokens), Depth (depth of sentence structure trees), and TopConstituents (the grammatical structure of sentences), and all three are closely related to the performance of sentence embedding.We emphasize that the worst performance occurs at or near the boundary between Phase 1 and Phase 2 for the three linguistic abilities, indicating a strong correlation with representation rank.The trend of deterioration followed by recovery was also observed for alignment, where the deterioration occurs while uniformity sharply improves.\n\nThe second group (Figure 4b) consists of three tasks that exhibit an upward performance trend in both Phase 1 and Phase 2. They are WordContent (deducing words from sentence representations), \u2021 https://github.com/facebookresearch/SentEval/tree/main/data/probing  SubjNumber, and ObjNumber (matching the number of subjects and objects in sentence clauses, respectively), and all three are intimately related to the sentence embedding task.These three linguistic abilities do not deteriorate in Phase 1 despite uniformity's sharp improvement, suggesting that the three do not form a trade-off relationship with uniformity.\n\nThe third group (Figure 4c) consists of the four remaining tasks.Their performance either deteriorates (BigramShift and CoordinationInversion) or oscillates (Tense and OddManOut) throughout the fine-tuning.",
                    "score": 0.3781347436051081,
                    "section_title": "Linguistic Abilities",
                    "char_start_offset": 11611,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 113
                        },
                        {
                            "start": 113,
                            "end": 249
                        },
                        {
                            "start": 249,
                            "end": 410
                        },
                        {
                            "start": 410,
                            "end": 597
                        },
                        {
                            "start": 597,
                            "end": 729
                        },
                        {
                            "start": 731,
                            "end": 1070
                        },
                        {
                            "start": 1070,
                            "end": 1262
                        },
                        {
                            "start": 1262,
                            "end": 1408
                        },
                        {
                            "start": 1410,
                            "end": 1649
                        },
                        {
                            "start": 1649,
                            "end": 1848
                        },
                        {
                            "start": 1848,
                            "end": 2030
                        },
                        {
                            "start": 2032,
                            "end": 2097
                        },
                        {
                            "start": 2097,
                            "end": 2238
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9873046875
                }
            ],
            "relevance_judgement": 0.9873046875,
            "relevance_judgment_input_expanded": "# Title: Unveiling Key Aspects of Fine-Tuning in Sentence Embeddings: A Representation Rank Analysis\n# Venue: IEEE Access\n# Authors: Euna Jung, Jaeill Kim, Jungmin Ko, Jinwoo Park, Wonjong Rhee\n## Abstract\nThe latest advancements in unsupervised learning of sentence embeddings predominantly involve employing contrastive learning-based (CL-based) fine-tuning over pre-trained language models. In this study, we analyze the latest sentence embedding methods by adopting representation rank as the primary tool of analysis. We first define Phase 1 and Phase 2 of fine-tuning based on when representation rank peaks. Utilizing these phases, we conduct a thorough analysis and obtain essential findings across key aspects, including alignment and uniformity, linguistic abilities, and correlation between performance and rank. For instance, we find that the dynamics of the key aspects can undergo significant changes as fine-tuning transitions from Phase 1 to Phase 2. Based on these findings, we experiment with a rank reduction (RR) strategy that facilitates rapid and stable fine-tuning of the latest CL-based methods. Through empirical investigations, we showcase the efficacy of RR in enhancing the performance and stability of five state-of-the-art sentence embedding methods. The code is available at (https://github.com/SNU-DRL/SentenceEmbedding_Rank).\n## Linguistic Abilities\nThe purpose of fine-tuning a language model is to leverage the language model's pre-learned linguistic abilities.However, due to the use of cosine similarity as the measure of sentence embedding, fine-tuning with contrastive loss becomes imperative.For the case of BERT, it becomes essential to enhance uniformity through fine-tuning despite the possibility of negatively affecting BERT's linguistic abilities.To explore linguistic aspects, we followed the methodology of Conneau et al. (2018) and utilized SentEval toolkit \u2021 to investigate training dynamics of ten different linguistic abilities.Based on the observed trends in training dynamics, we grouped the ten probing tasks into three categories, as presented in Figure 4.\n\nThe first group (Figure 4a) consists of three tasks that exhibit deterioration in Phase 1 followed by recovery in Phase 2. They are Length (number of tokens), Depth (depth of sentence structure trees), and TopConstituents (the grammatical structure of sentences), and all three are closely related to the performance of sentence embedding.We emphasize that the worst performance occurs at or near the boundary between Phase 1 and Phase 2 for the three linguistic abilities, indicating a strong correlation with representation rank.The trend of deterioration followed by recovery was also observed for alignment, where the deterioration occurs while uniformity sharply improves.\n\nThe second group (Figure 4b) consists of three tasks that exhibit an upward performance trend in both Phase 1 and Phase 2. They are WordContent (deducing words from sentence representations), \u2021 https://github.com/facebookresearch/SentEval/tree/main/data/probing  SubjNumber, and ObjNumber (matching the number of subjects and objects in sentence clauses, respectively), and all three are intimately related to the sentence embedding task.These three linguistic abilities do not deteriorate in Phase 1 despite uniformity's sharp improvement, suggesting that the three do not form a trade-off relationship with uniformity.\n\nThe third group (Figure 4c) consists of the four remaining tasks.Their performance either deteriorates (BigramShift and CoordinationInversion) or oscillates (Tense and OddManOut) throughout the fine-tuning.",
            "reference_string": "[269921716 | Jung et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Learning Dynamics of LLM Finetuning",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 66,
            "citation_count": 21,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.10490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115242507",
                    "name": "Yi Ren"
                },
                {
                    "authorId": "2262445067",
                    "name": "Danica J. Sutherland"
                }
            ],
            "abstract": "Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, gives us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during different types of finetuning, by analyzing the step-wise decomposition of how influence accumulates among different potential responses. Our framework allows a uniform interpretation of many interesting observations about the training of popular algorithms for both instruction tuning and preference tuning. In particular, we propose a hypothetical explanation of why specific types of hallucination are strengthened after finetuning, e.g., the model might use phrases or facts in the response for question B to answer question A, or the model might keep repeating similar simple phrases when generating responses. We also extend our framework and highlight a unique\"squeezing effect\"to explain a previously observed phenomenon in off-policy direct preference optimization (DPO), where running DPO for too long makes even the desired outputs less likely. This framework also provides insights into where the benefits of on-policy DPO and other variants come from. The analysis not only provides a novel perspective of understanding LLM's finetuning but also inspires a simple, effective method to improve alignment performance.",
            "corpus_id": 271213641,
            "sentences": [
                {
                    "corpus_id": "271213641",
                    "title": "Learning Dynamics of LLM Finetuning",
                    "text": "Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, gives us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during different types of finetuning, by analyzing the step-wise decomposition of how influence accumulates among different potential responses. Our framework allows a uniform interpretation of many interesting observations about the training of popular algorithms for both instruction tuning and preference tuning. In particular, we propose a hypothetical explanation of why specific types of hallucination are strengthened after finetuning, e.g., the model might use phrases or facts in the response for question B to answer question A, or the model might keep repeating similar simple phrases when generating responses. We also extend our framework and highlight a unique\"squeezing effect\"to explain a previously observed phenomenon in off-policy direct preference optimization (DPO), where running DPO for too long makes even the desired outputs less likely. This framework also provides insights into where the benefits of on-policy DPO and other variants come from. The analysis not only provides a novel perspective of understanding LLM's finetuning but also inspires a simple, effective method to improve alignment performance.",
                    "score": 0.48628703358765857,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98681640625
                }
            ],
            "relevance_judgement": 0.98681640625,
            "relevance_judgment_input_expanded": "# Title: Learning Dynamics of LLM Finetuning\n# Venue: International Conference on Learning Representations\n# Authors: Yi Ren, Danica J. Sutherland\n## Abstract\nLearning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, gives us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during different types of finetuning, by analyzing the step-wise decomposition of how influence accumulates among different potential responses. Our framework allows a uniform interpretation of many interesting observations about the training of popular algorithms for both instruction tuning and preference tuning. In particular, we propose a hypothetical explanation of why specific types of hallucination are strengthened after finetuning, e.g., the model might use phrases or facts in the response for question B to answer question A, or the model might keep repeating similar simple phrases when generating responses. We also extend our framework and highlight a unique\"squeezing effect\"to explain a previously observed phenomenon in off-policy direct preference optimization (DPO), where running DPO for too long makes even the desired outputs less likely. This framework also provides insights into where the benefits of on-policy DPO and other variants come from. The analysis not only provides a novel perspective of understanding LLM's finetuning but also inspires a simple, effective method to improve alignment performance.\n",
            "reference_string": "[271213641 | Ren et al. | 2024 | Citations: 21]"
        },
        {
            "title": "Geometric Signatures of Compositionality Across a Language Model's Lifetime",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 104,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.01444, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2323997802",
                    "name": "Jin Hwa Lee"
                },
                {
                    "authorId": "2187058673",
                    "name": "Thomas Jiralerspong"
                },
                {
                    "authorId": "2324060912",
                    "name": "Lei Yu"
                },
                {
                    "authorId": "1865800402",
                    "name": "Y. Bengio"
                },
                {
                    "authorId": "2323783941",
                    "name": "Emily Cheng"
                }
            ],
            "abstract": "By virtue of linguistic compositionality, few syntactic rules and a finite lexicon can generate an unbounded number of sentences. That is, language, though seemingly high-dimensional, can be explained using relatively few degrees of freedom. An open question is whether contemporary language models (LMs) reflect the intrinsic simplicity of language that is enabled by compositionality. We take a geometric view of this problem by relating the degree of compositionality in a dataset to the intrinsic dimension (ID) of its representations under an LM, a measure of feature complexity. We find not only that the degree of dataset compositionality is reflected in representations' ID, but that the relationship between compositionality and geometric complexity arises due to learned linguistic features over training. Finally, our analyses reveal a striking contrast between nonlinear and linear dimensionality, showing they respectively encode semantic and superficial aspects of linguistic composition.",
            "corpus_id": 273025704,
            "sentences": [
                {
                    "corpus_id": "273025704",
                    "title": "Geometric Signatures of Compositionality Across a Language Model's Lifetime",
                    "text": "Most research on LMs focuses on the final configuration of the model at the end of pre-training. Yet, recent work shows that learning dynamics can elucidate the behavior and computational mechanisms of LMs (Chen et al., 2024;Singh et al., 2024;Tigges et al., 2024). It has been found that, over training, LMs' weight matrices become higher-rank (Abbe et al., 2023), their representations higher dimensional (Cheng et al., 2024), and their gradients increasingly diffuse (Weber et al., 2024). Over finetuning, representational dimensionality has been found to change in concert with geometric properties like cluster reorganization (Doimo et al., 2024). \n\nPhase transitions during LM training have been found for some, but not all, aspects of language learning. Negative evidence includes that LM circuits involved in linguistic subtasks are stable (Tigges et al., 2024) and gradually reinforced (Weber et al., 2024) over training. Positive evidence for learning phase transitions includes that the ID of BERT's final [CLS] representation tracks sudden syntax acquisition and drops in training loss (Chen et al., 2024), with similar observations on Transformers trained on formal languages (Lubana et al., 2024). Our work supplements these results by investigating how the interaction between compositional understanding of language and geometric complexity of its representation arises over training.",
                    "score": 0.44253557297245344,
                    "section_title": "Language model training dynamics",
                    "char_start_offset": 5069,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 96
                        },
                        {
                            "start": 97,
                            "end": 265
                        },
                        {
                            "start": 266,
                            "end": 491
                        },
                        {
                            "start": 492,
                            "end": 652
                        },
                        {
                            "start": 655,
                            "end": 760
                        },
                        {
                            "start": 761,
                            "end": 930
                        },
                        {
                            "start": 931,
                            "end": 1211
                        },
                        {
                            "start": 1212,
                            "end": 1400
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 206,
                            "end": 225,
                            "matchedPaperCorpusId": "261822542"
                        },
                        {
                            "start": 225,
                            "end": 244,
                            "matchedPaperCorpusId": "265157721"
                        },
                        {
                            "start": 345,
                            "end": 364,
                            "matchedPaperCorpusId": "259138551"
                        },
                        {
                            "start": 470,
                            "end": 490,
                            "matchedPaperCorpusId": "270370801"
                        },
                        {
                            "start": 895,
                            "end": 915,
                            "matchedPaperCorpusId": "270370801"
                        },
                        {
                            "start": 1098,
                            "end": 1117,
                            "matchedPaperCorpusId": "261822542"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98681640625
                }
            ],
            "relevance_judgement": 0.98681640625,
            "relevance_judgment_input_expanded": "# Title: Geometric Signatures of Compositionality Across a Language Model's Lifetime\n# Venue: arXiv.org\n# Authors: Jin Hwa Lee, Thomas Jiralerspong, Lei Yu, Y. Bengio, Emily Cheng\n## Abstract\nBy virtue of linguistic compositionality, few syntactic rules and a finite lexicon can generate an unbounded number of sentences. That is, language, though seemingly high-dimensional, can be explained using relatively few degrees of freedom. An open question is whether contemporary language models (LMs) reflect the intrinsic simplicity of language that is enabled by compositionality. We take a geometric view of this problem by relating the degree of compositionality in a dataset to the intrinsic dimension (ID) of its representations under an LM, a measure of feature complexity. We find not only that the degree of dataset compositionality is reflected in representations' ID, but that the relationship between compositionality and geometric complexity arises due to learned linguistic features over training. Finally, our analyses reveal a striking contrast between nonlinear and linear dimensionality, showing they respectively encode semantic and superficial aspects of linguistic composition.\n## Language model training dynamics\nMost research on LMs focuses on the final configuration of the model at the end of pre-training. Yet, recent work shows that learning dynamics can elucidate the behavior and computational mechanisms of LMs (Chen et al., 2024;Singh et al., 2024;Tigges et al., 2024). It has been found that, over training, LMs' weight matrices become higher-rank (Abbe et al., 2023), their representations higher dimensional (Cheng et al., 2024), and their gradients increasingly diffuse (Weber et al., 2024). Over finetuning, representational dimensionality has been found to change in concert with geometric properties like cluster reorganization (Doimo et al., 2024). \n\nPhase transitions during LM training have been found for some, but not all, aspects of language learning. Negative evidence includes that LM circuits involved in linguistic subtasks are stable (Tigges et al., 2024) and gradually reinforced (Weber et al., 2024) over training. Positive evidence for learning phase transitions includes that the ID of BERT's final [CLS] representation tracks sudden syntax acquisition and drops in training loss (Chen et al., 2024), with similar observations on Transformers trained on formal languages (Lubana et al., 2024). Our work supplements these results by investigating how the interaction between compositional understanding of language and geometric complexity of its representation arises over training.",
            "reference_string": "[273025704 | Lee et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Non-asymptotic Convergence of Training Transformers for Next-token Prediction",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 43,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.17335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2140386847",
                    "name": "Ruiquan Huang"
                },
                {
                    "authorId": "2294511987",
                    "name": "Yingbin Liang"
                },
                {
                    "authorId": "2261298891",
                    "name": "Jing Yang"
                }
            ],
            "abstract": "Transformers have achieved extraordinary success in modern machine learning due to their excellent ability to handle sequential data, especially in next-token prediction (NTP) tasks. However, the theoretical understanding of their performance in NTP is limited, with existing studies focusing mainly on asymptotic performance. This paper provides a fine-grained non-asymptotic analysis of the training dynamics of a one-layer transformer consisting of a self-attention module followed by a feed-forward layer. We first characterize the essential structural properties of training datasets for NTP using a mathematical framework based on partial orders. Then, we design a two-stage training algorithm, where the pre-processing stage for training the feed-forward layer and the main stage for training the attention layer exhibit fast convergence performance. Specifically, both layers converge sub-linearly to the direction of their corresponding max-margin solutions. We also show that the cross-entropy loss enjoys a linear convergence rate. Furthermore, we show that the trained transformer presents non-trivial prediction ability with dataset shift, which sheds light on the remarkable generalization performance of transformers. Our analysis technique involves the development of novel properties on the attention gradient and further in-depth analysis of how these properties contribute to the convergence of the training process. Our experiments further validate our theoretical findings.",
            "corpus_id": 272910946,
            "sentences": [
                {
                    "corpus_id": "272910946",
                    "title": "Non-asymptotic Convergence of Training Transformers for Next-token Prediction",
                    "text": "The transformer architecture (Vaswani et al., 2017) has revolutionized the field of machine learning, establishing itself as a foundation model for numerous applications, including natural language processing (NLP) (Devlin et al., 2018), computer vision (Dosovitskiy et al., 2020), and multi-modal signal processing (Tsai et al., 2019). In particular, transformers achieve tremendous empirical success in large language models (LLMs) such as GPT-3 (Brown et al., 2020). Despite the empirical success, limited theoretical understanding of transformers have caused a series of critical concerns about their robustness, interpretability, and bias issues (Bommasani et al., 2021;Belkin, 2024). \n\nTo overcome these issues, recent advances in transformer theory have investigated the convergence of training transformers under theoretically amenable setting such as linear regression (Mahankali et al., 2023;Zhang et al., 2023;Huang et al., 2023) and binary classification (Tarzanagh et al., 2023b,a;Vasudeva et al., 2024;Li et al., 2023). Nevertheless, one of the fundamental task in LLMs and other generative models is next-token prediction (NTP), which involves predicting the next word or token in a sequence, given the previous tokens. In NTP, a few recent theoretical studies have started to investigate the training dynamics of transformers (Tian et al., 2023a;Li et al., 2024). However, those works lack of fine-grained non-asymptotic convergence analysis of the training process, posing the following open questions for further investigation: \n\nHow fast does the training of a transformer converge in NTP? \n\nIn addition, a pre-trained transformer empirically exhibits non-trivial generalization ability. A follow-up question from a theoretical point of view is that Can we show the generalization capability of a trained transformer on unseen data? \n\nIn this paper, we take a first step towards addressing the aforementioned questions by studying the training dynamics of a single layer transformer consisting of a self-attention layer and a feed-forward layer for NTP. We summarize our contribution as follows.",
                    "score": 0.39264548970667257,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 336
                        },
                        {
                            "start": 337,
                            "end": 469
                        },
                        {
                            "start": 470,
                            "end": 689
                        },
                        {
                            "start": 692,
                            "end": 1033
                        },
                        {
                            "start": 1034,
                            "end": 1234
                        },
                        {
                            "start": 1235,
                            "end": 1379
                        },
                        {
                            "start": 1380,
                            "end": 1545
                        },
                        {
                            "start": 1548,
                            "end": 1608
                        },
                        {
                            "start": 1611,
                            "end": 1706
                        },
                        {
                            "start": 1707,
                            "end": 1851
                        },
                        {
                            "start": 1854,
                            "end": 2072
                        },
                        {
                            "start": 2073,
                            "end": 2114
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 29,
                            "end": 51,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 316,
                            "end": 335,
                            "matchedPaperCorpusId": "173990158"
                        },
                        {
                            "start": 448,
                            "end": 468,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 675,
                            "end": 688,
                            "matchedPaperCorpusId": "267303083"
                        },
                        {
                            "start": 1342,
                            "end": 1362,
                            "matchedPaperCorpusId": "258947127"
                        },
                        {
                            "start": 1362,
                            "end": 1378,
                            "matchedPaperCorpusId": "268379753"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98681640625
                }
            ],
            "relevance_judgement": 0.98681640625,
            "relevance_judgment_input_expanded": "# Title: Non-asymptotic Convergence of Training Transformers for Next-token Prediction\n# Venue: Neural Information Processing Systems\n# Authors: Ruiquan Huang, Yingbin Liang, Jing Yang\n## Abstract\nTransformers have achieved extraordinary success in modern machine learning due to their excellent ability to handle sequential data, especially in next-token prediction (NTP) tasks. However, the theoretical understanding of their performance in NTP is limited, with existing studies focusing mainly on asymptotic performance. This paper provides a fine-grained non-asymptotic analysis of the training dynamics of a one-layer transformer consisting of a self-attention module followed by a feed-forward layer. We first characterize the essential structural properties of training datasets for NTP using a mathematical framework based on partial orders. Then, we design a two-stage training algorithm, where the pre-processing stage for training the feed-forward layer and the main stage for training the attention layer exhibit fast convergence performance. Specifically, both layers converge sub-linearly to the direction of their corresponding max-margin solutions. We also show that the cross-entropy loss enjoys a linear convergence rate. Furthermore, we show that the trained transformer presents non-trivial prediction ability with dataset shift, which sheds light on the remarkable generalization performance of transformers. Our analysis technique involves the development of novel properties on the attention gradient and further in-depth analysis of how these properties contribute to the convergence of the training process. Our experiments further validate our theoretical findings.\n## Introduction\nThe transformer architecture (Vaswani et al., 2017) has revolutionized the field of machine learning, establishing itself as a foundation model for numerous applications, including natural language processing (NLP) (Devlin et al., 2018), computer vision (Dosovitskiy et al., 2020), and multi-modal signal processing (Tsai et al., 2019). In particular, transformers achieve tremendous empirical success in large language models (LLMs) such as GPT-3 (Brown et al., 2020). Despite the empirical success, limited theoretical understanding of transformers have caused a series of critical concerns about their robustness, interpretability, and bias issues (Bommasani et al., 2021;Belkin, 2024). \n\nTo overcome these issues, recent advances in transformer theory have investigated the convergence of training transformers under theoretically amenable setting such as linear regression (Mahankali et al., 2023;Zhang et al., 2023;Huang et al., 2023) and binary classification (Tarzanagh et al., 2023b,a;Vasudeva et al., 2024;Li et al., 2023). Nevertheless, one of the fundamental task in LLMs and other generative models is next-token prediction (NTP), which involves predicting the next word or token in a sequence, given the previous tokens. In NTP, a few recent theoretical studies have started to investigate the training dynamics of transformers (Tian et al., 2023a;Li et al., 2024). However, those works lack of fine-grained non-asymptotic convergence analysis of the training process, posing the following open questions for further investigation: \n\nHow fast does the training of a transformer converge in NTP? \n\nIn addition, a pre-trained transformer empirically exhibits non-trivial generalization ability. A follow-up question from a theoretical point of view is that Can we show the generalization capability of a trained transformer on unseen data? \n\nIn this paper, we take a first step towards addressing the aforementioned questions by studying the training dynamics of a single layer transformer consisting of a self-attention layer and a feed-forward layer for NTP. We summarize our contribution as follows.",
            "reference_string": "[272910946 | Huang et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Towards Optimal Learning of Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 61,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17759, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116405624",
                    "name": "Yuxian Gu"
                },
                {
                    "authorId": "2286153844",
                    "name": "Li Dong"
                },
                {
                    "authorId": "34128716",
                    "name": "Y. Hao"
                },
                {
                    "authorId": "2287927238",
                    "name": "Qingxiu Dong"
                },
                {
                    "authorId": "2285704485",
                    "name": "Minlie Huang"
                },
                {
                    "authorId": "2253471545",
                    "name": "Furu Wei"
                }
            ],
            "abstract": "This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an\"LM-training-as-lossless-compression\"view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task. Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods. Our code can be found at https://aka.ms/LearningLaw.",
            "corpus_id": 268041376,
            "sentences": [
                {
                    "corpus_id": "268041376",
                    "title": "Towards Optimal Learning of Language Models",
                    "text": "This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an\"LM-training-as-lossless-compression\"view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task. Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods. Our code can be found at https://aka.ms/LearningLaw.",
                    "score": 0.3817591764273396,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.986328125
                }
            ],
            "relevance_judgement": 0.986328125,
            "relevance_judgment_input_expanded": "# Title: Towards Optimal Learning of Language Models\n# Venue: arXiv.org\n# Authors: Yuxian Gu, Li Dong, Y. Hao, Qingxiu Dong, Minlie Huang, Furu Wei\n## Abstract\nThis work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an\"LM-training-as-lossless-compression\"view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task. Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods. Our code can be found at https://aka.ms/LearningLaw.\n",
            "reference_string": "[268041376 | Gu et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 37,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.15499, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2342570163",
                    "name": "Ya Wang"
                },
                {
                    "authorId": "2210797120",
                    "name": "Zhijian Zhuo"
                },
                {
                    "authorId": "122290781",
                    "name": "Yutao Zeng"
                },
                {
                    "authorId": "2323563576",
                    "name": "Xun Zhou"
                },
                {
                    "authorId": "2347294114",
                    "name": "Jian Yang"
                },
                {
                    "authorId": "2346899198",
                    "name": "Xiaoqing Li"
                }
            ],
            "abstract": "Training stability is a persistent challenge in the pre-training of large language models (LLMs), particularly for architectures such as Post-Norm Transformers, which are prone to gradient explosion and dissipation. In this paper, we propose Scale-Distribution Decoupling (SDD), a novel approach that stabilizes training by explicitly decoupling the scale and distribution of the weight matrix in fully-connected layers. SDD applies a normalization mechanism to regulate activations and a learnable scaling vector to maintain well-conditioned gradients, effectively preventing $\\textbf{gradient explosion and dissipation}$. This separation improves optimization efficiency, particularly in deep networks, by ensuring stable gradient propagation. Experimental results demonstrate that our method stabilizes training across various LLM architectures and outperforms existing techniques in different normalization configurations. Furthermore, the proposed method is lightweight and compatible with existing frameworks, making it a practical solution for stabilizing LLM training. Code is available at https://github.com/kaihemo/SDD.",
            "corpus_id": 276558280,
            "sentences": [
                {
                    "corpus_id": "276558280",
                    "title": "Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models",
                    "text": "Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing tasks (Li et al., 2024;Zhu et al., 2024;Huang et al., 2025), fueled by advances in model architectures, large-scale datasets, and computational resources. However, the training stability of LLMs remains a critical challenge, especially as model size and complexity continue to grow. Instabilities during pre-training often lead to issues such as gradient explosion, vanishing gradients, or optimization stagnation, hindering the efficient and effective training of these models. Although Pre-Norm Transformer (Xiong et al., 2020;Zhuo et al., 2025) architectures exhibit greater stability during training, they suffer from feature collapse (Wang et al., 2024a;Xie et al., 2023), where representations across different layers become increasingly similar as depth increases. This phenomenon may contribute to the scaling bottleneck in large models. On the other hand, Post-Norm configurations remain significantly more difficult to train, exhibiting severe gradient explosion or vanishing issues, making stability in such settings a challenge in LLM research. \n\nA fundamental source of these instabilities lies in the complexity of optimizing weight matrices in high-dimensional spaces. Specifically, the scale of weight parameters becomes challenging to regulate as the matrix grows in size, making convergence increasingly delicate. While existing strategies, such as sophisticated initialization schemes (Zhang et al., 2019) and normalization techniques (Ding et al., 2021;Xiong et al., 2020), offer partial mitigation, they fail to resolve the core issue: the entanglement between the weight matrix's scale and distribution. This coupling induces suboptimal optimization dynamics, amplifying training insta-bilities, particularly in large-scale models where gradient propagation is susceptible to divergence or attenuation. \n\nTo tackle these challenges, we introduce Scale-Distribution Decoupling (SDD), a novel approach that restructures fullyconnected layers to explicitly separate the scale and distribution of weight matrices. In contrast to conventional formulations, SDD applies a normalization step to standardize activations, ensuring optimization focuses on learning the distribution rather than jointly optimizing both scale and distribution.",
                    "score": 0.3897650933975636,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 259
                        },
                        {
                            "start": 260,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 583
                        },
                        {
                            "start": 584,
                            "end": 876
                        },
                        {
                            "start": 877,
                            "end": 950
                        },
                        {
                            "start": 951,
                            "end": 1161
                        },
                        {
                            "start": 1164,
                            "end": 1288
                        },
                        {
                            "start": 1289,
                            "end": 1436
                        },
                        {
                            "start": 1437,
                            "end": 1730
                        },
                        {
                            "start": 1731,
                            "end": 1929
                        },
                        {
                            "start": 1932,
                            "end": 2136
                        },
                        {
                            "start": 2137,
                            "end": 2358
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 111,
                            "end": 128,
                            "matchedPaperCorpusId": "268379184"
                        },
                        {
                            "start": 614,
                            "end": 634,
                            "matchedPaperCorpusId": "211082816"
                        },
                        {
                            "start": 634,
                            "end": 652,
                            "matchedPaperCorpusId": "273850118"
                        },
                        {
                            "start": 744,
                            "end": 764,
                            "matchedPaperCorpusId": "247187905"
                        },
                        {
                            "start": 1578,
                            "end": 1597,
                            "matchedPaperCorpusId": "211082816"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98486328125
                }
            ],
            "relevance_judgement": 0.98486328125,
            "relevance_judgment_input_expanded": "# Title: Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models\n# Venue: arXiv.org\n# Authors: Ya Wang, Zhijian Zhuo, Yutao Zeng, Xun Zhou, Jian Yang, Xiaoqing Li\n## Abstract\nTraining stability is a persistent challenge in the pre-training of large language models (LLMs), particularly for architectures such as Post-Norm Transformers, which are prone to gradient explosion and dissipation. In this paper, we propose Scale-Distribution Decoupling (SDD), a novel approach that stabilizes training by explicitly decoupling the scale and distribution of the weight matrix in fully-connected layers. SDD applies a normalization mechanism to regulate activations and a learnable scaling vector to maintain well-conditioned gradients, effectively preventing $\\textbf{gradient explosion and dissipation}$. This separation improves optimization efficiency, particularly in deep networks, by ensuring stable gradient propagation. Experimental results demonstrate that our method stabilizes training across various LLM architectures and outperforms existing techniques in different normalization configurations. Furthermore, the proposed method is lightweight and compatible with existing frameworks, making it a practical solution for stabilizing LLM training. Code is available at https://github.com/kaihemo/SDD.\n## Introduction\nLarge Language Models (LLMs) have demonstrated remarkable success in various natural language processing tasks (Li et al., 2024;Zhu et al., 2024;Huang et al., 2025), fueled by advances in model architectures, large-scale datasets, and computational resources. However, the training stability of LLMs remains a critical challenge, especially as model size and complexity continue to grow. Instabilities during pre-training often lead to issues such as gradient explosion, vanishing gradients, or optimization stagnation, hindering the efficient and effective training of these models. Although Pre-Norm Transformer (Xiong et al., 2020;Zhuo et al., 2025) architectures exhibit greater stability during training, they suffer from feature collapse (Wang et al., 2024a;Xie et al., 2023), where representations across different layers become increasingly similar as depth increases. This phenomenon may contribute to the scaling bottleneck in large models. On the other hand, Post-Norm configurations remain significantly more difficult to train, exhibiting severe gradient explosion or vanishing issues, making stability in such settings a challenge in LLM research. \n\nA fundamental source of these instabilities lies in the complexity of optimizing weight matrices in high-dimensional spaces. Specifically, the scale of weight parameters becomes challenging to regulate as the matrix grows in size, making convergence increasingly delicate. While existing strategies, such as sophisticated initialization schemes (Zhang et al., 2019) and normalization techniques (Ding et al., 2021;Xiong et al., 2020), offer partial mitigation, they fail to resolve the core issue: the entanglement between the weight matrix's scale and distribution. This coupling induces suboptimal optimization dynamics, amplifying training insta-bilities, particularly in large-scale models where gradient propagation is susceptible to divergence or attenuation. \n\nTo tackle these challenges, we introduce Scale-Distribution Decoupling (SDD), a novel approach that restructures fullyconnected layers to explicitly separate the scale and distribution of weight matrices. In contrast to conventional formulations, SDD applies a normalization step to standardize activations, ensuring optimization focuses on learning the distribution rather than jointly optimizing both scale and distribution.",
            "reference_string": "[276558280 | Wang et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.04619, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2289835509",
                    "name": "Tian Qin"
                },
                {
                    "authorId": "2330978295",
                    "name": "Naomi Saphra"
                },
                {
                    "authorId": "2330971704",
                    "name": "David Alvarez-Melis"
                }
            ],
            "abstract": "Language models (LMs), like other neural networks, often favor shortcut heuristics based on surface-level patterns. Although LMs behave like n-gram models early in training, they must eventually learn hierarchical syntactic representations to correctly apply grammatical rules out-of-distribution (OOD). In this work, we use case studies of English grammar to explore how complex, diverse training data drives models to generalize OOD. We construct a framework that unifies our understanding of random variation with training dynamics, rule selection with memorization, and data diversity with complexity. We show that these factors are nuanced, and that intermediate levels of diversity and complexity lead to inconsistent behavior across random seeds and to unstable training dynamics. Our findings emphasize the critical role of training data in shaping generalization patterns and illuminate how competing model strategies lead to inconsistent generalization outcomes across random seeds. Code is available at https://github.com/sunnytqin/concept_comp.git.",
            "corpus_id": 274581427,
            "sentences": [
                {
                    "corpus_id": "274581427",
                    "title": "Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization",
                    "text": "Language models (LMs), like other neural networks, often favor shortcut heuristics based on surface-level patterns. Although LMs behave like n-gram models early in training, they must eventually learn hierarchical syntactic representations to correctly apply grammatical rules out-of-distribution (OOD). In this work, we use case studies of English grammar to explore how complex, diverse training data drives models to generalize OOD. We construct a framework that unifies our understanding of random variation with training dynamics, rule selection with memorization, and data diversity with complexity. We show that these factors are nuanced, and that intermediate levels of diversity and complexity lead to inconsistent behavior across random seeds and to unstable training dynamics. Our findings emphasize the critical role of training data in shaping generalization patterns and illuminate how competing model strategies lead to inconsistent generalization outcomes across random seeds. Code is available at https://github.com/sunnytqin/concept_comp.git.",
                    "score": 0.4222669038718771,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98388671875
                }
            ],
            "relevance_judgement": 0.98388671875,
            "relevance_judgment_input_expanded": "# Title: Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization\n# Venue: arXiv.org\n# Authors: Tian Qin, Naomi Saphra, David Alvarez-Melis\n## Abstract\nLanguage models (LMs), like other neural networks, often favor shortcut heuristics based on surface-level patterns. Although LMs behave like n-gram models early in training, they must eventually learn hierarchical syntactic representations to correctly apply grammatical rules out-of-distribution (OOD). In this work, we use case studies of English grammar to explore how complex, diverse training data drives models to generalize OOD. We construct a framework that unifies our understanding of random variation with training dynamics, rule selection with memorization, and data diversity with complexity. We show that these factors are nuanced, and that intermediate levels of diversity and complexity lead to inconsistent behavior across random seeds and to unstable training dynamics. Our findings emphasize the critical role of training data in shaping generalization patterns and illuminate how competing model strategies lead to inconsistent generalization outcomes across random seeds. Code is available at https://github.com/sunnytqin/concept_comp.git.\n",
            "reference_string": "[274581427 | Qin et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Sharpness-Aware Minimization and the Edge of Stability",
            "venue": "Journal of machine learning research",
            "year": 2023,
            "reference_count": 23,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.12488",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.12488, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144007105",
                    "name": "Philip M. Long"
                },
                {
                    "authorId": "2244620501",
                    "name": "Peter L. Bartlett"
                }
            ],
            "abstract": "Recent experiments have shown that, often, when training a neural network with gradient descent (GD) with a step size $\\eta$, the operator norm of the Hessian of the loss grows until it approximately reaches $2/\\eta$, after which it fluctuates around this value. The quantity $2/\\eta$ has been called the\"edge of stability\"based on consideration of a local quadratic approximation of the loss. We perform a similar calculation to arrive at an\"edge of stability\"for Sharpness-Aware Minimization (SAM), a variant of GD which has been shown to improve its generalization. Unlike the case for GD, the resulting SAM-edge depends on the norm of the gradient. Using three deep learning training tasks, we see empirically that SAM operates on the edge of stability identified by this analysis.",
            "corpus_id": 262217060,
            "sentences": [
                {
                    "corpus_id": "262217060",
                    "title": "Sharpness-Aware Minimization and the Edge of Stability",
                    "text": "Next, we report on experiments training a language model. As before, we start with SGD, here in Figure 12. \n\nNext, we plot the same quantities when the network is trained with SAM, with \u03c1 = 0.3, in Figure 13. Here, the operator norm of the Hessian is significantly less than when SGD is used, and we see evidence that training in SAM operates at the edge of stability analyzed in Section 2. In Figure 14, we zoom in on the lower part of the curve, and plot the operator norm of the Hessian, to examine the relationship between this quantity and the SAM edge in more detail. \n\nFigure 15 contains plots of the training loss, once again estimated per-minibatch. We included these mainly to motivate the combinations of hyperparameters where we examined other aspects of the dynamics of SAM. As expected, while SAM does take longer to achieve a certain loss, it ultimately achieves training error similar to SGD, but with less sharpness. \n\nFigure 16 contains plots of the alignment, once again estimated per-minibatch. For the large learning rates, late in training, despite the sampling noise arising from the use of minibatches, we see a systematic tendency for the SAM gradients align more closely with the principal eigenvector of the Hessian than the gradients at the initial solution. However, for the smallest learning rates, the opposite holds. Figure 16: Alignments between gradients and the principal direction of the Hessian in the language modeling experiments.",
                    "score": 0.3826298782839983,
                    "section_title": "Language modeling",
                    "char_start_offset": 14963,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 57
                        },
                        {
                            "start": 58,
                            "end": 106
                        },
                        {
                            "start": 109,
                            "end": 208
                        },
                        {
                            "start": 209,
                            "end": 573
                        },
                        {
                            "start": 576,
                            "end": 658
                        },
                        {
                            "start": 659,
                            "end": 787
                        },
                        {
                            "start": 788,
                            "end": 933
                        },
                        {
                            "start": 936,
                            "end": 1014
                        },
                        {
                            "start": 1015,
                            "end": 1286
                        },
                        {
                            "start": 1287,
                            "end": 1348
                        },
                        {
                            "start": 1349,
                            "end": 1469
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98291015625
                }
            ],
            "relevance_judgement": 0.98291015625,
            "relevance_judgment_input_expanded": "# Title: Sharpness-Aware Minimization and the Edge of Stability\n# Venue: Journal of machine learning research\n# Authors: Philip M. Long, Peter L. Bartlett\n## Abstract\nRecent experiments have shown that, often, when training a neural network with gradient descent (GD) with a step size $\\eta$, the operator norm of the Hessian of the loss grows until it approximately reaches $2/\\eta$, after which it fluctuates around this value. The quantity $2/\\eta$ has been called the\"edge of stability\"based on consideration of a local quadratic approximation of the loss. We perform a similar calculation to arrive at an\"edge of stability\"for Sharpness-Aware Minimization (SAM), a variant of GD which has been shown to improve its generalization. Unlike the case for GD, the resulting SAM-edge depends on the norm of the gradient. Using three deep learning training tasks, we see empirically that SAM operates on the edge of stability identified by this analysis.\n## Language modeling\nNext, we report on experiments training a language model. As before, we start with SGD, here in Figure 12. \n\nNext, we plot the same quantities when the network is trained with SAM, with \u03c1 = 0.3, in Figure 13. Here, the operator norm of the Hessian is significantly less than when SGD is used, and we see evidence that training in SAM operates at the edge of stability analyzed in Section 2. In Figure 14, we zoom in on the lower part of the curve, and plot the operator norm of the Hessian, to examine the relationship between this quantity and the SAM edge in more detail. \n\nFigure 15 contains plots of the training loss, once again estimated per-minibatch. We included these mainly to motivate the combinations of hyperparameters where we examined other aspects of the dynamics of SAM. As expected, while SAM does take longer to achieve a certain loss, it ultimately achieves training error similar to SGD, but with less sharpness. \n\nFigure 16 contains plots of the alignment, once again estimated per-minibatch. For the large learning rates, late in training, despite the sampling noise arising from the use of minibatches, we see a systematic tendency for the SAM gradients align more closely with the principal eigenvector of the Hessian than the gradients at the initial solution. However, for the smallest learning rates, the opposite holds. Figure 16: Alignments between gradients and the principal direction of the Hessian in the language modeling experiments.",
            "reference_string": "[262217060 | Long et al. | 2023 | Citations: 10]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "271860164",
            "title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models",
            "text": "Recent studies identify phase transition of model training (Olsson et al., 2022;Wei et al., 2022), where new capabilities or behaviors suddenly emerge when certain thresholds of model complexity are reached. The aspects of complexity often include model size, amount of training by FLOPs or tokens, and model architecture. Several prior works studied the training dynamics of language models by analyzing the internals of train-fromscratch models (Tirumala et al., 2022;Chen et al., 2023;Tian et al., 2023;Chen et al., 2024;Chang et al., 2024). The results of these works suggest that the behaviors that are often overlooked after training could be valuable signals for model analysis. In addition to train-from-scratch models, Ren and Sutherland (2024) studied the fine-tuning dynamics of language models. This work focuses on the effect of pre-training dynamics on downstream fine-tuning results by fine-tuning intermediate pretraining checkpoints on various tasks. Due to the scarcity of publically accessible intermediate pretraining checkpoints, the effect of fine-tuning at different pre-training stages is largely unexplored. Concurrent work (Snell et al., 2024) also fine-tunes intermediate pre-training checkpoints and finds that supervised fine-tuning results can be used as a signal to predict when emergence occurs, while our findings point out a dichotomy of model behavior on different datasets, with the potential for data-efficient and budget-friendly training by understanding the stages of model training.",
            "score": 0.605957493975886,
            "section_title": "Related Work",
            "char_start_offset": 25228,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1523
                }
            ],
            "ref_mentions": [
                {
                    "start": 447,
                    "end": 470,
                    "matchedPaperCorpusId": "248986465"
                },
                {
                    "start": 488,
                    "end": 506,
                    "matchedPaperCorpusId": "258947127"
                },
                {
                    "start": 524,
                    "end": 543,
                    "matchedPaperCorpusId": "261277016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99609375
        },
        {
            "corpus_id": "269042762",
            "title": "Rho-1: Not All Tokens Are What You Need",
            "text": "Investigating the training dynamics of language models is essential for understanding their behavior throughout the training process. This research includes studying internal representations [Saphra and Lopez, 2018], the acquisition of linguistic knowledge [Choshen et al., 2021, Liu et al., 2021], and the phenomenon of grokking [Power et al., 2022]. The analysis by Xia et al. [2022] is the most related to ours, which examines token-level training trajectories in models of varying sizes. Our findings, however, diverge from those of Xia et al. [2022], who posit that tokens with little change in perplexity are \"already learned\". We identify a spectrum of token patterns, including \"easy tokens\" and \"hard tokens\" that resist convergence. Recognizing this, we propose a method of selective language modeling that targets the influential tokens, optimizing the learning process.",
            "score": 0.5576720725638022,
            "section_title": "B.3 Language Model Training Dynamics",
            "char_start_offset": 26780,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 881
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99169921875
        },
        {
            "corpus_id": "268820276",
            "title": "The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis",
            "text": "This investigation into large language models (LLMs) enhances our understanding of model training complexities and scaling laws. Analyzing model dynamics across various tasks and training stages, we have gleaned insights to improve training and optimization strategies. \n\nOur study reveals the predictive power of task dynamics within a domain for unseen tasks, suggesting the benefit of adaptable training protocols. Drawing parallels between AI learning and human cognition, we see potential in applying cross-domain insights for better training outcomes.",
            "score": 0.545375491425341,
            "section_title": "Conclusion",
            "char_start_offset": 21179,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 269
                },
                {
                    "start": 272,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 557
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99169921875
        },
        {
            "corpus_id": "267897372",
            "title": "Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition",
            "text": "This paper presents work aimed at advancing the field of mechanistic understanding and training dynamics of large language models. As our work is purely theoretical, we acknowledge the potential societal consequences but do not think of any specific issues need to be highlighted in this context.",
            "score": 0.53969543277804,
            "section_title": "Impact Statement",
            "char_start_offset": 28571,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 296
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71923828125
        },
        {
            "corpus_id": "264935245",
            "title": "Training Dynamics of Contextual N-Grams in Language Models",
            "text": "Training dynamics Several works (Olsson et al., 2022;Nanda et al., 2023) discovered circuits in language models that seem causally relevant to emergent capabilities and can be ablated to reduce the capabilities. Michaud et al. (2023) postulated that many capabilities in language models emerge rapidly in phase transitions and proposed a model in which any capabilities which appear to instead form gradually can be decomposed into discrete, additive capabilities which emerge in independent phase transitions. Finally, several works (Varma et al., 2023;Nanda et al., 2023) observed the phenomenon of grokking, where a model that has already achieved near-perfect performance on the training data transitions to a mechanism that generalizes. \n\nWhile only a minority of circuits-style analysis has focused on model development, a wealth of results in the field of developmental interpretability have characterized the training dynamics of language models using other units of analysis. Behavioral analysis has shown that many model capabilities, such as predicting parts of speech and individual tokens, are learned during independent phase transitions characterized by varying slopes and points of maximum curvature (Chiang et al., 2020;Wei et al., 2022). Raghu et al. (2017) proposed singular vector canonical correlation analysis, a tool which allows different model layers and networks to be compared, and showed that early network layers converge earlier in training than late layers. Saphra and Lopez (2020b,a) found that LSTM models learn hierarchical sequential features in natural language bottom up -shorter sequences are learned first and form the basis for the representation of longer sequences that contain the shorter ones. Finally, Saxe et al. (2019) found that the features are learned in an order determined by the magnitude of the corresponding singular value in the model. Dodge et al. (2020) found that differences in random parameter initializations and training data order contribute comparably to differences between learned representations of fine-tuning runs of the same model on identical data, and McCoy et al. (2020) found that identically trained BERT transformer models are consistent with this finding, varying widely in generalization performance.",
            "score": 0.5338107389463258,
            "section_title": "Related Work",
            "char_start_offset": 17886,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2279
                }
            ],
            "ref_mentions": [
                {
                    "start": 1216,
                    "end": 1237,
                    "matchedPaperCorpusId": "222140842"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "233289478",
            "title": "Probing Across Time: What Does RoBERTa Know and When?",
            "text": "Learning dynamics Early work (McClelland and Rumelhart, 1986) observed the dynamics in a feedforward neural network to assess the cognitive plausibility of a connectionist model. They found staged learning in past tense acquisition, similar to humans. More recently, Saphra and Lopez (2019) studied linguistic and topic learning dynamics in hidden states of an LSTM language model. They found that syntactic information is encoded at an early training stage, which is consistent with our finding despite the difference in training objective and network architecture. Our work, instead, uses a rich set of probes to examine more diverse aspects of language and analyzes training iterations.\n\nConcurrent work (Chiang et al., 2020) is the closest work to ours and uses probes to investigate the learning dynamics as well. However, they find that linguistic knowledge and factual knowledge do not generally improve as pretraining proceeds, we find that factual and commonsense knowledge do ( \u00a73); we attribute such difference to our more systematic choice of probes -not only adding two more categories (i.e. commonsense and reasoning) but also more tasks in the linguistic and factual knowledge categories. For example, we found that the factual knowledge probed using SQuAD and ConceptNet data still increases as the pretraining progresses. However, Chiang et al. (2020) only used a subset of T-REx, which plateaus quickly according to our experiments. Sharing their concern of how data affects pretraining, we empirically investigate how the domain of pretraining corpus affects the dynamics of different types of knowledge.\n\nZhang et al. (2020) investigate masked language models trained on corpora of varying sizes in a domain. They experiment with linguistic probes and show that 90% of the improvement in syntactic probing performance can be achieved with a pretraining corpus of only about 10M words. In contrast, the probing performance in commonsense knowledge suffers from small training data. Different from the final models obtained in their work, we consider the entire pretraining trajectories. Nevertheless, our findings by varying corpus size ( \u00a74) are consistent with their conclusion and additionally we find that adding more data in diverse domains can improve both types of knowledge. Their results on the relation between the corpus size and probing performance also support our finding that linguistic knowledge is generally easy to learn",
            "score": 0.5161455617518966,
            "section_title": "Related Work and Further Discussion",
            "char_start_offset": 21352,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98046875
        },
        {
            "corpus_id": "256460933",
            "title": "Calibrating Student Models for Emotion-related Tasks",
            "text": "In an attempt to provide the teacher models with more useful information and impart the student models with better supervision signals during the distillation of the emotion-related models, we empirically examine standard mixup and propose a simple yet effective strategy to hone the perfor-mance and calibration of the student model in a distillation setup. \n\nGiven a training dataset of sentence-label pairs \n\ni=1 and a language model f , standard mixup creates the vicinal dataset by calculating a weighted average of training points based on the following simple rule by (Zhang et al., 2018): \n\nwhere (x i , y i ) and (x j , y j ) are two input examples that are randomly drawn from the training set, and weight \u03bb is sampled from a beta distribution, \u03b2(\u03b1, \u03b1) with parameter \u03b1 > 0, generally taken to be relatively small, so that the weighted averages do not stray too far from the original data points. Mixup augments the training data by linearly interpolating training samples and their corresponding labels in the input space. \n\nWe propose to use a novel mixup data augmentation technique on the teacher models that is informed by training dynamics to improve the student model calibration on the distillation objective. Our proposed mixup creates vicinal distribution steered by the data maps (Swayamdipta et al., 2020) as described below. \n\nMixup with Training Dynamics. Figure 1 depicts our proposed mixup strategy for teacher-student framework in the self-distillation setting (where both the teacher and student models have the same architecture, e.g., RoBERTa). We first contextualize each training instance of our D train into three categories, namely easy-to-learn, ambiguous, and hard-to-learn, based on training dynamics (statistics deriving from the behavior of the model across time). The training dynamics of instance (x i , y i ) are defined as statistics, i.e., confidence and variability computed across the E epochs. Confidence is calculated as the mean model probability of the true label y i across epochs: \n\nwhere \u03b8 denotes our model parameters and p \u03b8e indicates the model's probability at the end of the e th epoch. Intuitively, a high-confidence instance is easier for the given learner.",
            "score": 0.4979473917980352,
            "section_title": "Mixup Training",
            "char_start_offset": 12524,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 409
                },
                {
                    "start": 412,
                    "end": 596
                },
                {
                    "start": 599,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1033
                },
                {
                    "start": 1036,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1347
                },
                {
                    "start": 1350,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2032
                },
                {
                    "start": 2035,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 575,
                    "end": 595,
                    "matchedPaperCorpusId": "3162051"
                },
                {
                    "start": 1301,
                    "end": 1327,
                    "matchedPaperCorpusId": "221856637"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8955078125
        },
        {
            "corpus_id": "265281289",
            "title": "Energy and Carbon Considerations of Fine-Tuning BERT",
            "text": "Another related direction of research examines the dynamics of pre-training and fine-tuning of language models and the influence of factors like random seeds and early stopping (Dodge et al., 2020), scaling (Tay et al., 2022) and learning dynamics (Hao et al., 2020). While all of these studies have shed important light on these processes, in practice most of the decisions made remain empirical, with practitioners either referring to previous work (when hyperparameters and other training details are reported), or using techniques such as grid or random search (Bergstra and Bengio, 2012) to converge on optimal parameter values. Our own work builds upon both of these research directions. We study both the pre-training and fine-tuning process, and our experiments for studying their energy intensity are based on the works cited above.",
            "score": 0.4955728088946064,
            "section_title": "Related Work",
            "char_start_offset": 5154,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 841
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 225,
                    "matchedPaperCorpusId": "260498358"
                },
                {
                    "start": 248,
                    "end": 266,
                    "matchedPaperCorpusId": "227905362"
                },
                {
                    "start": 565,
                    "end": 592,
                    "matchedPaperCorpusId": "15700257"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8994140625
        },
        {
            "corpus_id": "269449894",
            "title": "Temporal Scaling Law for Large Language Models",
            "text": "Our research introduces the novel concept of Temporal Scaling Law within the context of Large Language Models (LLMs), shifting the focus from static attributes of model scalability to the dynamic, temporal aspects of training. We analyze the loss patterns associated with each token and discovered that these patterns conform to a reciprocallaw across different scales and stages of training. Utilizing the reciprocal-law, we could precisely predict the pre-training trajectory for the test loss (or perplexity) of LLMs, marking a significant improvement over baseline methods. This capability is crucial for numerous possible applications during training, like tuning data mixture proportions, selecting hyperparameters, and determining the precise point for early stopping. With the temporal scaling law, we provide empirical evidence supporting that LLMs enjoy an equalized learning approach across different token positions. Our findings validate the existing training paradigms where no position-based re-weighting strategies are applied to different token positions. \n\nLimitations. Our temporal scaling law is derived on the mainstream structure of LLMs, the GPT-based decoder transformers. The applicabil-ity of our findings to other architectures, such as Mixed of Experts models, remains to be explored. Furthermore, our research primarily focuses on the pre-training stage. The dynamics in other scenarios, such as transfer learning, are not covered. We leave the further investigations to our future research.",
            "score": 0.49181502987762865,
            "section_title": "Conclusion",
            "char_start_offset": 25281,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1072
                },
                {
                    "start": 1075,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1520
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99365234375
        },
        {
            "corpus_id": "271909320",
            "title": "Scaling Law with Learning Rate Annealing",
            "text": "We apply our proposed formulation to validate and provide a theoretical explanation for numerous existing experimental findings regarding the training dynamics of language models. These key insights also guide researchers in selecting critical LRS before initiating model training. An interesting summary is that \n\nThe art of learning rate schedule lies in the delicate balancing act between forward area and annealing area.",
            "score": 0.49062239613508524,
            "section_title": "TAKEAWAYS: EXPERIMENTAL FINDINGS VERIFICATION AND EXPLANATION",
            "char_start_offset": 20132,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 312
                },
                {
                    "start": 315,
                    "end": 424
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98828125
        },
        {
            "corpus_id": "247656607",
            "title": "Emergent Structures and Training Dynamics in Large Language Models",
            "text": "Training dynamics is an emerging field of research, promising to improve our understanding of knowledge acquisition in neural networks and offering insights into the utility of pre-trained models and embedded representations for downstream tasks. Most studies of Transformers (e.g. RoBERTa (Zhuang et al., 2021)) and LSTMs (Hochreiter and Schmidhuber, 1997) agree that models acquire linguistic knowledge early in the learning process.\n\nLocal syntactic information, such as parts of speech, is learned earlier than information encoding long-distance dependencies (e.g. topic) Saphra, 2021). Exploration of AL-BERT (Lan et al., 2019) and LSTM-based networks reveals different learning patterns for function and content words with more fine-grained distinctions within these categories including part of speech and verb form (Saphra, 2021;Chiang et al., 2020).\n\nDifferences in learning trajectory were also observed between layers. In LSTMs, recurrent layers become more task-independent over the course of training, while embeddings become more taskspecific (Saphra, 2021). In Transformer-based architectures, i.e.: ALBERT and ELECTRA, Chiang et al. (2020) observe differences in performance patterns between the top and last layers. Similarly to other areas of research in NLP, most of the literature on training dynamics concentrate on English-language models. Another possible direction for future work is extending studies conducted on LSTMs to more widely used Transformers.",
            "score": 0.4872818586341583,
            "section_title": "Training Dynamics of Internal Representation Development",
            "char_start_offset": 17316,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 290,
                    "end": 311,
                    "matchedPaperCorpusId": "237101158"
                },
                {
                    "start": 323,
                    "end": 357,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 837,
                    "end": 857,
                    "matchedPaperCorpusId": "222140842"
                },
                {
                    "start": 1135,
                    "end": 1155,
                    "matchedPaperCorpusId": "222140842"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99267578125
        },
        {
            "corpus_id": "271213641",
            "title": "Learning Dynamics of LLM Finetuning",
            "text": "Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, gives us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during different types of finetuning, by analyzing the step-wise decomposition of how influence accumulates among different potential responses. Our framework allows a uniform interpretation of many interesting observations about the training of popular algorithms for both instruction tuning and preference tuning. In particular, we propose a hypothetical explanation of why specific types of hallucination are strengthened after finetuning, e.g., the model might use phrases or facts in the response for question B to answer question A, or the model might keep repeating similar simple phrases when generating responses. We also extend our framework and highlight a unique\"squeezing effect\"to explain a previously observed phenomenon in off-policy direct preference optimization (DPO), where running DPO for too long makes even the desired outputs less likely. This framework also provides insights into where the benefits of on-policy DPO and other variants come from. The analysis not only provides a novel perspective of understanding LLM's finetuning but also inspires a simple, effective method to improve alignment performance.",
            "score": 0.48628703358765857,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98681640625
        },
        {
            "corpus_id": "276937763",
            "title": "PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs",
            "text": "Our experiments on downstream performance ( \u00a73), intermediate representations ( \u00a74), and model parameters ( \u00a75) allow us to examine the stability of training and find outlier runs using different methods across the model training pipeline. In this section, we analyse commonalities across the resulting metrics to identify broader characteristics of LM pre-training dynamics. \n\nLanguage modelling is largely stable. Generally, we observe LM pre-training dynamics to follow consistent trajectories. Across seeds and model sizes, downstream performance and representational efficiency consistently increase during pre-training, and training maps are linear (except for the outlier seeds). Furthermore, model scaling laws seem to hold across seeds, not only for downstream performance but also for information content and representational efficiency of model representations. \n\nSimilarly, both at the performance and representational level, we observe the effect of \"saturation\" (Michaelov and Bergen, 2023;Godey et al., 2024) in smaller models. \n\nLinguistic information is encoded in the initial learning phase (10 3 -10 4 steps). Across all experiments, metrics start moving away from the initial random baseline around step 10 3 (2B tokens circa) and reach their convergence level around step 10 4 (20B tokens circa). In this initial phase, representational shift peaks and linguistic information begins to be encoded into the models' latent representations. Through the lens of multiple metrics, we can analyse model behaviour in this phase in detail. Specifically, while the amount and the efficiency with which linguistic information is encoded in model representations have already increased substantially at step 10 3 , the model does not yet generate coherent outputs, as indicated by low performance on linguistic acceptability benchmarks like BLiMP (Gender Agreement). Simultaneously, self-consistency is low while inter-seed agreement is high, which we hypothesise to be an artefact of all models initially choosing an incorrect baseline answer. In terms of training maps, this phase corresponds to the 0 \u2192 1 transition, which occurs consistently in this initial training phase for all model sizes (except for the 410M outliers). \n\nMost improvements happen in the \"critical\" learning phase (10 4 -10 5 steps). In the range of 10 3 to 10 4 steps, most learning occurs, as measured by all of our metrics.",
            "score": 0.48574325876560387,
            "section_title": "DISCUSSION",
            "char_start_offset": 28106,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 872
                },
                {
                    "start": 875,
                    "end": 1042
                },
                {
                    "start": 1045,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2238
                },
                {
                    "start": 2241,
                    "end": 2318
                },
                {
                    "start": 2319,
                    "end": 2411
                }
            ],
            "ref_mentions": [
                {
                    "start": 976,
                    "end": 1004,
                    "matchedPaperCorpusId": "258865474"
                },
                {
                    "start": 1004,
                    "end": 1023,
                    "matchedPaperCorpusId": "269042847"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99609375
        },
        {
            "corpus_id": "258947127",
            "title": "Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer",
            "text": "The Transformer architecture [1] has demonstrated wide applications in multiple research domains, including natural language processing [2,3,4], computer vision [5,6,7], speech [8,9], multimodality [10,11], etc. Recently, large language models (LLMs) based on decoder-only Transformer architecture also demonstrate impressive performance [4,12,13], after fine-tuned with instruction data [14] or reward models [15]. Why a pre-trained model, often supervised by simple tasks such as predicting the next word [4,3,13] or filling in the blanks [2,16,17], can learn highly valuable representations for downstream tasks, remains a mystery. \n\nTo understand how Transformer works, many previous works exist. For example, it has been shown that Transformer is a universal approximator [18], can approximate Turing machines [19,20], and can perform a diverse set of tasks, e.g., hierarchical parsing of context-free grammar [21], if its weights are set properly. However, it is unclear whether the weights designed to achieve specific tasks are at a critical point, or can be learned by SoTA optimizers (e.g., SGD, Adam [22], AdaFactor [23], AdamW [24]). In fact, many existing ML models, such as k-NN, Kernel SVM, or MLP, are also universal approximators, while their empirical performance is often way below Transformer. \n\nTo demystify such a behavior, it is important to understand the training dynamics of Transformer, i.e., how the learnable parameters change over time during training. In this paper, as a first step, we formally characterize the SGD training dynamics of 1-layer position-encoding-free Transformer for 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2305.16380v4 [cs.CL] 30 Oct 2023 next token prediction, a popular training paradigm used in GPT series [3,4], in a mathematically rigorous manner.",
            "score": 0.4796516418966448,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1313
                },
                {
                    "start": 1316,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1841
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 141,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 141,
                    "end": 143,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 164,
                    "end": 166,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 166,
                    "end": 168,
                    "matchedPaperCorpusId": "243985980"
                },
                {
                    "start": 198,
                    "end": 202,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 202,
                    "end": 205,
                    "matchedPaperCorpusId": "246652264"
                },
                {
                    "start": 338,
                    "end": 341,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 410,
                    "end": 414,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 507,
                    "end": 510,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 510,
                    "end": 512,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 544,
                    "end": 547,
                    "matchedPaperCorpusId": "252780443"
                },
                {
                    "start": 547,
                    "end": 550,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 815,
                    "end": 819,
                    "matchedPaperCorpusId": "236469455"
                },
                {
                    "start": 819,
                    "end": 822,
                    "matchedPaperCorpusId": "233364065"
                },
                {
                    "start": 1127,
                    "end": 1131,
                    "matchedPaperCorpusId": "4786918"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9697265625
        },
        {
            "corpus_id": "248986465",
            "title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models",
            "text": "Memorization in Language Models: Unintended memorization is a known challenge for language models [14,85], which makes them open to extraction attacks [15,89] and membership inference attacks [41,64], although there has been work on mitigating these vulnerabilities [51,88]. Recent work has argued that memorization is not exclusively harmful, and can be crucial for certain types of generalization (e.g., on QA tasks) [11,46,87], while also allowing the models to encode significant amounts of world or factual knowledge [4,35,71]. There is also a growing body of work analyzing fundamental properties of memorization in language models [16,47,60,95]. Most related to our work Carlini et al. [16] analyzes memorization of fully trained language models and observes a dependence on model scale, training data duplication, and prompting context length. While we also study scaling behavior, our focus instead is on the memorization dynamics throughout training. \n\nLanguage Model Training Dynamics: Previous work has extensively analyzed training dynamics to understand how neural models acquire information over training [1,30,34,66,74]. Saphra and Lopez [80] were the first to analyze training dynamics for language modeling, focusing on the evolution of internal representations over pre-training. This inspired a line of work analyzing how neural language models learn linguistic structure/world knowledge [20,21,53], individual words [17], and cross-lingual structure [10] over pre-training. This analysis has been extended to many downstream tasks, including text summarization [33], machine/speech translation [81,86,92], and various NLP tasks [36,61]. \n\nForgetting in Language Models: There has also been work studying memory degradation (forgetting) in language models. Catastrophic forgetting or catastrophic interference, first reported in [59,77], studies how neural networks tend to forget the information from previous trained tasks or training batches, when trained on new data. This provides a key challenge for continual learning (or life-long learning) [19], where the goal is to gradually learn from a single pass over a, typically very large, stream of data.",
            "score": 0.4788464259136644,
            "section_title": "Background and Related Work",
            "char_start_offset": 1743,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 960
                },
                {
                    "start": 963,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1657
                },
                {
                    "start": 1660,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2176
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 102,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 102,
                    "end": 105,
                    "matchedPaperCorpusId": "159041003"
                },
                {
                    "start": 151,
                    "end": 155,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 155,
                    "end": 158,
                    "matchedPaperCorpusId": "220658693"
                },
                {
                    "start": 1120,
                    "end": 1123,
                    "matchedPaperCorpusId": "108298098"
                },
                {
                    "start": 1129,
                    "end": 1132,
                    "matchedPaperCorpusId": "49271358"
                },
                {
                    "start": 1437,
                    "end": 1441,
                    "matchedPaperCorpusId": "238408423"
                },
                {
                    "start": 1615,
                    "end": 1619,
                    "matchedPaperCorpusId": "250390453"
                },
                {
                    "start": 1619,
                    "end": 1622,
                    "matchedPaperCorpusId": "237239585"
                },
                {
                    "start": 1622,
                    "end": 1625,
                    "matchedPaperCorpusId": "224818197"
                },
                {
                    "start": 1649,
                    "end": 1653,
                    "matchedPaperCorpusId": "227905362"
                },
                {
                    "start": 1653,
                    "end": 1656,
                    "matchedPaperCorpusId": "216914339"
                },
                {
                    "start": 1849,
                    "end": 1853,
                    "matchedPaperCorpusId": "61019113"
                },
                {
                    "start": 1853,
                    "end": 1856,
                    "matchedPaperCorpusId": "18556305"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9990234375
        },
        {
            "corpus_id": "274823085",
            "title": "Training Dynamics of a 1.7B LLaMa Model: A Data-Efficient Approach",
            "text": "Our findings, along with the provided training logs, checkpoints, and sample outputs, aim to guide future researchers and practitioners in refining both pre-training and post-training strategies. By documenting the full trajectory from pre-training dynamics to post-training refinement, we emphasize a process-focused perspective where transparency and detailed insights foster reproducibility and more effective model development. This approach lays a foundation for future studies to explore additional best practices, methodological refinements, and novel techniques in both pre-training and instruction-tuning phases of large language models.",
            "score": 0.4781339282833546,
            "section_title": "Conclusion",
            "char_start_offset": 36146,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 646
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98486328125
        },
        {
            "corpus_id": "248986465",
            "title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models",
            "text": "The rate and extent to which a model memorizes its training data are key statistics that provide evidence about how it is likely to generalize to new test instances. Classical frameworks, such as bias-variance tradeoff [31], argued for fitting a training set without full memorization. However, recent work has established a more symbiotic relationship between memorization and generalization in deep learning [13,26,28]. This paper empirically studies memorization in causal and masked language modeling, across model sizes and throughout the training process. \n\nMuch of the recent performance gains for language models have come from scale, with the most recent models reaching up to 10 11 parameters [22,73,83]. Larger models are also known to memorize more training data [16], which is a crucial component of their improved generalization. However, perhaps surprisingly, relatively little work has been done in understanding the impact of scale on the dynamics of language model memorization over training. Existing work focuses on analyzing memorization post-training [16,47,88,95]. In this work, we study the memorization and forgetting dynamics in language models, with a focus on better measuring how they change as we scale up model size. Our primary contributions include: \n\n1. We measure the dependence of memorization dynamics over training on model size (and other factors such as dataset size, overfitting, and learning rate). We find that larger language models memorize training data faster ( \u00a7 4). 2. We design controlled experiments that allow us to characterize the forgetting curves in language models (i.e., how language models naturally forget memories throughout training).",
            "score": 0.47389060568496133,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 561
                },
                {
                    "start": 564,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1696
                }
            ],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 223,
                    "matchedPaperCorpusId": "58068920"
                },
                {
                    "start": 410,
                    "end": 414,
                    "matchedPaperCorpusId": "228376138"
                },
                {
                    "start": 417,
                    "end": 420,
                    "matchedPaperCorpusId": "225366984"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9990234375
        },
        {
            "corpus_id": "14188931",
            "title": "Language Dynamics and Capitalization using Maximum Entropy",
            "text": "Results presented so far use isolated training. A new approach is now proposed, which consists of training with new data, but starting with previously calculated models. In other words, previously trained models provide initialized models for the new train. As the training is still performed with the new data, the old models are iteratively adjusted to the new data. This approach is a very clean framework for language dynamics adaptation, offering a number of advantages: (1) new events are automatically considered in the new models; \n\n(2) with time, unused events slowly decrease in weight; \n\n(3) by sorting the trained models by their relevance, the amount of data used in next training stage can be limited without much impact in the results. Table 3 shows the re- sults achieved with this approach, revealing higher performance as more training data is available. \n\nThe next experiment shows that training order is important. In fact, from previous results, the increase of performance may be related only with the number of events seen so far. For this reason, another experiment have been performed, using the same training data, but retraining backwards. Corresponding results are illustrated in Figure 3, revealing that: the backwards training results are worse than forward training results, and that backward training results do not allways increase, rather stabilize after a certain amount of data. Despite the fact that both training use all training data, in the case of forward training the time gap between the training and testing data gets smaller for each iteration, while in the backwards training is grows. From these results we can conclude that a strategy based on retraining is suitable for using large amounts of data and for language adaptation.",
            "score": 0.47287424440068626,
            "section_title": "Retraining",
            "char_start_offset": 8364,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 538
                },
                {
                    "start": 541,
                    "end": 596
                },
                {
                    "start": 599,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 872
                },
                {
                    "start": 875,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1775
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94287109375
        },
        {
            "corpus_id": "272826711",
            "title": "Rethinking Conventional Wisdom in Machine Learning: From Generalization to Scaling",
            "text": "Training instability in large language models (LLMs) is a widely recognized challenge within the research community (Liu et al., 2020;Chowdhery et al., 2023;Dehghani et al., 2023;Zhang et al., 2023;Molybog et al., 2023;Cohen et al., 2022;Wortsman et al., 2023). In this section, we reproduce this phenomenon and explore potential solutions. While our initial fix appears to resolve the issue, it hides a deeper problem in scaling strategies. , 2, 3, 4, 6, 8, 12, 16, 20, 24, 32, 40, 48]. All models share the same batch size (256), number of layers ( 6), sequence length (512), and training steps (200,000, not Chinchilla-Optimal). We use a constant learning rate LR = 2/1024 for all model sizes. \n\nIn Figure 11a, we plot the evaluation loss against computational cost (flops) for each model. While performance generally improves with scale, a discontinuity emerges around 10 2 exaflops (D = 3072). Examining the learning dynamics for D = 3072 (Figure 11b) reveals training instability in larger models: the evaluation loss spikes during training (between 1,000-10,000 steps) but self-corrects later. Clearly, our scaling approach requires adjustment. \n\nFortunately, several techniques exist to address training instability: Z-loss (Chowdhery et al., 2023), extended warmup periods (Wortsman et al., 2023), learning rate reduction, increased weight decay, and QK-Norm (to prevent attention logit explosion (Dehghani et al., 2023)). \n\nWe try QK-Norm, and it proves effective. Not only does it resolve the instability for D \u2265 3072 (Figure 11b), but it also enhances performance for larger model sizes (Figure 11c). However, a crucial question remains: have we truly addressed the underlying issue, or have we merely patched a symptom? We evaluate these proposals with and without QK-Norm. Results are presented in Figure 11.",
            "score": 0.4726219073544885,
            "section_title": "Warmup: Training Instability",
            "char_start_offset": 23091,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1151
                },
                {
                    "start": 1154,
                    "end": 1431
                },
                {
                    "start": 1434,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1822
                }
            ],
            "ref_mentions": [
                {
                    "start": 134,
                    "end": 157,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 157,
                    "end": 179,
                    "matchedPaperCorpusId": "256808367"
                },
                {
                    "start": 1232,
                    "end": 1256,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1406,
                    "end": 1429,
                    "matchedPaperCorpusId": "256808367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "271213641",
            "title": "Learning Dynamics of LLM Finetuning",
            "text": "Deep neural networks usually acquire new knowledge by updating their parameters via gradient descent (GD). This procedure can be described by learning dynamics, which links changes in the model's predictions to the gradients generated by learning specific examples. With the help of learning dynamics, researchers have not only explained many interesting phenomena during training, but used these insights to propose novel, improved algorithms in different problems (e.g. Pruthi et al. 2020;Ren, Guo, et al. 2023;Xia et al. 2024). \n\nThe study of large language models (LLM) is gaining popularity due to their surprising capabilities on various tasks. To ensure the LLMs follow human instructions and align well with human preferences, finetuning has attracted much recent attention. Practitioners often start with instruction tuning, where the model learns extra knowledge necessary for the downstream task, and then preference tuning, where the model aligns its outputs to human preference (Ouyang et al. 2022). Various finetuning algorithms have been proposed to fit into this pipeline, with differing explanations as to why they improve the model's performance. \n\nContrary to most existing analyses of LLM finetuning, which use the perspective of their training targets, their status at the end of training, or their relationships to reinforcement learning (e.g. Ji et al. 2024;Rafailov et al. 2024;Tajwar et al. 2024), this paper tries to understand LLMs' evolution from a dynamical perspective. Specifically, we formalize the learning dynamics of LLM finetuning by decomposing the change of the model's prediction into three terms which play different roles. This framework can be easily adapted to various finetuning algorithms with different goals, including supervised finetuning (SFT, Wei et al. 2022), self-play finetuning (SPIN, Z. Chen et al. 2024), direct preference optimization (DPO, Rafailov et al. 2023), and related variants.",
            "score": 0.46506167550132826,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1164
                },
                {
                    "start": 1167,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 991,
                    "end": 1011,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9775390625
        },
        {
            "corpus_id": "248300248",
            "title": "Context-Aware Language Modeling for Goal-Oriented Dialogue Systems",
            "text": "also interpret a language model as learning the distribution over \u03c4 . This distribution can be factored into the product of the policy \u03c0(a t |\u03c4 <t ) and the dynamics T (\u03c4 <t+1 |\u03c4 <t , a t ), and so we can say that a language model also represents the policy and the dynamics. Therefore, the maximum likelihood objective for training or finetuning a language model on a dialogue dataset D off consisting of dialogue trajectories \u03c4 can be written as\n\nwhere \u03c0 \u03b8 (a t |o t ) represents a policy that generates new dialogue based on the observed context and dialogue history, and T \u03b8 (\u03c4 <t+1 |\u03c4 <t , a t ) represents the observed dynamics characterizing human responses, and \u03b8 denotes parameters in \u03c0 and T . Note that \u03c4 <t consists only of the conversation history, and does not contain any task-specific context. A na\u00efve approach to train dialogue systems is to jointly parameterize both \u03c0 and T as one language model, and optimize Equation 2 on pre-collected conversations D off . This method corresponds to behavioral cloning (BC) (Pomerleau, 1989). Context conditioning. While an agent trained using Equation 2 can learn policies and dynamics that imitate human conversations, this objective does not incorporate the task goal, and may not produce a policy that is more performant than the dataset D off . While it is possible to input c o into the language model to maximize the conditional probability of P (\u03c4 |c o ) using a conditional version of the language modeling objective, L CT X (\u03b8),\n\ncontexts with particular task structures (e.g., a set of entries in a table) may not be simply processed as a sequence similarly to \u03c4 . Additionally, the language model is not pretrained to read structured context, and oftentimes the recent dialogue history is much more predictive of the next utterance than the task context is. As a result, language models can ignore the task context and only learn P (\u03c4 ) despite being conditioned on c o . Our approach builds on this conditional modeling approach, but makes a number of improvements that allow it to be more aware of the context information, which attains significantly better results in our experiments.",
            "score": 0.46391423219385575,
            "section_title": "Preliminaries",
            "char_start_offset": 11252,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1030,
                    "end": 1047,
                    "matchedPaperCorpusId": "18420840"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9296875
        },
        {
            "corpus_id": "277349236",
            "title": "How do language models learn facts? Dynamics, curricula and hallucinations",
            "text": "Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distributions lead to shorter plateaus. Finally, hallucinations emerge simultaneously with knowledge, and integrating new knowledge into the model through fine-tuning is challenging, as it quickly corrupts its existing parametric memories. Our results emphasize the importance of data distribution in knowledge acquisition and suggest novel data scheduling strategies to accelerate neural network training.",
            "score": 0.46254871793668956,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "247447316",
            "title": "Staged Training for Transformer Language Models",
            "text": "The current standard approach to scaling transformer language models trains each model size from a different random initialization. As an alternative, we consider a staged training setup that begins with a small model and incrementally increases the amount of compute used for training by applying a\"growth operator\"to increase the model depth and width. By initializing each stage with the output of the previous one, the training process effectively re-uses the compute from prior stages and becomes more efficient. Our growth operators each take as input the entire training state (including model parameters, optimizer state, learning rate schedule, etc.) and output a new training state from which training continues. We identify two important properties of these growth operators, namely that they preserve both the loss and the\"training dynamics\"after applying the operator. While the loss-preserving property has been discussed previously, to the best of our knowledge this work is the first to identify the importance of preserving the training dynamics (the rate of decrease of the loss during training). To find the optimal schedule for stages, we use the scaling laws from (Kaplan et al., 2020) to find a precise schedule that gives the most compute saving by starting a new stage when training efficiency starts decreasing. We empirically validate our growth operators and staged training for autoregressive language models, showing up to 22% compute savings compared to a strong baseline trained from scratch. Our code is available at https://github.com/allenai/staged-training.",
            "score": 0.4604943092020033,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97021484375
        },
        {
            "corpus_id": "268820276",
            "title": "The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis",
            "text": "In this work, we first investigate the dynamics of several open-sourced large language models (i.e., Baichuan-7B (Yang et al., 2023), DeepSeek-7B (DeepSeek-AI et al., 2024), Amber-7B (Liu et al., 2023), OpenLLaMA-7B (Geng & Liu, 2023;Touvron et al., 2023;Computer, 2023), Yi-34B (AI et al., 2024) and DeepSeek-67B), and analyze their performance results across diverse tasks using the corresponding intermediate checkpoints based on the number of pre-trained tokens. Then, based on the theoretical framework of the Scaling Law (Kaplan et al., 2020), we analyze the performance patterns of different models across various downstream tasks, and provide findings to facilitate further research on the training dynamics of the LLMs, where the findings are shown as follows: \n\n\u2022 Findings on task dynamic prediction: Within the training process of LLMs, we observe that the dynamics of existing downstream tasks within a domain can predict the dynamics of unseen tasks. This suggests that a model's performance on known tasks can inform us about how it might perform on similar, yet unseen tasks within the same domain. (Section 4.1.1) \n\n\u2022 Findings on cross-domain promotion: Similar to the human cognitive process, the enhancement of different abilities across various domains progresses from basic to advanced levels following curriculum learning. The curriculum between crossdomain tasks can guide the training direction of models and the insights gained from one domain can potentially promote the learning process of other domains. (Section 4.1.2) \n\n\u2022 Findings on the effect of training strategies, model architecture, etc. : Based on the results of several 7b-scale models (Baichuan2-7b, DeepSeek-7b, OpenLLaMA-7b, and Amber-7b), we comprehensively analyze the effect of training strategies, dataset quality, and model architecture in training LLMs. For example, we observe that the training dataset, learning rate adjustments, batch size, and regularization techniques play a significant role in the learning efficiency in the early training stage (Section 4.2.1).",
            "score": 0.46013654483073024,
            "section_title": "Preprint",
            "char_start_offset": 968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1546
                },
                {
                    "start": 1549,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 2065
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99169921875
        },
        {
            "corpus_id": "268720815",
            "title": "Distilling dynamical knowledge from stochastic reaction networks",
            "text": "To design such a student model, we draw inspiration from the development of the transformer-based large language models that have been pretrained on web-scale datasets. These models have demonstrated exceptional text generation capabilities and exhibit zero-shot and few-shot generalization across various natural language understanding tasks (28,29). New capabilities can be unlocked by providing specific text prompts to shape the behavior of the pretrained foundation model. This tuning process allows the model to handle new tasks and data distributions that were not encountered during training (30). When valid prompts are provided, the language models generate words whose conditional joint distributions align with the training data given the provided context. We want to highlight that a direct correspondence can be established between natural language and stochastic dynamics. This can be accomplished by constructing a word space where the size of the vocabulary represents the range of states, and the length of the text corresponds to the dimension of the state space. Therefore, we believe that language models have the potential to represent the conditional joint probability of stochastic dynamical systems, given prompts that contain information about the formula of the CME, the equation parameters, and the initial conditions. Consequently, we have designed the student neural network model to serve as a knowledge repository for SRNs, following the transformer architecture with a multi-head self-attention mechanism, which we term the master equation transformer (MET). The architecture of MET is depicted in Fig. 1A and is discussed in detail in the Materials and Methods. A comparative analysis of MET over alternative approaches, such as the conditional normalizing flow method, can be found in SI Appendix. \n\nFor training prompt-based large language models, reinforcement learning with human feedback has proven to be a potent approach, aligning pretrained language models with intricate human tendencies (31). In our work, we adopt a similar approach, albeit with a noteworthy departure: We substitute human feedback with model feedback derived from the CME through the aid of teacher models, a major component of our knowledge distillation process. We employ reinforcement learning for two key reasons: Its capacity for efficient state space exploration and its stable convergence properties. First, reinforcement learning enables the model to select states based on its variational distributions in response to various prompts. Consequently, states with higher probabilities are sampled more often.",
            "score": 0.45776216382629153,
            "section_title": "Distilling Dynamical Knowledge with Reinforcement Learning.",
            "char_start_offset": 16651,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1832
                },
                {
                    "start": 1835,
                    "end": 2036
                },
                {
                    "start": 2037,
                    "end": 2276
                },
                {
                    "start": 2277,
                    "end": 2420
                },
                {
                    "start": 2421,
                    "end": 2556
                },
                {
                    "start": 2557,
                    "end": 2627
                }
            ],
            "ref_mentions": [
                {
                    "start": 343,
                    "end": 347,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 600,
                    "end": 604,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 2031,
                    "end": 2035,
                    "matchedPaperCorpusId": "221665105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91259765625
        },
        {
            "corpus_id": "236709408",
            "title": "Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? - A computational investigation",
            "text": "Hoang & Wang, 2015, for an overview), typically tested on TIMIT corpus (Garofolo et al., 1993). In addition, relatively low precision of the segmentation scores means that there is a substantial amount of oversegmentation involved in the process. In other words, the network dynamics have some correspondence to linguistic unit boundaries in time, but the segmentation behavior is far from being perfect. The observed performance is also somewhat worse than that reported by Harwath and Glass (2019) despite using the same model architecture and training protocol (CNN0). The reason for this difference is unclear, but may have to do with the different selection of the test data, as Harwath and Glass used read speech from TIMIT to test their model trained on Places. In general, it appears that temporal dynamics of the models are informative of phonetic, and to a smaller degree, syllabic and word boundaries. However, the effects of training are much less pronounced than with the selectivity analyses. Again, the main pattern of results does not seem to depend on whether the data are real or synthetic speech in nature.",
            "score": 0.45732318312132936,
            "section_title": "Results from Temporal Segmentation Analyses",
            "char_start_offset": 95975,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 475,
                    "end": 499,
                    "matchedPaperCorpusId": "67855487"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.339599609375
        },
        {
            "corpus_id": "270620163",
            "title": "Probing the Emergence of Cross-lingual Alignment during LLM Training",
            "text": "Concurrently, there is a rising interest in understanding the training dynamics of LLMs.Works that mainly examine monolingual English models report a steady trend in the acquisition of linguistic knowledge.Both Xia et al. (2023) and Choshen et al. (2022) argue that language acquisition undergoes the same order of phase transitions consistently across model scales, training objectives and random seeds.Chen et al. (2024) find that the emergence of syntactic structure in the attention scores of Transformer-based LMs is essential for grammar acquisition in LMs, but does not account for semantic knowledge acquisition.For multilingual training, Choenni et al. (2023) examine how data size and language variance affect the performance during fine-tuning.The experiments presented by Blevins et al. (2022) are the most reminiscent of      our work.They focus on the inconsistency between the emergence of in-language and cross-language abilities for encoder LMs, whereas we study the dynamics of neuron overlaps and the corresponding impact on downstream performance in autoregressive LMs.",
            "score": 0.4552262212933857,
            "section_title": "Knowledge Acquisition during Pre-training.",
            "char_start_offset": 21398,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 88,
                    "end": 206
                },
                {
                    "start": 206,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 620
                },
                {
                    "start": 620,
                    "end": 755
                },
                {
                    "start": 755,
                    "end": 848
                },
                {
                    "start": 848,
                    "end": 1089
                }
            ],
            "ref_mentions": [
                {
                    "start": 211,
                    "end": 228,
                    "matchedPaperCorpusId": "254877112"
                },
                {
                    "start": 233,
                    "end": 254,
                    "matchedPaperCorpusId": "237491997"
                },
                {
                    "start": 404,
                    "end": 422,
                    "matchedPaperCorpusId": "261822542"
                },
                {
                    "start": 647,
                    "end": 668,
                    "matchedPaperCorpusId": "258832559"
                },
                {
                    "start": 784,
                    "end": 805,
                    "matchedPaperCorpusId": "249017999"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97265625
        },
        {
            "corpus_id": "273375030",
            "title": "The Fair Language Model Paradox",
            "text": "Large Language Models (LLMs) are widely deployed in real-world applications, yet little is known about their training dynamics at the token level. Evaluation typically relies on aggregated training loss, measured at the batch level, which overlooks subtle per-token biases arising from (i) varying token-level dynamics and (ii) structural biases introduced by hyperparameters. While weight decay is commonly used to stabilize training, we reveal that it silently introduces performance biases detectable only at the token level. In fact, we empirically show across different dataset sizes, model architectures and sizes ranging from 270M to 3B parameters that as weight decay increases, low-frequency tokens are disproportionately depreciated. This is particularly concerning, as these neglected low-frequency tokens represent the vast majority of the token distribution in most languages, calling for novel regularization techniques that ensure fairness across all available tokens.",
            "score": 0.45496535313763825,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97607421875
        },
        {
            "corpus_id": "273186031",
            "title": "SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe",
            "text": "Data characterization (Albalak et al., 2024;Wang et al., 2024) seeks to assess and analyze the quality and relevance of training data, enabling more effective data filtering and elevated model performance. In particular, Swayamdipta et al. (2020) leveraged the training dynamics of a pre-trained language model (Liu, 2019) to create data maps, which have subsequently inspired advancements in active learning (Zhang & Plank, 2021;Zhang et al., 2022;Kung et al., 2023), curriculum learning (Christopoulou et al., 2022;Lin et al., 2024;Poesina et al., 2024), and dataset pruning (Chimoto et al., 2024;He et al., 2024;Lin et al., 2024;Seedat et al., 2024). Here, we explore applying training dynamics to causal language generation by categorizing an SFT dataset into confident and relatively unconfident subsets, which facilitates the subsequent Mixup-based regularization during LLM instruction tuning. \n\nMixup-Based Learning. To alleviate memorization and sensitivity to adversarial examples during training, Zhang et al. (2018) proposed Mixup, which trains models on convex combinations of pairs of input features and their corresponding labels. Its variants (Verma et al., 2019;Hendrycks et al., 2020;Uddin et al., 2021;Choi et al., 2022) further suggest interpolating feature representations at different stages, guided by various training signals. Theoretical analyses (Zhang et al., 2021;Carratino et al., 2022;Chidambaram et al., 2022;Park et al., 2022;Pinto et al., 2022) have demonstrated its data-adaptive regularization and generalization effects, leading to strong out-ofdistribution robustness and well-calibrated uncertainty estimation.",
            "score": 0.453627241924989,
            "section_title": "RELATED WORK",
            "char_start_offset": 7618,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1648
                }
            ],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 246,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 409,
                    "end": 430,
                    "matchedPaperCorpusId": "237453313"
                },
                {
                    "start": 430,
                    "end": 449,
                    "matchedPaperCorpusId": "248665619"
                },
                {
                    "start": 449,
                    "end": 467,
                    "matchedPaperCorpusId": "264832712"
                },
                {
                    "start": 489,
                    "end": 517,
                    "matchedPaperCorpusId": "247694098"
                },
                {
                    "start": 534,
                    "end": 555,
                    "matchedPaperCorpusId": "269921463"
                },
                {
                    "start": 577,
                    "end": 599,
                    "matchedPaperCorpusId": "270123213"
                },
                {
                    "start": 599,
                    "end": 615,
                    "matchedPaperCorpusId": "259108479"
                },
                {
                    "start": 1008,
                    "end": 1027,
                    "matchedPaperCorpusId": "3162051"
                },
                {
                    "start": 1159,
                    "end": 1179,
                    "matchedPaperCorpusId": "59604501"
                },
                {
                    "start": 1202,
                    "end": 1221,
                    "matchedPaperCorpusId": "219179868"
                },
                {
                    "start": 1221,
                    "end": 1239,
                    "matchedPaperCorpusId": "252907925"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95703125
        },
        {
            "corpus_id": "261822542",
            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
            "text": "While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2023), or phase transitions (Olsson et al., 2022). The interpretability literature rarely illuminates how these capabilities emerge, in part because most analyses only examine the final trained model. Instead, we consider developmental analysis as a complementary explanatory lens. \n\nTo better understand the role of interpretable artifacts in model development, we analyze and manipulate these artifacts during training. We focus on a case study of Syntactic Attention Structure (SAS), a model behavior thought to relate to grammatical structure. By measuring and controlling the emergence of SAS, we deepen our understanding of the relationship between the internal structural traits and extrinsic capabilities of masked language models (MLMs). \n\nSAS occurs when a model learns specialized attention heads that focus on a word's syntactic neighbors. This behavior emerges naturally during conventional MLM pre-training (Clark et al., 2019;Voita et al., 2019;Manning et al., 2020). We observe an abrupt spike in SAS at a consistent point in training, and explore its impact on MLM capabilities by manipulating SAS during training. Our observations paint a picture of how interpretability artifacts may represent simplicity biases that compete with other learning strategies during MLM training. In summary, our main contributions are: \n\n\u2022 Monitoring latent syntactic structure (defined in Section 2.1) throughout training, we identify (Section 4.1) a precipitous loss drop composed of multiple phase transitions (defined in Section 2.3) relating to various linguistic abilities. At the onset of this stage (which we call the structure onset), SAS spikes.",
            "score": 0.4524773919435431,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 888
                },
                {
                    "start": 891,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 1942
                },
                {
                    "start": 1945,
                    "end": 2186
                },
                {
                    "start": 2187,
                    "end": 2262
                }
            ],
            "ref_mentions": [
                {
                    "start": 588,
                    "end": 612,
                    "matchedPaperCorpusId": "253117181"
                },
                {
                    "start": 1567,
                    "end": 1588,
                    "matchedPaperCorpusId": "219315567"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99169921875
        },
        {
            "corpus_id": "251800110",
            "title": "Shortcut Learning of Large Language Models in Natural Language Understanding",
            "text": "The learning dynamics could reveal what knowledge has been learned during the course of model training. There are some observations. First, standard training procedures have a bias toward learning simple features, which we can refer to as the simplicity bias. The models are based mainly on the simplest features and remain invariant to complex predictive features. Moreover, it has been observed that the models give overconfident predictions for easy samples and lowconfidence predictions for hard samples. Second, models tend to learn non-robust and easy-to-learn features at the early stage of training. For example, reading comprehension models have learned the shortcut in the first few training iterations, which has influenced further exploration of the models for more robust features [17]. Third, it has been experimentally validated that longer fine-tuning could lead to better generalization. Specifically, a larger number of training epochs will dramatically improve the generalizability of LLMs in NLU tasks [46]. The preference for non-robust features can be explained from the following perspective. The present LLM training methods can be considered as data-driven, corpus-based, statistical, and machine learning approaches. It is postulated that while this data-driven paradigm may prove effective in certain NLP tasks, it falls short in relevance to the challenging NLU tasks that necessitate a deeper understanding of natural language.",
            "score": 0.45005767085143034,
            "section_title": "Model Fine-tuning Process",
            "char_start_offset": 17621,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1456
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.853515625
        },
        {
            "corpus_id": "261277016",
            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
            "text": "In the previous sections, we report general patterns during language model pre-training ( \u00a74), define ways to characterize learning curves ( \u00a75), and isolate specific features that predict the speed and stability of learning for individual tokens in context ( \u00a76). Our results contribute to ongoing work studying language model pre-training dynamics, with implications for robust model deployment. \n\nSequential learning. Previous work has demonstrated that language models exhibit fine-grained learning patterns that are not captured by the corpus-level loss curve (related work in \u00a72). In particular, sudden increases and decreases in example loss ( \u00a75 and Xia et al., 2023) may be somewhat surprising given that the pre-training text is i.i.d. for all pre-training steps. By demonstrating that many of these sudden changes are consistent regardless of random initialization and data shuffling ( \u00a75.2), our work indicates that some instances of sudden learning and \"forgetting\" are not due to random chance or the specific examples observed in a given step. 10 Rather, they reflect some change in model processing that consistently occurs partially into pre-training (roughly step t \u0338 = 0). Because such a sudden change cannot be attributed to the specific examples observed (robust to random shuffling) or any change in the pre-training distribution at time t (the data is always i.i.d.), the primary remaining explanation is that the models' sudden \"learning\" at step t \u0338 = 0 is made possible by some systematic difference between models (and their optimizers) just before step t vs. at step 0. \n\nFramed from a potentially more interesting perspective, some types of language model \"learning\" appear to be dependent on previous learning and the linguistic abilities already present in the model. This aligns with previous work showing that language models acquire linguistic abilities in a systematic order during pre-training (Liu et al., 2021;Choshen et al., 2022), although not necessarily due to sequential dependencies. For example, Evanson et al. (2023) show that despite similar acquisition orders across models, different syntactic abilities are learned in parallel; performance for most individual abilities increases from the onset of pre-training. Our work provides evidence that there exist other capabilities or types of generalizations (e.g.",
            "score": 0.4497071349042731,
            "section_title": "Discussion",
            "char_start_offset": 27189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 397
                },
                {
                    "start": 400,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1597
                },
                {
                    "start": 1600,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2261
                },
                {
                    "start": 2262,
                    "end": 2358
                }
            ],
            "ref_mentions": [
                {
                    "start": 658,
                    "end": 674,
                    "matchedPaperCorpusId": "254877112"
                },
                {
                    "start": 1930,
                    "end": 1948,
                    "matchedPaperCorpusId": "233289478"
                },
                {
                    "start": 2041,
                    "end": 2062,
                    "matchedPaperCorpusId": "259089351"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.994140625
        },
        {
            "corpus_id": "254877112",
            "title": "Training Trajectories of Language Models Across Scales",
            "text": "Scaling up language models has been shown to improve language modeling perplexity (Kaplan et al., 2020;Hernandez et al., 2022) as well as zero-or few-shot end task accuracies (Brown et al., 2020;Rae et al., 2021;Chowdhery et al., 2022;Zhang et al., 2022). However, relatively little is understood about why or how this happens. How do the training dynamics differ as models get larger? What do language models of different sizes learn during pre-training in terms of both generating texts and solving end tasks?\n\nWe attempt to make progress to answer these questions by studying the training trajectories of differently-sized OPT models (Zhang et al., 2022) through analyzing their intermediate checkpoints.\n\nIn contrast to prior work, which studies the trajectories of small models with up to 300M parameters (Liu et al., 2021;Choshen et al., 2022;Blevins et al., 2022) or focuses on the language modeling objective alone (Kaplan et al., 2020;Hernandez et al., 2021Hernandez et al., , 2022, we are the first to comprehensively study the training trajectories of large-scale autoregressive language models with up to 175B parameters across a wide range of settings.\n\nRepeatedly across training and different model scales, we analyze three aspects of model performance: (i) next-token prediction on subsets of tokens (ii) sequence-level generation and (iii) downstream task performance. We use perplexity, which is closely tied to language model evaluation, as the major metric throughout the study.\n\nFor next-token prediction ( \u00a73), we study the trajectory by categorizing each token's prediction as stagnated, upward or downward according to its perplexity trend as training progresses. We find each category comprising a significant number of tokens: while a significant number of tokens' perplexity stagnate, a subset of tokens with an increasing perplexity in smaller models exhibit a doubledescent trend (Nakkiran et al., 2020) where perplexity increases and then decreases in larger models. These behaviors primarily emerge at a similar validation perplexity across model scales",
            "score": 0.44957114632138095,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "268379408",
            "title": "The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models",
            "text": "Here, the only observation we can make is that the composition of the dataset might influence the timing of the bifurcation event (Ott, 2002).Specifically, when training involves non-deduplicated (NDD) datasets, the redundant information contained within may hinder the rapid approach to the bifurcation point.\n\nThe observation that the models' weights reach a stationary state through their own dynamics, as mentioned in the introduction, naturally suggests a practical protocol for spontaneously ending the training process: this bifurcation marks a transition to a stationary state, indicating that further training is unlikely to significantly alter the weight values.Thus, by observing the absence of significant fluctuations in the dynamics, one can efficiently terminate the training upon achieving such a stationary state.Moreover, our observations also suggest that a specific quantization of the weights can be performed.Indeed, as shown in Ma & et al. (2024), using a ternary set of {\u22121, 0, 1} for each parameter, the performance of large language models (LLMs) remains unchanged.In our observations, particularly in the unembedding layer, a quantization to distinct values for the weights occurs naturally.To better quantify the above observations, we have computed the mean square displacement over time for the models.To do this, we integrate the demeaned slices of w tk over the first dimension t from 1 to \u03c4 obtaining a new array \u0175 as:\n\nwhere \u27e8w tk \u27e9 = 1 d\u00d7v \u2211 d\u00d7v k=1 w tk .We then indicate the variance of \u0175\u03c4k over each temporal slice as the mean square displacement MSD(\u03c4), defined as:\n\nFigure 3 illustrates the evolution of MSD(\u03c4) over the available checkpoints for the 70M and 160M models as well for the 14M and 31M.In physics, Brownian motion (Uhlenbeck & Ornstein, 1930) displays a linear relation between mean-squared displacement and time (Zwanzig, 2001).In this case for both models, we observe a quasi-linear growth.We believe this has to do with unbounded brownian diffusion in a first phase of training, where uniform spreading of network parameters takes place within the loss landscape.",
            "score": 0.44951483913560103,
            "section_title": "Temporal dynamic of unembedding layer parameters",
            "char_start_offset": 17947,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 142,
                    "end": 310
                },
                {
                    "start": 312,
                    "end": 672
                },
                {
                    "start": 672,
                    "end": 830
                },
                {
                    "start": 830,
                    "end": 931
                },
                {
                    "start": 931,
                    "end": 1091
                },
                {
                    "start": 1091,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1332
                },
                {
                    "start": 1332,
                    "end": 1451
                },
                {
                    "start": 1453,
                    "end": 1491
                },
                {
                    "start": 1491,
                    "end": 1604
                },
                {
                    "start": 1606,
                    "end": 1738
                },
                {
                    "start": 1738,
                    "end": 1881
                },
                {
                    "start": 1881,
                    "end": 1944
                },
                {
                    "start": 1944,
                    "end": 2118
                }
            ],
            "ref_mentions": [
                {
                    "start": 1766,
                    "end": 1794,
                    "matchedPaperCorpusId": "93073745"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94921875
        },
        {
            "corpus_id": "271860164",
            "title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models",
            "text": "We explore the relationship between fine-tuning and pre-training LLMs through fine-tuning multiple pre-training checkpoints of large language models. Our results on 18 datasets and two models provide insights into LLM training. We identify the aspects that LLM learns and forgets during supervised fine-tuning; By analyzing pre-training history, we find that pre-training improves the model in a latent way that is only observable after finetuning. The model may excel at some tasks without fine-tuning. However, the model can rapidly learn datasets that it does not demonstrate capabilities during pre-training with a small amount of supervision. Overall, our study highlights the value of analyzing language model training dynamics. We encourage model developers to release pre-training checkpoints to facilitate research on LLM training.",
            "score": 0.44740278391535426,
            "section_title": "Conclusion",
            "char_start_offset": 26766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 840
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99072265625
        },
        {
            "corpus_id": "247447316",
            "title": "Staged Training for Transformer Language Models",
            "text": "Language models form the backbone of many modern NLP systems, and these language models have become progressively larger in recent years. Parameter counts for these models have grown significantly from ELMo (94 M) (Peters et al., 2018) Corresponding author: beltagy@allenai.org to   (Brown et al., 2020). While larger models with more learnable parameters perform better on a wide range of tasks, the computational cost to train or even just to evaluate these models has become prohibitively expensive (Schwartz et al., 2020). In this paper, we demonstrate a method to reduce the compute cost of training transformer language models (Vaswani et al., 2017) through a staged training setup that iteratively builds a large model from a smaller one. \n\nMost prior work on scaling language models initializes each model size from a random initialization and trains to convergence (Kaplan et al., 2020). This work illustrated an intriguing property of model training shown in Figure 1, namely that smaller models are initially more compute efficient then larger models, but eventually the larger model will reach a lower loss. Our central idea is to take advantage of this property by first training a smaller model in the compute efficient region, applying a growth operator to the entire training state, and restarting training with a larger sized model. We introduce two operators to perform this growing operation along the model depth and width dimensions. We also identify two important properties of the operators: first, the loss before growing the model is preserved, and second, the training dynamics of the grown model match that of an equivalent model trained from scratch. To maintain training dynamics, growth operators must take into the entire training state, including the optimizer state and learning rate schedule, in addition to the model weights. As can be seen in Figure 1 our growth operator is loss-preserving, and we show in subsequent sections that it also preserves the training dynamics. These properties also make applying the growth operators conceptually and algorithmically simple as they won't disrupt the training process, and prior results regarding the model size needed to converge to a particular loss still hold.",
            "score": 0.44705642588077593,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 745
                },
                {
                    "start": 748,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2244
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 235,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 283,
                    "end": 303,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 502,
                    "end": 525,
                    "matchedPaperCorpusId": "198229505"
                },
                {
                    "start": 633,
                    "end": 655,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96533203125
        },
        {
            "corpus_id": "277349236",
            "title": "How do language models learn facts? Dynamics, curricula and hallucinations",
            "text": "The study of neural network learning dynamics has deep roots in early connectionist research (e.g., Baldi and Hornik, 1995;Hinton, 1986;McClelland, 1995). Parallel to these developments, Nguyen (2024) showed that Transformer-based language models' predictions can be effectively approximated by \u2212gram statistics, with the optimal  increasing throughout training. Our work observes similar dynamics during the transition from bigram to trigram predictions during the plateau phase. However, a key distinction lies in the level of abstraction: while Nguyen (2024)'s \u2212gram associations occur directly at the token level, our work observes these sequential dependencies at a higher level of abstraction. \n\nFinally, and most related to ours, is the study of factual knowledge acquisition in large language models from Chang et al. (2024). Their key finding is that factual knowledge acquisition occurs through accumulating small probability increases when the model encounters the same knowledge, followed by gradual forgetting when not being exposed to it. These results are consistent with ours, although we find forgetting to be more pronounced, likely because of larger training distribution shifts. Additionally, while their findings were obtained by slightly altering the training distribution of a large language model, our work goes one step further by studying in depth the impact of training distributions on learning speed.",
            "score": 0.4470135800478032,
            "section_title": "A.2. Learning dynamics of neural networks",
            "char_start_offset": 33294,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 699
                },
                {
                    "start": 702,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1429
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 123,
                    "matchedPaperCorpusId": "6555395"
                },
                {
                    "start": 123,
                    "end": 136,
                    "matchedPaperCorpusId": "265039794"
                },
                {
                    "start": 187,
                    "end": 200,
                    "matchedPaperCorpusId": "271244615"
                },
                {
                    "start": 813,
                    "end": 832,
                    "matchedPaperCorpusId": "270559235"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9765625
        },
        {
            "corpus_id": "273025704",
            "title": "Geometric Signatures of Compositionality Across a Language Model's Lifetime",
            "text": "Most research on LMs focuses on the final configuration of the model at the end of pre-training. Yet, recent work shows that learning dynamics can elucidate the behavior and computational mechanisms of LMs (Chen et al., 2024;Singh et al., 2024;Tigges et al., 2024). It has been found that, over training, LMs' weight matrices become higher-rank (Abbe et al., 2023), their representations higher dimensional (Cheng et al., 2024), and their gradients increasingly diffuse (Weber et al., 2024). Over finetuning, representational dimensionality has been found to change in concert with geometric properties like cluster reorganization (Doimo et al., 2024). \n\nPhase transitions during LM training have been found for some, but not all, aspects of language learning. Negative evidence includes that LM circuits involved in linguistic subtasks are stable (Tigges et al., 2024) and gradually reinforced (Weber et al., 2024) over training. Positive evidence for learning phase transitions includes that the ID of BERT's final [CLS] representation tracks sudden syntax acquisition and drops in training loss (Chen et al., 2024), with similar observations on Transformers trained on formal languages (Lubana et al., 2024). Our work supplements these results by investigating how the interaction between compositional understanding of language and geometric complexity of its representation arises over training.",
            "score": 0.44253557297245344,
            "section_title": "Language model training dynamics",
            "char_start_offset": 5069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1400
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 225,
                    "matchedPaperCorpusId": "261822542"
                },
                {
                    "start": 225,
                    "end": 244,
                    "matchedPaperCorpusId": "265157721"
                },
                {
                    "start": 345,
                    "end": 364,
                    "matchedPaperCorpusId": "259138551"
                },
                {
                    "start": 470,
                    "end": 490,
                    "matchedPaperCorpusId": "270370801"
                },
                {
                    "start": 895,
                    "end": 915,
                    "matchedPaperCorpusId": "270370801"
                },
                {
                    "start": 1098,
                    "end": 1117,
                    "matchedPaperCorpusId": "261822542"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98681640625
        },
        {
            "corpus_id": "268379408",
            "title": "The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models",
            "text": "In this study we have analyzed both the temporal and spatial dimensions of training a large language model.As discussed above, our work is the first one dealing with distribution of network weights as a whole, by means of computational methods borrowed from statistical mechanics.More specifically, this work shows that a bifurcation occurs in the dynamics of the weights during the training process.Such transitions are observed across various models of different sizes trained with distinct datasets.We have conducted a thorough and meticulous analysis of this aspect and concluded that this bifurcation marks a transition to a stationary state, indicating that further training is unlikely to significantly alter the weight values.Thus, training can be efficiently terminated upon reaching such a stationary state.Moreover, our study has offered a possible interpretation of the bifurcation phenomenon in terms of model perplexity.\n\nJust as in the early days of thermodynamics, when empirical observations drove technological advancements, we advocate for the development of Large Language Models (LLMs) to be grounded in the observation of their internal dynamics.The identification of stationary states in the weight dynamics exemplifies this philosophy, marking a step toward a more observational and theoretically informed approach to LLM development.\n\nIntriguingly, we finally note how recent works in the physics of complex networks point at diversity of information pathways as the main driver of sparsity in real networks Ghavasieh & De Domenico (2024): in this sense, we hypothesize that the poli-semanticity of natural language may act as the main driving force for network self-sparsification.\n\nThe presented results can have far-reaching implications as they demonstrate that keeping track of the collective behaviour of network weights could be a powerful indicator of training convergence, as opposed to the classical methods based on evaluation metrics which suffer from the confounding effects of nonlinearity, hence giving raise to false claims about \"emergent\" properties.As a future work, we would like to further investigate the training loss dynamics, to check whether the sudden changes in quantitative microscopic parameters align with the development of induction heads in the model, as shown by (Bietti et al., 2023).",
            "score": 0.44235627110847087,
            "section_title": "Conclusions",
            "char_start_offset": 26866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 400
                },
                {
                    "start": 400,
                    "end": 502
                },
                {
                    "start": 502,
                    "end": 734
                },
                {
                    "start": 734,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 934
                },
                {
                    "start": 936,
                    "end": 1168
                },
                {
                    "start": 1168,
                    "end": 1358
                },
                {
                    "start": 1360,
                    "end": 1707
                },
                {
                    "start": 1709,
                    "end": 2093
                },
                {
                    "start": 2093,
                    "end": 2345
                }
            ],
            "ref_mentions": [
                {
                    "start": 1533,
                    "end": 1563,
                    "matchedPaperCorpusId": "267188216"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9951171875
        },
        {
            "corpus_id": "265351565",
            "title": "On the Calibration of Large Language Models and Alignment",
            "text": "We choose Pythia as our base model (Biderman et al., 2023). Pythia is a suite of transformer-based, auto-regressive language models designed for scientific research. It contains 8 models whose scales range from 70m to 12B parameters and for each of the scale it provides 154 checkpoints including 143 checkpoints saved every 1,000 training steps (i.e. 1 epoch) and 11 checkpoints trained for less than 1,000 steps. All of these models are trained on exactly the same data-the PILE (Gao et al., 2020) dataset in the same order. For parameter scale study, we experiment on models with all 8 scales. As for training dynamics, we choose Pythia-1B4 considering time and computational cost, and use 2 n * 1, 000(n = 1, 2...) steps checkpoints (up to step143,000) for our study. We also include checkpoints of step256 and step512 in our experiments to observe the behavior of under-fitted models.",
            "score": 0.4408800816669371,
            "section_title": "Experimental Setups",
            "char_start_offset": 15171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 889
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.943359375
        },
        {
            "corpus_id": "272826711",
            "title": "Rethinking Conventional Wisdom in Machine Learning: From Generalization to Scaling",
            "text": "To investigate the optimal learning rate for training language models, we trained our base model to Chinchilla-Optimal while varying the learning rates. We further test the robustness of our findings by introducing several interventions: Experimental Results. Across all these setups, the relationship between loss and learning rate consistently exhibited a U-shape curve. The optimal learning rate was far from the maximal stable learning rate often favored in traditional neural network training. \n\nDiscussion. Contrary to conventional wisdom, our findings reveal that the optimal learning rate for large language models is significantly lower than the maximal stable value3 previously assumed. This suggests that traditional regularization-based theories may not fully explain the dynamics of optimal learning rates in the context of training language models. Further research is needed to elucidate the complex relationship between optimal learning rate, model scale, and other training factors (see Paquette et al. (2024)).",
            "score": 0.4405644776430623,
            "section_title": "Does Maximal Stable Learning Rate Perform Better?",
            "char_start_offset": 16902,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 498
                },
                {
                    "start": 501,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1028
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9814453125
        },
        {
            "corpus_id": "277104008",
            "title": "Cognitive Activation and Chaotic Dynamics in Large Language Models: A Quasi-Lyapunov Analysis of Reasoning Mechanisms",
            "text": "Large Language Models (LLMs) have demonstrated remarkable capabilities in recent years, not only generating fluent natural language text but also performing complex reasoning tasks, solving multi-step problems [1] [2], and even exhibiting human-like thinking processes in certain scenarios [3][4] [5]. This phenomenon raises a fundamental question: how can these neural network systems, which essentially have fixed parameters, produce seemingly flexible and varied reasoning abilities? Traditional perspectives suggest that large language models merely perform high-dimensional interpolation [6] [7] or implicit memorization of training data [8] [9], rather than executing genuine reasoning. However, this view struggles to explain these models' excellent performance in zero-shot and few-shot scenarios, as well as their ability to handle complex logic and abstract concepts. \n\nThis research proposes a new theoretical framework to understand the reasoning mechanisms of large language models through the phenomenon of \"Cognitive Activation.\" Our core hypothesis is that reasoning abilities in LLMs stem from a dynamic, context-dependent information extraction process, wherein the model dynamically determines how to extract information from its parameters based on current inputs. This process resembles state evolution in dynamical systems and exhibits certain characteristics of chaotic systems. \n\nTraditional neural network research has primarily focused on network architecture design [10] [11] [12], training methods [13] [14], and parameter optimization [15] [16], with less exploration of network dynamics during the inference phase. This study approaches from a different perspective, viewing large language models as dynamic systems where each layer's output not only depends on the current input but also determines how subsequent layers extract information from network parameters. This perspective helps explain why models with fixed parameters can exhibit flexible, context-sensitive behaviors. \n\nChaos theory provides powerful tools for analyzing complex dynamical systems [17]. By introducing quasi-Lyapunov exponents(QLE), we can quantitatively describe the dynamic information extraction process in large language models, revealing the formation mechanism of their reasoning capabilities. This chaos theory-based analytical approach not only helps understand the behavior of existing models but may also provide theoretical guidance for designing more efficient and reliable language models. \n\nThe main contributions of this paper include: \uf09f Proposing the concept of \"Cognitive Activation\" to describe the dynamic information extraction process in large language models.",
            "score": 0.43844426283108195,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1401
                },
                {
                    "start": 1404,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2011
                },
                {
                    "start": 2014,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2309
                },
                {
                    "start": 2310,
                    "end": 2512
                },
                {
                    "start": 2515,
                    "end": 2691
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 217,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 297,
                    "end": 300,
                    "matchedPaperCorpusId": "268551442"
                },
                {
                    "start": 593,
                    "end": 596,
                    "matchedPaperCorpusId": "235422672"
                },
                {
                    "start": 643,
                    "end": 646,
                    "matchedPaperCorpusId": "246863735"
                },
                {
                    "start": 647,
                    "end": 650,
                    "matchedPaperCorpusId": "245502053"
                },
                {
                    "start": 1498,
                    "end": 1502,
                    "matchedPaperCorpusId": "232045912"
                },
                {
                    "start": 1503,
                    "end": 1507,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 2091,
                    "end": 2095,
                    "matchedPaperCorpusId": "73554069"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9423828125
        },
        {
            "corpus_id": "269237441",
            "title": "Analysis of learning curves in predictive modeling using exponential curve fitting with an asymptotic approach",
            "text": "Nevertheless, investigating asymptotic convergence of data models' outcomes can provide an understanding of how learning performance correlates with the sizes of samples [43], which tend to increase over time. Considering the substantial computational resources invested in training large language models (another current state-of-the-art context in 2023), analyzing the asymptotic behavior of learning curves can anticipate the behavior of data models before extensive training efforts. Consequently, monitoring a learning curve supports the decision regarding the need to retrain a large language model, induced by its temporal quality decay, thereby preventing the expenditure of substantial resources. \n\nThe key question driving our research arose from the context of the pandemic: how much data is requisite for reliable predictions? As we evaluated the outcomes obtained from our developed method for learning curve analysis, we assumed its applicability in monitoring large language models. Therefore, we considered another inquiry: at what point does retraining large language models provide substantial performance improvements, factoring in the considerable resources invested in the process? We assumed that the proposed method for learning curve analysis could be aptly suited for this task, founded on the experiment results demonstrating its capability of analyzing the expected future behavior of the models. Consequently, unraveling the answer to this subsequent question remains a prospect for future investigations.",
            "score": 0.43809889720420003,
            "section_title": "Discussion",
            "char_start_offset": 44610,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1533
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 174,
                    "matchedPaperCorpusId": "237417334"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96533203125
        },
        {
            "corpus_id": "270765562",
            "title": "Acoustic barycenters as exemplar production targets",
            "text": "Wedel, 2006;Ettlinger and Johnson, 2010, inter alia).This approach can be straightforwardly extended to handle parametric spaces of higher dimensionality e.g.encoding richer acoustic information with spectral frames, or sociophonetic context such as interlocutor identity, etc.However, it is unclear how it might be extended to incorporate the dynamic nature of human language, which unfolds in time and cannot be reduced to point measures.That is, the problem of straightforwardly accommodating the temporal variability and generalization of human speech in implemented production models remains underexplored.5",
            "score": 0.4348726027367971,
            "section_title": "Generalizing production from exemplar knowledge",
            "char_start_offset": 7508,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 53,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 277
                },
                {
                    "start": 277,
                    "end": 440
                },
                {
                    "start": 440,
                    "end": 612
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14453125
        },
        {
            "corpus_id": "274823085",
            "title": "Training Dynamics of a 1.7B LLaMa Model: A Data-Efficient Approach",
            "text": "Large language models (LLMs) have achieved unprecedented performance across a variety of natural language understanding and generation tasks. These advancements have been driven by innovations in model architectures, pre-training methodologies, and the availability of large-scale, high-quality data (Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell et al., 2020;Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann et al., 2022;Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Roziere, Goyal, Hambro, Azhar et al., 2023a). However, despite significant progress, many aspects of LLM training remain underexplored. Practical challenges-ranging from data curation and training stability to qualitative and quantitative evaluation-play a critical role in determining model performance but are often overlooked in favor of final benchmark results. \n\nIn this paper, we present our experience pre-training and post-training DMaS-LLaMa-Lite, a 1.7-billion-parameter LLaMa-based model, on 20 billion tokens of carefully curated data. Unlike many existing studies that focus solely on the final outputs of large-scale training, we emphasize the training process itself and the insights gleaned from it. Specifically, we document the following key aspects of our work: \n\n1. Training Dynamics: We analyze how validation loss and downstream benchmarks (e.g., Hellaswag, ARC) evolve over 40,000+ training steps and correlate these metrics with improvements in text fluency, coherence, and factual accuracy.",
            "score": 0.4348085577332137,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1326
                },
                {
                    "start": 1329,
                    "end": 1561
                }
            ],
            "ref_mentions": [
                {
                    "start": 300,
                    "end": 396,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9951171875
        },
        {
            "corpus_id": "237491997",
            "title": "The Grammar-Learning Trajectories of Neural Language Models",
            "text": "Children present remarkable consistency in their patterns of language acquisition. They often acquire linguistic phenomena in a similar order (Kuhl et al., 1992;Ingram, 1989), and make similar generalizations and over-generalizations (Kuczaj II, 1977;Pinker, 1995). This consistency provides an important starting point for linguistic study. For example, arguments in favor of single or dual system accounts of morphological representation are often backed by computational models of children learning trajectories (e.g., Rumelhart and McClelland, 1986;Pinker and Prince, 1988;Kirov and Cotterell, 2018). In this paper, we embrace this program for the study of computational language models, investigating learning trajectories. 1 The representations that language models (LM) acquire have been studied extensively, including studying their learning dynamics to improve training (see \u00a76). However, very little work aimed at drawing connections between the training dynamics and the learned representations. In this work we adopt a behavioral approach, thus revealing that NLMs share learning trajectories and generalize in similar ways during training. This implies that studying trajectories of NLMs is worthwhile, in the sense that results on one architecture or size are expected to be reproducible by others.\n\nThese findings call for a characterization of these trajectories, a new and promising territory for research. We take first steps to explore these directions, emphasizing their potential benefit to a better future understanding of what models learn.\n\nSpecifically, we train NLMs on next-word prediction, but evaluate and compare them by tracking their performance on grammar learning in English, using the BLIMP dataset (See 2.1). BLIMP is a dataset that consists of 67K minimal pairs, where each pair includes a grammatically correct and a grammatically erroneous sentence. NLMs are tested for their ability to assign higher probability to the correct one. See example in Table 1, and details of our experimental methodology in \u00a72.\n\nWe begin ( \u00a73) by establishing that NLMs learn grammatical phenomena in a consistent order. We evaluate NLMs at different time points along their training, showing that the performance on linguis- tic phenomena across initializations is highly correlated. We further find many similarities in the set of examples that they correctly classify.",
            "score": 0.43473486421809165,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 161,
                    "matchedPaperCorpusId": "45721427"
                },
                {
                    "start": 234,
                    "end": 251,
                    "matchedPaperCorpusId": "145429434"
                },
                {
                    "start": 522,
                    "end": 553,
                    "matchedPaperCorpusId": "237372475"
                },
                {
                    "start": 577,
                    "end": 603,
                    "matchedPaperCorpusId": "49742817"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9658203125
        },
        {
            "corpus_id": "264172260",
            "title": "Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression",
            "text": "We posit that GVA is a generic phenomenon that can manifest in disparate settings: whenever a model's predictions are applied within a (marginally stable or unstable) feedback loop, the closed-loop dynamics can amplify small fluctuations in a deleterious manner. A natural and timely setting with this structurewhich complements continuous control-is autoregressive language modeling. Here, a network's parameters \u03b8 are optimized on a 1-step prediction loss, which takes the role of \u2113 BC (\u03c0 \u03b8 ). The network \u03c0 \u03b8 is then used to generate a sequence of symbols w 1:H by iteratively rolling out \u03c0 \u03b8 : w 1:h \u2192 w h+1 . Such models have been paradigm-shattering in NLP, code synthesis, and beyond. Motivated by the similarity of this pipeline to behavior cloning, 7 we perform a smaller set of analogous experiments on language generation. Our findings 2500 5000 7500 10000 12500 15000 17500\n\nTraining steps here parallel our findings for continuous control, and show (i) the presence of GVA, and (ii) substantial benefits of iterate averaging. In more detail, we train 270M-parameter 12-layer Transformer models on the TinyStories dataset (Eldan & Li, 2023), which serves as an inexpensive surrogate for a full-scale pretraining pipeline. Highlights are shown in Fig. 4, while Appendix B.2 provides full documentation, including largerscale training runs with a non-synthetic corpus (Wikipedia). We summarize our findings below:\n\n(R5) Autoregressive LMs exhibit significant rollout oscillations throughout training. EMA stabilizes the trajectory, accelerates training, and improves generalization, complementing (and potentially obviating) standard practices in learning rate annealing.",
            "score": 0.43437243753202803,
            "section_title": "Autoregressive sequence models and language generation",
            "char_start_offset": 25870,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98046875
        },
        {
            "corpus_id": "269042752",
            "title": "On Training Data Influence of GPT Models",
            "text": "This is particularly critical for models expected to adapt to the dynamic and evolving trajectory of NLP tasks. \n\nIn response to these gaps, we introduce GPTfluence, a novel framework designed to extend the analysis of training data influence beyond the limitations of existing methodologies and across a broader spectrum of tasks. Employing a featurized simulation approach, GPTfluence estimates the impact of individual training examples on the performance of GPT models, covering both natural language understanding and generation tasks. This expanded focus facilitates a comprehensive understanding of model training dynamics, provid-ing insights into a wide array of evaluation metrics beyond mere test loss. \n\nExtensive experiments on selected subsets from FLAN datasets (Wei et al., 2022), across a variety of tasks and GPT model variants (Biderman et al., 2023), ranging in size from 14 million to 2.8 billion parameters, validate the effectiveness and superiority of our approach. Notably, our method not only sheds light on the training dynamics of GPT models but also demonstrates remarkable generalization capabilities to unseen data. \n\nContribution To summarize, our contributions are as follows: \n\n\u2022 We introduce GPTfluence, a featurized simulation approach that significantly advances the analysis of training data influence on GPT models. This approach not only enables a comprehensive comparison with existing methodologies but also marks the first extensive foray into the extensive investigation of training data's impact on the performance of GPT models across various scales. \u2022 Our approach demonstrates effectiveness on GPT models across different scales, showing its generalization capability on unseen data. \u2022 We release the GPTDynamics dataset, a collection encompassing over 350 runs of training dynamics data spanning six distinct model sizes and five NLP tasks, to facilitate further research advancement.",
            "score": 0.43391739554222064,
            "section_title": "Introduction",
            "char_start_offset": 1991,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 114,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 713
                },
                {
                    "start": 716,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1146
                },
                {
                    "start": 1149,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1933
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.978515625
        },
        {
            "corpus_id": "272826680",
            "title": "Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts",
            "text": "Here, we argue informally that our normative algorithm provides a notion of a rational pre-training process, and thus helps explain the training dynamics of practical LMs, and is also capable of rule extrapolation. We support our arguments by showing the role of simplicity bias (towards low Kolmogorov complexity) in the dynamics of learning the a n b n language with Transformers. Explaining training dynamics. We analize the dynamics of learning rule extrapolation. We report results on the Transformer (training dynamics of Mamba and the LSTM are in Appx. A), trained on the a n b n language-where high rule extrapolation ability is achieved. Fig. 3 shows that first, the model learns the sequences obeying (R2), then it learns the language (R1) \u2229 (R2) as its subset. We argue that the order in which rules are learnt is governed by the relative simplicity of the rules, quantified by Kolmogorov complexity. Given a formal language with rules (R1) and (R2), let p 1 , p 2 and p 1,2 be distributions defined by LMs that generate sequences that satisfy (R1), (R2) and (R1) \u2229 (R2), respectively. If, e.g., K(p 2 ) \u226a K(p 1,2 ), our normative algorithm will first learn (R2), and then learn the (R1) \u2229 (R2) as its subset. In the a n b n language, (R2) (a's before b's), is, on average, simpler to generate than (R1) (#a=#b) and (R1) \u2229 (R2). Therefore, we expect our normative algorithm to first learn (R2), and then learn (R1) \u2229 (R2) as its subset. Remarkably, our Transformer employs the same strategy ( Fig. 3), verifying the presence of simplicity bias. This result is matches past observations that Transformers are biased towards low Kolmogorov complexity [Goldblum et al., 2023]. Towards explaining rule extrapolation.",
            "score": 0.43178075737591326,
            "section_title": "Towards explaining training dynamics and rule extrapolation",
            "char_start_offset": 31665,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1723
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99365234375
        },
        {
            "corpus_id": "264451507",
            "title": "Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training",
            "text": "Contemporary advances in NLP are built on the representational power of latent embedding spaces learned by self-supervised language models (LMs). At their core, these approaches are built on the distributional hypothesis (Harris, 1954;Firth, 1957), for which the effects of scale have been implicitly and explicitly studied via the community's use of increasingly large models and datasets (Teehan et al., 2022;Wei et al., 2022). The learning dynamics by which these capabilities emerge during LM pre-training have, however, remained largely understudied. Understanding how and when the  LM training objective begins to encode information that is relevant to downstream tasks is crucial, as this informs the limits of what can be learned using current objectives.",
            "score": 0.4317331490381528,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 763
                }
            ],
            "ref_mentions": [
                {
                    "start": 235,
                    "end": 247,
                    "matchedPaperCorpusId": "208093066"
                },
                {
                    "start": 390,
                    "end": 411,
                    "matchedPaperCorpusId": "247656607"
                },
                {
                    "start": 411,
                    "end": 428,
                    "matchedPaperCorpusId": "249674500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98193359375
        },
        {
            "corpus_id": "273323881",
            "title": "Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization",
            "text": "Given a prompt x, the probability that the model \u03c0 \u03b8 assigns to a response z is determined by the hidden embeddings h x , h x,z<2 , . . . , h x,z <|z| and the token unembeddings W (Equation ( 1)). Our analysis relies on tracking their evolution when minimizing the loss L (Equation ( 2)). To do so, we adopt the unconstrained features model (Mixon et al., 2022), which amounts to treating hidden embeddings as directly trainable parameters. Formally, the trainable parameters are taken to be \u03b8 = {h z : z \u2208 V * } \u222a {W}. This simplification has proven useful for studying various deep learning phenomena, including neural collapse (e.g., Zhu et al. (2021); Ji et al. (2022); Tirer et al. (2023)) and the benefits of language model pretraining for downstream tasks (Saunshi et al., 2021). As verified in Sections 5 and 6, it also allows extracting salient sources of likelihood displacement in practice. 4 anguage model finetuning is typically done with small learning rates. Accordingly, we analyze the training dynamics of (stochastic) gradient descent at the small learning rate limit, i.e. gradient flow: \n\nwhere \u03b8(t) denotes the parameters at time t \u2265 0 of training. Note that under gradient flow the loss is monotonically decreasing.5 Thus, any reduction in the log probabilities of preferred responses constitutes likelihood displacement (cf. Definition 1).",
            "score": 0.43073131947901117,
            "section_title": "TECHNICAL APPROACH",
            "char_start_offset": 15561,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1106
                },
                {
                    "start": 1109,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1362
                }
            ],
            "ref_mentions": [
                {
                    "start": 656,
                    "end": 672,
                    "matchedPaperCorpusId": "238407888"
                },
                {
                    "start": 674,
                    "end": 693,
                    "matchedPaperCorpusId": "253237344"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.448486328125
        },
        {
            "corpus_id": "250390797",
            "title": "The Birth of Bias: A case study on the evolution of gender bias in an English language model",
            "text": "While there is a lot of important work on detecting and mitigating undesirable biases in language models, we still lack a good understanding of the mechanisms underlying the biased behaviour. The goal of this study was to take a step back and analyse the birth of bias in language models. To this end, we present a temporal investigation of how an English LSTM language model learns a representation of gender in the input embeddings and how this affects downstream biased behaviour. \n\nThere are many interesting directions for future research. An important open question is, for instance, how intrinsic representations of bias relate to other downstream tasks that may be closer to real-world systems where the representational and allocative harms to social groups are more clear (Blodgett et al., 2020). For future work, we also plan to do further investigations on how our training dynamics analysis may generalise to other undesirable social biases, model architectures, training corpora, and downstream tasks, as well as other possible representations in the internal states of the language model that are useful for understanding bias. Furthermore, the robustness of our results with respect to different random initialisations of the language model should be checked (Webster et al., 2020;D'Amour et al., 2020). \n\nIn this paper, we take a step towards a more thorough understanding of the evolution of bias in language models across the different stages of the language modelling pipeline. Hopefully, it will inspire more work on the dynamic behaviour of language models, with respect to bias, but also other still poorly understood features of these models.",
            "score": 0.4293038636567991,
            "section_title": "Conclusion",
            "char_start_offset": 36173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 483
                },
                {
                    "start": 486,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1319
                },
                {
                    "start": 1322,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1666
                }
            ],
            "ref_mentions": [
                {
                    "start": 782,
                    "end": 805,
                    "matchedPaperCorpusId": "218971825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96240234375
        },
        {
            "corpus_id": "6187154",
            "title": "Dynamic Language Models for Streaming Text",
            "text": "Language models are a key component in many NLP applications, such as machine translation and exploratory corpus analysis. Language models are typically assumed to be static-the word-given-context distributions do not change over time. Examples include n-gram models (Jelinek, 1997) and probabilistic topic models like latent Dirichlet allocation (Blei et al., 2003); we use the term \"language model\" to refer broadly to probabilistic models of text. \n\nRecently, streaming datasets (e.g., social media) have attracted much interest in NLP. Since such data evolve rapidly based on events in the real world, assuming a static language model becomes unrealistic. In general, more data is seen as better, but treating all past data equally runs the risk of distracting a model with irrelevant evidence. On the other hand, cautiously using only the most recent data risks overfitting to short-term trends and missing important timeinsensitive effects (Blei and Lafferty, 2006;Wang et al., 2008). Therefore, in this paper, we take steps toward methods for capturing long-range temporal dynamics in language use. Our model also exploits observable context variables to capture temporal variation that is otherwise difficult to capture using only text. Specifically for the applications we consider, we use stock market data as exogenous evidence on which the language model depends. For example, when an important company's price moves suddenly, the language model should be based not on the very recent history, but should be similar to the language model for a day when a similar change happened, since people are likely to say similar things (either about that company, or about conditions relevant to the change). Non-linguistic contexts such as stock price changes provide useful auxiliary information that might indicate the similarity of language models across different timesteps. \n\nWe also turn to a fully online learning framework (Cesa-Bianchi and Lugosi, 2006) to deal with nonstationarity and dynamics in the data that necessitate adaptation of the model to data in real time. In online learning, streaming examples are processed only when they arrive. Online learning also eliminates the need to store large amounts of data in memory.",
            "score": 0.4287026242822462,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 450
                },
                {
                    "start": 453,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1881
                },
                {
                    "start": 1884,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2241
                }
            ],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 366,
                    "matchedPaperCorpusId": "3177797"
                },
                {
                    "start": 946,
                    "end": 971,
                    "matchedPaperCorpusId": "262346294"
                },
                {
                    "start": 971,
                    "end": 989,
                    "matchedPaperCorpusId": "8092936"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8251953125
        },
        {
            "corpus_id": "261277016",
            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
            "text": "Previous work has studied the pre-training dynamics of language models (Saphra and Lopez, 2019). Choshen et al. (2022) and Evanson et al. (2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020;Liu et al., 2021). Olsson et al. (2022) find that copy mechanisms (\"induction heads\" for in-context learning) appear at an inflection point during pre-training. These results establish when a variety of abilities emerge in language models. Our work studies more fine-grained learning trajectories by evaluating individual tokens in context. \n\nIndeed, previous work has studied how individual tokens are learned during pre-training. \n\nFor example, word learning is highly dependent on word frequency (Chang and Bergen, 2022b). Larger models memorize more examples during pre-training without overfitting (Tirumala et al., 2022), but the time step that a model sees an example does not affect memorization (Biderman et al., 2023). Most similar to our work, Xia et al. (2023) collect learning curves for individual tokens in context, finding that some examples exhibit a \"doubledescent\" trend where they first increase then decrease in surprisal. All of the studies above collect language model learning curves during pre-training, either for individual examples or targeted benchmark performance. Here, we introduce metrics to characterize such curves, we identify general learning patterns, and we isolate text features that are predictive of learning speed and stability.",
            "score": 0.4279126903800746,
            "section_title": "Related Work",
            "char_start_offset": 3961,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1714
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 95,
                    "matchedPaperCorpusId": "267903883"
                },
                {
                    "start": 123,
                    "end": 144,
                    "matchedPaperCorpusId": "259089351"
                },
                {
                    "start": 422,
                    "end": 443,
                    "matchedPaperCorpusId": "222140842"
                },
                {
                    "start": 443,
                    "end": 460,
                    "matchedPaperCorpusId": "233289478"
                },
                {
                    "start": 942,
                    "end": 967,
                    "matchedPaperCorpusId": "238408423"
                },
                {
                    "start": 1046,
                    "end": 1069,
                    "matchedPaperCorpusId": "248986465"
                },
                {
                    "start": 1147,
                    "end": 1170,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 1198,
                    "end": 1215,
                    "matchedPaperCorpusId": "254877112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.994140625
        },
        {
            "corpus_id": "273346324",
            "title": "Training Dynamics of Transformers to Recognize Word Co-occurrence via Gradient Flow Analysis",
            "text": "Understanding the training dynamics of transformers is important to explain the impressive capabilities behind large language models. In this work, we study the dynamics of training a shallow transformer on a task of recognizing co-occurrence of two designated words. In the literature of studying training dynamics of transformers, several simplifications are commonly adopted such as weight reparameterization, attention linearization, special initialization, and lazy regime. In contrast, we analyze the gradient flow dynamics of simultaneously training three attention matrices and a linear MLP layer from random initialization, and provide a framework of analyzing such dynamics via a coupled dynamical system. We establish near minimum loss and characterize the attention model after training. We discover that gradient flow serves as an inherent mechanism that naturally divide the training process into two phases. In Phase 1, the linear MLP quickly aligns with the two target signals for correct classification, whereas the softmax attention remains almost unchanged. In Phase 2, the attention matrices and the MLP evolve jointly to enlarge the classification margin and reduce the loss to a near minimum value. Technically, we prove a novel property of the gradient flow, termed \\textit{automatic balancing of gradients}, which enables the loss values of different samples to decrease almost at the same rate and further facilitates the proof of near minimum training loss. We also conduct experiments to verify our theoretical results.",
            "score": 0.4277459607418496,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "268723657",
            "title": "Understanding the Learning Dynamics of Alignment with Human Feedback",
            "text": "Khanov et al. (2024) propose a decoding-time approach to alignment, which employs a reward mechanism that directly guides the text generation process of a language model thus bypassing the expensive RL training. Other modifications include the use of modelgenerated feedback (Bai et al., 2022b;Lee et al., 2023) and the use of different objectives or modeling assumptions (Munos et al., 2023;Hejna et al., 2023;Dai et al., 2023). \n\nTheoretical Analysis of Alignment. Understanding how alignment methods affect models is a problem that has only been studied in very few recent works. In particular, Wolf et al. (2023) introduce a theoretical framework that demonstrates a key limitation of alignment that any behavior with a positive probability can be triggered through prompting. Azar et al. (2023) analyze the asymptotics of DPO and a variation called IPO and finds that DPO can lead to overfitting. Wang et al. (2023) proves that RLHF can be solved with standard RL techniques and algorithms. Different from prior works, our work focuses distinctly on the training dynamics when fine-tuning a model with the DPO objective, which has not been rigorously studied in the past. Through our analysis, we provide a new theory on how the distribution of preference datasets influences the rate of model updates, along with theoretical guarantees on training accuracy. \n\nLearning Dynamics. Previous works have theoretically studied training dynamics under different objectives and their connections to generalization (Du et al., 2018;Jacot et al., 2018;Arora et al., 2019;Goldt et al., 2019;Papyan et al., 2020;Xu et al., 2023). Some of these works study how features arise in the early stages of training similar to our study of fine-tuning (Ba et al., 2022;Shi et al., 2022). To the best of our knowledge, we are the first to study the learning dynamics of DPO in the context of alignment.",
            "score": 0.4275559597920179,
            "section_title": "Related Works",
            "char_start_offset": 26214,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 429
                },
                {
                    "start": 432,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1886
                }
            ],
            "ref_mentions": [
                {
                    "start": 902,
                    "end": 920,
                    "matchedPaperCorpusId": "259252095"
                },
                {
                    "start": 1529,
                    "end": 1548,
                    "matchedPaperCorpusId": "49321232"
                },
                {
                    "start": 1548,
                    "end": 1567,
                    "matchedPaperCorpusId": "135463287"
                },
                {
                    "start": 1567,
                    "end": 1586,
                    "matchedPaperCorpusId": "236973420"
                },
                {
                    "start": 1586,
                    "end": 1606,
                    "matchedPaperCorpusId": "221172897"
                },
                {
                    "start": 1606,
                    "end": 1622,
                    "matchedPaperCorpusId": "254562341"
                },
                {
                    "start": 1737,
                    "end": 1754,
                    "matchedPaperCorpusId": "248505757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9404296875
        },
        {
            "corpus_id": "267770489",
            "title": "From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers",
            "text": "These frequencies will be evolving as the model keeps generating new states. In the context of language modeling, X = (x t ) L t=1 will correspond to the prompt that we input to the model and (x t ) t>L will be the model's response: Sections 3 and 4 will explore the learnability of underlying dynamics P from multiple diverse prompts and the corresponding model generations. On the other hand, Section 5 will study learnability from an infinite trajectory generated from a single prompt. \n\nAs we shall see, Definition 2.1 captures the dynamics of a 1-layer self-attention model when there are no positional encodings. In Section 6, we introduce a more general setting where the transition dynamics of CCMC incorporates the positional information of the state trajectory. This enriched model will similarly capture self-attention with absolute positional encoding.",
            "score": 0.4268795056642635,
            "section_title": "Context-Conditioned Markov Chain (CCMC)",
            "char_start_offset": 6924,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 488
                },
                {
                    "start": 491,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 864
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81494140625
        },
        {
            "corpus_id": "258762741",
            "title": "Diffusion Language Models Generation Can Be Halted Early",
            "text": "Exploring Large Language Models (LMs) is a dominant research direction in NLP. The two primary methods of training LMs for NLP are autoregressive training (Radford et al., 2019;Raffel et al., 2020;Chowdhery et al., 2022) and masked language modeling (Devlin et al., 2019;He et al., 2020;Liu et al., 2019;Lan et al., 2020). \n\nThe exploration of alternative models, such as diffusion models (Ho et al., 2020;Song et al., 2020), is a promising avenue for research as diffusion allows native non-causal conditioning and simplified controllable generation methods (Nichol et al., 2022). In recent works, with models such as \"Diffusion LM\" and Plaid (Li et al., 2022;Gulrajani and Hashimoto, 2023), Simplex-based Diffusion * These authors contributed equally to this work Language Model (SSD) (Han et al., 2023), GENIE (Lin et al., 2022), and Continuous Diffusion for Categorical Data (CDCD) (Dieleman et al., 2022) being introduced, indicating an emerging interest for using diffusion models in text generation. \n\nA crucial distinction between autoregressive LMs and diffusion language models (DLMs) lies in their modeling approaches. Autoregressive LMs predominantly adhere to the common probabilistic model. In contrast, DLMs exhibit substantial divergence in their application for modeling categorical data. When exploring DLMs, it is essential to consider the lack of connectivity between such models. The majority of comparisons between them have primarily focused on evaluating sample quality (Gulrajani and Hashimoto, 2023;Han et al., 2023). \n\nWhile it is essential to study the sample quality of DLMs, it does not further our understanding of the differences between these DLMs models. This work addresses this issue and evaluates various DLMs with a unified view of their generation process. Given this unified view, we study the dynamics of the generation process within different DLMs and focus on the changes in the samples during that process.",
            "score": 0.425833566090526,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 322
                },
                {
                    "start": 325,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1543
                },
                {
                    "start": 1546,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1951
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 197,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 250,
                    "end": 271,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 304,
                    "end": 321,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 389,
                    "end": 406,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 559,
                    "end": 580,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 787,
                    "end": 805,
                    "matchedPaperCorpusId": "253237701"
                },
                {
                    "start": 1525,
                    "end": 1542,
                    "matchedPaperCorpusId": "253237701"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8544921875
        },
        {
            "corpus_id": "274981612",
            "title": "Tracking the Feature Dynamics in LLM Training: A Mechanistic Study",
            "text": "Understanding training dynamics and feature evolution is crucial for the mechanistic interpretability of large language models (LLMs). Although sparse autoencoders (SAEs) have been used to identify features within LLMs, a clear picture of how these features evolve during training remains elusive. In this study, we: (1) introduce SAE-Track, a novel method to efficiently obtain a continual series of SAEs; (2) mechanistically investigate feature formation and develop a progress measure for it ; and (3) analyze and visualize feature drift during training. Our work provides new insights into the dynamics of features in LLMs, enhancing our understanding of training mechanisms and feature evolution.",
            "score": 0.4244425015891494,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99462890625
        },
        {
            "corpus_id": "276937707",
            "title": "Contrastive Speaker-Aware Learning for Multi-party Dialogue Generation with LLMs",
            "text": "This negative sampling approach is specifically designed to penalize the model if it fails to properly account for speaker roles and dynamics within the dialogue. It encourages the model to be sensitive to the speaker sequence and to generate responses that are consistent with the expected speaker turns and contributions. \n\nTo quantify the compatibility between a given dialogue context and a response, we define a scoring function f (Context, Response). In our SA-LLM framework, we utilize the conditional probability of generating the response given the context as this scoring function. This choice is natural as it directly reflects the model's generative capability and its learned understanding of contextresponse relationships: \n\nWith the scoring function defined and negative samples constructed, the Contrastive Learning Objective L Contrastive is formulated as a margin-based ranking loss. This loss function is designed to enforce a margin between the score of the positive example and the scores of the negative examples: \n\nwhere m > 0 is a pre-defined margin hyperparameter that controls the desired separation between positive and negative scores, and D represents the training data distribution. This loss function operates by increasing the loss value whenever the score of a negative example is within a margin m of the score of the positive example. By minimizing L Contrastive , we explicitly train the SA-LLM to assign a higher compatibility score to the ground-truth response Y when paired with the correct dialogue context X, in comparison to when it is paired with either a contextually incoherent response Y neg_context or a speaker-inconsistent context X neg_speaker . This pushes the model to learn robust representations that capture both contextual coherence and speaker awareness. \n\nOverall Training Objective The final training objective for the Speaker-Attentive LLM (SA-LLM) is a carefully balanced combination of the Language Modeling Loss and the Contrastive Learning Objective. This combined objective is expressed as: \n\nwhere \u03bb is a non-negative hyperparameter that governs the relative importance and contribution of the Contrastive Learning Objective to the overall training process. The hyperparameter \u03bb allows us to control the emphasis placed on learning contextual coherence and speaker awareness relative to the standard language modeling task.",
            "score": 0.423657647146957,
            "section_title": "Contrastive Learning Objective",
            "char_start_offset": 18701,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 323
                },
                {
                    "start": 326,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 736
                },
                {
                    "start": 739,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1811
                },
                {
                    "start": 1814,
                    "end": 2014
                },
                {
                    "start": 2015,
                    "end": 2055
                },
                {
                    "start": 2058,
                    "end": 2223
                },
                {
                    "start": 2224,
                    "end": 2389
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8466796875
        },
        {
            "corpus_id": "202577673",
            "title": "Global Autoregressive Models for Data-Efficient Sequence Learning",
            "text": "Training-2 While Training-1 results in a welldefined model P \u03bb (x), which may fit the data closely in principle, we should not conclude that P \u03bb (x) is convenient to use for inferencenamely, in language modeling, efficiently sampling from its normalized version p \u03bb (x); as seriously, because of the partition factor Z \u03bb , it is also not obvious to evaluate the perplexity of P \u03bb (x) on test data. In order to do both, one approach consists in using a distillation technique (Hinton et al., 2015), where, during training, one expends generous time towards producing a set of samples from P \u03bb , for instance by Monte-Carlo (e.g. Rejection Sampling) techniques, and where this set (which may be arbitrarily larger than the original D) is in turn used to train a new autoregressive model \u03c0 \u03b8 (x), which can then be used directly for sampling or for computing data likelihood. This is the approach that we use in our current experiments, again using the original r(x) as a proposal distribution.",
            "score": 0.4227106823263234,
            "section_title": "r(x)",
            "char_start_offset": 10705,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 991
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52978515625
        },
        {
            "corpus_id": "247694098",
            "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU",
            "text": "Since training a teacher model can add overhead to the general training process (training a teacher model plus a similar-sized student), we further conduct a minimal experiment on PAWS, where we collect training dynamics for a teacher XLM-R model for different number of epochs (stopping training early) and then train a student XLM-R model for longer, 10 epochs. Results are reported in Table 5 for our best overall curriculum for this dataset CORR+VAR ANNEAL as the average of the validation set languages performance. \n\nWe observe that it is not necessary to collect training dynamics for a long period of training (e.g. 10 epochs) as even with much less training, for in-",
            "score": 0.4225333563798619,
            "section_title": "Training with limited budget",
            "char_start_offset": 22578,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 520
                },
                {
                    "start": 523,
                    "end": 675
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87109375
        },
        {
            "corpus_id": "274581427",
            "title": "Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization",
            "text": "Language models (LMs), like other neural networks, often favor shortcut heuristics based on surface-level patterns. Although LMs behave like n-gram models early in training, they must eventually learn hierarchical syntactic representations to correctly apply grammatical rules out-of-distribution (OOD). In this work, we use case studies of English grammar to explore how complex, diverse training data drives models to generalize OOD. We construct a framework that unifies our understanding of random variation with training dynamics, rule selection with memorization, and data diversity with complexity. We show that these factors are nuanced, and that intermediate levels of diversity and complexity lead to inconsistent behavior across random seeds and to unstable training dynamics. Our findings emphasize the critical role of training data in shaping generalization patterns and illuminate how competing model strategies lead to inconsistent generalization outcomes across random seeds. Code is available at https://github.com/sunnytqin/concept_comp.git.",
            "score": 0.4222669038718771,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98388671875
        },
        {
            "corpus_id": "273162483",
            "title": "How Hard is this Test Set? NLI Characterization by Exploiting Training Dynamics",
            "text": "To show that our method is model-agnostic, we further provide a comparison between the dataset characterizations obtained by RoBERTa and De-BERTa. Table 4 showcases the accuracies of the two models on each others' data characterizations. The difficulty splits are maintained cross-model. Across datasets and difficulty levels, the performance sharply drops for the \"hard\" split for both models. DeBERTa achieved higher accuracy for \"hard\" set, most likely due to better overall performance compared to RoBERTa. In Figure 5, we show that overall heuristic values for \"Contains Negation\" are maintained across both models. Extended results for are presented in Appendix A. \n\nOur proposed methodology is general and independent of the underlying encoder model since we process training dynamics computed from raw logit scores. This characterization procedure may be adapted to using Large Language Models (LLMs) (Lee et al., 2023) in a zero-shot classification setting by manipulating the log-likelihood for the tokens of the correct classes. However, using LLMs requires a different approach than the one presented here since the networks are usually used without further training, in an in-context-learning manner (Dong et al., 2022). Furthermore, even if the LLMs are fine-tuned (Hu et al., 2021), it is not straightforward how the logits of each of the three classes are tracked across training. We leave this approach for future work. Our method highlights significant shortcomings in widely used NLI evaluation datasets (SNLI and MultiNLI) due to spurious correlations in the annotation process. To address these issues, we proposed an automatic method for constructing more challenging test sets, effectively filtering out problematic instances and providing a more realistic measure of model performance. Our approach, which categorizes examples in increasing difficulty levels using a wide range of training dynamics features, enhances evaluation reliability and offers insights into underlying challenges in NLI. Importantly, our methodology is general and model-agnostic, and can be applied across different datasets and models, promising improved evaluation practices in NLP.",
            "score": 0.4213422989720612,
            "section_title": "Impact of the Underlying Encoder",
            "char_start_offset": 20609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 909,
                    "end": 927,
                    "matchedPaperCorpusId": "258841424"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.931640625
        },
        {
            "corpus_id": "263908945",
            "title": "Towards Foundation Models for Materials Science: The Open MatSci ML Toolkit",
            "text": "Markovian dynamics, which are violated for large batch training, and result in a disconnect in the learning dynamics between layers whereby gradients decay to the order of  used to prevent division by zero. While Molybog et al. [29] discuss this in the context of large language models, the instabilities are general to \"Adam-like\" optimizers and likely to apply here, particularly as the use of GNN encoders can lead to over-smoothing (see [35] for a review) and therefore decrease signal contained within gradients; a symptom described by Molybog et al. [29] in early layers of language models. \n\nA thorough analysis into this phenomenon is outside our scope, and in the interim, we found a balance between convergence characteristics and training throughput with  = 256, or 16 nodes. While this kind of optimization is conventionally offloaded to hyperparameter optimization, further work is required to assess a more principled approach for large scale pretraining in Open MatSci ML Toolkit. Subsequent experiments involve using a model trained for 20 epochs, taking approximately \u223c80 min.-an adequate turnover rate for experimentation.",
            "score": 0.4196635557627997,
            "section_title": "Symmetry group pretraining",
            "char_start_offset": 17688,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 596
                },
                {
                    "start": 599,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1140
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79345703125
        },
        {
            "corpus_id": "267334853",
            "title": "Arrows of Time for Large Language Models",
            "text": "In the first experiment, we generate a linear language model in F 25 2 , with p = 0 perturbations, and matrices with a number 25 + k of non-zero elements, where k \u2208 [0,2,4,8,10,14,18,20,25,30,35,40,45,50]. We then train a transformer model of size GPT1 (see Table 6) on 600k sentences, with batch size 200. Note that the context size of the model matches exactly the number of tokens in one sentence. Final losses are reported in 4. Note that for lower sparsities, the trend is not obvious: this is due to the high variance in the final learning rate, as the learning of only a few non-zero elements is binary, depending on the initialization, the model either learns the matrix perfectly quickly, or it usually struggles to find the last few non-zero elements. Fig. 9 displays this behavior in the case k = 4. Note that the perturbations somewhat reduce this behavior, but don't cancel it completely. Fig. 10 displays typical learning dynamics for this problem, for k = 8 (high sparsity) and k = 40 (medium sparsity). We remind that in principle, given a large enough model, and enough training steps, the model should be able to find the optimal solution (hardness of type (2), see Section 3).",
            "score": 0.4194818216370884,
            "section_title": "B.2.2. SPARSITY LEVELS",
            "char_start_offset": 46382,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1195
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9638671875
        },
        {
            "corpus_id": "277104430",
            "title": "Using the Tools of Cognitive Science to Understand Large Language Models at Different Levels of Analysis",
            "text": "The computational level of analysis provides a powerful lens for understanding large language models by focusing on their function. Examining the training objective (in this case, autoregression) can directly predict specific behavioral patterns, as illustrated by the \"embers of autoregression.\" Furthermore, comparing LLMs to optimal benchmarks, whether derived from Bayesian models of cognition or the axioms of probability and decision theory, can reveal both the surprising capabilities and the systematic limitations of these systems. By considering what computational problem LLMs are solving (or approximating), we can gain significant insight into their behavior and internal structure, even when the algorithmic and implementation details remain opaque.",
            "score": 0.4185848365485682,
            "section_title": "Summary",
            "char_start_offset": 15932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 763
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8955078125
        },
        {
            "corpus_id": "259859071",
            "title": "Curating Datasets for Better Performance with Example Training Dynamics",
            "text": "We presented a new method for computing Example Training Dynamics, which can be used to increase both ID and OOD performance, without any changes to model size or architecture. We demonstrated that ETD can be transferable, i.e., they can be computed once and used many times for different models, reducing the computation cost at a long term. Finally, we have shown that ETD can be computed on the fly using Dynamic Training, which may hold the key to improved performance using ETD at no extra compute cost. \n\nAs the field of NLP leans more and more into the self-supervised pre-training paradigm, further research on ETD may be focused on adjusting our method for larger and self-supervised datasets in order to improve and reduce the cost of pre-training as a whole.",
            "score": 0.4185848365485682,
            "section_title": "Conclusion",
            "char_start_offset": 13303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 508
                },
                {
                    "start": 511,
                    "end": 769
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89453125
        },
        {
            "corpus_id": "265128603",
            "title": "The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models",
            "text": "To further probe the evolution of anisotropy, we examine its progression through the training phases of various models. \n\nFigure 2 and Figure 3 capture this trajectory by plotting anisotropy values for decoders at different training checkpoints at all internal layers. The consistent growth pattern, followed by stabilization, is observed across various models, suggesting an inherent characteristic of the language modeling training dynamics of decoders.",
            "score": 0.41753083880578606,
            "section_title": "Anisotropy Dynamics During Training",
            "char_start_offset": 12405,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 122,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 455
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9775390625
        },
        {
            "corpus_id": "271909320",
            "title": "Scaling Law with Learning Rate Annealing",
            "text": "In recent years, large language models (LLMs) have garnered significant academic and industrial attention (Brown et al., 2020;Touvron et al., 2023). The scaling law suggests that the validation loss of language models follow a power-law pattern as model and data sizes increase (Hestness et al., 2017;Kaplan et al., 2020;Henighan et al., 2020). This law provides a powerful framework for forecasting LLM performances before large scale training by fitting losses at smaller scales (OpenAI, 2023;DeepSeek-AI, 2024;Dubey et al., 2024). Numerous studies have explored on the formulation to model the scaling effect of LLMs under various different settings (Bahri et al., 2021;Hernandez et al., 2021;Caballero et al., 2022;Michaud et al., 2023;Muennighoff et al., 2023). \n\nHowever, typical scaling law formulations focus only on the final loss at the end of training (Hoffmann et al., 2022). Specifically, previous approaches generally rely on a set of training runs and fit the scaling law curve solely on the final loss from each run. Essentially, the middle points with different degrees of LR annealing fail to follow typical scaling laws, which do not consider local loss drop brought by LR annealing. The previous approach under-utilizes the training compute and fails to capture the training dynamics within each run. Further, the application of scaling laws in LLM developments is limited since the loss curve through the whole training process is not modeled. An expressive formulation that models full loss curves enables prediction of future training dynamics and also offers insights on understanding the learning process of LLMs. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Step  and S 2 at the 20-th step of a cosine LR scheduler. S 1 is the forward area, i.e., sum of red grid areas, which can be approximately regarded as the total amount of movement for neural network parameters; S 2 is the decayed annealing area, i.e., weighted sum of blue grid areas, where lighter shades indicate smaller weights.",
            "score": 0.41723827318256446,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 766
                },
                {
                    "start": 769,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 2023
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 126,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 719,
                    "end": 740,
                    "matchedPaperCorpusId": "257687665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "256460933",
            "title": "Calibrating Student Models for Emotion-related Tasks",
            "text": "Our proposed approach shows that using training dynamics to generate mixup samples along with the dark knowledge of a pre-trained language teacher model can act as a regularization process, which helps to calibrate the student model's confidence in its predictions. It would be interesting to analyze the impact of adding mixed data augmentation techniques to the student networks on the calibration of the pre-trained language models. One potential limitation of our approach is using a small additional overhead for calculating statistics with the data maps tool. However, this is a common limitation for all approaches that use this data maps.",
            "score": 0.416712158373033,
            "section_title": "Limitations",
            "char_start_offset": 26984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 646
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "262217328",
            "title": "Learning to Diversify Neural Text Generation via Degenerative Model",
            "text": "Previous studies have reported that the generation quality is likely to be degraded due to inherent attributes within the training examples, such as token repetition (Welleck et al., 2020;Fu et al., 2021), a skewed frequency distribution of words (Fagan and Gen\u00e7ay, 2011), and genericness in responses (Cs\u00e1ky et al., 2020). We refer to such attributes as degenerative attributes in the paper. \n\nIn this section, we analyze how such degenerative attributes affect the learning dynamics of training examples. We conduct experiments on two openended text generation tasks: language modeling and dialogue generation.",
            "score": 0.416408720887057,
            "section_title": "Preliminary Study",
            "char_start_offset": 2912,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 392
                },
                {
                    "start": 395,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 612
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 188,
                    "matchedPaperCorpusId": "199551982"
                },
                {
                    "start": 188,
                    "end": 204,
                    "matchedPaperCorpusId": "229923515"
                },
                {
                    "start": 302,
                    "end": 322,
                    "matchedPaperCorpusId": "153312586"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93408203125
        },
        {
            "corpus_id": "277349236",
            "title": "How do language models learn facts? Dynamics, curricula and hallucinations",
            "text": "On the learning dynamics of language models. This work reveals the existence of phase transitions, the formation of induction heads being the canonical example (Garg et al., 2022;Olsson et al., 2022;Reddy, 2024), in tasks as simple as factual recall. While the results presented above were obtained on an isolated task, we speculate that even with natural text and its multi-task nature, task-specific learning dynamics retain the same abrupt transitions and plateaus appear when the formation of a new circuit is the main bottleneck for solving a specific task. We found the time spent in the transition phase to depend more on the data distribution than the model size. Emergent abilities (Wei et al., 2022) can thus result from increased training time as models are scaled. On the data distribution side, our results have two consequences: First, they suggest the benefits of using synthetic data early in pre-training, since data used before the end of the plateau is not retained in the final model. Second, data schedulers, potentially even adaptive ones that reduce diversity when performance plateaus on a task, appear to be a promising direction to improve learning speed. Finally, our identification of changes in feedforward associative memories leading to rapid performance drops during early fine-tuning provides a simple explanation for the practically-observed ineffectiveness of fine-tuning for new knowledge (e.g., Jain et al. (2024); Ovadia et al. ( 2023)). \n\nOn the learning dynamics of neural networks more broadly. This analysis reveals that, in a simple but relevant setting, attention-based recall circuits emerge before the formation of associative memories in feedforward layers. We hypothesize that this occurs as established circuits amplify the correlation between the inputs and backpropagated errors received by feedforward layers. Prior to circuit formation, task learning remains possible (e.g., by artificially increasing name token values), but progress is slow, performance plateaus, and generalization likely suffers. Phenomena like grokking (Nanda et al., 2023a;Power et al., 2022) may indicate that learning dynamics initially find this shortcut, before abandoning it due to regularization.",
            "score": 0.41621821062589864,
            "section_title": "Discussion",
            "char_start_offset": 27933,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 45,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1475
                },
                {
                    "start": 1478,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 179,
                    "matchedPaperCorpusId": "251253368"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99462890625
        },
        {
            "corpus_id": "277349236",
            "title": "How do language models learn facts? Dynamics, curricula and hallucinations",
            "text": "This study investigates the learning dynamics underlying factual recall and knowledge acquisition in language models. This presents two main methodological challenges: First, we need to isolate the model's knowledge from other abilities. Second, we want to evaluate the models over the course of learning, which requires computationally efficient measurement techniques. Following Allen-Zhu and Li (2023), we train language models on synthetic biographies that feature key properties of datasets used to train large language models. By carefully designing these synthetic biographies, we can attribute the model's ability to predict specific tokens solely to its acquired knowledge and efficiently measure its knowledge through its performance on these tokens. In this section, we first define knowledge and contrast it with memory, then describe our synthetic dataset and introduce the metrics we use to track knowledge during training, and finally describe training details.",
            "score": 0.4146767728668682,
            "section_title": "An experimental setup to track knowledge over the course of learning",
            "char_start_offset": 2523,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 976
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98779296875
        },
        {
            "corpus_id": "4394853",
            "title": "Meta-Learning a Dynamical Language Model",
            "text": "Given a sequence of T discrete symbols S = (w 1 , w 2 , . . . , w T ), the language modeling task consists in assigning a probability P (S) = p(w 1 , . . . , w T ) to the sequence. P (S) can be written as \n\nIn the case of a neural-network-based LM, the conditional probability P (w t | w t\u22121 , . . . , w 0 , \u03b8) is typically parametrized using an autoregressive neural network as P (w t | w t\u22121 , . . . , w 0 , \u03b8) = f \u03b8 (w t\u22121 , . . . , w 0 ), \u03b8 being a set of parameters of the language model network. \n\nIn the dynamical framework, the parameters \u03b8 of the language model are not tied over the sequence S but are allowed to evolve. Prior to computing the probability of a future token w t , a set of parameters \u03b8 t is estimated from the past parameters and tokens as \u03b8 t = argmax \u03b8 P (\u03b8 | w t\u22121 , . . . , w 0 , \u03b8 t\u22121 . . . \u03b8 0 ) and the updated parameters \u03b8 t are used to compute the probability of the next token w t . \n\nIn the hierarchical model, the updated parameters \u03b8 t are estimated by a higher level neural network: \n\nwhere \u03c6 is the set of (static) parameters of the higher level network (meta-learner).",
            "score": 0.41450130713776434,
            "section_title": "DYNAMICAL LANGUAGE MODELING",
            "char_start_offset": 3533,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 204
                },
                {
                    "start": 207,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 501
                },
                {
                    "start": 504,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 918
                },
                {
                    "start": 921,
                    "end": 1022
                },
                {
                    "start": 1025,
                    "end": 1110
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83935546875
        },
        {
            "corpus_id": "227278160",
            "title": "Subspace-Based Representation and Learning for Phonotactic Spoken Language Recognition",
            "text": "Another method to capture the phonetic dynamics is to use state space models (SSMs), which are similar to hidden Markov models, except that the hidden states in SMMs are continuous [59]. Discrete-time dynamic linear models (DLMs) with Gaussian noise are simple versions of SSMs. It is assumed that the state of a process can be summarized by a multivariate variable, which cannot be observed directly at each time step but can be linked to observables using a measurement equation, provided that all the conditional random variables are linear-Gaussian distributed [60]. In the last decade, DLMs have been used to characterize acoustic-feature dynamics and track objects in videos [61], [62]. Inspired by these studies, we assume that each utterance can be generated by a DLM-based system, which may be regarded as a sub-system related to an unknown language production system in the human brain. This assumption is similar to the acoustic theory of speech production, which assumes that speech production is a linear system comprising a source and a filter [63].\n\nLet {y k } K k=1 be a sequence of K phonetic vectors of an utterance, where y k \u2208 R M and M denotes the number of phonemes, and {x k } K k=1 be the corresponding latent variables representing the hidden states of the system, where x k \u2208 R d , d denotes the system complexity, and d \u2264 min(M, K). A DLM can be specified by the following two linear equations:\n\nIn (1), w \u2208 R d and v \u2208 R M denote two time-independent noises caused during state evolution and observation generation, respectively. They are white-Gaussian distributed and stochastically independent. In addition to the covariance matrices Q and R, which can be reduced to identity matrices I d and I M for simplicity, respectively, the system comprises two operators that must be estimated, i.e., the state transition matrix A \u2208 R d\u00d7d and the generative matrix C \u2208 R M \u00d7d . In this regard, the DLM transforms an utterance into a pair of linear operators, i.e., A and C, as shown in Fig. 2.\n\nTo learn the dynamic system, i.e.,",
            "score": 0.41450130713776434,
            "section_title": "2) Dynamic Linear Modeling:",
            "char_start_offset": 14650,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 565,
                    "end": 569,
                    "matchedPaperCorpusId": "2590898"
                },
                {
                    "start": 681,
                    "end": 685,
                    "matchedPaperCorpusId": "35824741"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4697265625
        },
        {
            "corpus_id": "247447316",
            "title": "Staged Training for Transformer Language Models",
            "text": "This appendix defines a constrained optimization problem that produces the optimal staged-training schedule. The scaling laws of Kaplan et al. (2020) derived empirical fits for the language model loss L as it relates to the total amount of compute C, number of non-embedding parameters N , number of gradient update steps S, and batch size B. The total compute is given by \n\nand the loss L for any model size N and number of steps S is given by: Our goal is to minimize the total compute to train a model of a given size N target to a given target loss L target . We assume we have access to perfect growth operators that are loss-preserving and training dynamics-preserving, and that can grow from any model size to any other size. We consider a training regime consisting of a number of This neglects contributions proportional to the context length, nctx, and may not be valid in regime of large nctx where nctx \u2265 12d model \n\nWe can restrict the model size increases by adding additional constraints.",
            "score": 0.4123358598583665,
            "section_title": "C Optimal Stage Schedule",
            "char_start_offset": 32306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 372
                },
                {
                    "start": 375,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 926
                },
                {
                    "start": 929,
                    "end": 1003
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92919921875
        },
        {
            "corpus_id": "248986465",
            "title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models",
            "text": "To investigate the dependence of our observations on the particular language modeling task, we repeat this analysis for the masked language modeling task on WIKITEXT103 with mask probability 0.15. \n\nUnlike in causal language modeling, Figure 2 shows that T (N, \u03c4 ) is not monotonically decreasing in N for lower values of \u03c4 , and is monotonically decreasing in N for higher values of \u03c4 , where the phase transition4 between these two regimes occurs between \u03c4 = 0.6 and \u03c4 = 0.7. Smaller models memorize the training data quicker initially and slower in the long run (e.g., right plot of Figure 11). Language model training is heavily dependent on the dataset size [44], and therefore we expect M (f ) to be similarly impacted. In Figure 3, we analyze training set memorization on the much bigger ROBERTA dataset for both masked and causal language modeling. With large datasets such as ROBERTA dataset, it becomes infeasible to perform multiple epochs and evaluate memorization on the entire training set, especially when training larger models. Consequently, we focus on smaller values of \u03c4 and investigate the number of gradient descent updates it takes to reach memorization thresholds, i.e., T update (N, \u03c4 ). In Figure 3 we observe a similar trend as Figure 1, where T update (N, \u03c4 ) is monotonically decreasing with N for various \u03c4 , in both masked and causal language modeling. Unlike with WIKITEXT103, masked language modeling does not have a phase transition for \u03c4 .",
            "score": 0.4121202598171314,
            "section_title": "Dependence on Language Modeling Task and Dataset Size",
            "char_start_offset": 9814,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 199,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1474
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "277104392",
            "title": "Aligned Probing: Relating Toxic Behavior and Model Internals",
            "text": "We analyze how model behavior and internals evolve during pre-training by studying six pretraining checkpoints of OLMo (Groeneveld et al., 2024) -see Figure 12 of the appendix. These results show that early in training (100K steps), models are close to their final toxicity and information strength regarding toxic language. Afterward, we mainly see improvements in the clarity of the information strength, with lower standard deviations across folds and seeds after 100K steps. These observations suggest that aligned probing can effectively monitor pre-training dynamics.",
            "score": 0.4120407043087111,
            "section_title": "Case Study: Pre-Training Dynamics",
            "char_start_offset": 31513,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 573
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 144,
                    "matchedPaperCorpusId": "267365485"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.744140625
        },
        {
            "corpus_id": "256631050",
            "title": "Data Cartography for Low-Resource Neural Machine Translation",
            "text": "We characterize movement across regions more directly by reporting the proportion of samples that move across the different regions over the three training phases in Figure 3. The main type of movement consists of hard samples becoming easy from one phase to the next: 30% of Swahili-English samples and 25% of Turkish-English samples that were hard in phase 1 become easy in phase 2, while 23% of Swahili-English samples and 24% of Turkish-English samples that were hard in phase 2 become easy in phase 3, reflecting that model confidence increases while variability remains in the same lower range for these samples as training progresses. Easy samples in one phase remain easy in the next. Ambiguous samples also remain ambiguous, except for Turkish-English where 15% of samples move from the ambiguous to the easy regions between phases 1 and 2. Notably, by the end of training, 42% and 35% of the Swahili and Turkish data respectively are still classified as hard examples. \n\nOverall, this analysis suggests that the role of samples changes during training, but in a way that reflects overall model improvement rather than the modeling focus of each phase: for instance we do not observe a trend where samples that are easy for language modeling in phase 1 become hard in phase 2 while learning lexical translations. We conduct further filtering experiments to assess how sample movement impacts training more directly.",
            "score": 0.4107695400104575,
            "section_title": "Sample Movement",
            "char_start_offset": 12947,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1424
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9755859375
        },
        {
            "corpus_id": "270372048",
            "title": "Aligning Large Language Models with Representation Editing: A Control Perspective",
            "text": "A pre-trained autoregressive language model processes a sequence of input tokens and predicts subsequent tokens by recursively processing the sequence. we focus on the transformer-based architecture [61] prevalent in modern language models [10,55,1]. \n\nDefinition 4.1 (Language dynamical system) The behavior of a language dynamical system is governed by a function f LM , which acts as the state transition function, defined as: \n\nHere, y t is the newly generated token at each time step. h t comprises key-value pairs accumulated from previous time steps, represented as \n\nt ) corresponds to the key-value pairs generated from the i-th layer at time t. W is a linear transformation that maps the logits o t+1 to a probability distribution over the vocabulary space V. The system's evolution continues until y t = EOS, where EOS represents a special stopping token that signifies the end of the system. \n\nIn this system, the hidden state h t along with the logits o t corresponds to the state s t in a traditional stochastic dynamical system. The newly sampled token y t at each time step plays a role similar to the random variable \u03c9 t , introducing stochasticity into the system. The initial state, s 0 = {h 0 , o 0 }, is set by a given prompt x, marking the starting point of the dynamical process. 2: At test time, we perform gradient-based optimization to determine the control signals added to the language dynamical system for alignment. The color represents the value score on the state space, with darker colors indicating higher scores. Our goal is not to update the state to the global optimum but to control the state to achieve a better value score while remaining close to the original state. \n\nOriginal state Updated state However, unlike typical dynamical systems, this model lacks a direct control signal, functioning as an uncontrolled system. Next, we will explore how optimal control techniques can be applied to align the behavior of pre-trained language models with specific objectives.",
            "score": 0.41045944639441756,
            "section_title": "Autoregressive LLMs are Discrete-Time Stochastic Dynamical Systems",
            "char_start_offset": 11189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 250
                },
                {
                    "start": 253,
                    "end": 429
                },
                {
                    "start": 432,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 903
                },
                {
                    "start": 906,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1707
                },
                {
                    "start": 1710,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 2009
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 244,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9443359375
        },
        {
            "corpus_id": "276775997",
            "title": "From Small to Large Language Models: Revisiting the Federalist Papers",
            "text": "The fundamental objective of language models is characterizing the probabilistic mechanism for W that captures how often tokens (in a particular order) occur in the language. This can be captured with a likelihood function \n\nwhich explains how likely w is under different values of the model parameters \u03b8. In LLMs, \u03b8 is entirely uninterpretable and has massive dimensionality, parametrizing layers and layers of encoders and decoders. In small language models, \u03b8 has moderate dimensionality and serves to provide insight into the language structure. Training a language model on real data D involves estimating a parameter \u03b8 for which the likelihood of observing aspects (sequences) of D is the largest. For LLMs, the training data D is a vast corpus of text (inaccessible to the user) while for small language models, D is a collection of observed text documents (such as our Federalist Papers). Training the language model (1) can be facilitated by making some simplifying structural assumptions about \u03c0 \u03b8 (w). We will roughly group language models according to the assumption made. Bag-of-words Models utilize the simplifying assumption that the order of the words in the sequence W does not matter. In statistics, this corresponds to the notion of exchangeability where \u03c0 \u03b8 (w) = \u03c0 \u03b8 (w \u03c3(1,...,T ) ) for any permutation \u03c3(1, . . . , T ). Such language models very often operate on a specific summary statistic (word counts) as opposed to raw data (word sequences). For example, our analysis of the Federalist Papers involves training data documents D = {D i } n i=1 where \n\nj=1 . The word count matrix X = {x ij } \u2208 R n\u00d7N can be then constructed by counting the occurrence of each word in each document x ij = n i=1 N j=1 I{w j \u2208 D i }, where N = |W| denotes the size of the vocabulary. We talk about bag-of-words models later in Section 3.2.1 in the context of Latent Dirichlet Allocation and Latent Semantic Analysis. \n\nAutoregressive models characterize the joint distribution using the chain rule of probability as follows:",
            "score": 0.41045944639441756,
            "section_title": "Probabilistic Language Models",
            "char_start_offset": 15227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 222
                },
                {
                    "start": 225,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1576
                },
                {
                    "start": 1579,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1924
                },
                {
                    "start": 1927,
                    "end": 2032
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73828125
        },
        {
            "corpus_id": "272600327",
            "title": "Stable Language Model Pre-training by Reducing Embedding Variability",
            "text": "Improving large language models (LLMs) typically involves increasing model size, especially through greater depth (Brown et al., 2020;Kaplan et al., 2020;Rae et al., 2022;Xue et al., 2023). However, this approach often causes instability during pre-training, indicated by sudden spikes in loss (Chowdhery et al., 2022;Zhai et al., 2023), while stable pre-training typically leads to stronger performance under controlled training configurations (Touvron et al., 2023a;Takase et al., 2024). Such instability can lead to catastrophic divergence or degradation, underscoring the importance of assessing pre-training stability (Chowdhery et al., 2022;Zhai et al., 2023;Takase et al., 2024). \n\nThe conventional methods for monitoring the pre-training stability are computationally expensive (Kaplan et al., 2020), such as observing the gradient variance which needs additional O(nd) for gradient matrix g t \u2208 R n\u00d7d (Zhao et al., 2024) or analyzing Figure 1: TEV distribution for OPT, Pythia, Llama-2, and GPT-2 reveals that as model size grows, both \u00b5 TEV and \u03c3 TEV decrease. This trend correlates with better model performance, as reduced noisy gradients lead to higher pre-training stability and improved performance. For a fair comparison, Pythia 6.9B and 12B were excluded due to their different vocabulary sizes. \n\nthe singular values of the second-order derivative of the loss with respect to model parameters (Yao et al., 2020;Gilmer et al., 2021;Cohen et al., 2024). Further details will be addressed in Appendix A.",
            "score": 0.41045944639441756,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 686
                },
                {
                    "start": 689,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1312
                },
                {
                    "start": 1315,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1518
                }
            ],
            "ref_mentions": [
                {
                    "start": 318,
                    "end": 336,
                    "matchedPaperCorpusId": "257496258"
                },
                {
                    "start": 647,
                    "end": 665,
                    "matchedPaperCorpusId": "257496258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "265067542",
            "title": "Large language models implicitly learn to straighten neural sentence trajectories to construct a predictive representation of natural language",
            "text": "Our work puts forward and provides support for a general hypothesis at the representational level about a mechanism that allows large language models to achieve good performance on next word prediction and potentially downstream tasks. This work adds to the growing body of research on the interpretability of AI models. A better, more mechanistic understanding of these models, and potentially other models with prediction objectives, can both i) suggest ways to improve model efficiency and robustness, and ii) provide insights into the relationship between neural networks and the human language system. We acknowledge that our work could be improved in several respects. The results as they stand are compatible with at least two possibilities: (i) that predicting future inputs intrinsically gives rise to implicit next-state prediction, thus directly favoring linear state dynamics, or (ii) that predicting future inputs in domains like language benefits from slowly-changing contextual information, thus indirectly favoring slower (and more approximately linear) state dynamics. These possibilities can be distinguished in the future by training models on artificially created datasets that vary in the length of context that affects the predictability of an incoming element. If the effects we report here obtain across these different training datasets, that would support the first possibility; if instead the effects only hold for models trained on data where relatively long predictive contexts, that would support the second possibility. Furthermore, we have not evaluated the effects on sentence curvature of other training objectives or fine-tuning for downstream tasks. Doing so can help understand the selectivity of the observed effects (i.e., do sentence representations get straightened in the middle layers only under the pressure of the next-word prediction objective?) and their robustness to adding other objectives to a pre-trained model. We have also not causally tested the straightening hypothesis, which would require ablating the model in such a way that only curvature is affected, and testing how next-word prediction behavior changes. \n\nIt is also important to note that we are not claiming that representation straightening is the only mechanism that models rely on to gain linguistic competence. However, to the extent that prediction is a core part of language learning and processing (in artificial as well as biological systems), we are showing that targeted inspection of geometric properties of sentence representation gives rise to a hypothesis about how prediction may be implemented in language models.",
            "score": 0.410088898048578,
            "section_title": "Broader Impact and Limitations",
            "char_start_offset": 28219,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2167
                },
                {
                    "start": 2170,
                    "end": 2330
                },
                {
                    "start": 2331,
                    "end": 2645
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96240234375
        },
        {
            "corpus_id": "272910595",
            "title": "Unveiling the Role of Pretraining in Direct Speech Translation",
            "text": "However, the authors show that many strategies need to be simultaneously used to achieve this, such as an exhaustive hyperparameter tunning, CTCbased regularization and their proposed parameterized distance penalty. \n\nComplementing previous interpretability works in ST (Xu et al., 2021;Alastruey et al., 2022), in this study, we conduct the first-ever analysis of the training dynamics of a ST system, and based on its results, we propose a subtle modification in the Transformer (Vaswani et al., 2017) architecture to bypass the pretraining stage. \n\nFirst, we compare the training dynamics of a conventional system that uses a pretrained encoder with one trained from scratch1 . Through this analysis, we observe significant disparities in their behaviors. Particularly, we note that when making predictions, the model trained from scratch delays the utilization of information extracted by the encoder until a later stage of training. \n\nWe hypothesize that this delay occurs due to the complexity of the acoustic modeling task, that in this setting needs to be learned together with the semantic modeling. Hence, it takes a significant amount of updates to sufficiently train the encoder so that it can extract meaningful information. Consequently, the model ignores the encoder outputs and focuses on training the decoder for language modeling. Once the encoder can extract valuable representations, the model has already converged towards language modeling and struggles to rely on the information obtained by the encoder. \n\nSecondly, we believe that by forcing the model to utilize encoder outputs earlier in the training process, the model would not converge towards language modeling and the encoder would be trained more rapidly, leading to higher-quality representations in its outputs. Through a modification in the residual connection in the decoder crossattention mechanism, we force the model trained from scratch to integrate source information from earlier training steps, and we observe a comparable performance than in the pretrained one. \n\nOverall, the main contributions of our work are: \n\n(1) the first study of training dynamics in ST, that unveils the role of the pretraining, and (2) a modification in the Transformer architecture to bypass the pretraining stage.",
            "score": 0.4091248062141499,
            "section_title": "Introduction",
            "char_start_offset": 1722,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 218,
                    "end": 549
                },
                {
                    "start": 552,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 937
                },
                {
                    "start": 940,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1527
                },
                {
                    "start": 1530,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 2056
                },
                {
                    "start": 2059,
                    "end": 2107
                },
                {
                    "start": 2110,
                    "end": 2287
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 310,
                    "matchedPaperCorpusId": "248239907"
                },
                {
                    "start": 481,
                    "end": 503,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9765625
        },
        {
            "corpus_id": "271270337",
            "title": "Latent Causal Probing: A Formal Perspective on Probing with Causal Models of Data",
            "text": "We describe the key steps according to the framework in Section 3.3; Appendix A.1 contains full experimental details (e.g., LM and probe architecture and training, LM representations). \n\nHypothesis class. The exogenous variables are the initial state and program. The latent variables are the intermediate states, and the observed variables are the initial and final state and the program. The hypothesis class M is all Markovian SCMs over the variables. \n\nTarget SCM and latent variables. The target SCM M \u2208 M is the true data generation process in Figure 2. The target latent variables consist of the robot's position, facing direction, and whether the robot is facing a rock for each intermediate state. \n\nAuxiliary dataset construction and bound and free latent variable outcomes. For the auxiliary dataset, we use the same data generation process, except that programs range in length between 1 and 15, and we replace the final state in the specification with the initial state. Due to the size of the training corpus (1 million samples), we assume the LM observed all combinations of the exogenous variables. Because the programs in the LM training corpus are between length 5 and 9, the bound latent variables are s 6 to s 10 (they are observed during training as the final state). The free latent variables are s 1 to s 5 and s 11 to s 15 . \n\nValid baseline. We construct valid baselines according to a counterfactual state of the world where the intermediate states are generated by executing the program according to a different set of causal dynamics. Specifically, we define M \u2032 using the same SCM, but permute the causal dynamics of the turn_right, turn_left, and move actions (e.g., the robot turns left when executing a turn_right action). As M and M \u2032 are clearly symmetric from a language modeling perspective, Definition 3.1 (and hence Proposition 3.2) holds.",
            "score": 0.40864757856366585,
            "section_title": "Methods",
            "char_start_offset": 20873,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 187,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 454
                },
                {
                    "start": 457,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 706
                },
                {
                    "start": 709,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1877
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2474365234375
        },
        {
            "corpus_id": "261030259",
            "title": "Latent State Models of Training Dynamics",
            "text": "The impact of randomness on model training is poorly understood. How do differences in data order and initialization actually manifest in the model, such that some training runs outperform others or converge faster? Furthermore, how can we interpret the resulting training dynamics and the phase transitions that characterize different trajectories? To understand the effect of randomness on the dynamics and outcomes of neural network training, we train models multiple times with different random seeds and compute a variety of metrics throughout training, such as the $L_2$ norm, mean, and variance of the neural network's weights. We then fit a hidden Markov model (HMM) over the resulting sequences of metrics. The HMM represents training as a stochastic process of transitions between latent states, providing an intuitive overview of significant changes during training. Using our method, we produce a low-dimensional, discrete representation of training dynamics on grokking tasks, image classification, and masked language modeling. We use the HMM representation to study phase transitions and identify latent\"detour\"states that slow down convergence.",
            "score": 0.4080869138194221,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85888671875
        },
        {
            "corpus_id": "254854466",
            "title": "A Natural Bias for Language Generation Models",
            "text": "While one could argue that learning or encoding the superficial statistical tendencies of language is not necessarily a good thing, 11 empirical results suggest that it may in fact be an important part of learning dynamics (see App. B.3, for example). Indeed, Takahashi and Tanaka-Ishii (2019) find evidence that more powerful language models may even have a natural bias for learning them. Here we ask if -when initialising model parameters -we can explicitly endow our models with prior knowledge about one such statistical ten-11 Models can feasibly overfit to surface cues in the dataset, which would impede their ability to generalise at inference time (Warstadt et al., 2020;Wei et al., 2021). dency: the unigram distribution.\n\nWhile the result that this initialisation strategy improves training efficiency is perhaps not surprising, the relatively consistent improvement in performance (5/6 models in Fig. 2) that it offers is indeed more surprising. We offer two possible explanations for this improvement. The first possible explanation is that this initialisation changes model learning dynamics at the beginning of training in a beneficial way. More concretely, early learning dynamics have been shown to have an outsized impact on final model performance (Achille et al., 2019); hence changing the initialisation procedure may alter the learning dynamics in a beneficial way.\n\nA second possible explanation is that this method disentangles frequency in the modelling of contextual probabilities. Note that we can rewrite eq. (4) as the product of the joint distribution of y <t and y and the marginal distribution over unigrams: p(y | y <t ) = p(y <t , y) \u00b7 p(y). Thus, if p b explicitly models the unigram distribution, then our model does not need to capture this component of the conditional distribution over tokens in its other parameters, which frees up the model capacity to focus on more complex phenomena within natural language. Its success thus motivates exploring the use of higher-order statistical models in place of p b , such as a bigram or trigram model, in an attempt to further disentangle surfaces statistics from more nuanced components of natural language in a modular fashion (Li et al., 2022).",
            "score": 0.40800424709690797,
            "section_title": "Discussion",
            "char_start_offset": 14369,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 260,
                    "end": 293,
                    "matchedPaperCorpusId": "195345672"
                },
                {
                    "start": 681,
                    "end": 698,
                    "matchedPaperCorpusId": "237513713"
                },
                {
                    "start": 1268,
                    "end": 1290,
                    "matchedPaperCorpusId": "108298098"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85205078125
        },
        {
            "corpus_id": "264288956",
            "title": "Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers",
            "text": "We provide the GenBench eval card (Table 9) to help centralize the generalization evaluation in state-of-the-art language models. We conduct many experiments in our paper, but we can safely divide them into 3 main categories by their datasets (CFQ, COGS, and SMCS). The motivation beyond our experiments is the same. We hypothesized that we could improve the compositional generalization performance of neural models by harnessing their training dynamics to construct a smaller training set. Therefore, our motivation is practical as we aim to achieve better generalization and intrinsic as we examine and utilize models' training dynamics at the same time. All of our datasets are compositional gener-",
            "score": 0.407908581398027,
            "section_title": "A.8 GenBench Eval Card",
            "char_start_offset": 37655,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 702
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90869140625
        },
        {
            "corpus_id": "7464562",
            "title": "An Analysis of the Ability of Statistical Language Models to Capture the Structural Properties of Language",
            "text": "We investigate the characteristics and quan-ti\ufb01able predispositions of both n-gram and recurrent neural language models in the framework of language generation. In modern applications, neural models have been widely adopted, as they have empirically provided better results. However, there is a lack of deep analysis of the models and how they relate to real language and its structural properties. We attempt to perform such an investigation by analyzing corpora generated by sampling from the models. The results are compared to each other and to the results of the same analysis applied to the training corpus. We carried out these experiments on varieties of Kneser-Ney smoothed n-gram models and basic recurrent neural language models. Our results reveal a number of distinctive characteristics of each model, and offer insights into their behavior. Our general approach also provides a framework in which to perform further analysis of language models.",
            "score": 0.40751379911416824,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "269042752",
            "title": "On Training Data Influence of GPT Models",
            "text": "The advent of generative language models, particularly the GPT series (Radford et al., 2019;Brown et al., 2020;Zhang et al., 2022), has marked a paradigm shift in natural language processing (NLP) (Touvron et al., 2023;Jiang et al., 2023), code generation (Lozhkov et al., 2024;Chai et al., 2023), visual and language understanding (Achiam et al., 2023;Team et al., 2023). These models have redefined performance standards across an extensive range of tasks, igniting detailed investigations into the process of training dynamics and the intricate nature of learned representations. Despite these strides, the specific influence of individual * Work done during QL's internship at Baidu. training examples on the performance of GPT models remains a significantly underexplored area. This oversight presents a critical challenge in optimizing training processes, a challenge that grows in tandem with the increasing complexity and scale of these models. \n\nCurrent research has yet to focus comprehensively on the influence of training data on autoregressive language models. Prior studies, such as those utilizing the BERT (Park et al., 2023) or T5 architecture (Guu et al., 2023), have predominantly concentrated on natural language understanding tasks, leaving a considerable void in the exploration of generative language models. Furthermore, the majority of this research (Pruthi et al., 2020;Guu et al., 2023;K and S\u00f8gaard, 2021;Koh and Liang, 2017;Yeh et al., 2018) has focused on test loss as the primary metric of interest, neglecting other vital performance indicators. Metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) scores are crucial for a thorough evaluation of a model's capabilities, particularly in the context of generative language models where downstream task performance is paramount. Additionally, the challenge of generalizability-extending methodologies to accommodate unseen data-persists as a significant barrier (Guu et al., 2023). This is particularly critical for models expected to adapt to the dynamic and evolving trajectory of NLP tasks.",
            "score": 0.40734603123912055,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2087
                }
            ],
            "ref_mentions": [
                {
                    "start": 70,
                    "end": 92,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 92,
                    "end": 111,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 256,
                    "end": 278,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 278,
                    "end": 296,
                    "matchedPaperCorpusId": "254591305"
                },
                {
                    "start": 1375,
                    "end": 1396,
                    "matchedPaperCorpusId": "211204970"
                },
                {
                    "start": 1433,
                    "end": 1453,
                    "matchedPaperCorpusId": "13193974"
                },
                {
                    "start": 1453,
                    "end": 1470,
                    "matchedPaperCorpusId": "53741665"
                },
                {
                    "start": 1633,
                    "end": 1644,
                    "matchedPaperCorpusId": "964287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8525390625
        },
        {
            "corpus_id": "247594916",
            "title": "How does the pre-training objective affect what large language models learn about linguistic properties?",
            "text": "In this work, we compared the linguistic capabilities of LMs. Surprisingly, our results show that pretraining with linguistically motivated objectives obtain comparable performance to non-linguistically motivated objectives. This suggests that the data and the size of the model could be more influential than the objectives themselves in language model-ing. In future work, we plan to extend our experiments into other languages and probing tasks.  Table 5: Results of the Sentence Length (SentLen) probing task for each layer of the pre-trained models.  Table 6: Results of the Tree Depth (TreeDepth) probing task for each layer of the pre-trained models. 72.6 \u00b1 0.6 80.6 \u00b1 0.3 69.1 \u00b1 0.6 74.2 \u00b1 0.6 69.8 \u00b1 0.4 5 74.8 \u00b1 0.5 81.9 \u00b1 0.6 69.8 \u00b1 0.7 78.1 \u00b1 0.7 71.5 \u00b1 0.9 6 75.2 \u00b1 0.4 81.9 \u00b1 0.5 73.2 \u00b1 0.1 79.3 \u00b1 0.6 69.7 \u00b1 0.8 7 76.9 \u00b1 0.5 83.0 \u00b1 0.5 75.7 \u00b1 0.7 78.5 \u00b1 0.5 70.7 \u00b1 0.6 8 72.        58.1 \u00b1 0.6 56.9 \u00b1 1.3 53.7 \u00b1 0.7 50.0 \u00b1 0.2 50.0 \u00b1 0.2 9\n\n59.1 \u00b1 0.4 57.9 \u00b1 1.5 54.1 \u00b1 1.0 53.2 \u00b1 0.9 50.0 \u00b1 0.2 10 60.6 \u00b1 0.5 58.5 \u00b1 0.9 56.3 \u00b1 0.7 53.4 \u00b1 0.2 50.4 \u00b1 0.3 11 61.7 \u00b1 0.5 58.9 \u00b1 0.6 56.5 \u00b1 0.4 53.9 \u00b1 1.0 50.2 \u00b1 0. 54.6 \u00b1 1.1 53.9 \u00b1 0.7 51.5 \u00b1 1.5 51.3 \u00b1 0.2 50.0 \u00b1 0.1 7 56.1 \u00b1 0.6 55.2 \u00b1 0.6 53.2 \u00b1 0.2 52.0",
            "score": 0.4064586197702927,
            "section_title": "Conclusions",
            "char_start_offset": 11493,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7275390625
        },
        {
            "corpus_id": "274656373",
            "title": "Reasoning-Aware Query-Focused Summarization over Multi-Table Data",
            "text": "Large language models (LLMs [Touvron et al., 2023a,b]) have become a cornerstone in natural language processing (NLP) due to their exceptional capacity for generating coherent and contextually relevant text. Over the years, extensive research has been conducted to improve the architecture, training paradigms, and application scope of LLMs. These models leverage massive datasets and transformer-based architectures to capture nuanced linguistic patterns, enabling them to perform a wide array of NLP tasks with high accuracy [Zhou et al., 2023b[Zhou et al., , 2024a]]. \n\nRecent surveys provide a comprehensive overview of LLMs, highlighting advancements in pre-training, fine-tuning, and adaptation strategies. The evolution of LLM families such as GPT, LLaMA, and PaLM has brought forward innovations in model scalability, optimization techniques, and evaluation methods [Xiao et al., 2024, Zhao et al., 2023a]. Furthermore, specialized training methods, including reinforcement learning with human feedback (RLHF) and prompt tuning, have demonstrated the ability to align LLM outputs with user-specific goals effectively. \n\nFrom a theoretical standpoint, studies have explored the underlying mechanisms of autoregressive language models, drawing parallels with Markov chains and examining their convergence properties under varying configurations [Zekri et al., 2024]. Such theoretical insights provide a foundation for understanding the dynamic behavior of LLMs and guiding their practical applications. \n\nAdditionally, domain-specific applications of LLMs have gained traction [Bai et al., 2023, Zhou et al., 2024b,c]. For instance, LLMs have been successfully adapted for telecommunications, expanding their utility in generation, optimization, and prediction tasks [Zhou et al., 2024d, 2021, Zhou, 2022, Zhou and Long, 2023]. They have also been utilized to support the expansion of spoken language understanding systems to under-resourced languages, demonstrating their versatility in handling multilingual contexts [Hoscilowicz et al., 2024].",
            "score": 0.4064586197702927,
            "section_title": "Large Language Models",
            "char_start_offset": 3714,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1125
                },
                {
                    "start": 1128,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1508
                },
                {
                    "start": 1511,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 527,
                    "end": 546,
                    "matchedPaperCorpusId": "254877030"
                },
                {
                    "start": 546,
                    "end": 569,
                    "matchedPaperCorpusId": "254877030"
                },
                {
                    "start": 1792,
                    "end": 1798,
                    "matchedPaperCorpusId": "235780049"
                },
                {
                    "start": 1798,
                    "end": 1810,
                    "matchedPaperCorpusId": "235780049"
                },
                {
                    "start": 1810,
                    "end": 1832,
                    "matchedPaperCorpusId": "256358690"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88037109375
        },
        {
            "corpus_id": "247628243",
            "title": "Evaluating Distributional Distortion in Neural Language Modeling",
            "text": "This paper contributes to recent work investigating the properties of the distributions defined by LMs. Prior studies have focused on exploring (Takahashi & Tanaka-Ishii, 2019;2017) and develop-6 Formally, for the ith component of the length K pre-softmax logits x, this operation is given as: \n\ning frameworks (Meister & Cotterell, 2021) to better understand whether the large-scale statistical tendencies of natural language, such as Zipf's law (Zipf, 1949), are captured by LMs. We take a more fine-grained approach, proposing a methodology which draws off of instance-level evaluation schemes (Zhong et al., 2021) and the experimental control afforded by artificial corpora (White & Cotterell, 2021;Papadimitriou & Jurafsky, 2020). Indeed, closely related to our work is Kulikov et al. (2021)'s, in which artificial corpora produced by generative models were used to explore mode recovery in neural language modeling. Our analysis exploring the overestimation of ill-formed sequences extends previous findings on locally normalized conditional models assigning arbitrary probability mass to unlikely sequences (Andor et al., 2016;Goyal et al., 2019;Lafferty et al., 2001), neural LMs assigning high likelihood to sequences with repetitions (Welleck et al., 2020b), the consistency of decoding algorithms (Welleck et al., 2020a), and on machine translation models placing significant probability mass on the empty sequence (Stahlberg & Byrne, 2019). \n\nWe additionally contribute to the body work seeking to characterize and adapt neural model performance on rare or novel examples and classes (Horn & Perona, 2017;Bengio, 2015). In the context of language modeling, Lent et al. (2021) explored performance on under-resourced languages, whereas Oren et al. (2019) did so on under-represented domains in training corpora. Mc-Coy et al. (2021) introduced analyses to assess sequential and syntactic novelty in LMs. Focusing on the word frequency distribution, Dudy & Bedrick (2020) found that LMs under-perform when less frequent examples are encountered at test time.",
            "score": 0.4061442879120741,
            "section_title": "RELATED WORK",
            "char_start_offset": 22815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 293
                },
                {
                    "start": 296,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1452
                },
                {
                    "start": 1455,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2068
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 176,
                    "matchedPaperCorpusId": "195345672"
                },
                {
                    "start": 678,
                    "end": 703,
                    "matchedPaperCorpusId": "235293810"
                },
                {
                    "start": 1153,
                    "end": 1175,
                    "matchedPaperCorpusId": "219683473"
                },
                {
                    "start": 1244,
                    "end": 1267,
                    "matchedPaperCorpusId": "199551982"
                },
                {
                    "start": 1426,
                    "end": 1451,
                    "matchedPaperCorpusId": "201646223"
                },
                {
                    "start": 1747,
                    "end": 1765,
                    "matchedPaperCorpusId": "202541009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4609375
        },
        {
            "corpus_id": "275820529",
            "title": "Spurious Forgetting in Continual Learning of Language Models",
            "text": "Recent advancements in large language models (LLMs) reveal a perplexing phenomenon in continual learning: despite extensive training, models experience significant performance declines, raising questions about task alignment and underlying knowledge retention. This study first explores the concept of\"spurious forgetting\", proposing that such performance drops often reflect a decline in task alignment rather than true knowledge loss. Through controlled experiments with a synthesized dataset, we investigate the dynamics of model performance during the initial training phases of new tasks, discovering that early optimization steps can disrupt previously established task alignments. Our theoretical analysis connects these shifts to orthogonal updates in model weights, providing a robust framework for understanding this behavior. Ultimately, we introduce a Freezing strategy that fix the bottom layers of the model, leading to substantial improvements in four continual learning scenarios. Our findings underscore the critical distinction between task alignment and knowledge retention, paving the way for more effective strategies in continual learning.",
            "score": 0.4061051028166671,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9755859375
        },
        {
            "corpus_id": "243865354",
            "title": "How Do Neural Sequence Models Generalize? Local and Global Cues for Out-of-Distribution Prediction",
            "text": "Generalization in count-based LMs Before the widespread use of neural approaches in NLP, statistical approaches to language modeling were typically defined by explicit independence assumptions governing their generalization in contexts never observed in the training data. For example, ngram models (Miller and Selfridge, 1950;Shannon, 1951) ignore global sentence structure in favor of a local context of at most n words. By contrast, latent-variable language models based on finitestate machines (Kuhn et al., 1994) (or more expressive automata; Chelba andJelinek 1998, Pauls andKlein 2012) explicitly incorporate information from the long-range context by conditioning next-word prediction on abstract global states constrained by global sentence structure. In models of both kinds, behavior in contexts unlike any seen at training time is be explicitly specified via backoff and smoothing schemes aimed at providing robust estimates of the frequency of rare events (Good, 1953;Katz, 1987;Kneser and Ney, 1995). Like past work on backoff and smoothing, our work in this paper attempts to provide a general mechanism for both prediction and control in more complex, black-box neural LMs.",
            "score": 0.40584869857780265,
            "section_title": "Background",
            "char_start_offset": 4467,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1189
                }
            ],
            "ref_mentions": [
                {
                    "start": 299,
                    "end": 327,
                    "matchedPaperCorpusId": "34743079"
                },
                {
                    "start": 498,
                    "end": 517,
                    "matchedPaperCorpusId": "8960527"
                },
                {
                    "start": 548,
                    "end": 558,
                    "matchedPaperCorpusId": "2877845"
                },
                {
                    "start": 581,
                    "end": 592,
                    "matchedPaperCorpusId": "17366870"
                },
                {
                    "start": 969,
                    "end": 981,
                    "matchedPaperCorpusId": "11945361"
                },
                {
                    "start": 981,
                    "end": 992,
                    "matchedPaperCorpusId": "6555412"
                },
                {
                    "start": 992,
                    "end": 1013,
                    "matchedPaperCorpusId": "9685476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81005859375
        },
        {
            "corpus_id": "259859071",
            "title": "Curating Datasets for Better Performance with Example Training Dynamics",
            "text": "Breakthroughs in NLP are often the result of scaling up existing models in size and depth, and perhaps even more importantly-data (Hoffmann et al., 2022). To improve data quality, it has become common practice to train models on data that has been cleaned to some extent using simple heuristics (e.g. by removing non-language tokens), but not otherwise optimized for better performance. While some data-filtering approaches have been suggested to improve Out-Of-Distribution (OOD) generalization (Le Bras et al., 2020, Swayamdipta et al., 2020), they usually result in a decrease of In-Distribution (ID) performance. \n\nWe propose a method for curating datasets for better performance in both ID and OOD testing, enhancing data quality rather quantity. Our method is orthogonal to model architecture or size, and as such can be used alongside any LM to further improve results over specific tasks. \n\nTo implement our method we use the concept of Example Training Dynamics (ETD; Swayamdipta et al., 2020), which builds on the idea that the training process of models sheds light on the relative importance of specific examples within the datasets used. Specifically, Swayamdipta et al. (2020) have shown that over several epochs of training, a model may predict some examples in a dataset less consistently than others, and that those \"ambiguous\" examples are important for OOD generalization. \n\nWe propose a new method for computing ETD, as well as a new paradigm for using them in training. We show that by computing ETD over separate training processes (rather than over consecutive epochs of the same training process), and using ETD to weigh the importance of each example in the dataset, we can train a DeBERTa model (He et al., 2020) on the weighted versions of several NLI and multiple-choice datasets, improving average performance by 0.35% for ID testing and by 0.95% for OOD.",
            "score": 0.40503559467977596,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 616
                },
                {
                    "start": 619,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 896
                },
                {
                    "start": 899,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1391
                },
                {
                    "start": 1394,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1884
                }
            ],
            "ref_mentions": [
                {
                    "start": 977,
                    "end": 1002,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 1165,
                    "end": 1190,
                    "matchedPaperCorpusId": "221856637"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76904296875
        },
        {
            "corpus_id": "258967177",
            "title": "Likelihood-Based Diffusion Language Models",
            "text": "Despite a growing interest in diffusion-based language models, existing work has not shown that these models can attain nontrivial likelihoods on standard language modeling benchmarks. In this work, we take the first steps towards closing the likelihood gap between autoregressive and diffusion-based language models, with the goal of building and releasing a diffusion model which outperforms a small but widely-known autoregressive model. We pursue this goal through algorithmic improvements, scaling laws, and increased compute. On the algorithmic front, we introduce several methodological improvements for the maximum-likelihood training of diffusion language models. We then study scaling laws for our diffusion models and find compute-optimal training regimes which differ substantially from autoregressive models. Using our methods and scaling analysis, we train and release Plaid 1B, a large diffusion language model which outperforms GPT-2 124M in likelihood on benchmark datasets and generates fluent samples in unconditional and zero-shot control settings.",
            "score": 0.40408348103689573,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9033203125
        },
        {
            "corpus_id": "277349445",
            "title": "Data Mixture Optimization: A Multi-fidelity Multi-scale Bayesian Framework",
            "text": "We train predictors using multilayer perceptrons (MLPs) consisting of three hidden layers with 64 hidden units each, ReLU activations, and dropout with a rate of 0.1, totaling approximately 5,000 parameters. The predictors accept three types of inputs: (1) the model size (number of parameters), (2) the number of training steps, and (3) the proportions of each of the five dataset categories used during pretraining, namely (Wikipedia, StackExchange, Github, ArXiv, and Book ). \n\nFor each pretrained language model, the predictor outputs predictions for multiple metrics: the training loss, validation losses across seven categories (Wikipedia, StackExchange, Github, ArXiv, Book, and held-out datasets CommonCrawl and C4 ), as well as downstream task accuracies on three evaluation tasks: HellaSwag, PIQA, and Arc Easy [33,31,32]. Thus, each language model corresponds to a single row in our predictor's dataset, comprising 9 inputs as described above and outputs spanning these 11 metrics. We train the predictors to maximize the coefficient of determination \n\n, where y i is the true metric value for data point i, \u0177i is the predictor's estimate, and \u0233 is the mean of true values. Training of our predictor is conducted for 20 epochs using a batch size of 64, an Adam optimizer with a learning rate of 0.001 and weight decay of 0.01, and data normalization (standard scaling) applied to both inputs and outputs. We begin by investigating the extent to which smaller model runs can inform the dynamics of larger ones, as the literature on scaling laws [4] would suggest. Table 1 details these experiments, and Table 3 lists the results of the experiment. We note that, as expected, information garnered from training runs on smaller models seems to considerably increase the accuracy of our predictions on larger models, motivating our hope that a carefully crafted optimization algorithm can exploit the relationship.",
            "score": 0.40341576137015794,
            "section_title": "Predictor Training",
            "char_start_offset": 12069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 478
                },
                {
                    "start": 481,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1061
                },
                {
                    "start": 1064,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1921
                }
            ],
            "ref_mentions": [
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "159041722"
                },
                {
                    "start": 825,
                    "end": 828,
                    "matchedPaperCorpusId": "208290939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61767578125
        },
        {
            "corpus_id": "222140842",
            "title": "Pretrained Language Model Embryology: The Birth of ALBERT",
            "text": "The world of NLP has gone through some tremendous revolution since the proposal of contextualized word embeddings. Some big names are ELMo (Peters et al., 2018), GPT (Radford et al.), and BERT (Devlin et al., 2019), along with its variants (Sanh et al., 2019;Liu et al., 2019b;Lan et al., 2019). Performance boosts on miscellaneous downstream tasks have been reported by finetuning these totipotent pretrained language models. With a view to better grasping what has been learned by these contextualized word embedding models, probing is generally applied to the pretrained models and the models finetuned from them. Probing targets can range from linguistic knowledge, including semantic roles and syntactic structures (Liu et al., 2019a;Tenney et al., 2019Tenney et al., , 2018;;Hewitt and Manning, 2019), to world knowledge (Petroni et al., 2019). \n\nWhile the previous work focuses on what knowledge has been learned after pretraining of transformer-based language models, few delve into their dynamics during pretraining. What happened during the training process of a deep neural network model has been widely studied, including Gur-Ari et al. (2018), Frankle et al. (2019), Raghu et al. (2017), Morcos et al. (2018). Some previous works also study the dynamics of the training process of an LSTM language model (Saphra andLopez, 2018, 2019), but the training dynamics of a large scale pretrained language models are not well-studied. In this work, we probe ALBERT (Lan et al., 2019) during its pretraining phase every N parameter update steps and study what it has learned and what it can achieve so far. We perform a series of experiments, detailed in the following sections, to investigate the development of predicting and reconstructing tokens (Section 3), how linguistic and world knowledge evolve through time (Section 4, Section 6), and whether amassing those information serves as an assurance of good downstream task performances (Section 5). \n\nWe have the following findings based on AL-BERT:",
            "score": 0.4031092529676852,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 850
                },
                {
                    "start": 853,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1957
                },
                {
                    "start": 1960,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 160,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 193,
                    "end": 214,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 277,
                    "end": 294,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 720,
                    "end": 739,
                    "matchedPaperCorpusId": "84841767"
                },
                {
                    "start": 781,
                    "end": 806,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 827,
                    "end": 849,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 1157,
                    "end": 1178,
                    "matchedPaperCorpusId": "211259030"
                },
                {
                    "start": 1180,
                    "end": 1199,
                    "matchedPaperCorpusId": "23890457"
                },
                {
                    "start": 1201,
                    "end": 1221,
                    "matchedPaperCorpusId": "49271358"
                },
                {
                    "start": 1317,
                    "end": 1328,
                    "matchedPaperCorpusId": "53633715"
                },
                {
                    "start": 1470,
                    "end": 1488,
                    "matchedPaperCorpusId": "202888986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96337890625
        },
        {
            "corpus_id": "260125327",
            "title": "In-Context Learning Learns Label Relationships but Is Not Conventional Learning",
            "text": "to labels, L = (l 1 , . . . , l N ), e.g. the indices of the tokens immediately following the string 'Label:' in the above example.\n\nTokens to Predictions. In the following, we use capital letters to denote random variables and lower-case letters for their realizations. Here, we describe the behavior of decoder-only language models (Liu et al., 2018;Radford et al., 2018), a popular architecture choice for LLMs. Given the observed sequence of input tokens (X 1 = x 1 , . . . , X M = x M ), a single forward pass through the language model gives an estimate of the joint probability,\n\nWe highglight that Eq. (1) gives the joint probability at the observed outcomes: we obtain M 'onestep ahead' predictions, each conditioned only on observed outcomes and not on model predictions. Equation (1) is a common objective in LLM training, where 'the joint probability the model assigns to the observations' is sometimes referred to as teacher forcing (Williams & Zipser, 1989).\n\nAt test time, LLMs are usually iteratively conditioned on their own predictions, generating novel outputs via multiple forward passes, i.e. one first samplesx M \u223c p(X M | . . . ), and thenx M +1 \u223c p(X M +1 | . . . , X M =x M ), and so on. We here use '. . . ' to stand in for any additional tokens also conditioned on, e.g. (x 1 , . . . , x M \u22121 ). One usually ignores all other terms of the joint herethe predictions for (X 1 , . . . , X M \u22121 ) that are generated in each forward pass-as only the last term p(X M | . . . ) is needed to sample the next token, i.e. the label in standard few-shot ICL applications.\n\nSingle-Forward Pass ICL Training Dynamics. We now explain our approach for efficient evaluation of ICL training dynamics. Given input tokens (X 1 , . . . , X M ) for the few-shot ICL setup described above, we first select those terms from Eq. (1) that correspond",
            "score": 0.40284766308172903,
            "section_title": "B EVALUATION APPROACH FOR CHEAP IN-CONTEXT LEARNING DYNAMICS",
            "char_start_offset": 35410,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 946,
                    "end": 970,
                    "matchedPaperCorpusId": "14711886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9453125
        },
        {
            "corpus_id": "270562800",
            "title": "Stealth edits to large language models",
            "text": "We focus on the Mamba family of selective state space language models [13]. This presentation elides most of the details of how this family of models is structured, but exposes just the components necessary for our exposition. Such a model (with latent space dimension d) is a map N : P \u2192 T formed of a sequence of state space blocks and blocks for input and output. For an index j, let N j : P \u2192 R d represent the action of the model up to the input to Mamba block j, and let M j : R d \u2192 T be the map from the output of this block to the logit confidence scores produced by the model. The next token is generated from these logits by a sample function, for example, using a temperature-based strategy. For a prompt p \u2208 P, the model N can be expressed as \n\nwhere s : R d \u00d7 P \u2192 R d denotes the state space component, W 1 is a matrix of size m \u00d7 d, W 2 is a matrix of size d \u00d7 m, and \u03c1 : R d \u2192 R d is as in (7).",
            "score": 0.4024982055363106,
            "section_title": "B.1.2 Selective state space language models",
            "char_start_offset": 40704,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 754
                },
                {
                    "start": 757,
                    "end": 909
                }
            ],
            "ref_mentions": [
                {
                    "start": 905,
                    "end": 908,
                    "matchedPaperCorpusId": "248227301"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.505859375
        },
        {
            "corpus_id": "270869536",
            "title": "MoE-CT: A Novel Approach For Large Language Models Training With Resistance To Catastrophic Forgetting",
            "text": "Large language models (LLMs) (Ouyang et al., 2022;Brown et al., 2020b) have achieved remarkable progress in recent years, particularly in areas such as language generation (Radford et al., 2019;Brown et al., 2020a), machine translation (Vaswani et al., 2017;Wu et al., 2019), text summarization (See et al., 2017;Rush et al., 2015), and language understanding (Devlin et al., 2019a;Peters et  2018).However, the majority of these models have focused on resource-rich languages such as English, leaving substantial potential for performance improvements in low-resource languages.To alleviate above mentioned issues, researchers have aimed to Parameter-Efficient Fine-Tuning (PEFT) methods.Specifically, continual training (CT) has been proposed, which have proven to be effective in enhancing the performance of low-resource languages by training on specific language data.Besides, other researchers have resorted to method of Low-Rank Adaptation (LoRA) (Hu et al., 2021).By introducing a low-rank structure to reduce the model parameters for efficient updating.Thereby, it achieves a successful success between maintaining model performance and saving computation abd storage.Despite these developments, research on how to extend the multilingual capabilities of large models are still limited.We conducted an in-depth analysis of existing capability extension technologies.\n\n1. Firstly, it is difficult to obtain original training data.In conventional training methods, the original training data accounts for a significant proportion, so that the lack of data poses a significant challenge for the training in use.In addition, due to the different distributions between the training data of different stages, the absence of original training data will significantly exacerbate the catastrophic forgetting problems.\n\n2. Secondly, adding large amounts of raw data will limit the improvement of multilingual capabilities.As illustrated in Figure 1, with the volume of original language data greatly alleviated the issue of catastrophic forgetting, significantly improving the generation capability in original data distribution.When the volume of original language data is five times greater than oth new one, the catastrophic forgetting issue is almost disappeared.However, it limits the model's final performance on multilingual tasks.",
            "score": 0.40228496366652733,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 399
                },
                {
                    "start": 399,
                    "end": 579
                },
                {
                    "start": 579,
                    "end": 689
                },
                {
                    "start": 689,
                    "end": 873
                },
                {
                    "start": 873,
                    "end": 972
                },
                {
                    "start": 972,
                    "end": 1062
                },
                {
                    "start": 1062,
                    "end": 1177
                },
                {
                    "start": 1177,
                    "end": 1295
                },
                {
                    "start": 1295,
                    "end": 1375
                },
                {
                    "start": 1377,
                    "end": 1438
                },
                {
                    "start": 1438,
                    "end": 1617
                },
                {
                    "start": 1617,
                    "end": 1817
                },
                {
                    "start": 1819,
                    "end": 1921
                },
                {
                    "start": 1921,
                    "end": 2128
                },
                {
                    "start": 2128,
                    "end": 2266
                },
                {
                    "start": 2266,
                    "end": 2337
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 214,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 258,
                    "end": 273,
                    "matchedPaperCorpusId": "225040574"
                },
                {
                    "start": 295,
                    "end": 313,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 313,
                    "end": 331,
                    "matchedPaperCorpusId": "1918428"
                },
                {
                    "start": 360,
                    "end": 382,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 382,
                    "end": 391,
                    "matchedPaperCorpusId": "3626819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.822265625
        },
        {
            "corpus_id": "270560808",
            "title": "ptt5-v2: A Closer Look at Continued Pretraining of T5 Models for the Portuguese Language",
            "text": "Transformer-based pretrained language models have established themselves as the core paradigm in the field of Natural Language Processing (NLP). Starting with the advent of BERT [15], which popularized the \"pretrain, then fine-tune\" approach and the use of the transformer architecture ifself, these models acquire a general-purpose language representation by unsupervised pretraining on extensive corpora of unlabeled text. The dynamics of the pretraining process had been studied in depth by many works, like Raffel et al. [38], that introduced T5, and scaled up models to billions of parameters and set new SOTAs across many tasks. The trend towards increasing model sizes and datasets to improve performance motivated studies like Kaplan et al. [26] on scaling laws and Hoffmann et al. [21], who demonstrated the importance of training data size relative to model size for compute-optimal training regimes; more recently work from Gadre et al. [17] specifically examined the influence of extended pretraining on downstream task performance. \n\nDespite the extensive study of pretraining dynamics, the focus has predominantly been on English, leaving non-English languages less explored. Continued pretraining presents a strategic approach to adapting these models to additional languages and domains using significantly less data and computational resources than training from scratch. This method involves further pretraining on language-specific corpora, which has been shown to substantially enhance model performance on downstream tasks in the target language [36,9,29,8,47,14]. However, there is a lack of detailed investigations into how different settings during the continued pretraining phase influence on downstream tasks performance, with most studies merely aiming for benchmark-leading results without a thorough examination of the underlying factors. \n\nIn this work, we study the continued pretraining of T5 models for the Portuguese language, analyzing the impact of various settings on downstream task performance. Rather than solely focusing on achieving state-of-the-art results, our study also investigates how factors like model size, optimization schedules, and pretraining data quality affect the performance. We continue the pretraning of Google's T5 with up to 3 billion parameters on Portuguese texts.",
            "score": 0.4018316387876706,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 1044
                },
                {
                    "start": 1047,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1867
                },
                {
                    "start": 1870,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2234
                },
                {
                    "start": 2235,
                    "end": 2329
                }
            ],
            "ref_mentions": [
                {
                    "start": 1567,
                    "end": 1571,
                    "matchedPaperCorpusId": "258179504"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96875
        },
        {
            "corpus_id": "231637215",
            "title": "Evaluating models of robust word recognition with serial reproduction",
            "text": "Ideally, the language models here could be evaluated using the probability each assigns to a large set of utterances sampled from participants after strong evidence that sampling chains have fully converged to participants' expectations-both that inter-quartile variance approaches zero and that the probability of utterances is no longer increasing. Because utterance probabilities continue to increase, these chains are likely not directly sampling from participants' expectations by the end of the experiment.\n\nInstead, we use the fact that serial reproduction yields sentences that are increasingly representative of people's linguistic expectations in the task, even if they have not yet converged. As such, the pattern of changes towards the target distribution can be used to evaluate models: more representative models should have a greater magnitude increase in probability (manifested as a decrease in average per-word surprisal) over the course of the experiment. A model reflecting the true expectations of participants should exhibit the largest possible decrease in surprisal over the course of serial transmission. Though we lack direct access to people's expectations, the magnitude of the increase in utterance probability for each language model-as an approximation of those human expectations-is sufficient to rank it with respect to others.\n\nTo eliminate the effect of shorter utterances on probability, which would trivially be assigned higher probabilities under all language models, we divide each utterance's negative log probability by the number of words in that utterance. The resulting measure, \"average per-word surprisal\" has an intuitive interpretation as the average surprisal, q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Models vary in their surprisal estimates for reasons outside of the scope of the current analysis, especially the choice of smoothing scheme. For example, consider two unigram models that withhold different amounts of probability mass for word tokens not encountered during training, e.g., .05 and .01. If these two models were used to produce probability estimates for a set of sentences comprised of exclusively known tokens, the first model would assign a lower probability estimate compared to the second model, even though the probability estimates would be perfectly correlated.",
            "score": 0.4013308000012581,
            "section_title": "Evaluating Language Models",
            "char_start_offset": 36676,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89111328125
        },
        {
            "corpus_id": "264832734",
            "title": "The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback",
            "text": "The original training object used in popular language model architectures, autoregressive next-token prediction also suffers from an objective mismatch problem, as almost all LLM evaluation techniques evaluate the entire output rather than individual tokens. While this is true, the development signal that the next-token prediction loss provides is more orthogonal to the goals of RLHF. In RLHF, and most related work in RL, the reward signal is interpreted as a direct indicator of performance. This assumption creates a much more unintentionally nuanced research setup, warranting the specific study of its impacts. \n\nIn MBRL, the learning of a dynamics model is also often done via one-step transitions, with recent work studying autoregressive models (Janner, Li, & Levine, 2021;Lambert, Wilcox, Zhang, Pister, & Calandra, 2021), where the compounding error of multiple one-step predictions is well known as a deeply related issue to objective mismatch (Lambert, Pister, & Calandra, 2022). In the case where mismatch becomes a fundamental problem of LLMs, similar solutions could be investigated.",
            "score": 0.40119931630353445,
            "section_title": "Mismatch of next-token prediction",
            "char_start_offset": 18657,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1101
                }
            ],
            "ref_mentions": [
                {
                    "start": 756,
                    "end": 784,
                    "matchedPaperCorpusId": "235313679"
                },
                {
                    "start": 784,
                    "end": 833,
                    "matchedPaperCorpusId": "229211063"
                },
                {
                    "start": 958,
                    "end": 993,
                    "matchedPaperCorpusId": "264405813"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.408447265625
        },
        {
            "corpus_id": "273351273",
            "title": "How Transformers Get Rich: Approximation and Dynamics Analysis",
            "text": "Proof idea. We highlight that our fine-grained analysis of entire learning process is guided by two key observations: 1) the dynamics of the two heads can be decoupled; 2) there exist a distinct transition point in the dynamics of each head, as shown in Figure 2 (right). These insights lead us to divide the analysis of each head into two phases: a monotonic phase and a convergence phase. Particularly, for the convergence phase, we introduce a novel Lyapunov function that leverages the unique dynamical structure of self-attention. This Lyapunov function may be of independent interest and offers potential for studying broader issues in self-attention dynamics. \n\nRemark 4.6. We conduct additional experiments to validate our theoretical insights into the training dynamics and learning transition across a wider range of scenarios. These includes using data distribution (Figure 6) and optimization algorithms (Figure 7) in high-dimensional settings, as well as training real-world transformers on natural language datasets (Figure 5). (Jiang and Li, 2023), practical computer programs (Giannou et al., 2023), long but sparse memories (Wang et al., 2024), induction head (Sanford et al., 2024a;b;Rajaraman et al., 2024), and memorization and reasoning (Chen and Zou, 2024). Besides, many studies suggest that transformers achieve in-context learning by approximating gradient-based iterations across various layers (Garg et al., 2022;Aky\u00fcrek et al., 2022;Von Oswald et al., 2023;Mahankali et al., 2023;Bai et al., 2023;Shen et al., 2023). Besides, several studies explored the limitation of transformer's expressivity, particularly in modeling formal languages or simulating circuits (Hahn, 2020;Weiss et al., 2021;Bhattamishra et al., 2020;Merrill et al., 2022;Merrill and Sabharwal, 2023). Among all these works, the most closely related to ours are Rajaraman et al. (2024), which examined a generalized induction head similar to our Eq. ( 6).",
            "score": 0.40088524652406515,
            "section_title": "Training Stage II: Transition from 4-gram to Induction Head",
            "char_start_offset": 27453,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 666
                },
                {
                    "start": 669,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 1951
                }
            ],
            "ref_mentions": [
                {
                    "start": 1421,
                    "end": 1440,
                    "matchedPaperCorpusId": "251253368"
                },
                {
                    "start": 1461,
                    "end": 1485,
                    "matchedPaperCorpusId": "254685643"
                },
                {
                    "start": 1690,
                    "end": 1702,
                    "matchedPaperCorpusId": "189928186"
                },
                {
                    "start": 1702,
                    "end": 1721,
                    "matchedPaperCorpusId": "235421630"
                },
                {
                    "start": 1747,
                    "end": 1768,
                    "matchedPaperCorpusId": "248085924"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6259765625
        },
        {
            "corpus_id": "235248063",
            "title": "Language Models Use Monotonicity to Assess NPI Licensing",
            "text": "an LM bases its NPI predictions on simple co-occurrence heuristics, or if it can extrapolate from a general notion of monotonicity to cases of NPIs in environments in which they have never been encountered during training ( \u00a75.5). We again train new LMs on modified corpora, this time removing NPIs only in one specific environment, and repeat the NPI acceptability and DC ranking experiments. The results of this setup demonstrate that LMs indeed use a general notion of monotonicity to predict NPI licensing.\n\nContributions With this work, we contribute to the ongoing study of the linguistic abilities of language models in several ways:\n\n\u2022 With a series of experiments we demonstrate that LMs are able to acquire a general notion of monotonicity that is employed for NPI licensing.\n\n\u2022 We present two novel experimental setups: filtered corpus training and DC ranking, that can be used to assess the impact of specific information during training and compare the information used by DCs with the information used with the model, respectively.\n\n\u2022 By using experimental results from psychosemantics to motivate hypotheses for LM behavior, we find that our models reflect behavior similar to human language processing.\n\nIn the remainder of this paper, we will first provide some linguistic background that helps to situate and motivate our experiments and results ( \u00a72).\n\nWe then discuss related work on NPI processing in LMs in \u00a73. In \u00a74, we discuss our methods and experimental setup. \u00a75.1 through \u00a75.5 explain and present the results. We conclude in \u00a76 with a general discussion and pointers to future work.",
            "score": 0.4005165789493639,
            "section_title": "Introduction",
            "char_start_offset": 3723,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8076171875
        },
        {
            "corpus_id": "32648772",
            "title": "Achieving Compositional Language in a Population of Iterated Learners",
            "text": "The authors of the ILM themselves point out that complex population dynamics were traded off for computational power in the original model. Population structure was not taken into account, and every agent only ever learns from one other agent4 . Given that the iterated learning model aims to shed light on the relationship between the properties of individuals and the population-level behaviour that they exhibit, and that much of the work done in this area thus far has been concerned primarily with vertical cultural transmission, it is of significant interest to explore the behaviour of this ILM within a population of agents. \n\nHere we introduce a model in which, at each iteration, a population of N language users comprises N M mature individuals and N I immature individuals, where N M +N I = N . During each iteration of the model, every immature language user is assigned a number of trainers from whom they Figure 2: Replication of ILM behaviour. The solid line represents language expressivity, X, the proportion of the meaning space that is covered by the learner's language. The dotted line represents the language instability, 256 \u2212 S, the difference between the language mappings of the mature and immature language users. Here, N M =1, N I =1, B=50, E=100, M T =1, I T =0. infer the structure of their language through a series of training episodes. This set of trainers may involve both a number, M T , of randomly chosen mature trainers, and also, possibly, a number, I T , of randomly chosen immature trainers (see figure 4). The presence of immature trainers represents scenarios in which language learners are not kept isolated from one another, but may influence each others' language learning. An immature individual's total number of training episodes, T , is evenly split between their trainers with each trainer being involved in B M T +I T episodes per training epoch5 . As in the original ILM, it remains the case that",
            "score": 0.40035609913046144,
            "section_title": "Population-based Iterated Learning",
            "char_start_offset": 14298,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 1949
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7177734375
        },
        {
            "corpus_id": "267523002",
            "title": "Neural Networks Learn Statistics of Increasing Complexity",
            "text": "Xu et al. (2019b) proposed the Frequency Principle, the tendency of neural networks to first fit low-frequency Fourier components of a given target function, before moving on to fit higher frequency components, and empirically demonstrated this tendency on real image classification problems and synthetic datasets. Subsequent works further explored how neural network learning dynamics relate to the representation of training data in the frequency domain (Rahaman et al., 2019;Xu et al., 2019a;Basri et al., 2020;Xu & Zhou, 2021). Our work is similar in that we also aim to connect neural network learning order to simple mathematical properties of the training data, though we use distributional statistics, rather than frequency. Choshen et al. (2022) empirically studied learning dynamics of neural language models by tracking which grammatical patterns different networks learn to model across their training trajectories, and comparing network behavior across training to alternative language modeling approaches, such as n-gram models. They found that neural language models initially match the behaviors of unigram and bigram models early in training, then diverge as training progresses. These results are inline with our own findings on learning order in neural language models, and are consistent with a DSB-driven perspective on neural network learning dynamics.",
            "score": 0.40012241243952495,
            "section_title": "A.2. Learning order",
            "char_start_offset": 31470,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1375
                }
            ],
            "ref_mentions": [
                {
                    "start": 457,
                    "end": 479,
                    "matchedPaperCorpusId": "53012119"
                },
                {
                    "start": 496,
                    "end": 515,
                    "matchedPaperCorpusId": "212644664"
                },
                {
                    "start": 515,
                    "end": 531,
                    "matchedPaperCorpusId": "220831156"
                },
                {
                    "start": 734,
                    "end": 755,
                    "matchedPaperCorpusId": "237491997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95703125
        },
        {
            "corpus_id": "278602121",
            "title": "Block-Biased Mamba for Long-Range Sequence Processing",
            "text": "Mamba extends earlier state space models (SSMs) by introducing input-dependent dynamics, and has demonstrated strong empirical performance across a range of domains, including language modeling, computer vision, and foundation models. However, a surprising weakness remains: despite being built on architectures designed for long-range dependencies, Mamba performs poorly on long-range sequential tasks. Understanding and addressing this gap is important for improving Mamba's universality and versatility. In this work, we analyze Mamba's limitations through three perspectives: expressiveness, inductive bias, and training stability. Our theoretical results show how Mamba falls short in each of these aspects compared to earlier SSMs such as S4D. To address these issues, we propose $\\text{B}_2\\text{S}_6$, a simple extension of Mamba's S6 unit that combines block-wise selective dynamics with a channel-specific bias. We prove that these changes equip the model with a better-suited inductive bias and improve its expressiveness and stability. Empirically, $\\text{B}_2\\text{S}_6$ outperforms S4 and S4D on Long-Range Arena (LRA) tasks while maintaining Mamba's performance on language modeling benchmarks.",
            "score": 0.4000425554339121,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.587890625
        },
        {
            "corpus_id": "263829596",
            "title": "FTFT: efficient and robust Fine-Tuning by transFerring Training dynamics",
            "text": "Despite the massive success of fine-tuning Pre-trained Language Models (PLMs), they remain susceptible to out-of-distribution input. Dataset cartography is a simple yet effective dual-model approach that improves the robustness of fine-tuned PLMs. It involves fine-tuning a model on the original training set (i.e. reference model), selecting a subset of important training instances based on the training dynamics, and fine-tuning again only on these selected examples (i.e. main model). However, this approach requires fine-tuning the same model twice, which is computationally expensive for large PLMs. In this paper, we show that (1) training dynamics are highly transferable across model sizes and pre-training methods, and that (2) fine-tuning main models using these selected training instances achieves higher training efficiency than empirical risk minimization (ERM). Building on these observations, we propose a novel fine-tuning approach: Fine-Tuning by transFerring Training dynamics (FTFT). Compared with dataset cartography, FTFT uses more efficient reference models and aggressive early stopping. FTFT achieves robustness improvements over ERM while lowering the training cost by up to $\\sim 50\\%$.",
            "score": 0.3999109210459819,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9599609375
        },
        {
            "corpus_id": "278534923",
            "title": "Learning Dynamics in Continual Pre-Training for Large Language Models",
            "text": "In recent years, large language models (LLMs) exhibit versatile abilities and have garnered significant academic and industrial attention (Dubey et al., 2024;OpenAI, 2023). the LLMs' abilities in specific downstream domains (e.g. coding, finance, math, etc.) while mitigating the substantial costs associated with re-training (Chen et al., 2023a;C \u00b8agatay Y\u0131ld\u0131z et al., 2024;Ibrahim et al., 2024). \n\nCPT primarily focuses on the evolution of performance across general and downstream domains. It is widely observed and believed that improvements on downstream datasets may come at the expense of performance on general domain tasks, or the well-known catastrophic forgetting (French, 1999;Gupta et al., 2023). \n\nRecently, some scaling laws for transfer are proposed in continual pre-training scenarios. For example, Hernandez et al. (2021b) and Barnett (2024) discover a law to describe how effective data transferred scales with fine-tuning dataset size and model size. Que et al. (2024) and Gu et al. (2024) propose a law to find the optimal replay ratio to balance general and downstream performances. However, very few studies have attempted to quantitatively describe the learning dynamics, particularly many performance variations on general and downstream domains throughout the CPT process. We have two primary research questions (RQs): (1) Can we find such an accurate law containing as many variables that affect the final CPT performance as possible? (2) Can we trace the performance of LLMs throughout CPT process, rather than only final performance like previous works? Studying the first RQ helps researchers clarify the various factors that affect CPT performance and then optimize hyper-parameters through prediction; Studying the second RQ helps community understand the learning dynamics of LLMs at each step of the CPT process, providing deeper insights and theoretical guidance for subsequent CPT research. \n\nFollowing previous works (Gupta et al., 2023;Ibrahim et al., 2024;Que et al., 2024), we trace performance changes using validation loss of the corresponding domains. We find that the CPT loss curve is a transfer curve and can be described by decoupling the effects of distribution shift and LR annealing.",
            "score": 0.39949118640737047,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 398
                },
                {
                    "start": 401,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 710
                },
                {
                    "start": 713,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1927
                },
                {
                    "start": 1930,
                    "end": 2095
                },
                {
                    "start": 2096,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 326,
                    "end": 346,
                    "matchedPaperCorpusId": "258833488"
                },
                {
                    "start": 376,
                    "end": 397,
                    "matchedPaperCorpusId": "268379604"
                },
                {
                    "start": 676,
                    "end": 690,
                    "matchedPaperCorpusId": "2691726"
                },
                {
                    "start": 994,
                    "end": 1010,
                    "matchedPaperCorpusId": "271404362"
                },
                {
                    "start": 1975,
                    "end": 1996,
                    "matchedPaperCorpusId": "268379604"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.990234375
        },
        {
            "corpus_id": "273375526",
            "title": "Semantics-Adaptive Activation Intervention for LLMs via Dynamic Steering Vectors",
            "text": "This work presents Semantics-Adaptive Dynamic Intervention (SADI), a method designed to enhance the performance of large language models (LLMs) by dynamically adjusting their activations without additional training. We have conducted extensive experiments across diverse tasks to evaluate SADI's effectiveness, but we recognize that biases in the underlying models and datasets may still affect outcomes. We encourage practitioners to use SADI responsibly, with careful consideration of fairness, accountability, and transparency. No human subjects were involved in this research, and all experiments were conducted using publicly available models and datasets, adhering to their respective licenses and use policies.",
            "score": 0.3989999418535446,
            "section_title": "ETHICS STATEMENT",
            "char_start_offset": 23340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 717
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47509765625
        },
        {
            "corpus_id": "239886013",
            "title": "Mind the Gap: Assessing Temporal Generalization in Neural Language Models",
            "text": "Our world is open-ended, non-stationary, and constantly evolving; thus what we talk about and how we talk about it change over time. This inherent dynamic nature of language contrasts with the current static language modelling paradigm, which trains and evaluates models on utterances from overlapping time periods. Despite impressive recent progress, we demonstrate that Transformer-XL language models perform worse in the realistic setup of predicting future utterances from beyond their training period, and that model performance becomes increasingly worse with time. We find that, while increasing model size alone -- a key driver behind recent progress -- does not solve this problem, having models that continually update their knowledge with new information can indeed mitigate this performance degradation over time. Hence, given the compilation of ever-larger language modelling datasets, combined with the growing list of language-model-based NLP applications that require up-to-date factual knowledge about the world, we argue that now is the right time to rethink the static way in which we currently train and evaluate our language models, and develop adaptive language models that can remain up-to-date with respect to our ever-changing and non-stationary world. We publicly release our dynamic, streaming language modelling benchmarks for WMT and arXiv to facilitate language model evaluation that takes temporal dynamics into account.",
            "score": 0.39878400895288807,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "258888114",
            "title": "Training Data Extraction From Pre-trained Language Models: A Survey",
            "text": "Language models represent a probability distribution over the sequences of tokens. Based on the pre-training method, language modeling can be categorized into two types (Yang et al., 2023): autoregressive language modeling, which predicts words sequentially from left to right (Bengio et al., 2000;Mikolov et al., 2010), and masked language modeling, which hides some parts of a sentence and fills in the gaps (Devlin et al., 2019). The former is sometimes called causal language modeling (Tirumala et al., 2022). This study is focused on autoregressive language models with transformer (Vaswani et al., 2017), following many recent studies on training data extraction. Note that some studies have focused on masked language models such as BERT (Lehman et al., 2021;Mireshghallah et al., 2022a;He et al., 2022) and T5 (Carlini et al., 2023b). Most studies address pre-training rather than fine-tuning (Mireshghallah et al., 2022b). \n\nAutoregressive language models take a series of tokens as input and output a probability distribution for the next token. We show a schema of training and generation by following Carlini et al. (2021). \n\nTraining. The following statistical model was assumed for distribution: \n\nwhere x 1 , x 2 , . . . , x n is a sequence of tokens from a vocabulary using the chain rule of probability: \n\n) denote the likelihood of token x i when evaluating neural network f with parameters \u03b8. Language models are trained to optimize the probability of the data in a training set. Formally, training involves minimizing the loss function as follows: \n\nfor each data in the training set. This setting can be qualitatively regarded as memorizing the flow of sentences in each training data. \n\nGenerating. New tokens can be generated by iterating the following process: \n\nThis decoding process continues until conditions are satisfied. The simplest is greedy decoding, selecting the most probable tokens one by one.",
            "score": 0.39857759446045876,
            "section_title": "Language Models",
            "char_start_offset": 2578,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1135
                },
                {
                    "start": 1138,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1567
                },
                {
                    "start": 1570,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1706
                },
                {
                    "start": 1709,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1784
                },
                {
                    "start": 1787,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1930
                }
            ],
            "ref_mentions": [
                {
                    "start": 277,
                    "end": 298,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 298,
                    "end": 319,
                    "matchedPaperCorpusId": "17048224"
                },
                {
                    "start": 410,
                    "end": 431,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 489,
                    "end": 512,
                    "matchedPaperCorpusId": "248986465"
                },
                {
                    "start": 587,
                    "end": 609,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 745,
                    "end": 766,
                    "matchedPaperCorpusId": "233289659"
                },
                {
                    "start": 766,
                    "end": 794,
                    "matchedPaperCorpusId": "247315260"
                },
                {
                    "start": 794,
                    "end": 810,
                    "matchedPaperCorpusId": "253080602"
                },
                {
                    "start": 818,
                    "end": 841,
                    "matchedPaperCorpusId": "246863735"
                },
                {
                    "start": 901,
                    "end": 930,
                    "matchedPaperCorpusId": "256461422"
                },
                {
                    "start": 1113,
                    "end": 1134,
                    "matchedPaperCorpusId": "229156229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9306640625
        },
        {
            "corpus_id": "267782481",
            "title": "Unveiling Linguistic Regions in Large Language Models",
            "text": "Pre-training is a crucial process by which LLMs acquire linguistic competence and gain general knowledge about the real world. Formally, given a large corpus D, the training objective for autoregressive language modeling is to find the optimal \u03b8 that minimizes the following loss L: \n\nwhere x = {x 1 , ..., x n } denotes an input token sequence and \u03b8 denotes parameters of the model.",
            "score": 0.39857759446045876,
            "section_title": "Model Pre-training",
            "char_start_offset": 6134,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 383
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83837890625
        },
        {
            "corpus_id": "268063857",
            "title": "Diffusion Language Models Are Versatile Protein Learners",
            "text": "The dominant paradigm of language models is autoregressive language models, which breaks down the mutual distribution over the tokens of a sequence into conditional probabilities via the chain rule p(x \n\nand generates tokens by ancestral sampling from left to right (Bengio et al., 2000;Sutskever et al., 2014;Vaswani et al., 2017). Recently, researchers propose the nonautoregressive language models as an alternative (Gu et al., 2018). These models do not need to obey the left to right generation order (Qian et al., 2022;Huang et al., 2023) and demonstrate competitive or superior performance compared to their autoregressive counterpart across a wide range of domains including languages (Qian et al., 2021b;Huang et al., 2023;Qian et al., 2022;Huang et al., 2023;Zheng et al., 2023a), speeches (Kim et al., 2021), proteins (Zheng et al., 2023b), and molecules (Hoogeboom et al., 2022). Among the numerous non-autoregressive language models, diffusion language models (Li et al., 2022;Gong et al., 2022;Zheng et al., 2023a) have emerged as a solid and promising framework. Pretraining language models on a massive scale of unlabeled data markedly improves their downstream task performance (Mikolov et al., 2013;Peters et al., 2018;Radford et al., 2018;Devlin et al., 2019). As data volume and model sizes scale up, the training loss of language models predictably declines (Kaplan et al., 2020;Hoffmann et al., 2022;Muennighoff et al., 2023), and enhancing downstream task performance even without specific tuning (Radford et al., 2019). GPT3 (Brown et al., 2020) is a significant point in the journey, taking model sizes to 175B parameters, proposing in-context learning to bolster language models' competence in solving certain tasks with only a handful of demonstrations.",
            "score": 0.39857759446045876,
            "section_title": "E Related Work E.1 Language Models",
            "char_start_offset": 53727,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 204,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1780
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 287,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 287,
                    "end": 310,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 310,
                    "end": 331,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 419,
                    "end": 436,
                    "matchedPaperCorpusId": "3480671"
                },
                {
                    "start": 525,
                    "end": 544,
                    "matchedPaperCorpusId": "258297880"
                },
                {
                    "start": 713,
                    "end": 732,
                    "matchedPaperCorpusId": "258297880"
                },
                {
                    "start": 750,
                    "end": 769,
                    "matchedPaperCorpusId": "258297880"
                },
                {
                    "start": 800,
                    "end": 818,
                    "matchedPaperCorpusId": "235417304"
                },
                {
                    "start": 829,
                    "end": 850,
                    "matchedPaperCorpusId": "256597956"
                },
                {
                    "start": 866,
                    "end": 890,
                    "matchedPaperCorpusId": "247839510"
                },
                {
                    "start": 973,
                    "end": 990,
                    "matchedPaperCorpusId": "249192356"
                },
                {
                    "start": 1195,
                    "end": 1217,
                    "matchedPaperCorpusId": "5959482"
                },
                {
                    "start": 1217,
                    "end": 1237,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1258,
                    "end": 1278,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 1520,
                    "end": 1542,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90966796875
        },
        {
            "corpus_id": "248177957",
            "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model",
            "text": "In this paper we introduce GPT-NeoX-20B, a 20 billion parameter open-source autoregressive language model. We make the models weights freely and openly available to the public through a permissive license, motivated by the belief that open access to LLMs is critical to advancing research in a wide range of areas-particularly in AI safety, mechanistic interpretability, and the study of how LLM capabilities scale. Many of the most interesting capabilities of LLMs only emerge above a certain number of parameters, and they have many properties that simply cannot be studied in smaller models. Although safety is often cited as a justification for keeping model weights private, we believe this is insufficient to prevent misuse, and is largely a limitation on the ability to probe and study LLMs for researchers not based at the small number of organizations that have access to state of the art language models. In addition, we make partially trained checkpoints avaliable at evenly spaced 1000 step intervals throughout the whole of training. We hope that by making a wide range of checkpoints throughout training freely available, we will facilitate research on the training dynamics of LLMs, as well as the aforementioned areas of AI safety and interpretability. \n\nIn studying GPT-NeoX-20B, we find several noteworthy phenomena at odds with the established literature. We train on a dataset that contains duplicated data for more than one epoch but see no evidence of performance loss. While (Hendrycks et al., 2021a) claims that few-shot prompting doesn't improve performance on their task, we find that this is actually a phenomenon unique to GPT-3 and doesn't apply to either GPT-NeoX-20B or FairSeq models. Finally, we find that GPT-NeoX-20B is a powerful few-shot learner, recieving a much larger performance boost from few-shot examples than comparable sized GPT-3 and FairSeq models. As we see the same with GPT-J-6B (Wang and Komatsuzaki, 2021), we hypothesize that this may be due to the shared choice of training data.",
            "score": 0.39783924277466626,
            "section_title": "Introduction",
            "char_start_offset": 1751,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1268
                },
                {
                    "start": 1271,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2034
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7822265625
        },
        {
            "corpus_id": "263829596",
            "title": "FTFT: efficient and robust Fine-Tuning by transFerring Training dynamics",
            "text": "Despite the success of few-shot and zero-shot learning (Brown et al., 2020), state-of-the-art performance in Natural Language Processing (NLP) is still largely achieved by fine-tuning large Pretrained Language Models (PLMs) (Mosbach et al., 2023). Scaling laws (Kaplan et al., 2020;Hoffmann et al., 2022) suggest that better downstream performance is achieved with larger PLMs. However, fine-tuning large PLMs is also more expensive, in terms of both computational resources and carbon emission (Strubell et al., 2019;Wu et al., 2022). \n\nwhere DMs are constructed based on computationally efficient reference models to fine-tune more capable -and often larger -main models. Our motivation is two-fold: 1) efficient reference models ease the computational burden of constructing DMs, and 2) less capable reference models might be better at identifying ambiguous or hard training instances, because they are less likely to memorize training data (Tirumala et al., 2022;Carlini et al., 2023). Our results show that, in most cases, training dynamics are highly transferable across different model sizes ( \u00a74.1) and pretraining methods ( \u00a74.2). We further show that the condition for successful transfers is a reasonably strong reference model, which we also make precise in \u00a74.3. \n\nSecond, we observe that fine-tuning with selected instances achieves consistently higher training efficiency than conventional fine-tuning ( \u00a76). \n\nThird, building on these findings, we propose Fine-Tuning by transFerring Training dynamics (FTFT, \u00a76): an efficient fine-tuning approach that leads to improved OOD performance. Compared to dataset cartography, FTFT uses efficient reference models and early stopping. Experiments on two tasks, NLI and HSD, show that FTFT achieves better performance on OOD input than conventional Empirical Risk Minimization (ERM), while lowering the training cost by up to \u223c 50%.",
            "score": 0.39782521491719786,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 535
                },
                {
                    "start": 538,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1275
                },
                {
                    "start": 1278,
                    "end": 1423
                },
                {
                    "start": 1426,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1890
                }
            ],
            "ref_mentions": [
                {
                    "start": 224,
                    "end": 246,
                    "matchedPaperCorpusId": "258947047"
                },
                {
                    "start": 495,
                    "end": 518,
                    "matchedPaperCorpusId": "174802812"
                },
                {
                    "start": 518,
                    "end": 534,
                    "matchedPaperCorpusId": "240354766"
                },
                {
                    "start": 944,
                    "end": 967,
                    "matchedPaperCorpusId": "248986465"
                },
                {
                    "start": 967,
                    "end": 988,
                    "matchedPaperCorpusId": "246863735"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95458984375
        },
        {
            "corpus_id": "233308532",
            "title": "How Children Process Reduced Forms: A Computational Cognitive Modeling Approach to Pronoun Processing in Discourse",
            "text": "into one, so that tasks that have been performed frequently take fewer cognitive steps, and thus less time (Taatgen & Anderson, 2002). Moreover, production compilation can create new rules that include chunk information and therefore removes the need for time-consuming and potentially erroneous memory retrievals.\n\nLanguage experience is provided to the model in the form of a training phase preceding the test phase. In the training phase, the model is trained on the processing of overt and null pronouns without a discourse (in a ratio of 30% overt pronouns and 70% null pronouns, reflecting the ratio of use of null and overt pronouns in Italian, see Lorusso, Caprin, & Guasti, 2005;Serratrice, 2005). This provides the model with experience in resolving pronouns without training it on specific discourses. During training, the model is instructed to interpret bare null and overt pronouns, through which it practices pronoun processing. This way, the model increases the efficiency of the production rules involved and the activation of the linguistic constraints that are relevant for pronoun resolution. The more training the model receives, the faster it will become. Importantly, all of this training consists of unsupervised learning, in the sense that the model receives no feedback on the optimal interpretation of a presented pronoun. The model learns to associate a form with a particular meaning solely based on the activation of the linguistic constraints and the successful completion of perspective taking (see Section 3.4.2). A plot of the model's development of the interpretation of null and overt pronouns with more training is presented in Fig. A1 in the Appendix. This plot shows that the number of overt pronouns interpreted as a topic continuation slowly decreases in the model with more training.",
            "score": 0.39767239564471973,
            "section_title": "Sentence processing",
            "char_start_offset": 32827,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 133,
                    "matchedPaperCorpusId": "1327840"
                },
                {
                    "start": 656,
                    "end": 688,
                    "matchedPaperCorpusId": "142488541"
                },
                {
                    "start": 688,
                    "end": 705,
                    "matchedPaperCorpusId": "145074432"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.391845703125
        },
        {
            "corpus_id": "261277016",
            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
            "text": "Second, we only consider language models with 124M parameters trained on 5.1B tokens. Previous work has demonstrated that learning curves differ across model sizes (Xia et al., 2023); larger models are able to \"learn\" some examples (usually late in pre-training) for which smaller models reach nonoptimal local minima or even diverge. Larger models also exhibit less forgetting of pre-training examples (Tirumala et al., 2022), although it remains unclear whether similar mechanisms are responsible for evaluation example forgetting (i.e. surprisal increases for seen vs. unseen examples). Further research is necessary to determine the effects of model size on learning speed, variability, and forgetting; with a larger compute budget, the methods presented in our work can easily be applied to larger models. Nonetheless, previous work has documented similar behaviors for different model sizes when they achieve similar perplexities (Choshen et al., 2022;Xia et al., 2023), suggesting that pretraining dynamics in smaller models may be similar to the early dynamics of larger models. A particularly exciting direction future work is to characterize the examples (e.g. based on types of reasoning, world knowledge, or commonsense) that fluctuate at different points during pre-training across model sizes.",
            "score": 0.39742057044778967,
            "section_title": "Discussion",
            "char_start_offset": 33481,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1307
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 182,
                    "matchedPaperCorpusId": "254877112"
                },
                {
                    "start": 403,
                    "end": 426,
                    "matchedPaperCorpusId": "248986465"
                },
                {
                    "start": 958,
                    "end": 975,
                    "matchedPaperCorpusId": "254877112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9873046875
        },
        {
            "corpus_id": "256631050",
            "title": "Data Cartography for Low-Resource Neural Machine Translation",
            "text": "We describe the main methods used to analyze the role of different samples in low-resource MT training through training dynamics. \n\n2.1 Applying Data Maps to MT Swayamdipta et al. (2020) introduced Data Maps, a model-based tool to characterize and diagnose datasets by placing training samples on a twodimensional map based on (1) the variability (\u03c3 i ) of model scores for each sample throughout training on the x axis, and (2) the model confidence (\u03bc i ) for each sample on the y axis. Their approach was developed for classification tasks such as Natural Language Inference. We use it here for sequenceto-sequence models by replacing the probability of predicting the gold class for a given input with the probability of the gold output sequence given the input sequence: \n\nand \n\nOn classification tasks, training on various subsets of the data revealed that samples in three of the map regions play a distinct role: \n\n1. samples with high confidence and low variability (upper left corner) are easy to learn, as the model consistently predicts their class correctly with high confidence, and their presence in the training data encourages convergence without over fitting. \n\n2. samples with low confidence and low variability (lower left corner) are hard to learn, possibly because they are noisy and mislabeled. \n\n3. samples with high variance (right) are ambiguous, and are found to contributing most to improving the models ability to generalize to out-of-domain data. \n\nWe divide the MT Data Maps into these three regions by using the midpoint between the max and min values of confidence and variability scores. Figure 1 previews Data Maps for our two lowresource MT systems (Swahili-English on the left, Turkish-English on the right). We use BLEU, an automatic translation quality metric, as an analog to the accuracy of the converged model's translation output compared against the reference. We will discuss the specific settings used to generate it and the findings in Sections 3 and 4 respectively.",
            "score": 0.39728281747337113,
            "section_title": "Methods",
            "char_start_offset": 2819,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 132,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 780
                },
                {
                    "start": 783,
                    "end": 919
                },
                {
                    "start": 922,
                    "end": 1176
                },
                {
                    "start": 1179,
                    "end": 1316
                },
                {
                    "start": 1319,
                    "end": 1475
                },
                {
                    "start": 1478,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2012
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84765625
        },
        {
            "corpus_id": "267061159",
            "title": "Critical Data Size of Language Models from a Grokking Perspective",
            "text": "We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data. Our results deepen the understanding of language model training, offering a novel perspective on the role of data in the learning mechanism of language models.",
            "score": 0.39680604860107155,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99560546875
        },
        {
            "corpus_id": "277501711",
            "title": "Enlightenment Period Improving DNN Performance",
            "text": "Remarkably, it is during this period that the model predominantly acquires its functional capabilities. Our experimental results show that strategically reducing noise during specific and brief epochs of the enlightenment period results in consistent performance improvements across various architectures. \n\nThe duration of the enlightenment period varies with different model architectures and datasets. \n\nAlthough the stochastic nature of early-stage training dynamics makes precise parametric characterization challenging, it may be possible in future research to establish quantitative relationships between the duration of enlightenment periods and factors such as model configurations and dataset characteristics. \n\nDue to computational constraints, we were unable to conduct experiments on large-scale tasks. However, our empirical results indicate that the performance gains from disabling Mixup during the enlightenment period become increasingly pronounced as model architectures and datasets scale. This suggests that the enlightenment period strategy may yield substantial improvements in large language models (LLMs) and large multimodal models. Furthermore, our theory remains valid for models pretrained and fine-tuned, indicating promising prospects for leveraging the enlightenment period to enhance performance in domains such as large model fine-tuning and continual learning.",
            "score": 0.39596241967631085,
            "section_title": "Experiments",
            "char_start_offset": 11628,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1395
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75927734375
        },
        {
            "corpus_id": "266844308",
            "title": "Stochastic Thermodynamics of Learning Parametric Probabilistic Models",
            "text": "We want to emphasize that the dynamics of subsystem X is not a mere conjecture or an arbitrary component in this study; rather, it's an integral part of training a generative PPM. This dynamics is inherent in the optimizer action, necessitating a fresh set of model-generated samples to compute the loss function or its gradients after each parameter update. For instance, in the context of EBM, a Langevin Monte Carlo (LMC) sampler can be employed to generate new samples from the model [31]. The computational cost of producing a fresh set of model-generated samples introduces a time delay in the parameter dynamics. For instance, when using an LMC sampler, the number of Monte Carlo steps dictates this lag time. Conversely, in the case of a language model, since the computation of the loss function relies on inferring subsequent tokens, the inference latency signifgies the time delay. We denote the lag time parameters with \u03c4 . Here, the model subsystem evolves on the timescale \u03b4t, while the parameters subsystem evolves on the timescale \u03b1 = \u03c4 \u03b4t. In the thermodynamic context, this parameter represents the relaxation time of the subsystem X, under fix microscopic state of subsystem \u0398. Conceptually, parameter \u03c4 acts as a complexity metric, quantifying the computational resources required for each parameter optimization step. Moreover, the dynamics of the joint (X, \u0398) exhibit a bipartite property. This implies that simultaneous transitions in the states of X and \u0398 are not allowed, given that the observation of a new set of model-generated samples occurs only after a parameter update. \n\nThe lagged bipartite dynamics described above can be represented using two time resolutions: \u03b4t and \u03b1. In the finer time resolution of \u03b4t, the Markov chain within the time interval [t i , t i+1 ] is as follows: \n\nWe can also analyze this dynamics at a coarser time resolution of \u03b1. Within the interval [t 0 , t n ], the Markov Chain appears as:",
            "score": 0.3956800388272884,
            "section_title": "Lagged bipartite dynamics",
            "char_start_offset": 16039,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1601
                },
                {
                    "start": 1604,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1814
                },
                {
                    "start": 1817,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 1948
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83984375
        },
        {
            "corpus_id": "276409016",
            "title": "Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning",
            "text": "Catastrophic forgetting (CF) poses a significant challenge in machine learning, where a model forgets previously learned information upon learning new tasks. Despite the advanced capabilities of Large Language Models (LLMs), they continue to face challenges with CF during continual learning. The majority of existing research focuses on analyzing forgetting patterns through a singular training sequence, thereby overlooking the intricate effects that diverse tasks have on model behavior. Our study explores CF across various settings, discovering that model forgetting is influenced by both the specific training tasks and the models themselves. To this end, we interpret forgetting by examining the function vector (FV), a compact representation of functions in LLMs, offering a model-dependent indicator for the occurrence of CF. Through theoretical and empirical analyses, we demonstrated that CF in LLMs primarily stems from biases in function activation rather than the overwriting of task processing functions. Leveraging these insights, we propose a novel function vector guided training methodology, incorporating a regularization technique to stabilize the FV and mitigate forgetting. Empirical tests on four benchmarks confirm the effectiveness of our proposed training method, substantiating our theoretical framework concerning CF and model function dynamics. We plan to make our code publicly accessible in the near future.",
            "score": 0.3950719257314939,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94970703125
        },
        {
            "corpus_id": "269042847",
            "title": "Why do small language models underperform? Studying Language Model Saturation via the Softmax Bottleneck",
            "text": "The representation degeneration problem is a common phenomenon that affects selfsupervised learning methods used for textual data (Gao et al., 2019;Lai et al., 2023), among other modalities (Jing et al., 2022;Godey et al., 2024).Many observations on the intermediate representations of Language Models (LMs) have shed light on their low angular variability (or anisotropy) (Zhou et al., 2021;Rajaee & Pilehvar, 2022) or on outlier dimensions that emerged during training (Puccetti et al., 2022).However, these observations were mostly made on relatively small-scale models of dimensions comparable to BERT (Devlin et al., 2019) or models from the GPT-2 family (Radford et al., 2019).\n\nThese models are usually composed of a neural network f \u03b8 that takes sequences of tokens (y <i ) \u2208 [1, V] i\u22121 as inputs and produces a relatively low-dimensional contextual representation in R d , where d is the hidden dimension of the model.They then rely on a language modeling head that produces logits for contextual token probabilities.A common choice for the language modeling head is a linear layer with parameter W \u2208 R V\u00d7d , where V is the number of possible tokens.The resulting next-token probability distribution is then given by: p(y i ) = \u03c3(W f \u03b8 (y <i ))\n\nwhere \u03c3 is the softmax function.\n\nIn the language modeling field, the current trend consists in scaling up the generative pretraining approach introduced with GPT-2, which implies training neural models made of several billions of parameters on gigantic web-mined text corpora (Brown et al., 2020;Touvron et al., 2023;Almazrouei et al., 2023;Jiang et al., 2023).However, training and serving such highly parameterized models raises energy and hardware-related problematics, which motivates for looking into achieving similar performance levels with smaller models (Sardana & Frankle, 2023).",
            "score": 0.3946961894953656,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 229,
                    "end": 495
                },
                {
                    "start": 495,
                    "end": 683
                },
                {
                    "start": 685,
                    "end": 927
                },
                {
                    "start": 927,
                    "end": 1026
                },
                {
                    "start": 1026,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1253
                },
                {
                    "start": 1255,
                    "end": 1287
                },
                {
                    "start": 1289,
                    "end": 1617
                },
                {
                    "start": 1617,
                    "end": 1845
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 148,
                    "matchedPaperCorpusId": "59317065"
                },
                {
                    "start": 148,
                    "end": 165,
                    "matchedPaperCorpusId": "258832989"
                },
                {
                    "start": 190,
                    "end": 209,
                    "matchedPaperCorpusId": "239016966"
                },
                {
                    "start": 209,
                    "end": 228,
                    "matchedPaperCorpusId": "267069312"
                },
                {
                    "start": 392,
                    "end": 416,
                    "matchedPaperCorpusId": "238583197"
                },
                {
                    "start": 471,
                    "end": 494,
                    "matchedPaperCorpusId": "248986509"
                },
                {
                    "start": 606,
                    "end": 627,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74560546875
        },
        {
            "corpus_id": "273350789",
            "title": "A Hitchhiker's Guide to Scaling Law Estimation",
            "text": "Substantial effort and cost are required to train even a single large language model (LLM). 1 There is thus an acute need for efficient decision-making aids that can evaluate the effectiveness of proposed changes to language models' architecture or training data without full-scale training runs. While there is a large body of work that motivates or evaluates these changes using small models (Warstadt et al., 2023;Hillier et al., 2024), synthetic tasks (Aky\u00fcrek et al., 2024;Wortsman et al., 2023) or theory (Jelassi et al., 2024), one of the most important tools for current practitioners is the estimation of scaling laws for LLMs (Ivgi et al., 2022;Dubey et al., 2024). \n\nA scaling law extrapolates the performance of a target model from the performance of a set of models with fewer parameters or smaller training sets. Typically, this extrapolation requires models to belong to the same model family, differing only in parameter count and training set size, but using the same architecture and training distribution. A high-quality scaling law accurately predicts the target model's test performance (Rosenfeld et al.;Kaplan et al., 2020;Hoffmann et al., 2022). \n\nMost past work describing and characterizing scaling laws has begun by exhaustively training models in a family across a full range of dataset sizes and parameter counts. One question that has received comparatively little attention is how, when training a new LLM, a practitioner with limited computational resources should choose which small-scale models to train in order to best estimate a target model's final performance. This paper offers a practical guide to when, and how, to use small models to efficiently obtain meaningful predictions about large models' behaviormaximizing prediction reliability while minimizing the budget for preliminary experimentation, which necessarily involves tradeoffs between the number of preliminary models trained, the size of the largest preliminary model, and size of the dataset used to train it. 1 Code, data and full numbers are found in our repository 1 arXiv:2410.11840v1 [cs.LG] 15 Oct 2024 arXiv Version",
            "score": 0.3946961894953656,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2126
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 417,
                    "matchedPaperCorpusId": "266182457"
                },
                {
                    "start": 478,
                    "end": 500,
                    "matchedPaperCorpusId": "262824542"
                },
                {
                    "start": 1108,
                    "end": 1126,
                    "matchedPaperCorpusId": "203592013"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8505859375
        },
        {
            "corpus_id": "29612547",
            "title": "Probabilistic graphical models for credibility analysis in evolving online communities",
            "text": "It is imperative to show that the resultant model is not only stable, but also improves the log-likelihood of the data. Although there are several measures to evaluate the quality of facet models, we report the following from [Wallach 2009]: j |\u03b2; \u03b1). A higher likelihood indicates a better model. Figure IV.9 contrasts the log-likelihood of the data from the continuous experience model and its discrete counterpart (refer to Section IV.3). We find that the continuous model is stable and has a smooth increase in the data log-likelihood per iteration. This can be attributed to how smoothly the language model evolves over time, preserving the Markov property of experience evolution. Empirically our model also shows a fast convergence, as indicated by the number of iterations.\n\nOn the other hand, the discrete model not only has a worse fit, but is also less smooth. It exhibits abrupt state transitions in the Hidden Markov Model, when the experience level changes (refer to Figure IV.7). This leads to abrupt changes in the language model, as it is coupled to experience evolution.",
            "score": 0.3946961894953656,
            "section_title": "Inference of our model is quite involved with different Markov Chain Monte Carlo methods.",
            "char_start_offset": 198088,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60986328125
        },
        {
            "corpus_id": "261822542",
            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
            "text": "Recent work in interpretability has begun to take advantage of the chronology of training in developing a better understanding of models. In some of the first papers explicitly interpreting the training process, Some studies find that specific capabilities are often learned in a particular order. In autoregressive language models, Xia et al. (2023) show that training examples tend to be learned in a consistent order independent of model size. In MLMs, Chiang et al. (2020) find that different part of speech tags are learned at different rates, while Warstadt et al. (2020b) find that linguistic inductive biases only emerge late in training. Our work likewise finds that extrinsic grammatical capabilities emerge at a consistent point in training. \n\nWhile our phase transition results mirror Murty et al. (2022)'s findings that the latent structure of autoregressive language models plateaus in its adherence to formal syntax, their work also finds the structure continues to become more tree-like long after syntacticity plateaus. Their results suggest that continued improvements in performance can still be attributed to interpretable hierarchical latent structure, which may be an inductive bias of some autoregressive model training regimes (Saphra & Lopez, 2020). \n\nAlthough Appendix I precludes the impact of thresholding effects (Schaeffer et al., 2023;Srivastava et al., 2022) on our results, the relationship between the structure onset and capabilities onset does reflect a dependency pattern similar to the checkmate-in-one task, which Srivastava et al. (2022) consider to be precipitated by smooth scaling in the ability to produce valid chess moves. Even in cases where there is no clear dependency between extrinsic capabilities, there may be internal structures like SAS that emerge smoothly, which can be interpreted as progress measures (Barak et al., 2022;Nanda et al., 2023;Merrill et al., 2023).",
            "score": 0.3946961894953656,
            "section_title": "C.2 INTERPRETING TRAINING",
            "char_start_offset": 35208,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1274
                },
                {
                    "start": 1277,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1921
                }
            ],
            "ref_mentions": [
                {
                    "start": 1251,
                    "end": 1273,
                    "matchedPaperCorpusId": "222272198"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.951171875
        },
        {
            "corpus_id": "275787780",
            "title": "FOCUS: First Order Concentrated Updating Scheme",
            "text": "The emergence of large language models (LLMs) represents a transformative advancement in artificial intelligence. These systems have demonstrated remarkable capabilities that extend far beyond their initial purpose of processing and generating human language. From engaging in nuanced conversations and crafting creative content (Brown et al., 2020;OpenAI, 2023;Qin et al., 2023) to solving complex mathematical problems (Lewkowycz et al., 2022;Taylor et al., 2022;Wolfram, 2023;Trinh et al., 2024) and assisting with software development (Chen et al., 2021;GitHub, 2022), LLMs continue to expand the boundaries of what artificial intelligence can achieve. \n\nTo further advance the capabilities of LLMs, improving the pre-training is a critical research direction (Kaplan et al., 1 Massachusetts Institute of Technology, Cambridge, MA 02139, USA. Correspondence to: Yizhou Liu <liuyz@mit.edu>. 2020; Hoffmann et al., 2022;Liu et al., 2023a). Better optimizers are desired to reduce time, cost, model size, etc. for reaching the same loss or to achieve lower losses given similar budgets. To this end, it is essential to understand the loss landscapes in LLM pre-training and design optimizers accordingly. \n\nTo gain insights into the training dynamics, we take the physicists' approach of simplifying the real world into a picture and a toy model (Figure 1a). A picture describes only the important features and ignores all other details. A toy model then specifies the picture with quantification. We can gain insights and generate testable hypotheses through a systematic study of the toy model. Then, we can apply our insights to engineer the real systems or we may fail and update our picture. \n\nIn this paper, we start in Section 2.1 to build a picture for pre-training landscapes, i.e., a narrowing valley (Figure 1b), based on previous results (Wei & Schwab, 2019;Liu et al., 2023a;Wen et al., 2024). Depending upon the topology of the loss landscape, different optimizers may respond differently.",
            "score": 0.3945714632392349,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 656
                },
                {
                    "start": 659,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1205
                },
                {
                    "start": 1208,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1697
                },
                {
                    "start": 1700,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2004
                }
            ],
            "ref_mentions": [
                {
                    "start": 329,
                    "end": 349,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 479,
                    "end": 498,
                    "matchedPaperCorpusId": "267032902"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "258937784",
            "title": "A Mathematical Interpretation of Autoregressive Generative Pre-Trained Transformer and Self-Supervised Learning",
            "text": "For instance, Radford et al. [4] revealed that language models, when subjected to training on a novel dataset consisting of millions of webpages, exhibit an inherent ability to learn specific tasks without the need for any explicit supervision. They identified the significance of the language model's capacity in the success of zero-shot task transfer, with performance improving in a log-linear fashion across tasks. Their most extensive model, GPT-2, showcased an unparalleled performance on a majority of tested language modeling datasets, underscoring the potential of language processing systems that learn to perform tasks via naturally occurring demonstrations. \n\nSimilarly, Brown et al. [3] discovered that scaling up language models substantially enhances task-agnostic, few-shot, and GPT-3. They trained an autoregressive language model with a large parameter set, which was significantly larger than any previous non-sparse language model, and tested its performance in the few-shot setting. Despite acknowledging certain areas where GPT-3's few-shot learning struggles, their findings attest to the remarkable performance of GPT-3 across a range of NLP tasks. \n\nKaplan et al. [29] investigated the empirical scaling laws pertinent to language model performance on the cross-entropy loss, observing that the loss scales as a power-law with model, dataset size, and the amount of computation used in the training process. They noted the minimal effects of other features such as network width, despite the fact that larger models are significantly more sample-efficient. \n\nHowever, notwithstanding the compelling empirical results of these studies, there exists a conspicuous gap in our understanding of the model's behavior from a mathematical perspective. This gap is exactly what our research aspires to fill. We aim to provide a functional analysis of GPT, contributing to a deeper comprehension of the model's properties and elucidating the mechanisms that drive its performance.",
            "score": 0.3945329403727722,
            "section_title": "Definition 3 (Positional Encoding",
            "char_start_offset": 13114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1581
                },
                {
                    "start": 1584,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1995
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 32,
                    "matchedPaperCorpusId": "49313245"
                },
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97216796875
        },
        {
            "corpus_id": "267657671",
            "title": "Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in Large Models",
            "text": "The advent of large models triggered a paradigm shift in artificial intelligence (AI), demonstrating unparalleled capabilities across many domains (e.g., natural language processing Devlin et al.;Raffel et al.;Brown et al.;Chowdhery Figure 1: Difference between traditional methods and our new method. We present a different perspective of how the model gets emergence from the training steps scale. et al.; He et al.). Recent studies observed that, compared to smaller scale neural networks (NNs), the large language models (LLMs) can exhibit advanced cognitive functions and a heightened level of comprehension (Bubeck et al., 2023). This phenomenon has been encapsulated in a scaling law of performance, indicating a linear correlation between the model's reducible loss and its size on a logarithmic scale (Kaplan et al., 2020;Hu et al., 2023a;b). Moreover, a sudden leap of model performance was observed when the model scale reaches a large level, being defined as the \"emergence\" phenomenon (Srivastava et al., 2022;Wei et al., 2022a). \n\nEmpirical studies on analyzing the LLMs' test performance identified power-law relationships but did not provide the intrinsic characteristics and rationale behind the emergence behavior. Previous research on the emergence in LLMs (Wei et al., 2022a;Fu et al., 2023) focused on model scale (e.g., number of parameters, model depth), but ignored the evolution of model performance during training. The iterative training process is essential for LLM development, demonstrating their evolving capabilities over time, as illustrated in Fig. 2. Large models often require more comprehensive training to achieve heightened abilities. Besides, traditional metrics (Paperno et al., 2016;Sakaguchi et al., 2021;Bisk et al., 2020) only evaluate the LLM's output performance. Moreover, LLMs exhibit variations in their internal structures and dynamic characteristics during the training process (Ding et al., 2023). Although previous work may observe the external manifestations of emergence, they overlook the complex dynamics and internal interactions during the model training process, which are key to understanding the emergence.",
            "score": 0.3945191965402742,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1042
                },
                {
                    "start": 1045,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2169
                }
            ],
            "ref_mentions": [
                {
                    "start": 1023,
                    "end": 1041,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1276,
                    "end": 1295,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1295,
                    "end": 1311,
                    "matchedPaperCorpusId": "256390607"
                },
                {
                    "start": 1725,
                    "end": 1748,
                    "matchedPaperCorpusId": "199370376"
                },
                {
                    "start": 1748,
                    "end": 1766,
                    "matchedPaperCorpusId": "208290939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9287109375
        },
        {
            "corpus_id": "270286038",
            "title": "Enhancing In-Context Learning Performance with just SVD-Based Weight Pruning: A Theoretical Perspective",
            "text": "The LLMs based on the Transformer architecture not only contain an Attention mechanism, but also include feed-forward (MLP) layers. It can be estimated that the feed-forward layers account for approximately two-thirds of the parameters in a Transformer model [45,44]. Thus, MLP layers are essential for LLMs and there has been considerable work hypothesizing and analyzing the model's feed-forward layers. For example, Geva et al. [17], Qiu et al. [33] show that feed-forward layers in Transformer based language models operate as key-value memories, Tian et al. [41] propose Joint MLP/Attention (JoMA) dynamics by integrating out the self-attention layers in Transformers, analyzes joint training of MLP and self-attention layers, and qualitatively explains dynamics of multi-layer Transformers. Based on the inspiration above, we take MLP into consideration and rewrite Eq. ( 3) as: \n\nare projection matrices and relu(\u2022) is the activation function. And we can further relax the activation function for ease of qualitative analysis: \n\nwhere W M LP \u2208 R (dout+din)\u00d7(dim1) , which can be seen as a dimensional adaptation. And then do the similar derivation that we do in Appendix A.1, we can get",
            "score": 0.3942430742034336,
            "section_title": "A.3 Feed-Forward and ICL Implicit GD",
            "char_start_offset": 30042,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 884
                },
                {
                    "start": 887,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1033
                },
                {
                    "start": 1036,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1193
                }
            ],
            "ref_mentions": [
                {
                    "start": 259,
                    "end": 263,
                    "matchedPaperCorpusId": "162183964"
                },
                {
                    "start": 263,
                    "end": 266,
                    "matchedPaperCorpusId": "184486755"
                },
                {
                    "start": 431,
                    "end": 435,
                    "matchedPaperCorpusId": "229923720"
                },
                {
                    "start": 448,
                    "end": 452,
                    "matchedPaperCorpusId": "267751363"
                },
                {
                    "start": 563,
                    "end": 567,
                    "matchedPaperCorpusId": "263334266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9228515625
        },
        {
            "corpus_id": "269293401",
            "title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data",
            "text": "Our goal is to understand the behaviors of various procedures for fine-tuning language models.As discussed above, typically these methods differ along the use of on-policy sampling (with additional differences pertaining to sample reuse) and the presence of a negative gradient.We build a setup to understand these differences empirically by answering the following questions:\n\nQuestion 1: When does on-policy sampling improve over offline fine-tuning, even though on-policy samples are annotated by a reward model, which itself is learned from offline data?Is sample reuse useful or harmful for on-policy methods?Question 2: When does an explicit negative gradient help the discovery of effective policies compared to maximum likelihood approaches such as distilling the Best-of-N policy?\n\nQuestion 3: Does on-policy sampling offer complementary benefits to negative gradient, resulting in better performance with effective contrastive approaches (e.g., DPO)?\n\nTo gain practically useful and actionable insights, we must answer these questions in the context of coverage and geometric relations between the training data, reference policy, and the reward function.These relations affect the shape of the optimally fine-tuned policy and dictate the dynamics of various objectives under consideration.We consider specific conditions and relations that we discuss next.",
            "score": 0.39399440397560787,
            "section_title": "Research Questions and Analysis Setup",
            "char_start_offset": 12796,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 94,
                    "end": 278
                },
                {
                    "start": 278,
                    "end": 376
                },
                {
                    "start": 378,
                    "end": 558
                },
                {
                    "start": 558,
                    "end": 614
                },
                {
                    "start": 614,
                    "end": 789
                },
                {
                    "start": 791,
                    "end": 960
                },
                {
                    "start": 962,
                    "end": 1165
                },
                {
                    "start": 1165,
                    "end": 1300
                },
                {
                    "start": 1300,
                    "end": 1367
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.869140625
        },
        {
            "corpus_id": "214693050",
            "title": "Information-Theoretic Probing with Minimum Description Length",
            "text": "Probing classifiers are the most common approach for associating neural network representations with linguistic properties (see Belinkov and Glass (2019) for a survey). Among the works highlighting limitations of standard probes (not mentioned earlier) is the work by Saphra and Lopez (2019), who show that diagnostic classifiers are not suitable for understanding learning dynamics. \n\nIn addition to task performance, learning curves have also been used before by Yogatama et al. (2019) to evaluate how quickly a model learns a new task, and by Talmor et al. (2019) to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. \n\nOther methods for analyzing NLP models include (i) inspecting the mechanisms a model uses to encode information, such as attention weights (Voita et al., 2018;Raganato and Tiedemann, 2018;Voita et al., 2019b;Clark et al., 2019;Kovaleva et al., 2019) or individual neurons (Karpathy et al., 2015;Pham et al., 2016;Bau et al., 2019), (ii) looking at model predictions using manually defined templates, either evaluating sensitivity to specific grammatical errors (Linzen et al., 2016;Gulordava et al., 2018;Tran et al., 2018;Marvin and Linzen, 2018) or understanding what language models know when applying them as knowledge bases or in question answering settings (Radford et al., 2019;Petroni et al., 2019;Poerner et al., 2019;Jiang et al., 2019). \n\nAn information-theoretic view on analysis of NLP models has been previously attempted in Voita et al. (2019a) when explaining how representations in the Transformer evolve between layers under different training objectives.",
            "score": 0.3933771494696048,
            "section_title": "Related work",
            "char_start_offset": 29152,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 383
                },
                {
                    "start": 386,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 1479
                },
                {
                    "start": 1482,
                    "end": 1705
                }
            ],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 153,
                    "matchedPaperCorpusId": "56657817"
                },
                {
                    "start": 268,
                    "end": 291,
                    "matchedPaperCorpusId": "267903883"
                },
                {
                    "start": 871,
                    "end": 891,
                    "matchedPaperCorpusId": "44062236"
                },
                {
                    "start": 891,
                    "end": 920,
                    "matchedPaperCorpusId": "53596423"
                },
                {
                    "start": 920,
                    "end": 940,
                    "matchedPaperCorpusId": "162183964"
                },
                {
                    "start": 940,
                    "end": 959,
                    "matchedPaperCorpusId": "184486746"
                },
                {
                    "start": 959,
                    "end": 981,
                    "matchedPaperCorpusId": "201645145"
                },
                {
                    "start": 1027,
                    "end": 1045,
                    "matchedPaperCorpusId": "16733173"
                },
                {
                    "start": 1045,
                    "end": 1062,
                    "matchedPaperCorpusId": "53215110"
                },
                {
                    "start": 1193,
                    "end": 1214,
                    "matchedPaperCorpusId": "14091946"
                },
                {
                    "start": 1214,
                    "end": 1237,
                    "matchedPaperCorpusId": "4460159"
                },
                {
                    "start": 1237,
                    "end": 1255,
                    "matchedPaperCorpusId": "3785155"
                },
                {
                    "start": 1255,
                    "end": 1279,
                    "matchedPaperCorpusId": "52113185"
                },
                {
                    "start": 1395,
                    "end": 1417,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1417,
                    "end": 1438,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 1571,
                    "end": 1591,
                    "matchedPaperCorpusId": "202541078"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.931640625
        },
        {
            "corpus_id": "247627806",
            "title": "minicons: Enabling Flexible Behavioral and Representational Analyses of Transformer Language Models",
            "text": "Accessing and using pre-trained language models (LMs)-now a mainstay in modern NLP researchhas become ever so convenient due to the advent of high-quality open source libraries such as transformers (Wolf et al., 2020), jiant (Pruksachatkun et al., 2020), etc. Parallel to the proliferation of newer pre-training methods and LM architectures is that of the development of analyses methods and diagnostic datasets which we collectively refer to here as the field of 'BlackboxNLP' (Alishahi et al., 2019). One of the foundational goals of this field is to develop and understanding of exactly what is learned by these complex LM architectures as a result of pre-training, as well as how pre-trained models operate. \n\nIn this paper, we provide an implementationlevel solution to conduct such analyses in the form of an open-source python library called minicons. minicons builds on top of the transformers library (Wolf et al., 2020), and provides a standard API to perform behavioral and representational analyses of transformer (Vaswani et al., 2017) language models. In particular, it contains methods that collectively support analyses at two levels, both of which are well established in current LM analysis and evaluation literature: (1) at the prediction level, by using LM's word prediction abilities to perform a range of analyses that target specific linguistic or reasoning ability without performing any supervised training/fine-tuning; and (2) at the representational level, where the goal is to characterize the information made available in the internal activations of a given model. Together, these methods make standard model analyses methods accessible to a wider audience. \n\nIn what follows, we first provide an overview of minicons and its core modules. We then apply it in two motivating case studies. First, we analyze the learning dynamics of the BERT architecture with respect to 67 different grammatical phenomena related to syntax, semantics, and morphology.",
            "score": 0.3926488529039753,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1980
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 217,
                    "matchedPaperCorpusId": "269498086"
                },
                {
                    "start": 225,
                    "end": 253,
                    "matchedPaperCorpusId": "211990050"
                },
                {
                    "start": 910,
                    "end": 929,
                    "matchedPaperCorpusId": "269498086"
                },
                {
                    "start": 1026,
                    "end": 1048,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94189453125
        },
        {
            "corpus_id": "272910946",
            "title": "Non-asymptotic Convergence of Training Transformers for Next-token Prediction",
            "text": "The transformer architecture (Vaswani et al., 2017) has revolutionized the field of machine learning, establishing itself as a foundation model for numerous applications, including natural language processing (NLP) (Devlin et al., 2018), computer vision (Dosovitskiy et al., 2020), and multi-modal signal processing (Tsai et al., 2019). In particular, transformers achieve tremendous empirical success in large language models (LLMs) such as GPT-3 (Brown et al., 2020). Despite the empirical success, limited theoretical understanding of transformers have caused a series of critical concerns about their robustness, interpretability, and bias issues (Bommasani et al., 2021;Belkin, 2024). \n\nTo overcome these issues, recent advances in transformer theory have investigated the convergence of training transformers under theoretically amenable setting such as linear regression (Mahankali et al., 2023;Zhang et al., 2023;Huang et al., 2023) and binary classification (Tarzanagh et al., 2023b,a;Vasudeva et al., 2024;Li et al., 2023). Nevertheless, one of the fundamental task in LLMs and other generative models is next-token prediction (NTP), which involves predicting the next word or token in a sequence, given the previous tokens. In NTP, a few recent theoretical studies have started to investigate the training dynamics of transformers (Tian et al., 2023a;Li et al., 2024). However, those works lack of fine-grained non-asymptotic convergence analysis of the training process, posing the following open questions for further investigation: \n\nHow fast does the training of a transformer converge in NTP? \n\nIn addition, a pre-trained transformer empirically exhibits non-trivial generalization ability. A follow-up question from a theoretical point of view is that Can we show the generalization capability of a trained transformer on unseen data? \n\nIn this paper, we take a first step towards addressing the aforementioned questions by studying the training dynamics of a single layer transformer consisting of a self-attention layer and a feed-forward layer for NTP. We summarize our contribution as follows.",
            "score": 0.39264548970667257,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1608
                },
                {
                    "start": 1611,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1851
                },
                {
                    "start": 1854,
                    "end": 2072
                },
                {
                    "start": 2073,
                    "end": 2114
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 51,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 316,
                    "end": 335,
                    "matchedPaperCorpusId": "173990158"
                },
                {
                    "start": 448,
                    "end": 468,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 675,
                    "end": 688,
                    "matchedPaperCorpusId": "267303083"
                },
                {
                    "start": 1342,
                    "end": 1362,
                    "matchedPaperCorpusId": "258947127"
                },
                {
                    "start": 1362,
                    "end": 1378,
                    "matchedPaperCorpusId": "268379753"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98681640625
        },
        {
            "corpus_id": "269449894",
            "title": "Temporal Scaling Law for Large Language Models",
            "text": "Results on both the in-distribution (IID) validation set and the out-ofdistribution (OOD) validation set show that our methodology marks a significant improvement over baseline approaches. \n\nOur temporal scaling law presents a novel approach to understanding the learning dynamics of LLMs at the token level. We discovered that despite the existence of loss imbalance on tokens in different positions, the rate of loss decrease for each token is uniform after an initial training period. This uniformity in learning suggests that LLMs inherently adopt an equal learning approach for all token positions. Such an observation validates the effectiveness of the default training paradigm, in which no weighting strategies on different token positions are applied. To verify this, we compare the performance of pre-trained LLMs involving different re-weighting strategies with the default paradigm. Experiments show that these strategies yield comparable performance to the default training paradigm. This validates our findings and provides deeper insights into LLM pre-training. \n\nOverall, our contributions are threefold: \n\n\u2022 First, we propose the concept of Temporal Scaling Law for Large Language Models, in which the temporal behaviors of large language models during pre-training are characterized mathematically. \n\n\u2022 Second, we provide a method for predicting the pre-training trajectory (i.e., test loss decrease pattern) of LLMs precisely based on our proposed temporal scaling law. \n\n\u2022 Lastly, based on our proposed temporal scaling law, we derive the learning dynamics at the token level and provide evidence that the default pre-training paradigm, in which no reweighting operations are attached to different token positions, is effective enough. Experiment results on pre-training LLMs with different scales further validated our argument. \n\n2 Related Works",
            "score": 0.3924778881482748,
            "section_title": "Introduction",
            "char_start_offset": 4062,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 191,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1120
                },
                {
                    "start": 1123,
                    "end": 1316
                },
                {
                    "start": 1319,
                    "end": 1488
                },
                {
                    "start": 1491,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1849
                },
                {
                    "start": 1852,
                    "end": 1867
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97900390625
        },
        {
            "corpus_id": "270062468",
            "title": "Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective",
            "text": "In this paper, we explored the LP-FT training dynamics in complex classification models using the NTK to analyze feature changes. Our analysis identified classifier weight norms at the start of the FT stage as a key factor influencing FT dynamics. These norms balance the NTK matrix components and help reduce feature changes. Our findings support the existing feature distortion theory from an NTK perspective and emphasize the role of classifier weight norms alongside prediction accuracy. We also found that increases in classifier weight norms, characteristic of training with CE loss, may negatively impact model calibration, and this can be mitigated by temperature scaling. Additionally, the approximation effectiveness of LoRA is theoretically validated in terms of the similarity of the NTK matrix components. Empirical experiments with Transformer-based language models supported our theoretical insights, validating our understanding of the NTK, feature changes, and the benefits of temperature scaling. Overall, our study substantiates the efficacy of LP-FT as a robust method for adapting pre-trained complex models while preserving their well-trained features. \n\nLimitations The main limitation of our study is that it is based on the NTK regime, which might not fully capture the training dynamics. Additionally, we consider just one epoch of gradient descent in FT, which may not effectively represent the overall training. In our experiments, we specifically focused on validating the effectiveness of LP-FT on language models. Therefore, areas other than natural language processing are outside the scope of our experiments. \n\n[57] Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou, Xingyi Cheng, Heng Ji, Zhiyuan Liu, and Maosong Sun. Revisiting out-of-distribution robustness in nlp: Benchmark, analysis, and llms evaluations. arXiv preprint arXiv:2306.04618, 2023. \n\n[58] Yuchen Zeng and Kangwook Lee. The expressive power of low-rank adaptation. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=likXVjmh3E. \n\n[59] Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik.",
            "score": 0.3924607378153187,
            "section_title": "Conclusion",
            "char_start_offset": 26226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1174
                },
                {
                    "start": 1177,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1642
                },
                {
                    "start": 1645,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 1899
                },
                {
                    "start": 1902,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2105
                },
                {
                    "start": 2108,
                    "end": 2193
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99560546875
        },
        {
            "corpus_id": "277955221",
            "title": "Text-to-Decision Agent: Learning Generalist Policies from Natural Language Supervision",
            "text": "Natural language is a complex and unbounded representation of human instruction, and training policies to harness language supervision is nontrivial. A standard approach is to convert language instructions to simpler latent embeddings using popular pre-trained language models that can draw on a wealth of knowledge learned from copious amounts of text. However, directly applying these models to decision domains could fail to solve user-specific problems, as they are typically learned independently of decision-making tasks. A major challenge is that LLMs trained on text corpora typically lack grounding in the physical world and fail to capture any environment dynamics. \n\nFor effective grounding in decision domains, the agent ought to comprehend the environment dynamics of underlying tasks. RL typically learns from active interactions with the outer environment. The world model, i.e., the reward and state transition functions p(s \u2032 , r|s, a), fully characterizes the environment and presents a promising alternative to capturing the environment dynamics of decision tasks. Therefore, we introduce a generalized world model to encode the decision data from different tasks into a dynamics-aware embedding space. We separate the reasoning about the world model into two parts: i) encoding the dynamics-specific information into a latent embedding, and ii) decoding the environment dynamics conditioned on that embedding. \n\nFirst, we use a trajectory encoder \u03d5 to abstract multi-task decision trajectories into a compact embedding z that captures task-specific dynamics. Specifically, we \"tokenize\" different elements (i.e., state, action, and reward) in the raw trajectory sequence \u03c4 = (s 0 , a 0 , r 0 , ..., s L , a L , r L ) \u223c D k , by lifting them to a common representation space as \n\nwhere f s \u03d5 , f a \u03d5 , f r \u03d5 are element-specific tokenizers. We arrange the tokens in a 1-D sequence of length 3 \u00d7 L as \u03c4 e = (e s 0 , e a 0 , e r 0 , ..., e s L , e a L , e r L ).",
            "score": 0.3914259658409235,
            "section_title": "Dynamics-Aware Decision Embedding",
            "char_start_offset": 10689,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1429
                },
                {
                    "start": 1432,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1796
                },
                {
                    "start": 1799,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1979
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.666015625
        },
        {
            "corpus_id": "276937763",
            "title": "PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs",
            "text": "In this work, we define stability as the change in a metric of interest (e.g., validation loss) caused by changes in randomness factors and quantify it using the standard deviation of that metric (see Du and Nguyen, 2023 for other approaches to quantify stability). To provide a basis for analysing the stability of LMs to randomness factors (e.g., their training dynamics or final performance) without incurring the costs to train contemporary LMs, we introduce the PolyPythias: an extension of the Pythia model suite (Biderman et al., 2023b) trained on the Pile dataset (Gao et al., 2021), for which we release 9 new training runs for 5 model sizes, from 14M up to 410M parameters. These new 45 training runs-in addition to the 5 already available in the suite-cover approximately 7k checkpoints across pre-training, and enable us to analyse training stability of large-scale transformer-based LM with respect to model size, parameter initialisation, and data order as quantified by metrics along the entire model training pipeline: downstream performance and consistency of predictions ( \u00a73), shifts in linguistic representations ( \u00a74), and dynamics of the model parameters and training phases ( \u00a75). \n\nBy studying the PolyPythias, we find that: (i) language modelling is largely stable and follows predictable scaling laws with respect to downstream performance; (ii) across training, we identify consistent learning phases: an initial learning phase between steps 103 -10 4 and a critical learning phase between steps 10 4 -10 5 ; (iii) using training maps constructed from statistics of the model parameters, we identify the characteristics of stable training runs and the early signals of instability. \n\nIn the following sections, we describe the PolyPythias release ( \u00a72) and how we use the multiple training runs per model size to study the stability of models across various stages of the model training pipeline ( \u00a73- \u00a75). We conclude by combining the insights from the individual analyses ( \u00a76).",
            "score": 0.39130519242555994,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1590,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1708
                },
                {
                    "start": 1711,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 220,
                    "matchedPaperCorpusId": "256868814"
                },
                {
                    "start": 519,
                    "end": 543,
                    "matchedPaperCorpusId": "257921893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9931640625
        },
        {
            "corpus_id": "249954054",
            "title": "Offline RL for Natural Language Generation with Implicit Language Q Learning",
            "text": "For example, Verma et al. and Jang et al. (Verma et al., 2022;Jang et al., 2022) define their action space at the \"per-utterance\" level (Verma et al., 2022), resulting in expensive decoding processes during training (Bender et al., 2021); and while Jaques et al. (Jaques et al., 2020;2017) remove this issue by defining actions at the \"per-token\" level, the offline RL algorithm proposed requires querying likelihoods from a language model at RL training time, which adds an additional compounding source of approximation error and increases systems complexity at training time. Our proposed method instead operates both at the \"per-token\" level and trains in a fully self-contained way, without the need to simulate generation at training time or query likelihoods from a separate language model. This is achieved by combining an implicit dataset support constraint (Kostrikov et al., 2021) with a novel policy extraction method that takes advantage of the discrete \"per-token\" action space. The result of these design choices is a simple, stable, and effective method that is easy for NLP practitioners to pick up and apply to a variety of language-based tasks. In Section 6.4 we demonstrate our method's effectiveness in meeting these criteria through a series of ablations and comparisons. \n\nMuch prior work on steering language models towards desired behavior has done so without an explicit utility function, instead focusing on curating finetuning datasets (Zhang et al., 2018;Zellers et al., 2019;Rajpurkar et al., 2018). A more closely related line of work uses classifiers to guide LMs towards generating desired textual attributes (Yang & Klein, 2021;Ghazvininejad et al., 2017;Holtzman et al., 2018;Li et al., 2017). These methods are closely related to the prior work on offline RL. In RL parlance, such methods could be considered \"policy extraction\" methods with Monte Carlo value estimates.",
            "score": 0.3908534054751986,
            "section_title": "RELATED WORK",
            "char_start_offset": 6787,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1293
                },
                {
                    "start": 1296,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1906
                }
            ],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 80,
                    "matchedPaperCorpusId": "251648921"
                },
                {
                    "start": 216,
                    "end": 237,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1662,
                    "end": 1689,
                    "matchedPaperCorpusId": "3274110"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5859375
        },
        {
            "corpus_id": "277621090",
            "title": "Discovering Dynamical Laws for Speech Gestures",
            "text": "So far, we have two well-fitting models, but at this point we must go beyond treating these simply as effective fits to data and understand the ways in which they govern the dynamical laws of speech. In this section, we take two models from Section 4 -a first-order model and a second-order model -and explore them deeper, focusing on how to interpret the different terms in the equations, as well as what predictions and assumptions they make about speech gestures. In doing so, we also explore the effects of adding complexity to the second-order model. \n\nThe analysis for each section proceeds as follows. We first plot representative examples of the data against model predictions, followed by plotting the qualitative dynamics of the system in the form of phase portraits and Hooke portraits (Beek & Beek, 1988;Mottet & Bootsma, 1999). This is an essential step in moving beyond model assessment via simple data fitting, because a fundamental characteristic of dynamical models of skilled movement is how they specify the relationship between position, velocity and acceleration. We then explore the equations analytically, deriving algebraic properties that expose similarities to other well-understood systems, before exploring the dynamics of the relevant terms using computational simulations. We note that a comprehensive investigation of every aspect of each model and its numerical parameterization is beyond the scope of the current article; instead, we here focus on elucidating fundamental aspects of each system.",
            "score": 0.3908534054751986,
            "section_title": "Overview",
            "char_start_offset": 46856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1528
                }
            ],
            "ref_mentions": [
                {
                    "start": 797,
                    "end": 816,
                    "matchedPaperCorpusId": "121195051"
                },
                {
                    "start": 816,
                    "end": 839,
                    "matchedPaperCorpusId": "2419012"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.274658203125
        },
        {
            "corpus_id": "202558804",
            "title": "Learning Dynamic Author Representations with Temporal Language Models",
            "text": "We quantitatively evaluate our model and baselines for the language modeling task. We compare models based on their token perplexity. Results reported in section V were obtained on held-out test sets. Model and hyperparameters selection were performed with a separate validation set. The split proportions between training, validation, and testing sets are always approximately 70% / 10% / 20%. Each experiment was run 5 times with different seeds, and the reported results are the mean and standard deviation across these 5 runs. \n\nWe compare our model and the baselines in 3 temporal settings, which are depicted in figure 3. Each evaluation setting corresponds to a distribution of the train / val / test splits across timesteps. A setting can be seen as a temporal task, and help us analyze different behaviors. The three temporal tasks are:   LSTM-iAT and LSTM-AT baselines are not equipped to predict latent representations. So, when evaluating documents published by author a at timesteps t where no document was visible during training, we use the latent representation h a,t , with t < t the most recent timesteps where documents were present during training. For our method, we use the dynamic function f \u03c6 to predict the representation h a,t .",
            "score": 0.3908534054751986,
            "section_title": "B. Evaluation and Tasks",
            "char_start_offset": 17129,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1254
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9091796875
        },
        {
            "corpus_id": "235458053",
            "title": "Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning",
            "text": "We analyze how pretraining on generic language modeling tasks can improve performance on diverse downstream tasks. In our analysis framework, the downstream task requires predicting properties of the posterior distribution over latent variables in an underlying generative model. When the generative model is a standard HMM, downstream recovery is possible with a simple classification head under strong non-degeneracy assumptions. We also show that we can relax the non-degeneracy conditions by changing the generative model to a memory-augmented HMM or using prompt tuning. The generative distributions studied here are meant to provide a first-cut result -we also conjecture similar theorems to hold for other generative models, which we leave as an interesting direction for future work. \n\nAnother direction for future work is to analyze finetuning. Existing work analyzes finetuning for linear neural networks and obtains empirically useful insights [17], but analyzing neural networks with nonlinear activations is very challenging. Our analysis of head and prompt tuning treats the model as a black box. Analyzing finetuning requires understanding how to open up the black box, which is a major open question. \n\nProof of Proposition 3.4. We write \n\n(by Bayes' rule) \n\n\" \n\nB Formal abstraction for prompt tuning and proofs for Section 3.1 \n\nWe first formalize the definition of the model G described in Section 3.1. The model G takes a sequence of embedding vectors v \" pv 1 , . . . , v t q as input and implements message passing to compute a sequence of t outputs. We first define left and right messages \u00d0 \u00dd \u03b4 i`1\u00d1i pvq and \u00dd \u00d1 \u03b4 i\u00b41\u00d1i pvq for i P rts, as follows: \n\nNext, we define the aggregated message at timestep i by \n\nNote that if Assumption 3.2 holds about the Markov chain H 0 , H 1 , . . ., \u03c4 i pvq is always well-defined because P rH i s will have full support. Note that for the proper embeddings epx i q \" P rX i \" x i | H i s, where for x \" px 1 , . . . , x t q, we use epxq \" pepx 1 q, . . .",
            "score": 0.3908534054751986,
            "section_title": "Conclusion",
            "char_start_offset": 31920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 791
                },
                {
                    "start": 794,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1216
                },
                {
                    "start": 1219,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1253
                },
                {
                    "start": 1256,
                    "end": 1272
                },
                {
                    "start": 1275,
                    "end": 1276
                },
                {
                    "start": 1279,
                    "end": 1344
                },
                {
                    "start": 1347,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1673
                },
                {
                    "start": 1676,
                    "end": 1731
                },
                {
                    "start": 1734,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2015
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "258865422",
            "title": "Increasing Probability Mass on Answer Choices Does Not Always Improve Accuracy",
            "text": "This paper investigates the interplay between probability mass on vocabulary items and accuracy in zero-shot-and few-shot-prompted autoregressive language models. Our efforts show that investigations into output scoring functions can change the conclusions drawn about the capabilities of models, which we believe is an important part of better understanding how to reliably and adequately use these systems. The existing NLP benchmarks used both have limitations in their dissimilarity to realworld use cases of LMs (Raji et al., 2021), and in the means in which they were collected, for example by scraping (potentially copyrighted) material off of the internet in the case of MMLU. The use of copyrighted material in the training and testing of AI systems is currently unsettled (Levendowski, 2021;Callison-Burch et al., 2023).",
            "score": 0.3908534054751986,
            "section_title": "Ethics and Broader Impacts",
            "char_start_offset": 28052,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 830
                }
            ],
            "ref_mentions": [
                {
                    "start": 517,
                    "end": 536,
                    "matchedPaperCorpusId": "244729397"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5927734375
        },
        {
            "corpus_id": "266053529",
            "title": "Improving Activation Steering in Language Models with Mean-Centring",
            "text": "The method that we propose aims to get an LLM to exhibit behaviours that are not well-defined, but that can be captured by a dataset of examples that demonstrate the behaviour. Therefore, in our method we use a target dataset made of examples of a target behaviour to extract a distillation vector that can be used to get an LLM to generate the  Let x 1 , . . . , x n be the residual stream activations at some layer l across all token positions of an LLM when performing inference on a target dataset D target of exemplary behaviour. From the set of activations x 1 , . . . , x n we want to extract a distillation vector f . Previous work (Cai et al. 2021) has demonstrated that the activations of GPT-2 Small and BERT activations typically have a non-zero mean (Section 2.3), across all layers. In Appendix A we replicate these findings for a range of open source language models. This means that we might decompose the activations x i as \n\nwhere f is the representation of the behaviour displayed in the dataset D target , b is the bias vector applied to all activations in the language model, and v i is a noise vector encoding information about behaviour not shared by the other datapoints in D target . The mean of these activations can now be described \n\nIf we assume that the noise vectors, v i , are distributed independently about the 0 \u2208 R dmodel vector, then by the law of large numbers the mean of all activations x i becomes \n\nSteering with \u00b5 target directly might be sub-optimal, since the bias vector b in Equation 3 has significant magnitude and does not encode any information specific to the dataset D target . We demonstrate its ineffectiveness empirically in Section 4. \n\nInstead, we remove the bias vector b from \u00b5 target . Assuming that averaging activations x \u2032 1 , . . . , x \u2032 n \u2032 of samples of the training distribution, D training , approximates b: \n\nallows us to extract the vector via f \u2248 \u00b5 target \u2212 \u00b5 training . \n\nWe call this method of extracting distillation vectors meancentring, which we illustrate in Figure 1. We present the algorithm used to implement mean-centred activation steering in Algorithm 1.",
            "score": 0.3908534054751986,
            "section_title": "Mean-Centred Activation Steering",
            "char_start_offset": 6501,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 940
                },
                {
                    "start": 943,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1259
                },
                {
                    "start": 1262,
                    "end": 1438
                },
                {
                    "start": 1441,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1690
                },
                {
                    "start": 1693,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1875
                },
                {
                    "start": 1878,
                    "end": 1941
                },
                {
                    "start": 1944,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 640,
                    "end": 656,
                    "matchedPaperCorpusId": "236460327"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8310546875
        },
        {
            "corpus_id": "266348815",
            "title": "Paloma: A Benchmark for Evaluating Language Model Fit",
            "text": "We believe that evaluations of LM fit provide an important view of language model performance that has been neglected in recent LM research and development. Perplexity cannot be na\u00efvely applied to language modeling at this scale due to challenges such as benchmark contamination. However, these obstacles are worth overcoming as perplexity offers several advantages not afforded by downstream evaluations. Instead of constructing tasks from scratch, we can rely on the ecological validity of real-world data drawn from known sources. By figuring out the best ways to evaluate fit of a model to a collection of documents, we create an interface for other fields to easily compose evaluations for language models. Researchers in other fields need not understand the architectures of such models to collect a corpus of text representing domains of interest that LM researchers would not know to consider. Once a significant data source is identified, such as an online community, evaluations can be updated over time by simply scraping more data, unlike downstream tasks where expensive annotation would be required. \n\nPALOMA advances the possibilities for evaluation of LM fit by providing finely categorized domains of text and controls to mitigate confounders of our metrics. We hope that, along with the baseline models that we train for this work, submissions will begin to fill in the space of possible LM configurations over choices such as data composition, data size, parameter count, and learning rate schedule. By encouraging standardization within this space we increase the density of comparisons that can be made between models. With greater resolution of evaluations, we can increase the resolution of our understanding of language model training dynamics.",
            "score": 0.3904946758072937,
            "section_title": "CONCLUSION",
            "char_start_offset": 58042,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1113
                },
                {
                    "start": 1116,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1768
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.787109375
        },
        {
            "corpus_id": "268820276",
            "title": "The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis",
            "text": "Uncovering early-stage metrics that reflect final model performance is one core principle for large-scale pretraining. The existing scaling law demonstrates the power-law correlation between pretraining loss and training flops, which serves as an important indicator of the current training state for large language models. However, this principle only focuses on the model's compression properties on the training data, resulting in an inconsistency with the ability improvements on the downstream tasks. Some follow-up works attempted to extend the scaling-law to more complex metrics (such as hyperparameters), but still lacked a comprehensive analysis of the dynamic differences among various capabilities during pretraining. To address the aforementioned limitations, this paper undertakes a comprehensive comparison of model capabilities at various pretraining intermediate checkpoints. Through this analysis, we confirm that specific downstream metrics exhibit similar training dynamics across models of different sizes, up to 67 billion parameters. In addition to our core findings, we've reproduced Amber and OpenLLaMA, releasing their intermediate checkpoints. This initiative offers valuable resources to the research community and facilitates the verification and exploration of LLM pretraining by open-source researchers. Besides, we provide empirical summaries, including performance comparisons of different models and capabilities, and tuition of key metrics for different training phases. Based on these findings, we provide a more user-friendly strategy for evaluating the optimization state, offering guidance for establishing a stable pretraining process.",
            "score": 0.3904934077150004,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98828125
        },
        {
            "corpus_id": "268363554",
            "title": "LaERC-S: Improving LLM-based Emotion Recognition in Conversation with Speaker Characteristics",
            "text": "Speaker characteristic injection is to learn clues of emotional dynamics in conversation scenarios, which endows the model with speaker characteristics for subsequent emotion analysis. Although pre-trained large language models cover speakerspecific information, they have not yet been activated the perception capability about this under In the speaker characteristics injection, the generated speaker-characteristics are employed to make the models perceive emotional dynamics. In the emotion analysis, the conversational contents and predefined emotional labels are converted into a formatted input for the final response. As depicted in the instance, LaERC-S bridges the gap between speaker characteristics and the response of \"sad\". conversational contexts. To this end, we adopt a instruction-tuning strategy tailored to endow LLMs with speaker characteristics at the initial stage, as shown in Figure 2 (b). \n\nTypically, we design an instruction template with a certain key element and basic elements for knowledge analysis. A key element is one of any relationships provided by above preliminary. The basic elements comprises four aspects, i.e., 'title', 'specific token' and 'objective', 'constraint'. The 'title' indicates that the role of LLMs expert apt in learning emotional clues in conversations. The 'specific token' is to separate conversation contents. The 'objective' refers to a concise elucidation of the task of knowledge analysis, which provides a response based on conversation contexts. The 'constraint' is used to limit the length of the response for avoiding hallucinations. For reference, we construct the input template to align with the instruction-following template of information retrieval at preliminary.",
            "score": 0.38990725342376237,
            "section_title": "Speaker Characteristic Injection",
            "char_start_offset": 10275,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 914
                },
                {
                    "start": 917,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1738
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1663818359375
        },
        {
            "corpus_id": "276558280",
            "title": "Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models",
            "text": "Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing tasks (Li et al., 2024;Zhu et al., 2024;Huang et al., 2025), fueled by advances in model architectures, large-scale datasets, and computational resources. However, the training stability of LLMs remains a critical challenge, especially as model size and complexity continue to grow. Instabilities during pre-training often lead to issues such as gradient explosion, vanishing gradients, or optimization stagnation, hindering the efficient and effective training of these models. Although Pre-Norm Transformer (Xiong et al., 2020;Zhuo et al., 2025) architectures exhibit greater stability during training, they suffer from feature collapse (Wang et al., 2024a;Xie et al., 2023), where representations across different layers become increasingly similar as depth increases. This phenomenon may contribute to the scaling bottleneck in large models. On the other hand, Post-Norm configurations remain significantly more difficult to train, exhibiting severe gradient explosion or vanishing issues, making stability in such settings a challenge in LLM research. \n\nA fundamental source of these instabilities lies in the complexity of optimizing weight matrices in high-dimensional spaces. Specifically, the scale of weight parameters becomes challenging to regulate as the matrix grows in size, making convergence increasingly delicate. While existing strategies, such as sophisticated initialization schemes (Zhang et al., 2019) and normalization techniques (Ding et al., 2021;Xiong et al., 2020), offer partial mitigation, they fail to resolve the core issue: the entanglement between the weight matrix's scale and distribution. This coupling induces suboptimal optimization dynamics, amplifying training insta-bilities, particularly in large-scale models where gradient propagation is susceptible to divergence or attenuation. \n\nTo tackle these challenges, we introduce Scale-Distribution Decoupling (SDD), a novel approach that restructures fullyconnected layers to explicitly separate the scale and distribution of weight matrices. In contrast to conventional formulations, SDD applies a normalization step to standardize activations, ensuring optimization focuses on learning the distribution rather than jointly optimizing both scale and distribution.",
            "score": 0.3897650933975636,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1161
                },
                {
                    "start": 1164,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1929
                },
                {
                    "start": 1932,
                    "end": 2136
                },
                {
                    "start": 2137,
                    "end": 2358
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 128,
                    "matchedPaperCorpusId": "268379184"
                },
                {
                    "start": 614,
                    "end": 634,
                    "matchedPaperCorpusId": "211082816"
                },
                {
                    "start": 634,
                    "end": 652,
                    "matchedPaperCorpusId": "273850118"
                },
                {
                    "start": 744,
                    "end": 764,
                    "matchedPaperCorpusId": "247187905"
                },
                {
                    "start": 1578,
                    "end": 1597,
                    "matchedPaperCorpusId": "211082816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98486328125
        },
        {
            "corpus_id": "261030259",
            "title": "Latent State Models of Training Dynamics",
            "text": "We possess strong intuition for how various tuned hyperparameters, such as learning rate or weight decay, affect model training dynamics and outcomes (Galanti et al., 2023;Lyu et al., 2022). For example, a larger learning rate may lead to faster convergence at the cost of sub-optimal solutions (Hazan, 2019;Smith et al., 2021;Wu et al., 2019). However, we lack similar intuitions for the impact of randomness. Like other hyperparameters, random seeds also have a significant impact on training (Madhyastha & Jain, 2019;Sellam et al., 2022), but we have a limited understanding of how randomness in training actually manifests in the model. \n\nIn this work, we study the impact of random seeds through a low-dimensional representation of training dynamics, which we use to visualize and cluster training trajectories with different parameter initializations and data orders. Specifically, we analyze training trajectories using a hidden Markov model (HMM) fitted on a set of generic metrics collected throughout training, such as the means and variances of the 1. We propose to use the HMM as a principled, automated, and efficient method for analyzing variability in model training. We fit the HMM to a set of off-the-shelf metrics and allow the model to infer latent state transitions from the metrics. We then extract from the HMM a \"training map,\" which visualizes how training evolves and describes the important metrics for each latent state (Section 2). \n\nTo show the wide applicability of our method, we train HMMs on training trajectories derived from grokking tasks, language modeling, and image classification across a variety of model architectures and sizes. In these settings, we use the training map to characterize how different random seeds lead to different training trajectories. Furthermore, we analyze phase transitions in grokking by matching them to their corresponding latent states in the training map, and thus the important metrics associated with each phase transition (Section 3.1). \n\n2. We discover detour states, which are learned latent states associated with slower convergence.",
            "score": 0.3896284160351123,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1459
                },
                {
                    "start": 1462,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 2010
                },
                {
                    "start": 2013,
                    "end": 2110
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 189,
                    "matchedPaperCorpusId": "249674833"
                },
                {
                    "start": 308,
                    "end": 327,
                    "matchedPaperCorpusId": "231719359"
                },
                {
                    "start": 327,
                    "end": 343,
                    "matchedPaperCorpusId": "263227165"
                },
                {
                    "start": 495,
                    "end": 520,
                    "matchedPaperCorpusId": "202719062"
                },
                {
                    "start": 520,
                    "end": 540,
                    "matchedPaperCorpusId": "235683534"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9296875
        },
        {
            "corpus_id": "276580463",
            "title": "Knowledge Distillation with Training Wheels",
            "text": "The student model's action space is the vocabulary of the language being modeled plus a special action to seek help from the teacher. The teacher-seeking action is invoked by the special token, < \u03c4 >, which results in a teacher token being sampled. \n\nIt is true that for all student tokens, when an action is sampled, x i , the next state deterministically becomes X \u2264i . However, when the token < \u03c4 > is sampled from the student, the value of the next state depends on the teacher token being sampled. Note that the teacher, here, is part of the environment and not part of the policy. Hence, when token < \u03c4 > is sampled from the student, the next state is non-determinstic, when teacher sampling is not greedy. This is not reflected in our treatment above, as the equations implicitly express deterministic dynamics, for the sake of brevity. However, PCL works equally well for non-deterministic dynamics and hence any method of sampling from the teacher model is supported during training. \n\nIn the sequel, the following notation is used: \n\ni represent an output token from the student at time step i, and x (t) i represent an output token from the teacher model at time step i. We have the following relations. \n\nNote that, x \n\ni \u2208 V \u222a {< \u03c4 >} and x (t) i \u2208 V, where V is the token alphabet of the language being modeled. C (S) and C (T ) represent sequence prefixes for the student and teacher, respectively. We define two sequences, X (out) and X (in) as follows. \n\nx \n\nx \n\nX (out) represents the output of the decoding algorithm, and X (in) <i represents the state of the decoding process at time i. Note that X (in) includes additional internal information regarding the decoding process that is useful to the student model to budget teacher-use. That is, C \n\nAs X (in) is processed by the student model, each token in X (in) is embedded by it. For tokens in the alphabet, V, we use the default embedding scheme in the student model, and for the special token, < \u03c4 >, we add a learnable embedding to the default embedding.",
            "score": 0.3884060648368576,
            "section_title": "Student-Teacher decoding and data representation",
            "char_start_offset": 10613,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 992
                },
                {
                    "start": 995,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1214
                },
                {
                    "start": 1217,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1469
                },
                {
                    "start": 1472,
                    "end": 1473
                },
                {
                    "start": 1476,
                    "end": 1477
                },
                {
                    "start": 1480,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2030
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.564453125
        },
        {
            "corpus_id": "1707814",
            "title": "Recurrent Dropout without Memory Loss",
            "text": "Data. Following Mikolov et al. (2011) we use the Penn Treebank Corpus to train our Language Modeling (LM) models. The dataset contains approximately 1 million words and comes with pre-defined training, validation and test splits, and a vocabulary of 10k words. Setup. In our LM experiments we use recurrent networks with a single layer with 256 hidden units. Network parameters were initialized uniformly in [-0.05, 0.05]. For training, we use plain SGD with batch size 32 with the maximum norm gradient clipping (Pascanu et al., 2013). Learning rate, clipping threshold and number of Backpropagation Through Time (BPTT) steps were set to 0.1, 30 and 15 respectively for vanilla RNNs, 1, 10 and 35 for LSTMs and 0.1, 20 and 35 for GRUs. For the learning rate decay we use the following strategy: if the validation error does not decrease after each epoch, we divide the learning rate by 1.5. The aforementioned choices were largely guided by the work of Mikolov et al. ( 2014) 1 . To ease reproducibility of our results on the LM and synthetic tasks, we have released the source code of our experiments 2 . Results. Table 2 reports the results for vanilla RNNs, LSTM and GRU networks. For LSTM and GRU networks we also present results when the dropout is applied directly to hidden states as in (Moon et al., 2015). \n\nWe make the following observations: (i) dropping hidden state updates yields better results than dropping hidden states; (ii) per-step mask sampling is better when dropping hidden state directly; (iii) contrary to our expectations, when we apply dropout to hidden state updates per-step sampling seems to yield results similar to per-sequence sampling; (iv) applying dropout to hidden state updates rather than hidden states in some cases leads to a perplexity decrease by more than 30 points; and finally (v) our approach is effective even when combined with the forward dropout -for LSTMs we are able to bring down perplexity on the validation set from 130 to 91.6.",
            "score": 0.38815528951561157,
            "section_title": "Language Modeling",
            "char_start_offset": 18216,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 5
                },
                {
                    "start": 6,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1315
                },
                {
                    "start": 1318,
                    "end": 1985
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 37,
                    "matchedPaperCorpusId": "16447573"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81005859375
        },
        {
            "corpus_id": "248368718",
            "title": "How Much Do Modifications to Transformer Language Models Affect Their Ability to Learn Linguistic Knowledge?",
            "text": "At the core of many natural language processing tasks are language models (LMs), which compute the probability distribution of the next token that follows a given input context. The Transformer (Vaswani et al., 2017), as one of the most popular architectures for language modeling, has been widely adopted for large-scale pre-training, such as in BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020). The success of large-scale LM pretraining has propelled a surge of analysis on the linguistic knowledge encoded by language models.\n\nWhile prior works have uncovered many exciting facts regarding the linguistic capability of those pretrained LMs (Hewitt and Manning, 2019;Liu et al., 2019;Jawahar et al., 2019), most of these analyses are conducted on publicly-released model checkpoints, and thus the impact of various LM training configurations remains relatively unexplored, limited to LSTM LM configurations (Linzen et al., 2016) or varying training data size (Zhang et al., 2021).\n\nIn this work, we focus on Transformer LMs (Vaswani et al., 2017) instead of LSTMs, and we investigate two aspects of LM training distinct from previous works -(1) the LM training objective, for which we experiment with the focal loss and multi-task training; and (2) the Transformer's self-attention mechanism, which we restrict to a local window of tokens. We train a suite of Transformer LMs that minimally differ from each other in one of these two aspects, and evaluate the effect of these changes via non-parametric probing on BLiMP (Warstadt et al., 2020a), a targeted evaluation benchmark of multiple English linguistic phenomena (e.g., island effects, anaphor agreement). Experimental results demonstrate that none of these modifications yields significant gains on BLiMP in aggregate. However, we do observe that modified training objectives (e.g, using focal loss instead of standard cross entropy loss) result in improvements to specific subtypes of linguistic phenomena. Overall, our experiments suggest that it could be promising to scale up Transformer L",
            "score": 0.3878628101290976,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 216,
                    "matchedPaperCorpusId": "214693050"
                },
                {
                    "start": 1000,
                    "end": 1020,
                    "matchedPaperCorpusId": "226289695"
                },
                {
                    "start": 1065,
                    "end": 1087,
                    "matchedPaperCorpusId": "214693050"
                },
                {
                    "start": 1561,
                    "end": 1585,
                    "matchedPaperCorpusId": "208527435"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94873046875
        },
        {
            "corpus_id": "276235362",
            "title": "An Analysis for Reasoning Bias of Language Models with Small Initialization",
            "text": "Transformer-based Large Language Models (LLMs) have revolutionized Natural Language Processing by demonstrating exceptional performance across diverse tasks. This study investigates the impact of the parameter initialization scale on the training behavior and task preferences of LLMs. We discover that smaller initialization scales encourage models to favor reasoning tasks, whereas larger initialization scales lead to a preference for memorization tasks. We validate this reasoning bias via real datasets and meticulously designed anchor functions. Further analysis of initial training dynamics suggests that specific model components, particularly the embedding space and self-attention mechanisms, play pivotal roles in shaping these learning biases. We provide a theoretical framework from the perspective of model training dynamics to explain these phenomena. Additionally, experiments on real-world language tasks corroborate our theoretical insights. This work enhances our understanding of how initialization strategies influence LLM performance on reasoning tasks and offers valuable guidelines for training models.",
            "score": 0.3873542107335443,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99072265625
        },
        {
            "corpus_id": "270063546",
            "title": "Phase Transitions in the Output Distribution of Large Language Models",
            "text": "We have proposed a method for automating the detection of phase transitions in LLMs, and demonstrated that it successfully reveals a variety of transitions.Leveraging access to the LLMs' next-token probability distributions, the proposed dissimilarity measures can efficiently quantify distribution shifts without fine-tuning or adaption to the specific scenario at hand.Because the method is solely based on analyzing a model's output distribution and access to the model weights is not required, it enables black-box interpretability studies.\n\nThe proposed method is not only applicable to language models, but can be straightforwardly adapted to any generative model with an explicit, tractable density [116,73].If one can draw samples from the output distribution but does not have explicit access to the underlying probabilities, then the dissimilarity measures can still be approximated using NN-based classifiers [117,75] tailored toward the particular data type, such as natural language.\n\nLimitations.Future large-scale investigations are needed to fully understand how the uncovered transitions depend on variables such as the specific prompt, the number of generated output tokens, or the selected model.In particular, due to computational resource constraints, the size of the studied language models has been limited.\n\nBroader Impact.Our method has the potential to enhance the development of future AI systems due to an improved understanding of their behavior.The dual-use nature of such systems carries inherent risks, which requires one to proceed with caution and implement mechanisms to ensure they are used safely and ethically.",
            "score": 0.38704866882156796,
            "section_title": "Conclusion and Outlook",
            "char_start_offset": 25147,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 544
                },
                {
                    "start": 546,
                    "end": 715
                },
                {
                    "start": 715,
                    "end": 996
                },
                {
                    "start": 998,
                    "end": 1010
                },
                {
                    "start": 1010,
                    "end": 1215
                },
                {
                    "start": 1215,
                    "end": 1330
                },
                {
                    "start": 1332,
                    "end": 1347
                },
                {
                    "start": 1347,
                    "end": 1475
                },
                {
                    "start": 1475,
                    "end": 1648
                }
            ],
            "ref_mentions": [
                {
                    "start": 920,
                    "end": 925,
                    "matchedPaperCorpusId": "1501199"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82421875
        },
        {
            "corpus_id": "46988473",
            "title": "Deriving the Constant Rate Effect",
            "text": ", N, the learner picks G 2 with probability Q s\u22121 (and G 1 with probability 1 \u2212 Q s\u22121 ), attempts to parse the sentence, and sets \n\nThus, Q is increased with successful parsing events and decreased with unsuccessful parsing events. Finally, we set q t+1 = Q N . Under the simplifying assumption that N \u2192 \u221e, the learner does not have to contend with a finite dataset or a critical period. It is of course false, but like much work in learnability and modelling we adopt it here in order to derive analytical approximations such as (4) which would otherwise be difficult, if not impossible, to derive. This approximation holds in the following sense: q t+1 converges to a normal distribution with mean q t+1 = c t /(c t + d t ) and a variance which tends to 0 as \u03b3 \u2192 0 and N\u03b3 \u2192 \u221e (Narendra and Thathachar 1989:162-163). Assuming a finite learning sample would introduce a stochastic component (noise) to the system, and exploring the consequences of this falls beyond the scope of the present paper. This is the gist of the competing grammars model of language change; for more details, see Kroch (1994), Yang (2002), Pintzuk (2003) and especially Heycock and Wallenberg (2013), who apply the model to a concrete case study involving the loss of verb movement in Scandinavian.",
            "score": 0.38704866882156796,
            "section_title": "Learning competing grammars",
            "char_start_offset": 20812,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 132,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1274
                }
            ],
            "ref_mentions": [
                {
                    "start": 1089,
                    "end": 1101,
                    "matchedPaperCorpusId": "12029439"
                },
                {
                    "start": 1116,
                    "end": 1130,
                    "matchedPaperCorpusId": "142898315"
                },
                {
                    "start": 1146,
                    "end": 1175,
                    "matchedPaperCorpusId": "85534313"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.376953125
        },
        {
            "corpus_id": "251320517",
            "title": "Fusing Sentence Embeddings Into LSTM-based Autoregressive Language Models",
            "text": "The task of masked language modelling is to predict a token (or multiple) under a mask in a sequence: p(w i |w 1 . . . , w i\u22121 , w i+1 , . . . w |s| ). Masked language modelling as a training objective is inspired by the cloze task [Taylor, 1953, Devlin et al., 2018] and has recently led to impressive improvements in various natural language tasks [Zaib et al., 2020]. In contrast, autoregressive LMs model the \"sequence probability\" p(w 1 , . . . , w |s| ) as |s| i=1 p(w i |w <i ). In practice, this allows the model to be used for decoding and generating sequences. The equivalent in human experiments is the word-level Shannon game [Shannon, 1951] (predicting the next word based on the left context). \n\nAutoregressive LMs are used in a plethora of places. The most intuitive and straightforward application is text auto-completion [Jaech and Ostendorf, 2018]. Moreover, the ability of the model to compute sequence probabilities and compare them is highly useful in many NLP applications which make use of sequences of text either as an intermediate or final output. These include machine translation [Brants et al., 2007, Gulcehre et al., 2017], speech recognition [Toshniwal et al., 2018] or free-form question answering [Dong et al., 2019]. A commonly used part of these pipelines is the decoding for which an autoregressive LM is essential [Chorowski and Jaitly, 2016]. \n\nOn the other hand, masked LM has shown to be a very efficient training task that yields encoder models which became widely used in the NLP community [Xia et al., 2020, Koroteev, 2021].",
            "score": 0.38704866882156796,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 707
                },
                {
                    "start": 710,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1567
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 245,
                    "matchedPaperCorpusId": "206666846"
                },
                {
                    "start": 350,
                    "end": 369,
                    "matchedPaperCorpusId": "211040895"
                },
                {
                    "start": 1128,
                    "end": 1152,
                    "matchedPaperCorpusId": "44472515"
                },
                {
                    "start": 1173,
                    "end": 1197,
                    "matchedPaperCorpusId": "51870362"
                },
                {
                    "start": 1230,
                    "end": 1249,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70654296875
        },
        {
            "corpus_id": "267412499",
            "title": "On Catastrophic Inheritance of Large Foundation Models",
            "text": "Fully understanding the impacts of catastrophic inheritance corresponds to finding the changes in both f and g from pre-training and downstream tasks respectively, including conducting empirical experiments and building novel evaluation metrics and benchmarks on various downstream tasks. \n\nProbing into Effects at Pre-training and Downstream. The initial focus would be identifying the exact effects and the changes of them w.r.t. pre-training data biases at both the pre-training stage and downstream transferring stage. It is critical to study various types of pre-training data biases in this stage and find out the effects of such biases through large-scale experiments. As discussed earlier, the pre-training data biases not only shape the learning dynamics but consequently imprint on the model's behavior on downstream tasks (Nakkiran et al., 2019;Dar et al., 2021;Caballero et al., 2022). A particular aspect of interest is the relationship between these biases and scaling laws. We propose future research encompassing a comprehensive empirical investigation of different LFMs including CLIP (Radford et al., 2021), language models (Touvron et al., 2023a,b), and etc., under controlled and varying pre-training bias conditions. This investigation involves introducing different types and scales of synthetic and realistic biases into clean and controllable largescale pre-training data, and illuminates the trend and changes in training dynamics, model behaviors, and generalization on downstream tasks. Changes of many other properties in f and g w.r.t biases is also worth studying, such as the expressive capacity (Zhang et al., 2016), transition from memorization to generalization (Power et al., 2022;Kumar et al., 2023;Davies et al., 2023), and structures of LFMs affected by the biases. Studying on the biases within controlled subset of concepts is also necessary (Feldman, 2020). \n\nOn the downstream side, we need to consider broader contexts to figure out the trend of g to biases.",
            "score": 0.3866680643725279,
            "section_title": "Understanding Catastrophic Inheritance",
            "char_start_offset": 29755,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 288
                },
                {
                    "start": 291,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1898
                },
                {
                    "start": 1901,
                    "end": 2001
                }
            ],
            "ref_mentions": [
                {
                    "start": 833,
                    "end": 856,
                    "matchedPaperCorpusId": "207808916"
                },
                {
                    "start": 856,
                    "end": 873,
                    "matchedPaperCorpusId": "259137324"
                },
                {
                    "start": 1102,
                    "end": 1124,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1882,
                    "end": 1897,
                    "matchedPaperCorpusId": "186206616"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90966796875
        },
        {
            "corpus_id": "269042847",
            "title": "Why do small language models underperform? Studying Language Model Saturation via the Softmax Bottleneck",
            "text": "The main limitation of this article is the relatively small amount of saturated language models we studied.As it is the only suite of language models trained in the range of interest to release an extensive amount of intermediate checkpoints, we could only observe the training dynamics of small Pythia models.Although we observe strong last-layer anisotropy for the smallest GPT-2 model, we cannot tell with certainty whether it suffered from saturation.The OPT-125m model does not display a strong last-layer anisotropy, which could indicate that it was not affected by the saturation phenomenon.\n\nNevertheless, we argue that this paper does not show that all small models should suffer from saturation, but rather that the saturation of small language models is symptomatic of a limitation that may affect language models that are based on a relatively small hidden dimension.\n\nAnother limitation of this work is the loose nature of the mathematical connection that we establish between the dimensionality of the ideal language modeling head and the rank-constrained performance (cf.Theorem 5.3).Moreover, it can also be argued that considering ideal x * i representations is an ill-defined notion.We argue that the reasoning behind Theorem 5.3 could be applied to any contextual representations, as the ideal nature of x * i is not necessary in the demonstrations.The word ideal reflects that our observations hold for x * i representations obtained from any underlying model, to an extent that depends on the structure that these representations impose on the W * matrix for a given training set T .",
            "score": 0.38666298435596547,
            "section_title": "Limitations",
            "char_start_offset": 20143,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 310
                },
                {
                    "start": 310,
                    "end": 455
                },
                {
                    "start": 455,
                    "end": 598
                },
                {
                    "start": 600,
                    "end": 879
                },
                {
                    "start": 881,
                    "end": 1086
                },
                {
                    "start": 1086,
                    "end": 1099
                },
                {
                    "start": 1099,
                    "end": 1201
                },
                {
                    "start": 1201,
                    "end": 1368
                },
                {
                    "start": 1368,
                    "end": 1604
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93115234375
        },
        {
            "corpus_id": "278602121",
            "title": "Block-Biased Mamba for Long-Range Sequence Processing",
            "text": "An extensive list of different SSM architectures has been given in the survey articles [89,58]. The expressiveness of SSMs has been studied in [99,47], their training dynamics in [76,98], and generalization properties in [98,46]. Several studies have also investigated the representation stability of SSMs, including [86,99]. Applications of SSMs in different scientific fields have been studied in several papers [104,90,63]. \n\nMamba Models. Mamba [26] extends SSMs by introducing input-dependent dynamics via a selective mechanism, resulting in a highly efficient sequential architecture. Unlike earlier SSMs that use fixed, input-independent recurrence, Mamba adapts its dynamics at each step, enabling it to rival transformer-based models in language tasks. The theoretical benefits of this input-dependent recurrence was studied in [53] through the lens of controlled differential equations (CDEs). Mamba has since inspired a number of extensions and studies. Mamba2 [20] incorporates a multihead structure with softmax gating. Mamba has also been applied beyond language modeling [58], including computer vision [109,105,44], time-series forecasting [91], multimodal learning [62], and audio processing [43]. In addition, [51] proposed using Mamba as a foundation model backbone across modalities. Despite its versatility, follow-up studies such as [3] have highlighted the limitations of Mamba on long-range sequence benchmarks such as LRA. The S7 model [79] has been proposed as another selective model with slightly improved performances on LRA tasks; though, the accuracies are still substantially worse than models like S4D and S5. Other efforts to enhance Mamba's long memory retention and selectivity mechanism are found in [96] and [65], respectively. The spectral properties of the state matrix are considered in [25] and related to Mamba's state-tracking capabilities. In addition, the recent work [32] proposes a theoretical generalization upper bound based on the Rademacher complexity of Mamba models.",
            "score": 0.3861012978519654,
            "section_title": "A Related works",
            "char_start_offset": 31409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2020
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 94,
                    "matchedPaperCorpusId": "269362649"
                },
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "269982444"
                },
                {
                    "start": 147,
                    "end": 150,
                    "matchedPaperCorpusId": "274422647"
                },
                {
                    "start": 183,
                    "end": 186,
                    "matchedPaperCorpusId": "273098244"
                },
                {
                    "start": 221,
                    "end": 225,
                    "matchedPaperCorpusId": "273098244"
                },
                {
                    "start": 321,
                    "end": 324,
                    "matchedPaperCorpusId": "269982444"
                },
                {
                    "start": 837,
                    "end": 841,
                    "matchedPaperCorpusId": "268063708"
                },
                {
                    "start": 1086,
                    "end": 1090,
                    "matchedPaperCorpusId": "269362649"
                },
                {
                    "start": 1118,
                    "end": 1123,
                    "matchedPaperCorpusId": "267028142"
                },
                {
                    "start": 1127,
                    "end": 1130,
                    "matchedPaperCorpusId": "268363759"
                },
                {
                    "start": 1156,
                    "end": 1160,
                    "matchedPaperCorpusId": "268513477"
                },
                {
                    "start": 1737,
                    "end": 1741,
                    "matchedPaperCorpusId": "277993667"
                },
                {
                    "start": 1828,
                    "end": 1832,
                    "matchedPaperCorpusId": "274141450"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.480224609375
        },
        {
            "corpus_id": "278310723",
            "title": "How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias",
            "text": "However, Kim & Suzuki (2024b) analyzed the training of an attention layer only, leaving more general characterization of joint training of feed-forward and attention layers yet to be studied. Wen et al. (2024) analyzed three iteration steps in training without establishing the convergence of the entire training process. Our goal is to develop a more general training dynamics characterization, including joint training of the attention and linear feed-forward layers, the convergence rate of the loss functions, and the implicit bias of the training parameters. Furthermore, while both the 'even pairs' and 'parity check' problems are classification tasks, existing theoretical studies on transformers in classification settings (Li et al., 2023;Tarzanagh et al., 2023b;a;Vasudeva et al., 2024;Deora et al., 2023;Yang et al., 2024a;Magen et al., 2024;Jiang et al., 2024;Sakamoto & Sato, 2024) have primarily focused on cases where class distinctions are based on identifiable features. In contrast, these language recognition tasks will pose unique challenges, which require the transformer to leverage its attention mechanism to uncover intricate dependencies inherent in data sequences. By exploring these tasks, our work will offer new insights into the fundamental mechanism of transformers. \n\nIn this work, we investigate how a one-layer transformer, consisting of an attention layer followed by a linear layer, learns to perform the 'even pairs' and 'parity check' tasks. We will theoretically analyze the model dynamics during the training process of gradient descent, and examine how transformer parameters will be guided to converge to a solution with implicit bias. Here, we will jointly analyze the training process of the attention layer and linear layer, which will be significantly different from most existing analysis of the training dynamics of transformers for classification problems, where joint training is not studied (Huang et al., 2024a;Tarzanagh et al., 2023b;Kim & Suzuki, 2024b;Li et al., 2024b). \n\nOur major contributions are four-fold: \n\nFirst, for the even pairs problem, we identify two distinct learning phases. In Phase 1, both linear and attention layers grow rapidly, inducing separable outputs of the attention layer.",
            "score": 0.385910021318745,
            "section_title": "Introduction",
            "char_start_offset": 2111,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1297
                },
                {
                    "start": 1300,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 2025
                },
                {
                    "start": 2028,
                    "end": 2066
                },
                {
                    "start": 2069,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2255
                }
            ],
            "ref_mentions": [
                {
                    "start": 748,
                    "end": 772,
                    "matchedPaperCorpusId": "259262027"
                },
                {
                    "start": 815,
                    "end": 834,
                    "matchedPaperCorpusId": "272910946"
                },
                {
                    "start": 1942,
                    "end": 1963,
                    "matchedPaperCorpusId": "272910946"
                },
                {
                    "start": 1963,
                    "end": 1987,
                    "matchedPaperCorpusId": "259262027"
                },
                {
                    "start": 2007,
                    "end": 2024,
                    "matchedPaperCorpusId": "268379753"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9296875
        },
        {
            "corpus_id": "278534923",
            "title": "Learning Dynamics in Continual Pre-Training for Large Language Models",
            "text": "In conclusion, we explore the learning dynamics in continual pre-training of large language models. We focus on the evolution of performance across general and downstream domains, with domain performance assessed via validation loss. By observations and analyses, we propose a CPT scaling law that integrates distribution shift and learning rate annealing to predict the validation loss at any intermediate training step under common learning rate schedules. Our scaling law provides a comprehensive understanding of key CPT factors and helps optimize the hyper-parameters in CPT for different training goals. Further experiments demon-strate that the law can be also extended to more complicated scenarios such as out-of-domain datasets and models with unknown information. We believe that our CPT scaling law is promising to reshape the understanding of researchers for LLM continual pre-training and scaling laws.",
            "score": 0.385754733396352,
            "section_title": "Conclusion",
            "char_start_offset": 25347,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 916
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99560546875
        },
        {
            "corpus_id": "269005923",
            "title": "How Bad is Training on Synthetic Data? A Statistical Analysis of Language Model Collapse",
            "text": "In this paper, we studied model collapse in language models through a simple statistical model.We provided theoretical analysis when training with only synthetic data and when adding real data from the original distribution.Our results demonstrate that model collapse always happens when the model is training solely on synthetic data, whereas controlling deviation from the initial distribution requires careful choice of the amount of synthetic data to inject in the training set.We also provided experiments showing that these findings extend beyond the simple theoretical settings.\n\nOur current results describe only the statistical approximation error since all generation models are unbiased in our theoretical framework.However, as we discussed in the previous section, this framework can be extended to account for the functional approximation error by considering high-dimension Gaussian vectors as context embeddings instead of canonical vectors.Another possible extension is to study the effect of in-context learning (Wu et al., 2023) on model collapse, which is a key feature of transformer-based models.\n\nDespite the simple setting of our current investigation, we believe that it lays the groundwork for better understanding and mitigation of model collapse in language models, thereby opening the way for the development of more general theoretical frameworks to study next-generation language models dynamics.",
            "score": 0.38567875147215425,
            "section_title": "Discussions & Conclusion",
            "char_start_offset": 25989,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 224
                },
                {
                    "start": 224,
                    "end": 482
                },
                {
                    "start": 482,
                    "end": 585
                },
                {
                    "start": 587,
                    "end": 727
                },
                {
                    "start": 727,
                    "end": 956
                },
                {
                    "start": 956,
                    "end": 1117
                },
                {
                    "start": 1119,
                    "end": 1426
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9716796875
        },
        {
            "corpus_id": "256460933",
            "title": "Calibrating Student Models for Emotion-related Tasks",
            "text": "Therefore, interpolating samples from different regions (e.g., easy-to-learn with ambiguous) in the teacher model can potentially result in a better-calibrated student model with improved ID and OOD perfor-mance. To contextualize examples in our datasets based on training dynamics, we utilize data maps (Swayamdipta et al., 2020). Data maps is a modelbased tool that characterizes datasets based on the model's behavior on each of the instances. By leveraging training dynamics, data maps estimates two measures, i.e., confidence and variability, the mean and standard deviation of the ground-truth probabilities, predicted for each instance across training epochs. \n\nWe further experimentally explore the effect of popular regularization techniques like temperature scaling (Guo et al., 2017) and label smoothing (Pereyra et al., 2017a) along with our informed mixup on the calibration of the student models in a teacher-student training set-up. We carried out extensive experiments to evaluate the proposed informed mixup data augmentation technique by creating teacher networks on two pre-trained models, BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2019). Student networks will then be distilled and evaluated against several data sets on three different text classification tasks, including emotion detection (Demszky et al., 2020), sentiment analysis (Zhang et al., 2015), and empathy detection (Sharma et al., 2020). Our contributions are thus summarized as follows: \n\n\u2022 We show that the dark knowledge of a pretrained language teacher model can act as a regularization process, helping to calibrate the student model's confidence in its predictions. \n\n\u2022 We demonstrate that using training dynamics to inform the interpolation process in the mixup data augmentation on a teacher model can effectively improve the calibration of the student model in a distillation setting. Based on the confidence and variability of each example, we divide training samples into distinct categories where we propose to mix easy-tolearn and ambiguous samples in the teacher model for the student model calibration.",
            "score": 0.3856148700560099,
            "section_title": "Introduction",
            "char_start_offset": 4960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 666
                },
                {
                    "start": 669,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1482
                },
                {
                    "start": 1485,
                    "end": 1666
                },
                {
                    "start": 1669,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 2112
                }
            ],
            "ref_mentions": [
                {
                    "start": 304,
                    "end": 330,
                    "matchedPaperCorpusId": "221856637"
                },
                {
                    "start": 776,
                    "end": 794,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 815,
                    "end": 838,
                    "matchedPaperCorpusId": "9545399"
                },
                {
                    "start": 1114,
                    "end": 1135,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1323,
                    "end": 1345,
                    "matchedPaperCorpusId": "218486942"
                },
                {
                    "start": 1366,
                    "end": 1386,
                    "matchedPaperCorpusId": "368182"
                },
                {
                    "start": 1410,
                    "end": 1431,
                    "matchedPaperCorpusId": "221761251"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87109375
        },
        {
            "corpus_id": "248780409",
            "title": "Life after BERT: What do Other Muppets Understand about Language?",
            "text": "T5 or BART (Mosbach et al., 2020;Phang et al., 2021;Jiang et al., 2021). However, only a small number of analysis papers have probed multiple (three or more) model families (Zhou et al., 2021;Ilharco et al., 2021).\n\nIn our work, we test 8 families of models on oLMpics tasks (Talmor et al., 2019) and 6 families on psycholinguistic tasks from Ettinger (2020). These models differ in size, architecture, pretraining objective, dataset size, and have other small yet important differences. Such a diverse set of models provides a broader view of what linguistic capabilities are affected by the change of any of these properties. We also include several distilled models in our analysis. We find that different models excel in different symbolic reasoning tasks, suggesting that slight differences related to optimization or masking strategy might be more important than the pre-training approach, dataset size, or architecture. Furthermore, in contrast to Radford et al. (2019), we find that for oLMpics tasks, model size rarely correlates with the model performance. In addition, we observe that all models fail on composition tasks when evaluated in a zero-shot fashion.\n\nPre-trained model analysis is a rapidly growing area in NLP today. There exists a number of methods for analyzing internal representations of a model, including structured head and FCN pruning (Michel et al., 2019;Voita et al., 2019;Prasanna et al., 2020), residual connection and LayerNormalization analysis (Kovaleva et al., 2021;Kobayashi et al., 2021), and analyzing attention patterns (Clark et al., 2019;Kovaleva et al., 2019).\n\nCompared to these methods, probing tasks (Conneau et al., 2018;Tenney et al., 2019) provide a more direct way to evaluate what a model can and cannot accomplish. While it is possible to probe embeddings or hidden representations directly (Tenney et al., 2019;Liu et al., 2019a), the adoption of pre-trained language models has made it possible to evaluate such models by framing probing tasks close to the original model",
            "score": 0.3855480270648808,
            "section_title": "Introduction",
            "char_start_offset": 1725,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 11,
                    "end": 33,
                    "matchedPaperCorpusId": "222141026"
                },
                {
                    "start": 52,
                    "end": 71,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 173,
                    "end": 192,
                    "matchedPaperCorpusId": "221865854"
                },
                {
                    "start": 192,
                    "end": 213,
                    "matchedPaperCorpusId": "225086076"
                },
                {
                    "start": 1366,
                    "end": 1387,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 1406,
                    "end": 1428,
                    "matchedPaperCorpusId": "218487454"
                },
                {
                    "start": 1482,
                    "end": 1505,
                    "matchedPaperCorpusId": "235313996"
                },
                {
                    "start": 1505,
                    "end": 1528,
                    "matchedPaperCorpusId": "237513393"
                },
                {
                    "start": 1563,
                    "end": 1583,
                    "matchedPaperCorpusId": "184486746"
                },
                {
                    "start": 1649,
                    "end": 1671,
                    "matchedPaperCorpusId": "24461982"
                },
                {
                    "start": 1671,
                    "end": 1691,
                    "matchedPaperCorpusId": "155092004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6513671875
        },
        {
            "corpus_id": "278534923",
            "title": "Learning Dynamics in Continual Pre-Training for Large Language Models",
            "text": "Continual Pre-Training (CPT) is a effective method to enhance the foundation large language models to specific downstream domains or tasks. Our work provide a scaling law to quantitatively describe the learning dynamics of CPT process, enabling to optimize the training hyper-parameters for balancing the general and downstream performance. While there will be important impacts resulting from the use of CPT in general, here we focus on the impact of using our scaling law to provide explanations for CPT process. There are many benefits to using our method, such as predict the loss curve dynamics and optimize hyper-parameters. This paper presents work whose goal is to advance the field of Large Language Models. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. language modeling performance, 2024. URL https: //arxiv.org/abs/2403.16952. \n\nC \u00b8agatay Y\u0131ld\u0131z, Ravichandran, N. K., Punia, P., Bethge, M., and Ermis, B. Investigating continual pretraining in large language models: Insights and implications, 2024. URL https://arxiv.org/abs/2402.17400.",
            "score": 0.3845291391330515,
            "section_title": "Impact Statement",
            "char_start_offset": 26284,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1121
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.990234375
        },
        {
            "corpus_id": "233189563",
            "title": "Analyzing the Forgetting Problem in Pretrain-Finetuning of Open-domain Dialogue Response Models",
            "text": "In this section, we conduct a set of detailed behavior analysis, characterising how different training strategies change the model's behavior. In particular, we aim to answer the crucial question about whether the model forgets precious language generation skills during standard finetuning, and whether mix-review helps the model remember the skills. We first present perplexity results for different finetuning methods in Table 2. We observe the big improvement in perplexity (larger than 40%) for the pretrained models comparing to the baseline models trained from scratch. Comparing to MASS, the NS pretraining has more than 7% relative improvement. This confirms our earlier discussion that the model pretrained by NS better utilizes contextual input (which is further verified in Section 5.2). Based on this observation, we focus our analysis below on the NS pretraining.\n\nComparing to standard finetuning, mix-review further gives solid improvement. The gain is due to its strong regularization effect (which we study in the next three sections). However, the performance gap between mix-review and WD(\u2713 pre ) is not significant. We believe the reason is that the benefit (e.g., knowledge transfer) from alleviate the forgetting problem is not be well demonstrated in single-turn response evaluation, because the context is limited to the narrow scope of the specific datasets. We address this concern with multi-turn human evaluation in the next section.",
            "score": 0.3843632280090497,
            "section_title": "Experiment Results",
            "char_start_offset": 14218,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9326171875
        },
        {
            "corpus_id": "273638217",
            "title": "A distributional simplicity bias in the learning dynamics of transformers",
            "text": "Notably, for the clone derived from a six-layer architecture, as well as for the clone sampled from a BERT model, the test loss continues to decrease throughout the entire training process, as indicated by the black arrows. \n\nSequential learning of many-body interactions by GPT Finally, we extend the analysis to the learning dynamics of an autoregressive transformer model trained for the next-token prediction task. Specifically, we examine a two-layer GPT-Neo model, as also investigated in the TinyStories paper [22], training it on the original dataset and evaluating its performance on the different clones (see appendix A.4 for full experimental details). While the loss curve shows a single decay of the loss, the sequential learning is again clearly apparent, as shown in the left panel of fig. 5. An alternative visualization of the same data presented in the left panel of the same figure reveals that the test loss on clones with low-order interactions saturates within the first 500 training steps, while the test loss on clones describing higher-order interactions continues to decrease throughout the entire training period. Crucially, these findings demonstrate the applicability of our framework also to autoregressive models, which are the foundation of state-of-the-art large language models.",
            "score": 0.3843317355003433,
            "section_title": "Sampling procedure",
            "char_start_offset": 28551,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 226,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1312
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97509765625
        },
        {
            "corpus_id": "259747253",
            "title": "Feature Normalization and Cartography-Based Demonstrations for Prompt-Based Fine-Tuning on Emotion-Related Tasks",
            "text": "A large and growing body of research has been conducted on training very large neural networks for language understanding across various tasks (Liu et al. 2019a;Devlin et al. 2019;Radford et al. 2018;Raffel et al. 2020). Although these models achieved remarkable results across a wide range of natural language tasks, effectively employing such paradigms depends on the presence of tens of thousands of task-specific training examples to fine-tune the model. GPT-3 model (Brown et al. 2020), on the other hand, has shown remarkable capabilities for few-shot predictions only by employing a natural language task description and some task demonstrations as input context. However, as an extremely large autoregressive language model, it is challenging to use GPT-3 in most real-world applications. \n\nFollowing GPT-3 in-context learning method where many demonstrations (depending on the model's context window size) are randomly selected from the training set, Gao, Fisch, and Chen (2021) proposed to append only one selected training sample from each class to the input based on their simi-larity to the input sample. In this way, the final input sample becomes shorter and easier to be leveraged by a mediumsized language model like RoBERTa. We are greatly inspired by (Gao, Fisch, and Chen 2021), although they randomly pick a single example from each class to create demonstrations and select to pair inputs with similar examples. We, on the other hand, leverage training dynamics to generate informative demonstrations based on the characteristics of the data samples and their contextual similarity to the input sample. We also leverage feature moments (i.e., mean and standard deviation) as a data augmentation technique and provide the language model with two salient signals, the normalized features and the moments of samples. \n\nAlong the lines of prompt-based learning, there are several studies on prompting for extracting knowledge from pre-trained models (Petroni et al. 2019;Davison, Feldman, and Rush 2019;Talmor et al. 2020). Petroni et al. (2019) proposed a probe for examining the commonsense knowledge contained in pre-trained language models.",
            "score": 0.38390232718750383,
            "section_title": "Related Work",
            "char_start_offset": 5704,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1835
                },
                {
                    "start": 1838,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2162
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 180,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 180,
                    "end": 200,
                    "matchedPaperCorpusId": "49313245"
                },
                {
                    "start": 200,
                    "end": 219,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 471,
                    "end": 490,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 960,
                    "end": 987,
                    "matchedPaperCorpusId": "229923710"
                },
                {
                    "start": 1270,
                    "end": 1297,
                    "matchedPaperCorpusId": "229923710"
                },
                {
                    "start": 1968,
                    "end": 1989,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 1989,
                    "end": 2021,
                    "matchedPaperCorpusId": "202541043"
                },
                {
                    "start": 2021,
                    "end": 2040,
                    "matchedPaperCorpusId": "209515274"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79541015625
        },
        {
            "corpus_id": "277104124",
            "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
            "text": "Reinforcement learning on large language models is not only a cutting-edge research direction but also an intrinsically complex systems engineering challenge, characterized by the interdependence of its various subsystems. Modifications to any single subsystem can propagate through the system, leading to unforeseen consequences due to the intricate interplay among these components. Even seemingly minor changes in initial conditions, such as variations in data and hyperparameters, can amplify through iterative reinforcement learning processes, yielding substantial deviations in outcomes. This complexity often confronts researchers with a dilemma: even after meticulous analysis and well-founded expectations that a modification will enhance specific aspects of the training process, the actual results frequently diverge from the anticipated trajectory. Therefore, monitoring of key intermediate results during experimentation is essential for swiftly identifying the sources of discrepancies and, ultimately, for refining the system. \n\n\u2022 The Length of Generated Responses is a metric closely related to training stability and performance, as shown in Figure 7a. The increase in length provides the model with a larger space for exploration, allowing more complex reasoning behaviors to be sampled and gradually reinforced through training. However, it is important to note that length does not always maintain a continuous upward trend during training. In some considerable periods, it can exhibit a trend of stagnation or even decline, which has also been demonstrated in [2]. We typically use length in conjunction with validation accuracy as indicators to assess whether an experiment is deteriorating. \n\n\u2022 The Dynamics of Reward during training has always been one of the crucial monitoring indicators in reinforcement learning, as shown in Figure 7b. In the majority of our experiments, the trend of reward increase is relatively stable and does not fluctuate or decline significantly due to adjustments in experimental settings. This indicates that, given a reliable reward signal, language models can robustly fit the distribution of training set. However, we find that the final reward on the training set often exhibits little correlation with the accuracy on the validation set, which indicates overfitting to the training set. \n\n\u2022 The Entropy of the Actor Model and Generation Probability are related to the model's exploration capability and are key metrics that we closely monitor in our experiments. Intuitively, the model's entropy needs to be maintained within an appropriate range.",
            "score": 0.3837588935504188,
            "section_title": "Training Dynamics",
            "char_start_offset": 16742,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1713
                },
                {
                    "start": 1716,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2345
                },
                {
                    "start": 2348,
                    "end": 2521
                },
                {
                    "start": 2522,
                    "end": 2606
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9736328125
        },
        {
            "corpus_id": "264802494",
            "title": "Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning",
            "text": "We start by pre-training LMs based on a popular and computationally affordable pre-trained language model, GPT-2 (Radford et al., 2019). We use the corpus dataset WikiText (Merity et al., 2016) and the common next-token prediction objective \n\nwhere w i is the ith language token in one sentence. The pre-trained GPT-2 is accessible in Hugging Face1 . To explore the effects of different pre-trained model quality on the downstream offline RL tasks, we also explore three variants of models: 1) a model that is pre-trained for fewer steps; 2) a model that is pre-trained on randomly shuffled text corpus; 3) a model with randomly initialized weights. Our results in Section 5.5 and Appendix G show that high language pre-training quality is helpful for downstream RL tasks, underscoring the importance and necessity of the pre-training. (1) pre-training LMs on language tasks, (2) freezing the pre-trained attention layers, replacing linear projections with MLPs, and using LoRA to adapt to RL tasks. We also apply the language loss during the offline RL stage as a regularizer.",
            "score": 0.38328362736143007,
            "section_title": "PRE-TRAINING ON LANGUAGE TASKS",
            "char_start_offset": 12955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1077
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72509765625
        },
        {
            "corpus_id": "273821140",
            "title": "Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study",
            "text": "The classic language modeling task involves predicting the next word in a sequence, given the preceding words. The model is trained in an autoregressive manner. A typical example is the GPT model [18]. Training an autoregressive model involves fitting a function f : R N \u00d7D \u2192 R N \u00d7D , where N represents the sequence length of tokens, each token having a dimension of D. The model aims to estimate the conditional probability distribution, denoted p, which can be expressed as: \n\nwhere x t \u2208 R D represents the t'th discrete token observation, c denotes the initial context or input sequence, and T signifies the length of the sequence of predicted tokens [19,Chapter 22]. LLMs are normally pre-trained on a huge amount of data, giving them a broad understanding of language patterns and semantics. This pre-training phase allows LLMs to capture intricate linguistic structures and contextual nuances, empowering them with a robust foundation for various natural language processing tasks. \n\nFine-tuning is a method in which a pre-trained model can be adapted to a more specific task, such as auto-completing source code. The process involves adjusting the weights of the pre-trained model using task-specific data, thereby enhancing its performance on a target task. When all parameters are tuned, this is often referred to as \"full fine-tuning\" [20]. If we imagine the model as a web of knowledge, the full fine-tuning loosens all the connections and lets them adapt entirely to the new domain. This flexibility allows for potentially superior performance but comes at a cost: \n\n\u2022 Overfitting: The model can become overly reliant on the training data, potentially struggling with unseen examples [21]. \u2022 Computational cost: Training all parameters requires significant computational resources [20]. \n\n\u2022 Catastrophic forgetting: The model might forget some valuable knowledge it acquired during pre-training [22,23].",
            "score": 0.3832814172581209,
            "section_title": "II. PRELIMINARIES A. LLM training",
            "char_start_offset": 4397,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1578
                },
                {
                    "start": 1581,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1800
                },
                {
                    "start": 1803,
                    "end": 1917
                }
            ],
            "ref_mentions": [
                {
                    "start": 1347,
                    "end": 1351,
                    "matchedPaperCorpusId": "269293949"
                },
                {
                    "start": 1698,
                    "end": 1702,
                    "matchedPaperCorpusId": "12440383"
                },
                {
                    "start": 1795,
                    "end": 1799,
                    "matchedPaperCorpusId": "269293949"
                },
                {
                    "start": 1909,
                    "end": 1913,
                    "matchedPaperCorpusId": "2691726"
                },
                {
                    "start": 1913,
                    "end": 1916,
                    "matchedPaperCorpusId": "261031244"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9052734375
        },
        {
            "corpus_id": "273022769",
            "title": "Investigating the Impact of Model Complexity in Large Language Models",
            "text": "Large Language Models (LLMs) based on the pre-trained fine-tuning paradigm have become pivotal in solving natural language processing tasks, consistently achieving state-of-the-art performance. Nevertheless, the theoretical understanding of how model complexity influences fine-tuning performance remains challenging and has not been well explored yet. In this paper, we focus on autoregressive LLMs and propose to employ Hidden Markov Models (HMMs) to model them. Based on the HMM modeling, we investigate the relationship between model complexity and the generalization capability in downstream tasks. Specifically, we consider a popular tuning paradigm for downstream tasks, head tuning, where all pre-trained parameters are frozen and only individual heads are trained atop pre-trained LLMs. Our theoretical analysis reveals that the risk initially increases and then decreases with rising model complexity, showcasing a\"double descent\"phenomenon. In this case, the initial\"descent\"is degenerate, signifying that the\"sweet spot\"where bias and variance are balanced occurs when the model size is zero. Obtaining the presented in this study conclusion confronts several challenges, primarily revolving around effectively modeling autoregressive LLMs and downstream tasks, as well as conducting a comprehensive risk analysis for multivariate regression. Our research is substantiated by experiments conducted on data generated from HMMs, which provided empirical support and alignment with our theoretical insights.",
            "score": 0.3832814172581209,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83740234375
        },
        {
            "corpus_id": "3934380",
            "title": "Dynamic encoding of structural uncertainty in gradient symbols",
            "text": "The processing time of w k is defined as the sum of the settling times in phase 1 and 3 and the duration of phase 2. \n\nThe parser, in phase 1, integrates a new word input with its internal language model (or structural prediction) and, in phase 2, updates the internal language model via the control of commitment level to make a new structural prediction. In the proposed model, the effect of instantaneous surprisal of w k (phase 1) is conceptually distinguished from the effect of model update (phase 2) (c.f., O'Reilly et al., 2013). 5  The role of phase 2 is to reduce the number of grid points reachable from the present activation state. 6 As q increases, the system passes through a series of bifurcations, the qualitative changes in the organization of the representation space. When q passes some critical values q c , more local optima emerge. Each local optimum forms a local hump (basin of attraction) on the Harmony surface. Those local optima are separated by Harmony valleys that block transitions from one hump to another: the state seeks higher Harmony. Metaphorically, the paths to some futures (corresponding to different parses) are separated from the present state by these valleys. That is, some structural hypotheses are rejected (Cho and Smolensky, 2016). \n\nGiven a length-N sentence, we define a commitment policy \u03c0 N as a sequence of q values (q 0 , \u2022 \u2022 \u2022 , q k , \u2022 \u2022 \u2022 , q N ) where q k is the commitment level after processing the k-th word in a sentence. gence as follows. Let Hmax(t) be the maximum total harmony in a phase up through time t. If Hmax has not been updated for a certain amount of time (= 0.5 in our simulation study; Section 5), the phase ends and the following phase begins. During phase 2, q increases at a constant rate dq/dt = 1 so the duration of phase 2 is simply q k \u2212 q k\u22121 . \n\n5 Alternatively, we can consider a GSC parser with a discrete commitment policy. Given a new word input w k , the model updates both q and ex discretely from q k\u22121 and ex k\u22121 to q k and ex k .",
            "score": 0.3832814172581209,
            "section_title": "GSC parser as a probabilistic model 3.1 GSC parser",
            "char_start_offset": 12777,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 119,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1280
                },
                {
                    "start": 1283,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1830
                },
                {
                    "start": 1833,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2025
                }
            ],
            "ref_mentions": [
                {
                    "start": 514,
                    "end": 536,
                    "matchedPaperCorpusId": "9133917"
                },
                {
                    "start": 1263,
                    "end": 1279,
                    "matchedPaperCorpusId": "18676005"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.282470703125
        },
        {
            "corpus_id": "273877918",
            "title": "Gradient Localization Improves Lifelong Pretraining of Language Models",
            "text": "Pretraining over diverse datasets has been shown to encode world knowledge in the parameters of large language models (LLMs) (Petroni et al., 2019;Roberts et al., 2020;Gueta et al., 2023) from massive static web-scale datasets. However, these models are normally trained on large static text corpora which do not reflect changes in world knowledge or language usage that occur after the initial data collection. In practice language models are deployed in dynamic real-world settings, and their learned knowledge becomes stale over time (Lazaridou et al., 2021;Luu et al., 2022;Dhingra et al., 2022;Yao et al., 2022;Nylund et al., 2023;Cheang et al., 2023); the temporal degradation can be evaluated according to intrinsic measures such as perplexity, or extrinsic downstream performance (e.g. question answering). \n\nIncrementally training language models on streams of data has been explored as a method to mitigate temporal performance degradation without incurring the heavy computational and environmental costs of retraining models on large pretraining corpora (Jang et al., 2021(Jang et al., , 2022;;Lin et al., 2022;Gururangan et al., 2020). However, naive online training on these data streams is known to: induce hallucinations in language generations (Kang et al., 2024), fail in the uptake of new information (Onoe et al., 2023;Hu et al., 2023), and catastrophically forget previously learned information (Zhu et al., 2020). \n\nTo address these problems, recent work has applied continual learning and online learning methods to adapting large language models to streams of documents (Loureiro et al., 2022;Scialom et al., 2022;Jang et al., 2022). While continual learning methods have been shown to mitigate temporal performance degradations, the mechanisms by which neural language models store and update information are not well understood.",
            "score": 0.3832814172581209,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1435
                },
                {
                    "start": 1438,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1854
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 147,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 168,
                    "end": 187,
                    "matchedPaperCorpusId": "256697368"
                },
                {
                    "start": 537,
                    "end": 561,
                    "matchedPaperCorpusId": "239886013"
                },
                {
                    "start": 561,
                    "end": 578,
                    "matchedPaperCorpusId": "244117116"
                },
                {
                    "start": 599,
                    "end": 616,
                    "matchedPaperCorpusId": "252761342"
                },
                {
                    "start": 636,
                    "end": 656,
                    "matchedPaperCorpusId": "258461491"
                },
                {
                    "start": 1066,
                    "end": 1084,
                    "matchedPaperCorpusId": "238419458"
                },
                {
                    "start": 1084,
                    "end": 1106,
                    "matchedPaperCorpusId": "248476156"
                },
                {
                    "start": 1106,
                    "end": 1123,
                    "matchedPaperCorpusId": "248512744"
                },
                {
                    "start": 1123,
                    "end": 1147,
                    "matchedPaperCorpusId": "216080466"
                },
                {
                    "start": 1320,
                    "end": 1339,
                    "matchedPaperCorpusId": "258437155"
                },
                {
                    "start": 1638,
                    "end": 1656,
                    "matchedPaperCorpusId": "248476156"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91357421875
        },
        {
            "corpus_id": "268379408",
            "title": "The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models",
            "text": "A substantial gap persists in understanding the reasons behind the exceptional performance of the Transformer architecture in NLP. A particularly unexplored area involves the mechanistic description of how the distribution of parameters evolves over time during training. In this work we suggest that looking at the time evolution of the statistic distribution of model parameters, and specifically at bifurcation effects, can help understanding the model quality, potentially reducing training costs and evaluation efforts and empirically showing the reasons behind the effectiveness of weights sparsification.",
            "score": 0.3832814172581209,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98291015625
        },
        {
            "corpus_id": "6473108",
            "title": "N-th Order Ergodic Multigram HMM for Modeling of Languages without Marked Word Boundaries",
            "text": "Based on the Hidden Markov Model, the Ergodic Multigram llidden Markov Model (l,aw and Chan, 1996), when applied as a language model, can process directly on unsegmented input corpus as it allows a variable mmfl)er of characters in each word class. Other than that its prol)erties are sin> liar to l';rgodic tlidden Markov Models (Kuhn ct al., 1994), that both training and scoring can be done directly on a raw, unCagged corpus, given a lexicon with word classes. Specifically, the N-Oh order F, rgodic Multigram It M M, as in conventional class-based (N+I)-gram model, assumes a (loubly stochastic process in sentence production. The word-class sequence in a scalene(: follows Che N-Oh order Markov assulnl> tion, i.e. tile identity of a (:lass in the s('.lite[Ic(~ delmn(Is only on tim previous N classes, and the word observed depelads only on the class it l)elongs to. The difference is thai, this is a multigram model (Doligne and Bimbot, 1995) in the sense Chat each state (i.e. node in the IIMM) (:a,t genera.re a wu-iable number of ot)served character sequences. Sentence boundaries are inodelcd as a sl)ecial class. \n\nThis model can be apl/lied to a.ll input sent(race or a characCer lattice as a language model. 'Fhe maxinnun likelihood scat(: sequence through l,he model, obtaine(t using the ViCerl)i or Stack I)(> coding AlgoriChln, ret)resenCs the 1)est particular segmentation and class-tagging for the input sentence or lattice, since transition of states denotes a wor(t boundary and state identity denotes tile ClU'rent word class.",
            "score": 0.3832814172581209,
            "section_title": "Overview",
            "char_start_offset": 2395,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1125
                },
                {
                    "start": 1128,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1549
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2371826171875
        },
        {
            "corpus_id": "271924204",
            "title": "Unraveling Text Generation in LLMs: A Stochastic Differential Equation Approach",
            "text": "The trajectory analysis underscores the strengths and limitations of the current LLM's text generation process. The model demonstrates a strong ability to predict text generation trajectories in simple linguistic contexts, where the predicted paths closely align with the actual generated sequences. However, as the complexity of the language increases, the model's predictions become less accurate, indicating a need for further refinement of the drift and diffusion terms within the stochastic differential equation (SDE) framework. \n\nThese findings suggest that enhancing the model's capacity to handle complex linguistic structures, possibly through more sophisticated drift term modeling or improved integration of contextual information, could lead to more accurate and coherent text generation. The trajectory analysis thus provides a critical diagnostic tool for guiding future developments in LLM architecture, ultimately contributing to the creation of more robust and versatile language models.",
            "score": 0.3832814172581209,
            "section_title": "Conclusion:",
            "char_start_offset": 31289,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1005
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74951171875
        },
        {
            "corpus_id": "258762609",
            "title": "Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions",
            "text": "Much of contemporary natural language processing (NLP) is driven by Transformer-based large language models, which are trained to make predictions about words in their context by aggregating representations through their self-attention mechanism. The breakthrough in many NLP tasks these models have achieved has led to active research into interpreting their predictions and probing the knowledge embodied by these models (Manning Figure 1: Schematic of input and output representations from Transformer-based autoregressive language models. Standard models (top) calculate one vector of final hidden states at a given timestep (x L,i ), which in this work (bottom) is decomposed exactly into the sum of output representations of each input token (x L,i,k ) and a cumulative bias term (b L,i ). Rogers et al., 2021;Belinkov, 2022). One line of such research focuses on quantifying the importance of each input token to the model's final output, but due to the complexity of the computations performed within the Transformer layers, analysis has been limited to studying the self-attention mechanism and the feedforward neural network independently (Kobayashi et al., 2020(Kobayashi et al., , 2021Geva et al., 2021Geva et al., , 2022Mickus et al., 2022) or has relied on e.g. gradient-based attribution methods (Sanyal and Ren, 2021;Zaman and Belinkov, 2022) that yield measures that are not interpretable in terms of output model probabilities.\n\nTo address these limitations, this work presents a linear decomposition of final language model hidden states into the sum of final output representations of each initial input token and a cumulative bias term, which is schematized in Figure 1. This work focuses on decomposing autoregressive language models, in which the final hidden states are used to calculate a probability distribution over the next token. The decomposition allows the definition of probability distributions that ablate the contribution of specific input tokens, which can be used to study their impact on next-token probabilities with only one forward pass from the model. This decomposition is exact if the activation function of the feedforward neural network is differentiable almost everywhere, 1 and therefore it does not require per",
            "score": 0.3832814172581209,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 796,
                    "end": 816,
                    "matchedPaperCorpusId": "211532403"
                },
                {
                    "start": 816,
                    "end": 831,
                    "matchedPaperCorpusId": "236924832"
                },
                {
                    "start": 1149,
                    "end": 1172,
                    "matchedPaperCorpusId": "222176890"
                },
                {
                    "start": 1172,
                    "end": 1197,
                    "matchedPaperCorpusId": "237513393"
                },
                {
                    "start": 1197,
                    "end": 1214,
                    "matchedPaperCorpusId": "229923720"
                },
                {
                    "start": 1214,
                    "end": 1233,
                    "matchedPaperCorpusId": "247762385"
                },
                {
                    "start": 1233,
                    "end": 1253,
                    "matchedPaperCorpusId": "249461519"
                },
                {
                    "start": 1311,
                    "end": 1333,
                    "matchedPaperCorpusId": "237363853"
                },
                {
                    "start": 1333,
                    "end": 1358,
                    "matchedPaperCorpusId": "248119025"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89599609375
        },
        {
            "corpus_id": "16376678",
            "title": "Discovery of a Recursive Principle: An Artificial Grammar Investigation of Human Learning of a Counting Recursion Language",
            "text": "The intermediate states show a subtle property that allows us to obtain some further information about the nature of the system. Consider another kind of noise model that is capable of describing intermediate stages: when the system is in transition between G k and G k+1 (k = 1, 2, 3) or between G 3 and G R , it probabilistically chooses between the two grammars at each word-transition. This model, which we call \"Probabilistic Mixture of Bearing Points\" predicts profiles in which the noisy behavior is limited to the points of differentiation between the two grammars. Indeed, such an account is roughly consistent with the profiles shown in Figure 4, thus improving on the Progression + Noise account. However, we will present evidence that Probabilistic Mixture fails to capture a distinctive pattern in the data. In Figure 4, the mean prediction accuracy on the last T 23 seems lower than the mean prediction accuracy on the last T 34 although both transitions are equally easy to predict. It suggests that there are interactions between nearby trials in the sequence such that the experience of the participant on one trial can influence what happens on successive trials in ways that are not captured by the grammar predictions about those trials. \n\nWe hypothesized that an error has a perturbing effect: it should knock the system off the path of correct prediction on the trials immediately following the error (T 23 s and T 34 trials), but the degree of distortion should diminish as the trials progress. We call this account \"Dynamical Stability.\" \n\nTo test the hypothesis, we first compared trial-level prediction accuracy for T 23 and T 34 ; the transition probability in Sequence 3 is 1 for both transitions. If noise is purely random, prediction accuracy should be roughly equal for both transition types. Binary trial-level prediction accuracy was modeled as a function of WordType (dummy-coded with word 2 as reference level); a by-subject random intercept, a by-subject random slope of WordType, and the correlation between them were included as random effects.",
            "score": 0.3832814172581209,
            "section_title": "Result (iii): Evidence for Dynamical Stability",
            "char_start_offset": 61237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1257
                },
                {
                    "start": 1260,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1561
                },
                {
                    "start": 1564,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 2082
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07366943359375
        },
        {
            "corpus_id": "257405439",
            "title": "Automatically Auditing Large Language Models via Discrete Optimization",
            "text": "Our experiments audit autoregressive language models, which compute probabilities of subsequent tokens given previous tokens. We report numbers on the 762M-parameter GPT-2-large [Radford et al., 2019] and 6B-parameter GPT-J [Wang and Komatsuzaki, 2021] hosted on HuggingFace [Wolf et al., 2019]. For all experiments and all algorithms, we randomly initialize prompts and outputs, then optimize the objective until both f (x) = o and \u03c6(x, o) is su ciently large, or we hit a maximum number of iterations. See Appendix B.1 for additional details and hyperparameters.",
            "score": 0.3832814172581209,
            "section_title": "Setup",
            "char_start_offset": 18956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 564
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 200,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79248046875
        },
        {
            "corpus_id": "270062442",
            "title": "Exploring the LLM Journey from Cognition to Expression with Linear Representations",
            "text": "This subsection explores the significance of the vocabulary linear layer in shaping the expressive capabilities of a model.Positioned as the final MLP (Multi-Layer Perceptron) layer with trainable parameters within a decoder-only LLM, the vocabulary linear layer maps the last transformer block's output to the vocabulary space T .During the model's greedy output generation, this layer effectively assesses the simi-larity between its input and each of its row vectors (each corresponding to a token in the vocabulary), ultimately selecting the token that exhibits the highest similarity for its prediction.This process not only underpins the model's ability to generate coherent and contextually relevant text but also forges a structural link between cognitive and expressive capabilities.Earlier sections have illustrated that as the model approaches the convergence point during Pretraining, the output performance of the last few transformer blocks stabilizes, reflecting the model's cognitive capabilities.Consequently, the effective training of the vocabulary linear layer is paramount, as it directly influences the model's ability to articulate its 'thoughts' and knowledge accurately.\n\nTo demonstrate the dynamics of the vocabulary linear layer, we designed an experiment using a set of prompts {x i } m i=1 and performed inference across a series of models.We define the output from the final transformer block for each prompt x i in model M as c \u22121 (x i , M).The weights of model M's vocabulary linear layer are represented by Vocal(M).We used a series of Baichuan-33B models {M j |j = 1, 2, . . ., n} across Pretraining, SFT, and RLHF stages.An average output c \u22121\n\n) is computed.Subsequently, Vocal(M i ) was adjusted to examine the resulting output distribution through:\n\nwhere c \u22121 (x i ) acts as a consistent reference for input x i across all models, enabling the assessment of changes in the vocabulary linear layer via the variation in average KL divergence:\n\nThe results are depicted in Figure 7.",
            "score": 0.3832814172581209,
            "section_title": "Establishment of Expressive Capability",
            "char_start_offset": 22629,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 123,
                    "end": 331
                },
                {
                    "start": 331,
                    "end": 608
                },
                {
                    "start": 608,
                    "end": 792
                },
                {
                    "start": 792,
                    "end": 1013
                },
                {
                    "start": 1013,
                    "end": 1195
                },
                {
                    "start": 1197,
                    "end": 1369
                },
                {
                    "start": 1369,
                    "end": 1472
                },
                {
                    "start": 1472,
                    "end": 1549
                },
                {
                    "start": 1549,
                    "end": 1610
                },
                {
                    "start": 1610,
                    "end": 1656
                },
                {
                    "start": 1656,
                    "end": 1678
                },
                {
                    "start": 1680,
                    "end": 1694
                },
                {
                    "start": 1694,
                    "end": 1786
                },
                {
                    "start": 1788,
                    "end": 1979
                },
                {
                    "start": 1981,
                    "end": 2018
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8779296875
        },
        {
            "corpus_id": "274527400",
            "title": "The analysis of generative adversarial network in sports education based on deep learning",
            "text": "The pre-training of the model's generator employs the Negative Log Likelihood Loss (NLL). Subsequently, the pre-trained generator serves as the foundation for the pre-training of the discriminator. The training of the discriminator involves the amalgamation of generated and real texts in a non-sequential manner, aiming to minimize cross-entropy. Consequently, the training dynamics of both the generator and discriminator are illustrated in Fig. 4. In Fig. 4, the textual composition of the dataset is not overly complex. Through multiple experimental iterations, the pre-training of the generator stabilizes after the 10th epoch, indicating that even with additional iterations, the loss does not exhibit a significant decrease. The pre-training of the discriminator begins to gradually stabilize after the 35th epoch.",
            "score": 0.3832814172581209,
            "section_title": "Model pre-training",
            "char_start_offset": 15598,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 821
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.437255859375
        },
        {
            "corpus_id": "257804771",
            "title": "Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning",
            "text": "Understanding the limitations of large language models (LLMs) becomes ever more important with their accelerated adoption and application to reallife tasks. After the original discovery that large LMs could perform simple NLP tasks without additional training (Radford et al., 2019), the use of these models has rapidly grown, as have their capabilities (Brown et al., 2020;Sanh et al., 2021;Chowdhery et al., 2022). As the community invests considerable effort into creating, training, and deploying these models (Zhang et al., 2022;Black et al., 2021), it is important to understand the types of data and tasks they might not be well-suited. \n\nThe field of analysis of pre-trained models has grown rapidly in recent years (Zagoury et al., 2021;Liu et al., 2021;Lialin et al., 2022;bench authors, 2023;Rogers et al., 2020). Methods such as attention pattern analysis (Kovaleva et al., 2019;Kobayashi et al., 2020), linear probing (Tenney et al., 2019), and zero-shot probing (Belinkov et al., 2020;Talmor et al., 2019;Ettinger, 2019;Lialin et al., 2022) allow us to evaluate specific capabilities of pre-trained models. Zero-shot methods give us arguably the most clear picture, as they directly probe what the model learned through the upstream task and allow the researcher to target very specific skills such as understanding of negation or role. \n\nHowever, even though these methods do not require training data, producing a good dataset for zero-shot evaluation of these language models is not an easy task. We want these datasets to be clean, diverse and to have enough statistical power to be useful for model comparison (Card et al., 2020). Many existing probing datasets struggle with at least one of these requirements. \n\nPsycholinguistic datasets used in a study by Ettinger (2019) have been particularly interesting in that they enabled a comparison between model behavior and human response, including both N400 effects and well-reasoned cloze judgments by human speakers.",
            "score": 0.3832814172581209,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 643
                },
                {
                    "start": 646,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1350
                },
                {
                    "start": 1353,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1730
                },
                {
                    "start": 1733,
                    "end": 1986
                }
            ],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 374,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 724,
                    "end": 746,
                    "matchedPaperCorpusId": "233024793"
                },
                {
                    "start": 746,
                    "end": 763,
                    "matchedPaperCorpusId": "233289478"
                },
                {
                    "start": 763,
                    "end": 783,
                    "matchedPaperCorpusId": "248780409"
                },
                {
                    "start": 868,
                    "end": 891,
                    "matchedPaperCorpusId": "201645145"
                },
                {
                    "start": 891,
                    "end": 914,
                    "matchedPaperCorpusId": "222176890"
                },
                {
                    "start": 931,
                    "end": 952,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 976,
                    "end": 999,
                    "matchedPaperCorpusId": "263866950"
                },
                {
                    "start": 999,
                    "end": 1019,
                    "matchedPaperCorpusId": "209515274"
                },
                {
                    "start": 1019,
                    "end": 1034,
                    "matchedPaperCorpusId": "199001173"
                },
                {
                    "start": 1034,
                    "end": 1054,
                    "matchedPaperCorpusId": "248780409"
                },
                {
                    "start": 1629,
                    "end": 1648,
                    "matchedPaperCorpusId": "222341713"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5791015625
        },
        {
            "corpus_id": "261030259",
            "title": "Latent State Models of Training Dynamics",
            "text": "Our work assumes that training dynamics can be represented by a linear, discrete, and Markovian model. Despite the successes of our approach, a higher-powered model might capture even more information about training dynamics. Relaxing the assumptions of the HMM is likely a fruitful area for future work. Additionally, in this work we perform dimensionality reduction via hand-picked metrics. We use these metrics as interpretable features for our training maps, but a fully unsupervised approach without explicit metrics also deserves exploration. For very large models, training an HMM across many random seeds may be infeasible. \n\nA possible follow-up work could look at whether models of training dynamics can generalize zero-shot across architectures and architecture sizes (Yang et al., 2021). If this were the case, then one could reuse dynamics models to interpret training. \n\nFinally, our findings are suggestive for future work on hyperparameter search. We demonstrate that 1) training instability to random seed is highly dependent on hyperparameters, and 2) instability manifests early in training. Thus, it may be more efficient to measure early variation across a few seeds to quickly evaluate a hyperparameter setting, rather than waiting to measure the final evaluation accuracy on the trained model.",
            "score": 0.3832814172581209,
            "section_title": "Limitations and Future Work",
            "char_start_offset": 26697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1316
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.363525390625
        },
        {
            "corpus_id": "268157336",
            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
            "text": "Kaplan et al. [97] analyzed the effect of model size, computing power, and training data on the performance of language models. The key finding from [97] was that LM performance improves smoothly and predictably as model size, data, and computation are scaled up appropriately. Additionally, large models were more sample-efficient than smaller models as they reached the same level of performance with fewer data points and optimization steps. As per [97], most of the increase in computation should go towards increasing the model size. Also, a relatively small increase in data is needed to avoid reuse, where larger batch sizes can help to boost and increase parallelism. Additionally, larger batches and training for more steps become possible as more computing becomes available. T5 [25] conducted experiments that started with the baseline model having 220 M parameters and then scaled it up to a model with 11 B parameters. The experiments conducted in T5 showed how performance degraded as the data size shrank and improved with the increase in model size and training time.",
            "score": 0.3831916719970929,
            "section_title": "Model Width (Parameter Size)",
            "char_start_offset": 88370,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1083
                }
            ],
            "ref_mentions": [
                {
                    "start": 789,
                    "end": 793,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.955078125
        },
        {
            "corpus_id": "257505035",
            "title": "The Life Cycle of Knowledge in Big Language Models: A Survey",
            "text": "Denoising Autoencoder first corrupts the input sequence with randomly mask symbols, then feed the input into a bidirectional encoder, and the likelihood of the whole original input is calculated with an auto-regressive decoder (Lewis et al., 2020a). \n\nAlthough PLMs are pre-trained without any supervision from external knowledge sources, they have been shown to capture a diverse range of knowledge within their parameters, such as linguistic knowledge (Lin et al., 2019;Tenney et al., 2019;Liu et al., 2019b;Htut et al., 2019;Hewitt and Manning, 2019;Goldberg, 2019;Warstadt et al., 2019), semantic knowledge (Tenney et al., 2019;Wallace et al., 2019;Ettinger, 2020) and world knowledge (Davison et al., 2019;Bouraoui et al., 2020;Forbes et al., 2019;Zhou et al., 2020b;Roberts et al., 2020;Lin et al., 2020a;Tamborrino et al., 2020). \n\nIntuitively, PLMs learn such knowledge because they can abstract, generalize and store the implicit knowledge in the text through self-supervised learning. Unfortunately, the underlying mechanism of how and why PLMs acquire or forget knowledge still remains to be explored. And it will be valuable to understand the behaviors of PLMs and inspire better knowledge acquisition strategies. \n\nTo understand the underlying mechanisms, some studies dive into the dynamics of LMs' pre-training procedure. Many researchers study the training dynamics of neural networks. For example, Achille et al. (2019) try to figure out whether there exist critical periods in the learning process of neural networks. Liu et al. (2021c) devote to finding a mathematical solution for the semantic development in deep linear networks. Other studies (Saphra andLopez, 2019, 2020) analyze the training dynamics of LSTM (Hochreiter and Schmidhuber, 1997) with techniques such as SVCCA (Raghu et al., 2017). \n\nWhile most existing studies focus on neural networks with relatively simple architectures. Only a few studies consider knowledge in large-scale pretrained language models.",
            "score": 0.3829565141121337,
            "section_title": "Learning from Text Data",
            "char_start_offset": 13596,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 252,
                    "end": 836
                },
                {
                    "start": 839,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1819
                },
                {
                    "start": 1822,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 248,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 454,
                    "end": 472,
                    "matchedPaperCorpusId": "174799346"
                },
                {
                    "start": 472,
                    "end": 492,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 492,
                    "end": 510,
                    "matchedPaperCorpusId": "84841767"
                },
                {
                    "start": 528,
                    "end": 553,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 553,
                    "end": 568,
                    "matchedPaperCorpusId": "58007068"
                },
                {
                    "start": 568,
                    "end": 590,
                    "matchedPaperCorpusId": "202233787"
                },
                {
                    "start": 611,
                    "end": 632,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 632,
                    "end": 653,
                    "matchedPaperCorpusId": "202583694"
                },
                {
                    "start": 653,
                    "end": 668,
                    "matchedPaperCorpusId": "199001173"
                },
                {
                    "start": 689,
                    "end": 711,
                    "matchedPaperCorpusId": "202541043"
                },
                {
                    "start": 772,
                    "end": 793,
                    "matchedPaperCorpusId": "211205183"
                },
                {
                    "start": 793,
                    "end": 811,
                    "matchedPaperCorpusId": "218486812"
                },
                {
                    "start": 811,
                    "end": 835,
                    "matchedPaperCorpusId": "216641845"
                },
                {
                    "start": 1415,
                    "end": 1436,
                    "matchedPaperCorpusId": "108298098"
                },
                {
                    "start": 1536,
                    "end": 1554,
                    "matchedPaperCorpusId": "233289478"
                },
                {
                    "start": 1733,
                    "end": 1767,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1798,
                    "end": 1818,
                    "matchedPaperCorpusId": "23890457"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96142578125
        },
        {
            "corpus_id": "262217060",
            "title": "Sharpness-Aware Minimization and the Edge of Stability",
            "text": "Next, we report on experiments training a language model. As before, we start with SGD, here in Figure 12. \n\nNext, we plot the same quantities when the network is trained with SAM, with \u03c1 = 0.3, in Figure 13. Here, the operator norm of the Hessian is significantly less than when SGD is used, and we see evidence that training in SAM operates at the edge of stability analyzed in Section 2. In Figure 14, we zoom in on the lower part of the curve, and plot the operator norm of the Hessian, to examine the relationship between this quantity and the SAM edge in more detail. \n\nFigure 15 contains plots of the training loss, once again estimated per-minibatch. We included these mainly to motivate the combinations of hyperparameters where we examined other aspects of the dynamics of SAM. As expected, while SAM does take longer to achieve a certain loss, it ultimately achieves training error similar to SGD, but with less sharpness. \n\nFigure 16 contains plots of the alignment, once again estimated per-minibatch. For the large learning rates, late in training, despite the sampling noise arising from the use of minibatches, we see a systematic tendency for the SAM gradients align more closely with the principal eigenvector of the Hessian than the gradients at the initial solution. However, for the smallest learning rates, the opposite holds. Figure 16: Alignments between gradients and the principal direction of the Hessian in the language modeling experiments.",
            "score": 0.3826298782839983,
            "section_title": "Language modeling",
            "char_start_offset": 14963,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 58,
                    "end": 106
                },
                {
                    "start": 109,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 933
                },
                {
                    "start": 936,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1469
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98291015625
        },
        {
            "corpus_id": "273025796",
            "title": "An Information Theory of Compute-Optimal Size Scaling, Emergence, and Plateaus in Language Models",
            "text": "Michaud et al. [21] assume power-law scaling and that each text piece contains only one skill, which may be very different than real-world scenarios. Moreover, inverse polynomial loss scaling is interpreted as the average behavior of emergence at different scales. These existing frameworks explain neither the Chinchilla rule nor the plateau phenomenon. These three frameworks abstract the gradient dynamics of language model training [2]; an alternate mathematical framework considers dynamics to explain the Chinchilla rule and loss function plateaus but does not consider emergence [6]. \n\nOur information-theoretic approach is inspired by skill-text bipartite graph frameworks of [2,18,21] and is closest to [18]. We make a small modification by separating notions of concepts and skills, as in well-established human cognitive architectures [22] that have simple hierarchies [17,1,16]. In our framework, skills are not directly learned from text; rather, concepts are learned from texts and skills at different levels are learned from concepts (see Section 2 for a detailed description). That is, our framework takes on the notion of skill-quanta from [21], and so the number of concepts a language model can learn is proportional to the model size. \n\nThe key difference in our work is to have much more detailed and expressive analysis using nonasymptotic techniques rather than asymptotic ones [8]. Indeed, such finitary analysis is necessary to even consider size scaling. Recall that [2,18] assume Chinchilla scaling, whereas we derive it without it being built into our framework. \n\nThe main contributions of this paper are as follows. \n\n1. We propose a simple unified mathematical framework that considers a language model's learning of concepts from texts and composition of skills from concepts. 2. Using this framework and tools from non-asymptotic information theory, we deduce compute-optimal scaling in language models. 3. With the help of random network theory, we provide a simple explanation for emergent abilities of language models in complex tasks when their sizes exceed a certain threshold. 4. We show that plateauing of performance with size-scaling is just a consequence of diversity of skills required for a task.",
            "score": 0.38223828266171095,
            "section_title": "Introduction",
            "char_start_offset": 2093,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 590
                },
                {
                    "start": 593,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1254
                },
                {
                    "start": 1257,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1590
                },
                {
                    "start": 1593,
                    "end": 1645
                },
                {
                    "start": 1648,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2241
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 19,
                    "matchedPaperCorpusId": "257687665"
                },
                {
                    "start": 586,
                    "end": 589,
                    "matchedPaperCorpusId": "267406160"
                },
                {
                    "start": 690,
                    "end": 693,
                    "matchedPaperCorpusId": "257687665"
                },
                {
                    "start": 880,
                    "end": 884,
                    "matchedPaperCorpusId": "10546302"
                },
                {
                    "start": 886,
                    "end": 889,
                    "matchedPaperCorpusId": "1463322"
                },
                {
                    "start": 1157,
                    "end": 1161,
                    "matchedPaperCorpusId": "257687665"
                },
                {
                    "start": 1401,
                    "end": 1404,
                    "matchedPaperCorpusId": "7001913"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9716796875
        },
        {
            "corpus_id": "264405937",
            "title": "Enhancing Zero-Shot Crypto Sentiment With Fine-Tuned Language Model and Prompt Engineering",
            "text": "Recently, there has been a significant emphasis on pre-trained language models (PLMs) that utilize self-supervised learning Discard the response r. \n\nfiltered ). 5: end for 6: Select an instruction y \u2208 Y . 7: Let function A : (x, y) \u2192 x which augments each entry \n\nx \u2208 X with an instruction y. 8: for i \u2190 (1 . . . len(X cleaned )) do 9: \n\ncleaned , y). 10: end for 11: Return Augmented dataset X augmented on extensive raw text data [38]. Notable examples of such models include GPT-3 [22], PaLM [39], Chinchilla [40], LLaMA [23], and Falcon 40B [41]. By training on largescale texts using self-learning tasks like masked word prediction, sentence sequence recognition, text completion, and text generation [19], [42] PLMs acquire a comprehensive understanding of language. In addition, these models enhance the semantic representation of words by considering contextual dynamics and provide a unified framework for various NLP tasks. Currently, there are three standard models [43] structures in PLMs: autoregressive language models, autoencoding language models, and hybrid language models. Representative models for each design are GPT [22], BERT [20], and T5 [21], respectively. Autoregressive language models follow a standard approach where language modeling is done decoder-only, predicting words one by one through one-way Algorithm 3 Fine-tuning of a pre-trained language model. Constant random_seed, input sequence size, number of layers, number of hidden layer nodes, number of classifier outputs. \n\nInput Training set (X \n\nend for 7: \n\nloss \u2190 E(Y \n\nCalculate \u2207\u0398 for backpropagation. 9: \n\nAdjust parameters using an optimizer to minimize the loss. \n\n10: \n\nend for 11: end for 12: Return Task-specific Fine-tuned model. language encoding-decoding and token-by-token prediction of words. Autoencoding language models randomly mask words in a sentence, use bidirectional encoding to capture context, and then predict the masked words based on the encoded information. Finally, hybrid language models combine the approaches of the previous two models.",
            "score": 0.38204531814687137,
            "section_title": "C. PRE-TRAINED LANGUAGE MODELS (PLMS)",
            "char_start_offset": 18789,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 150,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 262
                },
                {
                    "start": 265,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 336
                },
                {
                    "start": 339,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1508
                },
                {
                    "start": 1511,
                    "end": 1532
                },
                {
                    "start": 1535,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1558
                },
                {
                    "start": 1561,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1597
                },
                {
                    "start": 1600,
                    "end": 1658
                },
                {
                    "start": 1661,
                    "end": 1664
                },
                {
                    "start": 1667,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2058
                }
            ],
            "ref_mentions": [
                {
                    "start": 707,
                    "end": 711,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 978,
                    "end": 982,
                    "matchedPaperCorpusId": "258447166"
                },
                {
                    "start": 1163,
                    "end": 1167,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8271484375
        },
        {
            "corpus_id": "268041376",
            "title": "Towards Optimal Learning of Language Models",
            "text": "This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an\"LM-training-as-lossless-compression\"view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task. Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods. Our code can be found at https://aka.ms/LearningLaw.",
            "score": 0.3817591764273396,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.986328125
        },
        {
            "corpus_id": "268379604",
            "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
            "text": "We consider three realistic continual pre-training settings in the main body and provide results for a third which we believe is less warranted in the appendix. Each setting was carefully selected to expose different challenges and strengths of continual pre-training. Our setups assume that continually pre-trained LLMs undergo two or more pre-training phases sequentially. At the start of each phase, we reset the optimizer states, since optimizer states may not always be available, e.g. when using open-weight models from HuggingFace. That is, our results apply to situations where a continually pre-trained LLM is randomly initialized and pre-trained on datasets D 0 , D 1 , . . . , D N \u22121 in sequence where N \u2265 2. For the realistic settings we consider tokens(D i ) \u2265 100B. In each case, we consider the following natural baselines: \n\n\u2022 A model trained from random initialization on the union of all datasets i.e. \n\nN \u22121 i=0 D i , and \u2022 A model trained from random initialization on individual dataset D i , 0 \u2264 i \u2264 N . N = 2 settings -Here we assume a model is available (e.g. via hugging face or pre-trained in-house) that has been pre-trained for autoregressive language modeling on a dataset (D 0 ) using a linear warmup and cosine decay LR schedule. We also assume that the schedule follows existing conventions in the literature (e.g. decaying to the token budget; see Sec. 4 for details) as it is the case for most performant pre-trained LLMs (Rae et al., 2021;Hoffmann et al., 2022;Touvron et al., 2023a;b). Given a model pre-trained on D 0 , we now assume that a practitioner wants to update this model on a new dataset D 1 using the same self-supervised objective. We consider the following concrete variations of the two-dataset setting:",
            "score": 0.3815594082946892,
            "section_title": "Continual Learning Settings",
            "char_start_offset": 16500,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 838
                },
                {
                    "start": 841,
                    "end": 919
                },
                {
                    "start": 922,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1754
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89208984375
        },
        {
            "corpus_id": "260438420",
            "title": "Learning to Model the World with Language",
            "text": "The world model learns to predict the sequence of multimodal representations, which enables it to plan and ground language. The sequence model produces \u1e91t from the current model state (z t\u22121 , h t\u22121 ) and the current action a t\u22121 , which is trained to match the actual representation at the next timestep z t . Concretely, the future prediction objective is: \n\nwhere the gradient around the target distribution for z t is stopped since it is also a learned representation and \u03b2 pred = 0.5 is a hyperparameter. Intuitively, the codes z t contains both information from current observation and additional information that may be required to predict the reward and episode continuation. By training the world model to make predictions \u1e91t of its future representations, it effectively learns to predict future images, language, and rewards, encouraging the agent to extract information from language and learn the correlations between its multiple modalities. For example, when the language input describes that \"the book is in the bedroom\" and the agent later on visually observes the book, the agent will learn this multimodal association even if the reward signal does not directly relate the two.The world model is trained to optimize the overall loss L repr + L pred with respect to all its parameters. \n\nSingle-Modality Pretraining One potential benefit of separating world modeling from policy learning is that the world model can be trained offline, benefitting from largescale text-only and video-only datasets without actions. To pretrain the world model with text-only data as in Section 4.6, we zero out the image and action inputs and set the image, reward, and continuation decoder loss coefficients to 0 so the pretraining focuses on learning to represent text and text dynamics (i.e. language modeling). Dynalang can then be finetuned on experience with all modalities (language, images, and actions) by initializing the actor and critic from scratch, while continuing to train the world model. Note that unlike the typical language modeling objective, the model is not explicitly trained to predict the next token from the prefix, except through the next-representation prediction.",
            "score": 0.3813653949209649,
            "section_title": "Future prediction",
            "char_start_offset": 15504,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1303
                },
                {
                    "start": 1306,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2194
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.873046875
        },
        {
            "corpus_id": "270559235",
            "title": "How Do Large Language Models Acquire Factual Knowledge During Pretraining?",
            "text": "Recently, there has been a surge in interest in LLMs [9,13,21,23,49]. [23] and [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus. Extensive studies have examined the knowledge encoded in the parameters of LLMs [36,40]. [3], [15], [16], [19], [20], and [31] examined how language models learn and capture factual knowledge presented in training data. [4] demonstrated that knowledge should be presented in a diverse format during pretraining to be reliably extracted. However, recent investigations on LLMs have revealed that LLMs show poor acquisition of long-tail knowledge [26,34]. In addition, LLMs cannot manipulate knowledge from pretraining data effectively [5]. These works have mainly focused on investigating the factual knowledge encoded in LLMs after pretraining is complete. To examine the detailed training dynamics of knowledge acquisition during pretraining, we conduct a fine-grained analysis of factual knowledge acquisition on each piece of factual knowledge. \n\nMemorization and forgetting are closely related to knowledge acquisition in neural networks [6]. LLMs memorize a significant amount of training data [10,29], and the tendency to memorize training data increases as the size of the model gets larger, without harming the ability to generalize the knowledge [7,11]. In addition, [17] theoretically demonstrated that a specific degree of memorization is essential for attaining high performance. Notably, [46] conducted an extensive analysis of the behavior of LLMs on memorization and forgetting across various pretraining conditions. \n\nSeveral studies have investigated the training dynamics of LLMs, specifically how they evolve during training [12,18,22,32,33,45,51]. [44] and [46] focused on the dynamics of memorization in language model pretraining. Recently, [53] explored the relationship between the data size and grokking [37]. Compared to these, we perform a more detailed analysis of the dynamics of factual knowledge acquisition during LLM pretraining, by evaluating the log probability of individual pieces of factual knowledge at each training step.",
            "score": 0.3812564954333603,
            "section_title": "Related Work",
            "char_start_offset": 2942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1083
                },
                {
                    "start": 1086,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1667
                },
                {
                    "start": 1670,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2197
                }
            ],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 56,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 56,
                    "end": 59,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 320,
                    "end": 323,
                    "matchedPaperCorpusId": "211205183"
                },
                {
                    "start": 330,
                    "end": 334,
                    "matchedPaperCorpusId": "233296761"
                },
                {
                    "start": 342,
                    "end": 346,
                    "matchedPaperCorpusId": "229923720"
                },
                {
                    "start": 348,
                    "end": 352,
                    "matchedPaperCorpusId": "258417932"
                },
                {
                    "start": 358,
                    "end": 362,
                    "matchedPaperCorpusId": "247839380"
                },
                {
                    "start": 681,
                    "end": 685,
                    "matchedPaperCorpusId": "253522998"
                },
                {
                    "start": 685,
                    "end": 688,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 1178,
                    "end": 1181,
                    "matchedPaperCorpusId": "11455421"
                },
                {
                    "start": 1235,
                    "end": 1239,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1239,
                    "end": 1242,
                    "matchedPaperCorpusId": "235829052"
                },
                {
                    "start": 1412,
                    "end": 1416,
                    "matchedPaperCorpusId": "186206616"
                },
                {
                    "start": 1537,
                    "end": 1541,
                    "matchedPaperCorpusId": "248986465"
                },
                {
                    "start": 1780,
                    "end": 1784,
                    "matchedPaperCorpusId": "261822542"
                },
                {
                    "start": 1787,
                    "end": 1790,
                    "matchedPaperCorpusId": "227905362"
                },
                {
                    "start": 1790,
                    "end": 1793,
                    "matchedPaperCorpusId": "233289478"
                },
                {
                    "start": 1796,
                    "end": 1799,
                    "matchedPaperCorpusId": "247656607"
                },
                {
                    "start": 1804,
                    "end": 1808,
                    "matchedPaperCorpusId": "247450508"
                },
                {
                    "start": 1813,
                    "end": 1817,
                    "matchedPaperCorpusId": "248986465"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97900390625
        },
        {
            "corpus_id": "273351173",
            "title": "Tending Towards Stability: Convergence Challenges in Small Language Models",
            "text": "Increasing the number of parameters in language models is a common strategy to enhance their performance. However, smaller language models remain valuable due to their lower operational costs. Despite their advantages, smaller models frequently underperform compared to their larger counterparts, even when provided with equivalent data and computational resources. Specifically, their performance tends to degrade in the late pretraining phase. This is anecdotally attributed to their reduced representational capacity. Yet, the exact causes of this performance degradation remain unclear. We use the Pythia model suite to analyse the training dynamics that underlie this phenomenon. Across different model sizes, we investigate the convergence of the Attention and MLP activations to their final state and examine how the effective rank of their parameters influences this process. We find that nearly all layers in larger models stabilise early in training - within the first 20% - whereas layers in smaller models exhibit slower and less stable convergence, especially when their parameters have lower effective rank. By linking the convergence of layers' activations to their parameters' effective rank, our analyses can guide future work to address inefficiencies in the learning dynamics of small models.",
            "score": 0.38115113924083144,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9931640625
        },
        {
            "corpus_id": "248366317",
            "title": "Reward Reports for Reinforcement Learning",
            "text": "We can draw a direct analogy between the MDP setting and the evaluation of language models. While language models are not Markovian in an exact sense, the notion of \"state\" can be applied to the conversation text at stake in a particular user interaction. The \"observations\" of the language model would consist of the subset of the historic conversation text that fits within the context window (the \"time horizon\") of the language model. Likewise, the \"actions\" taken by a dialogue agent consist of the token sequences that are generated to form responses to user queries. The \"dynamics\" of this system correspond to the user responses to dialogue agent generations -updating the conversation state by moving the sequence of conversation forward. Finally, the performance metrics (e.g. loss and/or regularization function(s), user 'thumbs up'/'thumbs down' feedback) that shape the behaviour of the language model during pre-training, fine-tuning, and subsequent updates, can be considered as a source of scalar \"reward\" feedback that depends on the specific conversation state at a given point in time. This lens is central to the recent surge in the use of reinforcement learning from human feedback (RLHF) [17] to further fine-tune language models with respect to human values.",
            "score": 0.3810242573465261,
            "section_title": "Large Language Models through the RL Lens",
            "char_start_offset": 15004,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1281
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "273026625",
            "title": "Optimizing Prompt Length and Specificity for Enhanced AI Chatbot Responses",
            "text": "The language models tested here, namely, DistilBERT, BERT, RoBERTa, and XLNet, all have quite different performances when viewed theoretically against analytic difference and qualitative insight, rather than manual or automatic evaluation. Here is the analysis on the same: \uf0d8 Building the Theoretical Framework This was important in laying a solid theoretical framework at the beginning to guide expectations about the performance of the models. It gave an indication of how each model was supposed to perform depending on its architecture, training data, and previous research conducted using it with the different prompt types.",
            "score": 0.38055590918577886,
            "section_title": "Evaluation Process",
            "char_start_offset": 9203,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 629
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62939453125
        },
        {
            "corpus_id": "248006342",
            "title": "Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators",
            "text": "Instead of pretraining encoders on texts with random noise (e.g., randomly masked tokens in BERT (Devlin et al., 2019)), ELECTRA-style frameworks employ an auxiliary model to corrupt the original text sequence using its language modeling outputs, and pretrain the main Transformer to detect (Clark et al., 2020) or correct (Meng et al., 2021) the replaced tokens. These new selfsupervised frameworks significantly improve the efficiency and effectiveness of pretraining and lead to strong results in various downstream tasks with fine-tuning (Clark et al., 2020), prompt-based learning (Meng et al., 2021), and zero-shot cross-lingual representations (Chi et al., 2021). \n\nRecent studies revealed that the key to the ELECTRA's success is its new learning dynamics (Xu et al., 2020;Meng et al., 2021). By pretraining the auxiliary model jointly with the main Transformer, an implicit learning curriculum is formed: The noise produced by the auxiliary generator becomes more and more plausible during pretraining, posing greater challenges for the discriminator, which has to overcome the difficulty by reasoning more deeply using the contexts. This leads to significantly improved sample efficiency and effectiveness of ELECTRA-style pretrained models (Clark et al., 2020;Chi et al., 2021;Meng et al., 2021). \n\nOn the other hand, such a training dynamic also introduced new challenges in search of the optimal pretraining setting. First, the configurations of the auxiliary generator-its depth, width, and masking fraction-require costly trial-and-error pretraining runs. At the same time, they also significantly impact the discriminator's downstream task performance: A weak auxiliary model does not generate hard enough pretraining signal to push the discriminator, but a too strong one can confuse the discriminator and worsen its downstream task performance (Clark et al., 2020;Meng et al., 2021).",
            "score": 0.3801274664144457,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1307
                },
                {
                    "start": 1310,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1901
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 118,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9267578125
        },
        {
            "corpus_id": "250390536",
            "title": "Toward Automatic Discourse Parsing of Student Writing Motivated by Neural Interpretation",
            "text": "Recent work has shown there is evidence that neural models may be learning general heuristics and memorizing exceptions to those heuristics that increase performance on a given task (Fiacco et al., 2019a). Assuming this is the case, we attempt to exploit this behavior to offload some of the complexity of learning the RST discourse parsing task into multiple phases of training. A fully trained parent model, which includes all of the features in the previous sections, is executed concurrently to the child model and a subset of the parser state of the parent model is concatenated with the parser state of the child model. \n\nThe parser state for the parent model is updated along with the child model using the action chosen by the child model, though with its own stack and buffer representations. This ensures that even if the parent and child models diverge in their predicted actions, the parser states are consistent. Maintaining this consistency is important for the neural transition parser as the representation of the stack can contain a representation of a larger segment of the document than just a single EDU. \n\nNeuron Selection via Pathways: For datasets with noisy data, we prune the parser state from the parent model to only use the dimensions of the state that correspond to the neurons that are part of the neural pathways that explain the most variance of the model. The intuition for this pruning is that the groups of neurons that explain the largest amount of variance in the model will regularize the model via eliminating overfitted parameters. \n\nThese neurons are obtained by extracting the parser state for each training instance and constructing an activation matrix with the dimensions of the parser state as columns and the training instances as rows. A PCA is performed over the matrix, and the subset of resulting factors that cumulatively explain more than a tunable threshold of the variance are chosen as the subset of pathways of interest. For each selected factor, the factor loadings of each neuron are computed and the N neurons with the highest loadings are added to the set of neurons to be transferred. The value of N can be tuned by optimizing performance on a validation set.",
            "score": 0.37955109953353117,
            "section_title": "Parent Parser State",
            "char_start_offset": 20133,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1124
                },
                {
                    "start": 1127,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1571
                },
                {
                    "start": 1574,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 204,
                    "matchedPaperCorpusId": "196211427"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31884765625
        },
        {
            "corpus_id": "236241277",
            "title": "A survey on text generation using generative adversarial networks",
            "text": "A properly trained language model encodes features and rules from the text it was trained. In other words, the model has learned a probabilistic distribution that represents the training data and samples new data. Let P (w t |w t\u22121 , w t\u22122 , . . . , w t\u2212n ) be the probability distribution of a token at t timestep, given an n-sized sequence of previous tokens. The language model estimates the probabilities of a token w t given a previous sequence of tokens, allowing it to be sampled into a new token. Afterward, in t + 1 timestep, the probability distribution tries to represent w t+1 given w t , w t\u22121 , . . . , w t\u2212n+1 . The process is repeated until a convergence criterion is satisfied, such as the number of generated tokens. Finally, the resulting tokens are sequentially arranged to form the generated text.",
            "score": 0.37955109953353117,
            "section_title": "How to Generate Text?",
            "char_start_offset": 9444,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 818
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86181640625
        },
        {
            "corpus_id": "264451722",
            "title": "Unraveling Feature Extraction Mechanisms in Neural Networks",
            "text": "Neural networks have become indispensable across a variety of natural language processing (NLP) tasks. There has been growing interest in understanding their successes and interpreting their characteristics. One line of works attempts to identify possible features captured by them for NLP tasks (Li et al., 2016;Linzen et al., 2016;Jacovi et al., 2018;Hewitt and Manning, 2019;Vuli\u0107 et al., 2020). They mainly develop empirical methods to verify hypotheses regarding the semantic and syntactic features encoded in the output. Such works may result in interesting findings, but those models still remain black-boxes to us. Another line seeks to reveal internal mechanisms of neural models using mathematical tools (Levy and Goldberg, 2014;Saxe et al., 2013;Arora et al., 2018;Bhojanapalli et al., 2020;Merrill et al., 2020;Dong et al., 2021;Tian et al., 2023), which can be more straightforward and insightful. However, few of them have specifically focused on the feature extraction of neural NLP models. \n\nWhen applying neural models to downstream NLP tasks in practice, we often notice some modules perform better than others on specific tasks, while some exhibit similar behaviors. We may wonder what mechanisms are behind such differences and similarities between those modules. By acquiring deeper insights into the roles of those modules in a complex model with respect to feature extraction, we will be able to select or even design more suitable models for downstream tasks. \n\nIn this work, we propose a novel theoretical approach to understanding the mechanisms, through which fundamental models (often used as modules in complex models) acquire features during gradient descent in text classification tasks. The evolution of model output can be described as learning dynamics involving NTKs (Jacot et al., 2018;Arora et al., 2019), which are typically used to study various properties of neural networks, including convergence and generalization. While these representations can be complex in practice, when the width of the network approaches infinity, they tend to converge to less complex representations and remain asymptotically constant (Jacot et al., 2018), allowing us to intuitively interpret the learning dynamics and identify the relevant features cap-tured by the model.",
            "score": 0.37955109953353117,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1005
                },
                {
                    "start": 1008,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1483
                },
                {
                    "start": 1486,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2293
                }
            ],
            "ref_mentions": [
                {
                    "start": 296,
                    "end": 313,
                    "matchedPaperCorpusId": "14099741"
                },
                {
                    "start": 313,
                    "end": 333,
                    "matchedPaperCorpusId": "14091946"
                },
                {
                    "start": 333,
                    "end": 353,
                    "matchedPaperCorpusId": "52346770"
                },
                {
                    "start": 353,
                    "end": 378,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 378,
                    "end": 397,
                    "matchedPaperCorpusId": "222290596"
                },
                {
                    "start": 714,
                    "end": 739,
                    "matchedPaperCorpusId": "1190093"
                },
                {
                    "start": 757,
                    "end": 776,
                    "matchedPaperCorpusId": "3331630"
                },
                {
                    "start": 776,
                    "end": 802,
                    "matchedPaperCorpusId": "211132628"
                },
                {
                    "start": 802,
                    "end": 823,
                    "matchedPaperCorpusId": "215828261"
                },
                {
                    "start": 823,
                    "end": 841,
                    "matchedPaperCorpusId": "232134936"
                },
                {
                    "start": 1802,
                    "end": 1822,
                    "matchedPaperCorpusId": "49321232"
                },
                {
                    "start": 1822,
                    "end": 1841,
                    "matchedPaperCorpusId": "135463287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.822265625
        },
        {
            "corpus_id": "247628243",
            "title": "Evaluating Distributional Distortion in Neural Language Modeling",
            "text": "See A.1.3 for evidence indicating that this underestimation behaviour also occurs in pre-trained models fine-tuned on D train . test by their target sequence probabilities p L (x), and split the probability tuples into 50 equally-sized bins. We plot estimation curves in Figure 3: Each curve represents a 50th of the sequences, with darker curves denoting estimation error for sequences with lower target probabilities (rarer sequences). At any given point, then, the distance between estimation curves represents the degree to which estimation error is dependent on the target probability of the sequence. Our previous experiment trained languages models on a set of 1M sequences. A plausible explanation for the model's underestimation behaviour on unseen test sequences is therefore that the language model has not seen enough samples from the target distribution. Here we explore how Figure 5: GPT2-medium estimation behaviour for 15M sequences in \u03a3 * across two dimensions: sequence rarity (x-axis) and degree of perturbation (y-axis). The heat map is shaded based on estimation error severity; blue areas indicate overestimation, whereas brown areas indicate underestimation. We also include example sequences from two zones in this sequence space. estimation error varies as a function of the amount of training data. We train GPT2-medium, GPT2large and an LSTM model in the online \"Ideal World\" setting (Nakkiran et al., 2020) by sampling, at the beginning of each training iteration, a fresh set of 500,000 sequences from p L , and training M on this sample. Doing so for 60 iterations, we obtain LMs which have been trained on 30 million sequences. We compute model estimation error on D test at the end of each iteration i. Figure 4 visualizes underestimation error throughout training for these LMs. We again split test sequences by their true probability, with darker lines denoting estimation trends for less probable target sequences. \n\nThe estimation curves in Figure 4 suggest that while increasing the amount of data in training initially leads to lower estimation error, this reduction eventually asymptotes. In the insets of Figure 4, we visualize the relative change in mean estimation between epochs i \u2212 1 and i.",
            "score": 0.37955109953353117,
            "section_title": "ESTIMATION ERROR WITH FIXED DATA",
            "char_start_offset": 15763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1949
                },
                {
                    "start": 1952,
                    "end": 2127
                },
                {
                    "start": 2128,
                    "end": 2234
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84130859375
        },
        {
            "corpus_id": "216562482",
            "title": "The Curse of Performance Instability in Analysis Datasets: Consequences, Source, and Suggestions",
            "text": "Neural network models have significantly pushed forward performances on natural language processing benchmarks with the development of largescale language model pre-training (Peters et al., 2018;Radford et al., 2018;Devlin et al., 2019;Radford et al., 2019;Liu et al., 2019b). For example, on two semantically challenging tasks, Natu- ral Language Inference (NLI) and Reading Comprehension (RC), the state-of-the-art results have reached or even surpassed the estimated human performance on certain benchmark datasets (Wang et al., 2019;Rajpurkar et al., 2016aRajpurkar et al., , 2018)). These astounding improvements, in turn, motivate a new trend of research to analyze what language understanding and reasoning skills are actually achieved, versus what is still missing within these current models. Following this trend, numerous analysis approaches have been proposed to examine models' ability to capture different linguistic phenomena (e.g., named entities, syntax, lexical inference, etc.). \n\nThose studies are often conducted in three steps: \n\n(1) proposing assumptions about models' certain ability; \n\n(2) building analysis datasets by automatic generation or crowd-sourcing; \n\n(3) concluding models' certain ability based on results on these analy-sis datasets. \n\nPast analysis studies have led to many key discoveries in NLP models, such as over-stability (Jia and Liang, 2017), surface pattern overfitting (Gururangan et al., 2018), but recently McCoy et al. (2019a) found that the results of different runs of BERT NLI models have large non-negligible variances on the HANS (McCoy et al., 2019b) analysis datasets, contrasting sharply with their stable results on standard validation set across multiple seeds. This finding raises concerns regarding the reliability of individual results reported on those datasets, the conclusions made upon these results, and lack of reproducibility (Makel et al., 2012). Thus, to help consolidate further developments, we conduct a deep investigation on model instability, showing how unstable the results are, and how such instability compromises the feedback loop between model analysis and model development.",
            "score": 0.37955109953353117,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1049
                },
                {
                    "start": 1052,
                    "end": 1108
                },
                {
                    "start": 1111,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1271
                },
                {
                    "start": 1274,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2160
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 195,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 216,
                    "end": 236,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 236,
                    "end": 257,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 537,
                    "end": 560,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 1367,
                    "end": 1388,
                    "matchedPaperCorpusId": "52011616"
                },
                {
                    "start": 1418,
                    "end": 1443,
                    "matchedPaperCorpusId": "4537113"
                },
                {
                    "start": 1587,
                    "end": 1608,
                    "matchedPaperCorpusId": "59599752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58349609375
        },
        {
            "corpus_id": "230433908",
            "title": "Recoding latent sentence representations - Dynamic gradient-based activation modification in RNNs",
            "text": "In this chapter I present the results of several experiments applying the models described in the previous chapter to a Language Modeling task, assessing the impact of important hyperparameters and model components.\n\nThe data set is being introduced first in section 5.1 along with the training conditions in section 5.2. Secondly, I evaluate the different models with a language modeling objective in section 5.3. More precisely, the effect of the most prominent hyperparameters on performances is dissected in sections 5.3.1, 5.3.2 and 5.3.3. Additional experiments in section 5.3.4 focus on the impact of the dynamic step sizes like introduced back in chapter 4.4. Lastly, some ablation experiments try to explain the interplay between the LSTM and the recoder in chapter 5.3.5.\n\nImplementing these models in a framework like PyTorch come with some caveats, which are described in appendix B.2.",
            "score": 0.37955109953353117,
            "section_title": "Experiments",
            "char_start_offset": 67696,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95166015625
        },
        {
            "corpus_id": "271909583",
            "title": "SysBench: Can Large Language Models Follow System Messages?",
            "text": "On a broader scale, we categorize all conversations into multi-turn dependent and multi-turn parallel, as mentioned in \u00a73.1. To evaluate the stability maintenance capability of large language models across multi-turn conversations, we utilize the Session Stability Rate (SSR) as the metric. Additionally, we report the R n values, defining as the percentage of all sessions in which the first n rounds of model responses follow all given constraints. The linear regression slopes of R n (denoted as k) are also reported in Table 4, illustrating the degradation in model performance across successive conversation turns. \n\nThe model GPT-4o and GPT-4-Turbo outperform other models in terms of SSR, while Claude-3 follows closely behind. From the analysis of the slopes, we observe the R n values of multi-turn dependent conversations generally decline more rapidly than those of parallel ones. This difference may be attribute to the simpler and more straightforward instruction in the first turn of multi-turn dependent conversation, while subsequent rounds involve less irrelevant contextual references, which has been well-studied in prior works such as (Shi et al., 2023). Interestingly, some models like GPT-4-turbo do not exhibit significant differentiation between these two categories of conversation, while some other models such as ERINE-4 and Claude-3, perform moderately well in the first round (with high R 1 value), but degraded more rapidly in subsequent dialogues, as evidenced by R n values decaying at a higher rate, resulting in lower SSR. This observation highlights a potential area for improvement in the ability of some models to process multi-turn conversations or manage long contexts within system message constraints. \n\nOverall, the SSR metric measures models from a high-level perspective, focusing on stability across multiple conversation turns. The reported results show the performance differences and reveal variations in each model's ability to maintain stable over multiple rounds with system message. This provides developers with a macro-view reference of each model's capacity, guiding further development and optimization efforts to enhance their performance in complex conversational scenarios, since the best SSR is only 54.4%.",
            "score": 0.37955109953353117,
            "section_title": "MULTI-TURN STABILITY",
            "char_start_offset": 24222,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1742
                },
                {
                    "start": 1745,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2266
                }
            ],
            "ref_mentions": [
                {
                    "start": 1155,
                    "end": 1173,
                    "matchedPaperCorpusId": "256459776"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73193359375
        },
        {
            "corpus_id": "271571298",
            "title": "Big Cooperative Learning",
            "text": "However, the trained model capabilities, e.g., to independently predict the masked 15% tokens conditioned on the unmasked 85% tokens, are often discrepant from those required during the test phase, leading to the well-known problem of training-test (or pretraining-finetuning) discrepancy [47,48]. On the other hand, the next-token-prediction training uses another specific set of data-sampling demonstrations from q(x t |x <t ) to pretrain the auto-regressive model p AR \u03b8 (x t |x <t ) by minimizing E q(x<t) KL[q(x t |x <t )||p AR \u03b8 (x t |x <t )] (see Eqs. ( 3) and ( 4)). Fortunately, the trained model capabilities of various next-token predictions often align closely with those required during testing (considering current applications), which results in a significantly reduced training-test discrepancy than that of mask-and-predict and is likely a key factor contributing to the success of large language models [27,28,34,3]. For better comparison, we summarize the utilized datasampling demonstrations for both mask-and-predict and next-token-prediction, as well as other representative training methods for foundation models, in a consistent manner in Table 1. \n\nConsidering various testing scenarios for foundation models, especially those associated with artificial general intelligence, severe training-test discrepancy may arise from training with a limited portion of the available but underutilized data-sampling demonstrations, pinpointing the insufficiency of the existing training theory for foundation models [7]. We propose to exhaustively exploit these demonstrations during training to significantly expand the training scope to reduce the training-test discrepancy.",
            "score": 0.37955109953353117,
            "section_title": "Versatile but Underutilized Data-Sampling Demonstrations Within a Single Data Sample",
            "char_start_offset": 10605,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1171
                },
                {
                    "start": 1174,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1690
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8564453125
        },
        {
            "corpus_id": "229549248",
            "title": "Predicting speech from a cortical hierarchy of event-based time scales",
            "text": "The objective of our language models was to minimize the difference between the \"predicted\" probability distribution d p (i.e., a vector of probabilities ranging from 0 to 1) and the \"actual\" probability distribution corresponding to the next word in a text (i.e., a vector of zeros with a one-hot encoded target word). To this end, we trained models on mini batches of 16 independent text sequences with 500 words each and monitored model performance by means of categorical cross-entropy between the predicted and actual probability distribution of each word in a sequence. On the basis of model perform ance, trainable parameters were updated after each mini batch using the Adam algorithm for stochastic gradient optimization (87).\n\nOur text corpus comprised more than 130 million words including 4400 political speeches (88) and 1200 fictional and popular scientific books. All texts had at least 500 words; metadata, page numbers, references, and punctuations (except for hyphenated compound words) were removed from documents. A held-out set of 10 randomly selected documents was used for validation after each epoch of training (i.e., going through the complete training set once) and allowed us to detect overfitting on the training set. Training automatically stopped after model performance did not increase over two epochs for the validation dataset.\n\nUsing a context window of 500 words, we aimed at roughly modeling time scales of the length of common linguistic units in written language (i.e., words, phrases, sentences, and paragraphs). Therefore, we only used a small range of values from three to seven to find the number of layers-intended to represent distinct time scales-best suited to make good predictions. In addition, we tuned the number of units in LSTM and HM-LSTM cells of language models, using values from 50 to 500 in steps of 50. Hyperparameters were evaluated on a single epoch using grid search, and the best combination of hyperparameters was chosen on the basis of the performance on the validation set. Our final language models had five LSTM or HM-LSTM layers with 300 units each and an output module. The LSTM model had 31,428,745 and the HM-LSTM model had 31,431,570",
            "score": 0.37955109953353117,
            "section_title": "Training and evaluation of language models",
            "char_start_offset": 55998,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 825,
                    "end": 829,
                    "matchedPaperCorpusId": "21702522"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8779296875
        },
        {
            "corpus_id": "234093986",
            "title": "Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing Regressions In NLP Model Updates",
            "text": "To analyze whether a fine-tuned model can handle linguistic phenomena for a specific end task, perturbation techniques are often used (Belinkov and Bisk, 2018;Ribeiro et al., 2018;Prabhakaran et al., 2019;Wu et al., 2019;Talmor et al., 2020). In particular, CHECKLIST (Ribeiro et al., 2020) leverages and expands those techniques to efficiently evaluate a wide range of linguistic behavioral capabilities of NLP models. Our work applies CHECKLIST to inspect where the model update regressions come from and on which linguistic phenomena our proposed solutions help to reduce regressions.",
            "score": 0.37955109953353117,
            "section_title": "Behavioral Testing of NLP Models",
            "char_start_offset": 22300,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 134,
                    "end": 159,
                    "matchedPaperCorpusId": "3513372"
                },
                {
                    "start": 159,
                    "end": 180,
                    "matchedPaperCorpusId": "21740766"
                },
                {
                    "start": 180,
                    "end": 205,
                    "matchedPaperCorpusId": "203078302"
                },
                {
                    "start": 205,
                    "end": 221,
                    "matchedPaperCorpusId": "196199409"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65478515625
        },
        {
            "corpus_id": "256459309",
            "title": "Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data",
            "text": "Figure 1 provides a basic illustration of how we formulate FLAD as a MAB problem. \n\nTo empirically validate our approaches, we focus on few-shot training of language models and utilize P3 [23], a readily available resource with hundreds of auxiliary language datasets. We evaluate our methods on the same held-out tasks as the T0 language model [16] and show that, when using the same collection of auxiliary datasets, our algorithms outperform a directly fine-tuned T0 by 5.6% (EXP3-FLAD) and 5.7% (UCB1-FLAD) absolute. Furthermore, incorporating all available datasets in P3 (i.e. not just those used to train T0) increases the improvement to 9.1% and 9.2%. Finally, we compare models trained with our methods against state-of-the-art few-shot methods, finding that our methods improve performance by >3%, even though one model utilizes a large collection of unlabeled target dataset samples. Furthermore, to the best of our knowledge, our methods lead to the first 3 billion parameter model that improves over 175B GPT-3 using few-shot in-context learning. \n\nIn summary, our main contributions are: \n\n\u2022 We connect FLAD to the MAB setting and focus on the exploration-exploitation trade-off by designing two algorithms, EXP3-FLAD and UCB1-FLAD along with three reward functions that are both simple and efficient (in space and computational complexity). \u2022 We empirically validate that our methods improve few-shot performance of pretrained language models and show that strategies that employ only exploration or exploitation lead to sub-optimal performance. \u2022 We perform case studies to better understand the dynamics of our reward functions and their interaction with the dynamics of large language model training.",
            "score": 0.3790221436685906,
            "section_title": "Sample a batch",
            "char_start_offset": 3461,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 84,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1718
                }
            ],
            "ref_mentions": [
                {
                    "start": 188,
                    "end": 192,
                    "matchedPaperCorpusId": "246485605"
                },
                {
                    "start": 345,
                    "end": 349,
                    "matchedPaperCorpusId": "239009562"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97021484375
        },
        {
            "corpus_id": "267665577",
            "title": "Exploring emergent syllables in end-to-end automatic speech recognizers through model explainability technique",
            "text": "Automatic speech recognition systems based on end-to-end models (E2E-ASRs) can achieve comparable performance to conventional ASR systems while reproducing all their essential parts automatically, from speech units to the language model. However, they hide the underlying perceptual processes modelled, if any, and they have lower adaptability to multiple application contexts, and, furthermore, they require powerful hardware and an extensive amount of training data. Model-explainability techniques can explore the internal dynamics of these ASR systems and possibly understand and explain the processes conducting to their decisions and outputs. Understanding these processes can help enhance ASR performance and reduce the required training data and hardware significantly. In this paper, we probe the internal dynamics of three E2E-ASRs pre-trained for English by building an acoustic-syllable boundary detector for Italian and Spanish based on the E2E-ASRs\u2019 internal encoding layer outputs. We demonstrate that the shallower E2E-ASR layers spontaneously form a rhythmic component correlated with prominent syllables, central in human speech processing. This finding highlights a parallel between the analysed E2E-ASRs and human speech recognition. Our results contribute to the body of knowledge by providing a human-explainable insight into behaviours encoded in popular E2E-ASR systems.",
            "score": 0.3788125467470333,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79638671875
        },
        {
            "corpus_id": "270620163",
            "title": "Probing the Emergence of Cross-lingual Alignment during LLM Training",
            "text": "As already mentioned in \u00a74.1, we observe that an unexpected drop of cross-lingual alignment may appear during the training process, suggesting that multilingual LMs can pass through highly suboptimal regions in the loss landscape.As depicted by the drop points in Figure 1 and Figure 3, intermediate models saved on global step 600k of BLOOM 560m and 400k of BLOOM 1b1 have common traits: 1) there is nearly no neuron overlap detected in these models; 2) These models display weak zero-shot transfer abilities since their performance on target languages are mostly random, as shown in Figure 5; 3) These models' performance becomes worse also in-language, showing a performance degradation in the source language English.\n\nSeveral works studying linguistic acquisition in LMs across time find that the risk of models falling into bad minima depends on their scale.Xia et al. (2023) conduct a study on the training dynamics across scales in monolingual LMs.They find that while all models decrease their perplexity for hallucinated texts at the start of training, only large-scale models eventually escape this sub-optimal distribution.Conneau et al. (2020a) introduce the curse of multilinguality, which refers to the phenomenon that given a fixed number of parameters, continued increase in the number of languages leads to a performance degradation in terms of both monolingual and cross-lingual skills.Consequently, they argue that this bottleneck could be solved by increasing model scales.\n\nOur experimental results on the dynamics of cross-lingual alignment are in agreement with the aforementioned works.We suggest that crosslingual alignment follows the same trajectory of learning across scales, similar to what is observed in Chen et al. (2024).However, smaller scales appear more likely to pass through or fall into suboptimal parameter configurations, which lead to a simultaneous degradation in both in-language and cross-lingual abilities.Our work thus offers a more nuanced perspective on the multilingual abilities of LMs based on learning dynamics, which enriches the received wisdom based on converged models.",
            "score": 0.3787779412733308,
            "section_title": "Why does the drop point occur?",
            "char_start_offset": 17261,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 721
                },
                {
                    "start": 723,
                    "end": 864
                },
                {
                    "start": 864,
                    "end": 956
                },
                {
                    "start": 956,
                    "end": 1135
                },
                {
                    "start": 1135,
                    "end": 1405
                },
                {
                    "start": 1405,
                    "end": 1494
                },
                {
                    "start": 1496,
                    "end": 1611
                },
                {
                    "start": 1611,
                    "end": 1755
                },
                {
                    "start": 1755,
                    "end": 1953
                },
                {
                    "start": 1953,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 864,
                    "end": 881,
                    "matchedPaperCorpusId": "254877112"
                },
                {
                    "start": 1135,
                    "end": 1157,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 1736,
                    "end": 1754,
                    "matchedPaperCorpusId": "261822542"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9228515625
        },
        {
            "corpus_id": "252815771",
            "title": "A Kernel-Based View of Language Model Fine-Tuning",
            "text": "We use NTKs to mathematically formalize the general intuition that fine-tuning pretrained language models to solve downstream tasks requires only a \"small change.\" We derive a new kernel to describe Adam training (Section 4) and we use it in Section 5 to show how prompt-based fine-tuning can exhibit kernel behavior. Extensive experiments in Section 6 on 14 NLU tasks demonstrate that including a meaningful prompt often causes FT to exhibit kernel behavior ( Figure 1) and that kernel dynamics describe prompt-based FT on tasks that the eNTK can solve (Section 6.2). We demonstrate one possible use of the kernel view to explain empirical phenomena by applying it to understand subspace-based fine-tuning methods (Section 7), and we note that the kernel has many mathematically useful properties that can aid design and study of alternate fine-tuning methods.\n\nOur work suggests that a kernel-based view of language model fine-tuning is plausible, but there are several limitations. First, our experiments are limited to few-shot classification tasks and a single masked language model with specific prompts. Extending to additional settings (e.g., increasing k) and models require significant computational cost because the eNTK is expensive to compute. The theoretical results also apply only to \"early-stage\" training with Adam, and it is not clear how well they can describe longer training schemes; concurrent work in Littwin & Yang (2023) suggests that the reduction of Adam to SignGD is crucial to observe kernel dynamics. Nevertheless, our work provides substantial empirical and theoretical evidence that fine-tuning can be analyzed in terms of kernel behavior.\n\nAs a future direction, one can use the kernel analog to study the inductive bias of FT, as was done for gradient descent from a random initialization in the past ( Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv. Yang, G., Hu, E. J., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., Ryder, N.,",
            "score": 0.3781483152221908,
            "section_title": "Conclusion",
            "char_start_offset": 32551,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98779296875
        },
        {
            "corpus_id": "269921716",
            "title": "Unveiling Key Aspects of Fine-Tuning in Sentence Embeddings: A Representation Rank Analysis",
            "text": "The purpose of fine-tuning a language model is to leverage the language model's pre-learned linguistic abilities.However, due to the use of cosine similarity as the measure of sentence embedding, fine-tuning with contrastive loss becomes imperative.For the case of BERT, it becomes essential to enhance uniformity through fine-tuning despite the possibility of negatively affecting BERT's linguistic abilities.To explore linguistic aspects, we followed the methodology of Conneau et al. (2018) and utilized SentEval toolkit \u2021 to investigate training dynamics of ten different linguistic abilities.Based on the observed trends in training dynamics, we grouped the ten probing tasks into three categories, as presented in Figure 4.\n\nThe first group (Figure 4a) consists of three tasks that exhibit deterioration in Phase 1 followed by recovery in Phase 2. They are Length (number of tokens), Depth (depth of sentence structure trees), and TopConstituents (the grammatical structure of sentences), and all three are closely related to the performance of sentence embedding.We emphasize that the worst performance occurs at or near the boundary between Phase 1 and Phase 2 for the three linguistic abilities, indicating a strong correlation with representation rank.The trend of deterioration followed by recovery was also observed for alignment, where the deterioration occurs while uniformity sharply improves.\n\nThe second group (Figure 4b) consists of three tasks that exhibit an upward performance trend in both Phase 1 and Phase 2. They are WordContent (deducing words from sentence representations), \u2021 https://github.com/facebookresearch/SentEval/tree/main/data/probing  SubjNumber, and ObjNumber (matching the number of subjects and objects in sentence clauses, respectively), and all three are intimately related to the sentence embedding task.These three linguistic abilities do not deteriorate in Phase 1 despite uniformity's sharp improvement, suggesting that the three do not form a trade-off relationship with uniformity.\n\nThe third group (Figure 4c) consists of the four remaining tasks.Their performance either deteriorates (BigramShift and CoordinationInversion) or oscillates (Tense and OddManOut) throughout the fine-tuning.",
            "score": 0.3781347436051081,
            "section_title": "Linguistic Abilities",
            "char_start_offset": 11611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 113,
                    "end": 249
                },
                {
                    "start": 249,
                    "end": 410
                },
                {
                    "start": 410,
                    "end": 597
                },
                {
                    "start": 597,
                    "end": 729
                },
                {
                    "start": 731,
                    "end": 1070
                },
                {
                    "start": 1070,
                    "end": 1262
                },
                {
                    "start": 1262,
                    "end": 1408
                },
                {
                    "start": 1410,
                    "end": 1649
                },
                {
                    "start": 1649,
                    "end": 1848
                },
                {
                    "start": 1848,
                    "end": 2030
                },
                {
                    "start": 2032,
                    "end": 2097
                },
                {
                    "start": 2097,
                    "end": 2238
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9873046875
        },
        {
            "corpus_id": "237491997",
            "title": "The Grammar-Learning Trajectories of Neural Language Models",
            "text": "We begin ( \u00a73) by establishing that NLMs learn grammatical phenomena in a consistent order. We evaluate NLMs at different time points along their training, showing that the performance on linguis- tic phenomena across initializations is highly correlated. We further find many similarities in the set of examples that they correctly classify.\n\nStill, models of different architectures learn at a different pace, and hence cannot be directly compared at identical time points. In \u00a73.3, we overcome this by re-scaling the timeline. We then show that despite architectural differences, NLMs present highly correlated performance trajectories. In \u00a73.4, we further demonstrate that even the choice of training data has minor influence on the results. Finally, in \u00a73.5 we show that the learning dynamics essentially follows a single dimension. Namely, where the average performance is similar, success on linguistic phenomena is also similar.\n\nWe proceed by analyzing the early stages of learning in \u00a74. We find that, at first, NLMs rely mostly on local cues and not on word order. They thus resemble bag-of-words models over a window of the preceding tokens. Later stages seem to drift further away from bag-of-words models toward n-gram models, and with time seem to be more sensitive to structural cues. We also find evidence that some latent features that the model learns may not be related to linguistic phenomena.\n\nFinally, in \u00a75 we take the first steps in categorizing linguistic phenomena by their learning trajectories. We identify links between their representations by finding phenomena that progress in unison. For example, we find that morphological phenomena are mostly learned at similar stages. Of particular interest are cases where performance decreases with time, which may suggest either overgeneralization or biases in the BLIMP challenges.",
            "score": 0.3774206056549347,
            "section_title": "Introduction",
            "char_start_offset": 2063,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99267578125
        },
        {
            "corpus_id": "269042762",
            "title": "Rho-1: Not All Tokens Are What You Need",
            "text": "To explore how language models learn at the token level, we initially examined training dynamics, particularly how the token-level loss evolves during usual pretraining. In \u00a72.1, we evaluated the model's token perplexity at different checkpoints and categorized tokens into different types. Our findings reveal that significant loss reduction is limited to a select group of tokens. Many tokens are \"easy tokens\" that are already learned, and some are \"hard tokens\" that exhibit variable losses and resist convergence. These tokens can lead to numerous ineffective gradient updates. \n\nBased on these analyses, we introduce RHO-1 models trained with a novel Selective Language Modeling (SLM) objective3 . As shown in Figure 2 (Right), this approach inputs the full sequence into the model and selectively removes the loss of undesired tokens. The detailed pipeline is depicted in Figure 4: First, SLM trains a reference language model on high-quality corpora. This model establishes utility metrics to score tokens according to the desired distribution, naturally filtering out unclean and irrelevant tokens. Second, SLM uses the reference model to score each token in a corpus using its loss ( \u00a72.2). Finally, we train a language model only on those tokens that exhibit a high excess loss between the reference and the training model, selectively learning the tokens that best benefit downstream applications ( \u00a72.2). \n\nWe show through comprehensive experiments that SLM significantly enhances token efficiency during training and improves performance on downstream tasks. Furthermore, our findings indicate that SLM effectively identifies tokens relevant to the target distribution, resulting in improved perplexity scores on benchmarks for models trained with the selected tokens. \u00a73.2 shows the effectiveness of SLM on math continual pretraining: both 1B and 7B RHO-1 outperform CLM-trained baselines by over 16% on the GSM8k and MATH datasets. SLM reaches baseline accuracy up to 10x faster, as shown in Figure 1. Remarkably, RHO-1-7B matches the state-of-the-art performance of DeepSeekMath-7B using only 15B tokens, compared to the 500B tokens required by DeepSeekMath.",
            "score": 0.37741877661636886,
            "section_title": "body",
            "char_start_offset": 1945,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2175
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98486328125
        },
        {
            "corpus_id": "258564693",
            "title": "Coherent Wave Dynamics and Language Generation of a Generative Pre-trained Transformer",
            "text": "Large language models (LLMs) have demonstrated impressive abilities, surpassing people's expectations with their emergent capabilities Ouyang et al. [2022], Radford et al. [2018Radford et al. [ , 2019b,a],a], Brown et al. [2020], OpenAI [2023], Chowdhery et al. [2022], Wei et al. [2022]. However, the lack of in-depth understanding of their inner workings and technical control over these large models has raised concerns and questions. The sheer size of these models makes training and maintaining them extremely challenging, and finding better solutions to make them smaller and more manageable is critical. However, it is not yet clear how to create smaller models tailored to specific tasks without sacrificing significant functionality. Despite people's frequent interaction with LLMs, their empirical understanding mostly hovers on the surface. The ambiguity and lack of theoretical support surrounding the emergent ability continue to spark controversy. In reality, while people are excited about the models' long sequences of logical reasoning capabilities, they are often surprised by the models' inability to distinguish real information from fake or to provide sensible responses. There is currently no systematic approach to ensure that these large models master common sense. Deepening our understanding of LLMs' operating mechanisms is crucial to address the concerns, questions, and challenges that have arisen due to their emergent abilities. Only by gaining a solid understanding of these models' inner workings can we establish human control over them and develop confidence in their ability to operate in a safe and ethical manner. \n\nOur aim in this study is to deepen our understanding of LLMs by analyzing the hidden state wave dynamics in a small GPT. Specifically, we will focus on the coherence of wave dynamics in terms of cross-channel correlation and individual auto-correlation and investigate its impact on Poisson statistics of language patterns. We will leverage wave patterns, coherence, and clustering to demonstrate a systematic way of identifying and interpreting the functionality of the hidden state channels, paving the way to understand and control higher-level language pattern formation. Additionally, we will investigate the Poisson statistics of spelling errors in text sequence generation across various levels of model training.",
            "score": 0.3773985388493691,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1651
                },
                {
                    "start": 1654,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2229
                },
                {
                    "start": 2230,
                    "end": 2374
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 228,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8857421875
        },
        {
            "corpus_id": "264451707",
            "title": "CLEX: Continuous Length Extrapolation for Large Language Models",
            "text": "However, such methods are designed for extending the context length corresponding to a fixed scaling factor, which either restricts their ability to extrapolate to longer sequences (when using small factors) or impairs the performance even within the native context window (when using large factors) as shown in Figure 1. On the other hand, length extrapolation methods (Press et al., 2022;Sun et al., 2023;Chi et al., 2022;2023), typified by ALiBi (Press et al., 2022), strive to achieve test-time context length extension (i.e., \"training on short, testing on long\") by substituting position embeddings with additional biases, where the biases encode positional information to the attention scores. Despite their impressive capability in language modelling, ALiBi-like methods usually struggle in the practical tasks requiring long-context dependency (Pal et al., 2023) (also see \u00a74.3). \n\nIn this work, we present Continuous Length EXtrapolation (CLEX), a novel approach that efficiently extrapolates the context window of LLMs through continuous PE scaling. Concretely, we propose a unified view of PE scaling via generalising the PE scaling methods to the transition of frequency basis. Upon it, we formulate the PE scaling as a continuous dynamical system, which models the transition of frequency basis through the continuous dynamics over the length scaling factor. We argue that previous PE scaling methods, training models using fixed (discrete) scaling factors, overlook the progressively continuous dynamics over the gradually length-extending process. This ensnares themselves in the aforementioned dilemma between extrapolating the length and preserving the performance within shorter lengths. In contrast, our CLEX exploits a neural ordinary differential equation (ODE) (Chen et al., 2018), parameterised by an up-and-down projection layer with lightweight parameters to learn these continuous dynamics, enabling fine-grained extending to long context. More essentially, by extending the dynamics beyond training length, CLEX empowers models to progressively extrapolate to longer contexts even when trained with short sequences.",
            "score": 0.3772144043050154,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1382,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 888
                },
                {
                    "start": 891,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2143
                }
            ],
            "ref_mentions": [
                {
                    "start": 370,
                    "end": 390,
                    "matchedPaperCorpusId": "237347130"
                },
                {
                    "start": 390,
                    "end": 407,
                    "matchedPaperCorpusId": "15641339"
                },
                {
                    "start": 407,
                    "end": 424,
                    "matchedPaperCorpusId": "248965309"
                },
                {
                    "start": 449,
                    "end": 469,
                    "matchedPaperCorpusId": "237347130"
                },
                {
                    "start": 1784,
                    "end": 1803,
                    "matchedPaperCorpusId": "49310446"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7529296875
        },
        {
            "corpus_id": "277824008",
            "title": "Improving Instruct Models for Free: A Study on Partial Adaptation",
            "text": "Training Large Language Models (LLMs) involves multiple steps, broadly categorized into pretraining and post-training. In pre-training, the base model acquires the bulk of its knowledge through the next-token prediction objective. Post-training usually involves supervised fine-tuning (SFT) and multiple rounds of reinforcement learning from human feedback (RLHF), resulting in an instruct model that is better at following instructions and more aligned with user goals. \n\nHowever, both SFT and RLHF, to some degree, encourage the model to produce long and conversational responses. This may be an unwanted feature when testing on extractive and/or structured natural language processing (NLP) tasks such as classification, name entity recognition, or extractive question answering. In these cases, the responses need to be concise and exact, and any additional chattiness creates issues in parsing the responses. Before instruct models became available, this need was fulfilled decently by the emergent few-shot in-context learning (ICL) abilities of the base model (Wei et al., 2022). Few previous studies touch on the pros and cons of base and instruct models. One example is Cuconasu et al. (2024) which shows how base models work better than instruct models on RAG-related tasks. \n\nOur work aims to fill this gap and thoroughly explores the performance trajectory between base and instruct models. In order to study the learning dynamics between base and instruct models, we need access to the model checkpoints saved during instruct tuning, which are rarely available, especially for best performing open-weight models. Therefore as a surrogate of this (Na et al., 2024), we resort to a simple training-free technique, partial adaptation (or PAd) (Fleshman and Van Durme, 2024), to scale the instruction-tuning strength in a post-hoc manner. Concretely, we create in-between models by partially adapting the base model (with weights W B ) to instruct (with weights W I ): \n\nUsing 18 open-weight LLMs, we evaluate these partially adapted models on a benchmark containing 21 classic NLP tasks using few-shot in-context learning.",
            "score": 0.37708858191253863,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 470
                },
                {
                    "start": 473,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1284
                },
                {
                    "start": 1287,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1977
                },
                {
                    "start": 1980,
                    "end": 2132
                }
            ],
            "ref_mentions": [
                {
                    "start": 1067,
                    "end": 1085,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1659,
                    "end": 1676,
                    "matchedPaperCorpusId": "273502726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9638671875
        },
        {
            "corpus_id": "273229170",
            "title": "Probing Language Models on Their Knowledge Source",
            "text": "Large Language Models (LLMs) have demonstrated remarkable proficiency in memorizing and retrieving massive amounts of information. Despite these strengths, LLMs often struggle when exposed to novel information not seen during training (Ovadia et al., 2019) or when there is a conflict between their parametric knowledge (PK) and the context knowledge (CK) provided at inference (Xie et al., 2024). Such discrepancies can lead to erroneous outputs, a phenomenon that remains a significant challenge in LLMs applications (Ji et al., 2023). While several approaches, such as reinforcement learning and retrieval-augmented generation, have been proposed to mitigate these issues (Ziegler et al., 2020;Lewis et al., 2021), the mechanisms by which LLMs select and prioritize knowledge sources are not well understood, suggesting a gap in current research methodologies. Figure 1: Illustration of our method for probing knowledge sources in LLMs. We present the model with a prompt containing contradictory information to its learned knowledge to test whether it uses parametric knowledge (PK) or contextual knowledge (CK). The resulting activations are used to train a classifier to distinguish between PK and CK. \n\nThis paper explores the internal dynamics of LLMs, and more precisely decoder-only layers, focusing on their decision-making processes regarding the use of CK versus PK. By prompting the LLM in a way that contradicts its PK, we probe the model's knowledge-sourcing behaviors. By training a linear classifier on model activations, our experiments reveal that certain activations correlate with determining whether context or parametric knowledge predominates in the generated outputs. \n\nIn this paper, we make the following key findings and contributions: \n\n\u2022 We define a framework that characterizes the source of knowledge used by the model to generate its outputs -Sections 3 and 4. To facilitate further research and validation of our findings, we make our framework publicly available on GitHub 1 . \n\n\u2022 Specific activations are indicative of the knowledge source: by applying our framework to LLMs of different sizes, we establish that specific activations correlate with the model's use of contextual or parametric knowledge.",
            "score": 0.3761232855087367,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1693
                },
                {
                    "start": 1696,
                    "end": 1764
                },
                {
                    "start": 1767,
                    "end": 2012
                },
                {
                    "start": 2015,
                    "end": 2240
                }
            ],
            "ref_mentions": [
                {
                    "start": 378,
                    "end": 396,
                    "matchedPaperCorpusId": "263610324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7783203125
        },
        {
            "corpus_id": "276250237",
            "title": "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining",
            "text": "In this paper, we introduce a novel sample-level reweighting framework aimed at improving the efficiency and effectiveness of large language model (LLM) pretraining. By dynamically adjusting the importance of individual samples based on their loss values, our approach overcomes the limitations of traditional uniform averaging methods and adds a new dimension to existing domain-level reweighting methods by incorporating more fine-grained sample-level dynamics. Through extensive theoretical and empirical validation, we demonstrate that down-weighting low-loss samples accelerates convergence and improves performance. The experiments show that our proposed LinUpper strategy consistently outperforms uniform sampling on common LLM reasoning and commonsense benchmarks. Similarly, our approach creates synergies with existing domain reweighting techniques, which underpins the importance of sample-level dynamics. We observe that the benefits of our method are more pronounced in larger models, while smaller models show less significant improvements on benchmarks. This could be due to the fact that smaller models may have limited capacity to fully exploit the advantages of our approach. However, our overall findings highlight the potential of loss-based reweighting strategies to optimize LLM pretraining both in training efficiency and in model evaluation performance.",
            "score": 0.3761201648458789,
            "section_title": "CONCLUSION",
            "char_start_offset": 29084,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1377
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97314453125
        },
        {
            "corpus_id": "270380069",
            "title": "Evolving Subnetwork Training for Large Language Models",
            "text": "Large language models have ushered in a new era of artificial intelligence research. However, their substantial training costs hinder further development and widespread adoption. In this paper, inspired by the redundancy in the parameters of large language models, we propose a novel training paradigm: Evolving Subnetwork Training (EST). EST samples subnetworks from the layers of the large language model and from commonly used modules within each layer, Multi-Head Attention (MHA) and Multi-Layer Perceptron (MLP). By gradually increasing the size of the subnetworks during the training process, EST can save the cost of training. We apply EST to train GPT2 model and TinyLlama model, resulting in 26.7\\% FLOPs saving for GPT2 and 25.0\\% for TinyLlama without an increase in loss on the pre-training dataset. Moreover, EST leads to performance improvements in downstream tasks, indicating that it benefits generalization. Additionally, we provide intuitive theoretical studies based on training dynamics and Dropout theory to ensure the feasibility of EST. Our code is available at https://github.com/OpenDFM/EST.",
            "score": 0.37606497939343575,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9658203125
        },
        {
            "corpus_id": "267657703",
            "title": "The Mirrored Influence Hypothesis: Efficient Data Influence Estimation by Harnessing Forward Passes",
            "text": "Motivation. With the growing prevalence of large language models in various applications, such as conversational agents [10,32,33], the importance of providing reasonable supporting evidence has become paramount. As a result, the need to trace the origin of a model's output back to specific data samples has also become indispensable to identifying the responsible training data. Driven by this motivation, we study the task of model behavior tracing regarding factual assertion, which involves identifying the training examples responsible for inducing the model to make some factual assertion at test time. \n\nSetup. We utilize a MT5 model [39] to finetune on the FTRACE-TREx dataset [3]. We consider each training example that conveys the same fact as a \"proponent\" of the corresponding test example and as a \"distractor\" otherwise. \n\nWe provide the details of the dataset and hyperparameter selection in Appendix D. Due to the demanding computation required by other direct train-to-test influence estimation methods, such as IF, here we only compare with TracIn. \n\nIn order to make TracIn [1] more efficient, for each test sample, we limit the scope of our search to a candidate set, i.e., a subset of the entire training dataset, following previous studies [3,26]. We leverage the same evaluation metrics (i.e., precision and Mean Reciprocal Rank (MRR)) described in the previous studies [3,26]. To consider the efficiency and the performance simultaneously, we propose a time-dependent performance metric, i.e., performance in a limited time budget. This metric is realistic because, in practice, user-facing products cannot afford to spend an indefinite amount of time responding to a user's request.",
            "score": 0.375857175152596,
            "section_title": "Language Model Behavior Tracing",
            "char_start_offset": 26641,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 1067
                },
                {
                    "start": 1070,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1708
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 124,
                    "matchedPaperCorpusId": "253759631"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83984375
        },
        {
            "corpus_id": "237458430",
            "title": "Semantic Systematicity in Connectionist Language Production",
            "text": "Our model architecture (see Figure 1) is broadly similar to the one of Frank et al. [23], with the main difference being that the inputs and outputs are reversed: it maps DSS representations onto sentences. As Frank et al. [23] point out, this is not intended to model human language development. The model is an extension of a Simple Recurrent Network (SRN [31]). It consists of an input layer, a 120-units recurrent hidden (sigmoid) layer, and a 43-units (softmax) output layer. The dimensionality of the input layer is determined by the chosen semantic representation (150-dimensional situation vector or 44-dimensional belief vector), plus one bit indicating if the model should produce an active sentence (1) or a passive one (0). The output layer dimensionality is the number of words in the vocabulary plus the end-of-sentence marker (43). \n\nTime in the model is discrete. At each time step t, the activation of the input layer dss is propagated to the hidden recurrent layer. This layer also receives its own activation h t\u22121 at time-step t \u2212 1 (zeros at t = 0) through context units. Additionally, the hidden layer receives the word mon t\u22121 produced at time-step t \u2212 1 (zeros at t = 0) through monitoring units, where only the unit corresponding to the word produced at time-step t \u2212 1 is activated (set to 1). \n\nWe did not test with more sophisticated architectures such as LSTMs [32] or GRUs [33] because the focus of this work are the representations used by the model, rather than the model itself. Consequently, we tried to use the minimum machinery possible aside from the input and output representations. \n\nMore formally, activation of the hidden layer is given by: \n\nwhere W ih is the weight matrix connecting the input layer to the hidden layer, W hh connects the hidden layer to itself, W mh connects the monitoring units to the hidden layer, and b h is the bias unit of the hidden layer. \n\nThen, the activation of the hidden layer h t is propagated to the output layer, which yields a probability distribution over the vocabulary, and its activation is given by:",
            "score": 0.375857175152596,
            "section_title": "Language Production Model",
            "char_start_offset": 15224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 846
                },
                {
                    "start": 849,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1319
                },
                {
                    "start": 1322,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1621
                },
                {
                    "start": 1624,
                    "end": 1682
                },
                {
                    "start": 1685,
                    "end": 1908
                },
                {
                    "start": 1911,
                    "end": 2083
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 88,
                    "matchedPaperCorpusId": "16835145"
                },
                {
                    "start": 223,
                    "end": 227,
                    "matchedPaperCorpusId": "16835145"
                },
                {
                    "start": 358,
                    "end": 362,
                    "matchedPaperCorpusId": "2763403"
                },
                {
                    "start": 841,
                    "end": 845,
                    "matchedPaperCorpusId": "3958369"
                },
                {
                    "start": 1390,
                    "end": 1394,
                    "matchedPaperCorpusId": "1915014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.283935546875
        },
        {
            "corpus_id": "277621090",
            "title": "Discovering Dynamical Laws for Speech Gestures",
            "text": "There are also reasons beyond quantitative fit to search for new models, such as the trade-off between simplicity and accuracy, and the value of interpretable parameters. Standard models may also not capture all kinds of speech and may require modifications or extensions to reproduce phenomena observed in disordered speech (M\u00fccke, Roessig, Thies et al., 2024), speech development (Abakarova, Fuchs & Noiray, 2022), different languages (Geissler & Nellakra, 2024), and so on. Finally, discovering new models is a core part of evaluating the success of existing models; if we can develop better models then this would represent a major advance, but if extant models prove more successful than the discovered models then this is also an important empirical finding. \n\n2 Discovering dynamical models from data",
            "score": 0.375857175152596,
            "section_title": "Dynamical models of speech",
            "char_start_offset": 12139,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 807
                }
            ],
            "ref_mentions": [
                {
                    "start": 325,
                    "end": 361,
                    "matchedPaperCorpusId": "267076751"
                },
                {
                    "start": 382,
                    "end": 415,
                    "matchedPaperCorpusId": "251694325"
                },
                {
                    "start": 437,
                    "end": 464,
                    "matchedPaperCorpusId": "270871831"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14111328125
        },
        {
            "corpus_id": "263334266",
            "title": "JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention",
            "text": "Training Dynamics of Neural Networks. Earlier research has delved into training dynamics within multi-layer linear neural networks (Arora et al., 2018;Bartlett et al., 2018), the teacher-student setting (Brutzkus & Globerson, 2017;Tian, 2017;Soltanolkotabi, 2017;Goel et al., 2018;Du et al., 2017;2018a;Zhou et al., 2019;Liu et al., 2019;Xu & Du, 2023), and infinite-width limits (Jacot et al., 2018;Chizat et al., 2019;Du et al., 2018b;2019;Allen-Zhu et al., 2019;Arora et al., 2019;Oymak & Soltanolkotabi, 2020;Zou et al., 2020;Li & Liang, 2018;Chizat & Bach, 2018;Mei et al., 2018;Nguyen & Pham, 2020;Fang et al., 2021;Lu et al., 2020). This includes extensions to attention-based-models (Hron et al., 2020;Yang et al., 2022). For self-supervised learning, there are analyses of linear networks (Tian, 2022) and explorations into the impact of nonlinearity (Tian, 2023). \n\nDynamics of Attention-based models. Regarding attention-based models, Zhang et al. (2020) delves into adaptive optimization techniques. Jelassi et al. (2022) introduces an idealized context, demonstrating that the vision transformer (Dosovitskiy et al., 2020) trained via gradient descent can discern spatial structures. Li et al. (2023c) illustrates that a singlelayer Transformer can learn a constrained topic model, where each word is tied to a single topic, using \u2113 2 loss, BERT-like framework (Devlin et al., 2018), and certain assumptions on attention patterns. Snell et al. (2021) investigate the training dynamics of single-head attention in mimicking Seq2Seq learning.",
            "score": 0.375857175152596,
            "section_title": "Related Work",
            "char_start_offset": 4461,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1553
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 173,
                    "matchedPaperCorpusId": "3544497"
                },
                {
                    "start": 203,
                    "end": 231,
                    "matchedPaperCorpusId": "13000960"
                },
                {
                    "start": 231,
                    "end": 242,
                    "matchedPaperCorpusId": "14147309"
                },
                {
                    "start": 263,
                    "end": 281,
                    "matchedPaperCorpusId": "3647043"
                },
                {
                    "start": 297,
                    "end": 303,
                    "matchedPaperCorpusId": "3461537"
                },
                {
                    "start": 303,
                    "end": 321,
                    "matchedPaperCorpusId": "174800314"
                },
                {
                    "start": 321,
                    "end": 338,
                    "matchedPaperCorpusId": "202542887"
                },
                {
                    "start": 380,
                    "end": 400,
                    "matchedPaperCorpusId": "49321232"
                },
                {
                    "start": 400,
                    "end": 420,
                    "matchedPaperCorpusId": "189928159"
                },
                {
                    "start": 442,
                    "end": 465,
                    "matchedPaperCorpusId": "53250107"
                },
                {
                    "start": 465,
                    "end": 484,
                    "matchedPaperCorpusId": "59222746"
                },
                {
                    "start": 513,
                    "end": 530,
                    "matchedPaperCorpusId": "254740635"
                },
                {
                    "start": 530,
                    "end": 547,
                    "matchedPaperCorpusId": "51920936"
                },
                {
                    "start": 547,
                    "end": 567,
                    "matchedPaperCorpusId": "43945764"
                },
                {
                    "start": 567,
                    "end": 584,
                    "matchedPaperCorpusId": "4932688"
                },
                {
                    "start": 622,
                    "end": 638,
                    "matchedPaperCorpusId": "212675182"
                },
                {
                    "start": 691,
                    "end": 710,
                    "matchedPaperCorpusId": "219792519"
                },
                {
                    "start": 946,
                    "end": 965,
                    "matchedPaperCorpusId": "225069687"
                },
                {
                    "start": 1012,
                    "end": 1033,
                    "matchedPaperCorpusId": "252918331"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07080078125
        },
        {
            "corpus_id": "258833087",
            "title": "Autoregressive Modeling with Lookahead Attention",
            "text": "Standard estimators such as L2 -regularized maximum likelihood do not have this property. Instead, Shi et al. (2018); Mehta et al. (2020) created inductive bias through inverse reinforcement learning, attempting to explain observed text by identifying a simple reward function along with a possibly complex policy that generates high-reward text. This policy then serves as the autoregressive language model. \n\nA competing System 2 approach would drop autoregressive modeling altogether in favor of energy-based modeling (LeCun et al., 2007). This is akin to learning the reward function without also learning a sequential generation policy.1 Instead, high-reward sequences are generated at runtime using an expensive planning-based process such as rejection sampling, MCMC, or stochastic beam search (Kool et al., 2019). Linguistics famously made this move in the 1990's with the rise of Optimality Theory (Paradis, 1988;Prince & Smolensky, 2004), which replaced complex stepwise generation procedures (akin to autoregressive models) with simpler direct descriptions of the rewards that those procedures were apparently constructed to obtain (akin to energy-based models). Note that training energy-based models can be difficult, although noise-contrastive estimation is one approach. 2 \n\nIn this paper, we attempt to find a practical hybrid approach in which System 2 consults System 1 (Kahneman, 2011). We retain the autoregressive parameterization, but we allow the autoregressive factors to engage in a limited form of planning. Specifically, our definition of p(x t+1 | x \u2264t ) will use rollouts p 0 (x >t | x \u2264t ) to consider the sentences that different choices of the next word x t might lead to. In this paper, we do not explicitly learn any reward function that evaluates the rollouts and chooses among them (although that is a reasonable direction for future work). Rather, we train a Transformer model p (System 2) to predict the next word after freely examining rollouts from p 0 (System 1). \n\nOur thinking is as follows. Humans are able to speak in real time, so it is unlikely that they do exhaustive planning.",
            "score": 0.375857175152596,
            "section_title": "body",
            "char_start_offset": 5607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 408
                },
                {
                    "start": 411,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1287
                },
                {
                    "start": 1290,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2004
                },
                {
                    "start": 2007,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2125
                }
            ],
            "ref_mentions": [
                {
                    "start": 521,
                    "end": 541,
                    "matchedPaperCorpusId": "8531544"
                },
                {
                    "start": 801,
                    "end": 820,
                    "matchedPaperCorpusId": "76662039"
                },
                {
                    "start": 907,
                    "end": 922,
                    "matchedPaperCorpusId": "170769200"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58203125
        },
        {
            "corpus_id": "266755802",
            "title": "TinyLlama: An Open-Source Small Language Model",
            "text": "Recent progress in natural language processing (NLP) has been largely propelled by scaling up language model sizes (Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023a,b). Large Language Models (LLMs) pre-trained on extensive text corpora have demonstrated their effectiveness on a wide range of tasks (OpenAI, 2023;Touvron et al., 2023b). Some empirical studies demonstrated emergent abilities in LLMs, abilities that may only manifest in models with a sufficiently large number of parameters, such as few-shot prompting (Brown et al., 2020) and chain-of-thought reasoning (Wei et al., 2022). Other studies focus on modeling the scaling behavior of LLMs (Kaplan et al., 2020;Hoffmann et al., 2022). Hoffmann et al. (2022) suggest that, to train a compute-optimal model, the size of the model and the amount of training data should be increased proportionally. This provides a guideline on how to optimally select the model size and allocate the amount of training data when the compute budget is fixed. \n\nAlthough these works show a clear preference on large models, the potential of training smaller models with larger datasets remains under-explored. Instead of training compute-optimal language models, Touvron et al. (2023a) highlight the importance of the inference budget, instead of focusing solely on training compute-optimal language models. Inference-optimal language models aim for optimal performance within specific inference constraints. This is achieved by training models with more tokens than what is recommended by the scaling law (Hoffmann et al., 2022). Touvron et al. (2023a) demonstrates that smaller models, when trained with more data, can match or even outperform their larger counterparts. Also, Thadd\u00e9e (2023) suggest that existing scaling laws (Hoffmann et al., 2022) may not predict accurately in situations where smaller models are trained for longer periods. \n\nMotivated by these new findings, this work focuses on exploring the behavior of smaller models when trained with a significantly larger number of tokens than what is suggested by the scaling law (Hoffmann et al., 2022).",
            "score": 0.375857175152596,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1015
                },
                {
                    "start": 1018,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1902
                },
                {
                    "start": 1905,
                    "end": 2124
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 135,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 534,
                    "end": 554,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 586,
                    "end": 604,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 688,
                    "end": 710,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 712,
                    "end": 734,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1562,
                    "end": 1585,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1785,
                    "end": 1808,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80859375
        },
        {
            "corpus_id": "270285736",
            "title": "What Should Embeddings Embed? Autoregressive Models Represent Latent Generating Distributions",
            "text": "Autoregressive language models (LMs) are trained to predict the next token in a sequence [e.g., Bengio et al., 2000].Many large language models (LLMs) use the autoregressive objective for pretraining [e.g., Radford et al., 2019], and their document-level embeddings have been shown to capture elements of latent structure that appear in text, such as agent properties [Andreas, 2022] and syntax [Hewitt and Manning, 2019].However, it remains unclear why embeddings represent these elements, and, more formally, what information an embedding learned by a capable autoregressive predictor should capture.An understanding of what LLMs represent and why is important to the design and evaluation of architectures and optimizers, and to supporting transparency and safety.\n\nThe representations formed by LLMs are challenging to analyze partly due to polysemanticity, where a neuron may activate for several distinct concepts Cunningham et al. [2023].Previous work has developed methods for probing LLM internal representations for specific concepts [Li et al., 2023, Meng et al., 2022, Zheng et al., 2023, Tenney et al., 2019].These efforts can be guided more effectively by developing a general theory of what aspects of the data embeddings should represent.Figure 1: Three data generation processes where prediction of the next token x n+1 is independent from previous tokens x 1:n given a predictive sufficient statistic.The left corresponds to exchangeable data, the middle to latent state models, and the right to discrete hypotheses.The relevant predictive sufficient statistics are the sufficient statistic for \u03b8 (or p(\u03b8|x 1:n ), p(z n+1 |x 1:n ), and p(h|x 1:n ) respectively).We show the embeddings learned by autoregressive transformers represent this information.\n\nIn this work, we investigate several cases where the representations of autoregressive LMs can be formally connected with those of a Bayes-optimal agent.",
            "score": 0.375857175152596,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 422
                },
                {
                    "start": 422,
                    "end": 602
                },
                {
                    "start": 602,
                    "end": 767
                },
                {
                    "start": 769,
                    "end": 945
                },
                {
                    "start": 945,
                    "end": 1122
                },
                {
                    "start": 1122,
                    "end": 1254
                },
                {
                    "start": 1254,
                    "end": 1419
                },
                {
                    "start": 1419,
                    "end": 1534
                },
                {
                    "start": 1534,
                    "end": 1680
                },
                {
                    "start": 1680,
                    "end": 1769
                },
                {
                    "start": 1771,
                    "end": 1924
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 116,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 207,
                    "end": 228,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 368,
                    "end": 383,
                    "matchedPaperCorpusId": "254246305"
                },
                {
                    "start": 395,
                    "end": 421,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 1060,
                    "end": 1079,
                    "matchedPaperCorpusId": "246679964"
                },
                {
                    "start": 1099,
                    "end": 1121,
                    "matchedPaperCorpusId": "108300988"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.822265625
        },
        {
            "corpus_id": "270202881",
            "title": "Dynamic Gradient Scaling: A Fine-Grained Approach to Optimizing Large Language Models in Deep Learning",
            "text": "Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. However, the training of such models is often hindered by optimization challenges, leading to inefficiencies and suboptimal performance. In this paper, I propose Dynamic Gradient Scaling (DGS), a novel fine-grained optimization technique tailored for the unique demands of LLMs. DGS dynamically adjusts learning rates based on the importance of individual parameters, allowing for enhanced efficiency and control during the optimization process. Theoretical foundations of DGS are explored, elucidating the calculation of importance scores, scaling factors, and adaptive learning rates. Practical implementation within the training loop of deep learning models showcases the versatility of DGS across diverse tasks. My research demonstrates that DGS provides fine-grained control over the optimization process, yielding improvements in training efficiency and model performance.",
            "score": 0.375857175152596,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9765625
        },
        {
            "corpus_id": "267027819",
            "title": "Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text",
            "text": "The core objective during the training of autoregressive language models is captured by the equation: \n\nThis equation portrays the negative log-likelihood of a model with parameters \u03b8, where the objective is to predict each token in a sequence based on its preceding tokens. Minimizing this likelihood value signifies that the model assigns higher probabilities to observed sequences, indicating a better fit to the training data. \n\nHowever, as straightforward as this equation may appear, the ultimate behavior and artifacts of an LLM are influenced by several factors. The distribution and nature of the training data shape the model's behavior. For instance, an LLM trained on a broad, multi-domain dataset might demonstrate extensive generalization capabilities. In contrast, a model trained on niche data could display a more narrow understanding. Patterns present in the training data may manifest as artifacts in the machine-generated text. The model architecture further complicates how Equation 1 is learned. Choices surrounding the number of layers, attention mechanisms, and other hyperparameters can lead to differences in the artifacts present in the text outputs.",
            "score": 0.375857175152596,
            "section_title": "LLM Text Generation Objective",
            "char_start_offset": 12510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 104,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 430
                },
                {
                    "start": 433,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1177
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82080078125
        },
        {
            "corpus_id": "273661991",
            "title": "Abrupt Learning in Transformers: A Case Study on Matrix Completion",
            "text": "Is it possible to analyse training dynamics of individual model components to derive insights about the full model training? This is motivated by the findings in the previous section on embeddings, and in Section 3.1; the pre-shift model does not use Attention layers for its computation in that stage, and relies on other components to copy input entries. Hence, in our case, the sudden drop corresponds in large part to learning the right Attention patterns (see Appendix M.1). To analyse training dynamics of different model components, we choose (a set of) components -Attention layers, MLP layers, Positional Embeddings and Token Embeddings, randomly initialize them and freeze the weights of other components to their values at the final step of training (Fig. 8). \n\nWe find that (a) MLPs and Token Embeddings converge without any observable plateau or sudden drop in loss; (b) for other components, the dynamics resemble those for the full model training (i.e. plateau and then sudden drop), and (c) Positional embeddings show the longest plateau in loss. Step (x1000)  There is no loss plateau for token embeddings and MLP layers, in contrast to positional embeddings, where the sudden drop occurs just before step 40000. In all other cases the sudden drop occurs before the sudden drop in usual training.",
            "score": 0.375857175152596,
            "section_title": "Sudden Drop in Loss -Role of Model Components",
            "char_start_offset": 23859,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1313
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85546875
        },
        {
            "corpus_id": "267311875",
            "title": "An Information-Theoretic Analysis of In-Context Learning",
            "text": "In recent years, we have observed the capability of large language models (LLMs) to learn from data within just its context window. This puzzling phenomenon referred to as in-context learning (ICL) (Brown et al., 2020), has captured the attention of the theoretical machine learning community. As the data available in-context is dwarfed by the extensive pretraining set, meta-learning stands as a prevailing explanation for ICL (Xie et al., 2022). \n\nAs aforementioned, Xie et al. (2022) introduced the idea that ICL could be interpreted as implicit Bayesian inference within a mixture of HMMs. While their theoretical results rely on contrived assumptions and fail to explain how ICL is possible with such short sequences, their work initiated the study of modeling ICL as Bayesian inference 1 Department of Computer Science, Stanford University, Stanford, CA, USA 2 Princeton University, Princeton, NJ, USA 3 New York University, New York City, NY, USA 4 Stanford University, Stanford, CA, USA. Correspondence to: Hong Jun Jeon <hjjeon@stanford.edu>. \n\nor other thoroughly studied learning processes such as empirical risk minimization. As much of the theoretical community is most familiar with error analyses of empirical risk minimization, much of the existing results (Li et al., 2023a;Bai et al., 2023;Edelman et al., 2021) study the error of an ICL under the assumption that ICL is competitive in out-of-sample performance with empirical risk minimization. However, each of these error bounds is limited in some way such as exponential depth dependence (Edelman et al., 2021;Li et al., 2023a) or error which decays only with the number of sequences and not the length of the sequences (Edelman et al., 2021;Bai et al., 2023). The results which do demonstrate that error decays in both the number of training sequences and sequence length often rely on contrived mixing time assumptions (Zhang et al., 2023b) or stability conditions which are equivalent to fast mixing (Li et al., 2023a). \n\nOur work revisits the idea of modeling ICL as Bayesian inference.",
            "score": 0.375857175152596,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 448
                },
                {
                    "start": 451,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1995
                },
                {
                    "start": 1998,
                    "end": 2063
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 218,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1274,
                    "end": 1292,
                    "matchedPaperCorpusId": "256616253"
                },
                {
                    "start": 1583,
                    "end": 1600,
                    "matchedPaperCorpusId": "256616253"
                },
                {
                    "start": 1976,
                    "end": 1994,
                    "matchedPaperCorpusId": "256616253"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80419921875
        },
        {
            "corpus_id": "270688100",
            "title": "Efficient Continual Pre-training by Mitigating the Stability Gap",
            "text": "In this section, we first study the behavior of large language models (LLMs) during continual pretraining by measuring them at regular intervals.We observe that the performance on the target task initially drops and then rises during continual pre-training.To explain our observations, we introduce the concept of the stability gap to continual pre-training and verify our explanations with experiments.",
            "score": 0.375857175152596,
            "section_title": "Identifying the stability gap in continual pre-training",
            "char_start_offset": 4786,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 145,
                    "end": 257
                },
                {
                    "start": 257,
                    "end": 403
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.974609375
        },
        {
            "corpus_id": "259360395",
            "title": "A Survey on Evaluation of Large Language Models",
            "text": "Language models (LMs) [36,51,96] are computational models that have the capability to understand and generate human language. LMs have the transformative ability to predict the likelihood of word sequences or generate new text based on a given input. N-gram models [13], the most common type of LM, estimate word probabilities based on the preceding context. However, LMs also face challenges, such as the issue of rare or unseen words, the problem of overfitting, and the difficulty in capturing complex linguistic phenomena. Researchers are continuously working on improving LM architectures and training methods to address these challenges. \n\nLarge Language Models (LLMs) [19,91,257] are advanced language models with massive parameter sizes and exceptional learning capabilities. The core module behind many LLMs such as GPT-3 [43], InstructGPT [149], and  is the self-attention module in Transformer [197] J. ACM, Vol. 37  that serves as the fundamental building block for language modeling tasks. Transformers have revolutionized the field of NLP with their ability to handle sequential data efficiently, allowing for parallelization and capturing long-range dependencies in text. One key feature of LLMs is in-context learning [14], where the model is trained to generate text based on a given context or prompt. This enables LLMs to generate more coherent and contextually relevant responses, making them suitable for interactive and conversational applications. Reinforcement Learning from Human Feedback (RLHF) [25,268] is another crucial aspect of LLMs. This technique involves fine-tuning the model using human-generated responses as rewards, allowing the model to learn from its mistakes and improve its performance over time. \n\nIn an autoregressive language model, such as GPT-3 and PaLM [24], given a context sequence  , the LM tasks aim to predict the next token . The model is trained by maximizing the probability of the given token sequence conditioned on the context, i.e.,  (| ) =  (| 1 ,  2 , ...,   \u22121 ), where  1 ,  2 , ...,   \u22121 are the tokens in the context sequence, and  is the current position.",
            "score": 0.375857175152596,
            "section_title": "Large Language Models",
            "char_start_offset": 7143,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 643
                },
                {
                    "start": 646,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1739
                },
                {
                    "start": 1742,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 265,
                    "end": 269,
                    "matchedPaperCorpusId": "10986188"
                },
                {
                    "start": 679,
                    "end": 682,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 849,
                    "end": 854,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 924,
                    "end": 926,
                    "matchedPaperCorpusId": "231719337"
                },
                {
                    "start": 1234,
                    "end": 1238,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82666015625
        },
        {
            "corpus_id": "250072164",
            "title": "Your Autoregressive Generative Model Can be Better If You Treat It as an Energy-Based One",
            "text": "To further demonstrate E-ARM's consistency in reducing flaws of autoregressive generative models, we also conduct experiments on language modeling tasks. Three different datasets, WikiText-103 (Merity et al., 2017), Toronto Book Corpus (Zhu et al., 2015;Kiros et al., 2015), and CC-news (Bakhtin et al., 2019), are chosen as the testbed. In addition, we have studied the effect of different start epochs of E-ARM on the performance of language modeling, which can be seen in Table 4. From this, we may deduce that starting E-ARM training at the 15-th and 10th epoch yields the best results for Transformer-Base and Transformer-XL respectively, whereas starting earlier or later yields a performance decline. It is reasonable because, if E-ARM was introduced too early, the autoregressive model may not have been optimized well at that moment. As a result, the quality of generation for \"negative phase\" would be terrible, making energy-based training unstable. \n\nOn the other hand, the underlying autoregressive model can be modified only marginally if E-ARM was introduced when the ARGM training is virtually complete.",
            "score": 0.375857175152596,
            "section_title": "Application to Language Modeling",
            "char_start_offset": 21892,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 960
                },
                {
                    "start": 963,
                    "end": 1119
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 214,
                    "matchedPaperCorpusId": "16299141"
                },
                {
                    "start": 236,
                    "end": 254,
                    "matchedPaperCorpusId": "6866988"
                },
                {
                    "start": 254,
                    "end": 273,
                    "matchedPaperCorpusId": "9126867"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79931640625
        },
        {
            "corpus_id": "268531955",
            "title": "Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts",
            "text": "Pretraining large language models on a large corpus of unlabeled texts and further finetuning them on downstream tasks have been a common practice in natural language processing (NLP), resulting in significant advances in tasks such as sentiment classification, natural language inference, and text generation (Devlin et al., 2018;Liu et al., 2019;Lewis et al., 2019;Raffel et al., 2020).\n\nHowever, conventional finetuning of pretrained language models (PLMs) presents several challenges.First, PLMs are prone to instability in finetuning, characterized by high variance in performance for different weight initializations even when using the same hyperparameters, especially on small datasets (Ziser and Reichart, 2019;Devlin et al., 2018;Phang et al., 2018;Lee et al., 2019;Dodge et al., 2020;Zhang et al., 2020).Moreover, PLMs, due to their extremely large capacity, are prone to overfitting when finetuned on small downstream datasets, leading to poor generalization on test set (Belinkov et al., 2020;Aghajanyan et al., 2020;Kuang et al., 2021).Consequently, adapting PLMs to a variety of low-resource tasks, while preserving stability and maximizing generalization, remains a significant challenge in the field.\n\nFinetuning a strategically chosen sub-network on a downstream task, while keeping the remaining weights fixed to the pretrained weights, has effectively mitigated these challenges.Within this umbrella, CHILD-TUNING D (Xu et al., 2021) and DPS dense (Zhang et al., 2022) are promising.In CHILD-TUNING D (Xu et al., 2021), a static subnetwork, termed \"child network\", is first selected based on the Fisher Information Matrix (FIM) and this child network is subsequently updated during finetuning.Dynamic Parameter Selection (DPS) (Zhang et al., 2022) further refines this approach by dynamically selecting the child network during PLM finetuning using FIM as a guiding principle.\n\nNevertheless, these prior works exhibit certain limitations.",
            "score": 0.375857175152596,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 388
                },
                {
                    "start": 390,
                    "end": 488
                },
                {
                    "start": 488,
                    "end": 815
                },
                {
                    "start": 815,
                    "end": 1050
                },
                {
                    "start": 1050,
                    "end": 1217
                },
                {
                    "start": 1219,
                    "end": 1399
                },
                {
                    "start": 1399,
                    "end": 1503
                },
                {
                    "start": 1503,
                    "end": 1713
                },
                {
                    "start": 1713,
                    "end": 1896
                },
                {
                    "start": 1898,
                    "end": 1958
                }
            ],
            "ref_mentions": [
                {
                    "start": 367,
                    "end": 387,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 983,
                    "end": 1006,
                    "matchedPaperCorpusId": "235391000"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6865234375
        },
        {
            "corpus_id": "271745043",
            "title": "On the Generalization of Preference Learning with DPO",
            "text": "One approach is through the lens of feature learning, attempting to understand how models learn data-dependent features and how these features are structured [75,76,77,78,79,80,81,82,83,84]. Another approach is through providing generalization bounds that quantify the expected performance of the model beyond the training samples and over a data distribution [11,12,13,14,15,16,17,7,8,10,9]. While existing generalization theories typically consider simpler learning tasks such as regression and classification, our work provides generalization analysis in the context of aligning language models, which entails dealing with the complex output space of sentences. Moreover, existing generalization theory typically considers overparameterized models that achieve near-optimal loss [11,12,13,14] or are independent of the training process [15,16,17]. In contrast, our framework focuses on the generalization of models by analyzing the reward dynamics after finite gradient steps, which matches more closely with the real-world practices of aligning LLMs. Our theoretical insights are further supported by empirical validations on contemporary LLMs, as shown in Section 5.",
            "score": 0.37576508504470135,
            "section_title": "Related Works",
            "char_start_offset": 31215,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1171
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 162,
                    "matchedPaperCorpusId": "253018612"
                },
                {
                    "start": 162,
                    "end": 165,
                    "matchedPaperCorpusId": "225094694"
                },
                {
                    "start": 165,
                    "end": 168,
                    "matchedPaperCorpusId": "235825390"
                },
                {
                    "start": 168,
                    "end": 171,
                    "matchedPaperCorpusId": "249375392"
                },
                {
                    "start": 171,
                    "end": 174,
                    "matchedPaperCorpusId": "211296758"
                },
                {
                    "start": 186,
                    "end": 189,
                    "matchedPaperCorpusId": "258947127"
                },
                {
                    "start": 360,
                    "end": 364,
                    "matchedPaperCorpusId": "53287096"
                },
                {
                    "start": 364,
                    "end": 367,
                    "matchedPaperCorpusId": "91184109"
                },
                {
                    "start": 367,
                    "end": 370,
                    "matchedPaperCorpusId": "249375287"
                },
                {
                    "start": 370,
                    "end": 373,
                    "matchedPaperCorpusId": "59222746"
                },
                {
                    "start": 373,
                    "end": 376,
                    "matchedPaperCorpusId": "3566398"
                },
                {
                    "start": 376,
                    "end": 379,
                    "matchedPaperCorpusId": "254017755"
                },
                {
                    "start": 382,
                    "end": 384,
                    "matchedPaperCorpusId": "52920928"
                },
                {
                    "start": 389,
                    "end": 391,
                    "matchedPaperCorpusId": "13537400"
                },
                {
                    "start": 782,
                    "end": 786,
                    "matchedPaperCorpusId": "53287096"
                },
                {
                    "start": 786,
                    "end": 789,
                    "matchedPaperCorpusId": "91184109"
                },
                {
                    "start": 789,
                    "end": 792,
                    "matchedPaperCorpusId": "249375287"
                },
                {
                    "start": 792,
                    "end": 795,
                    "matchedPaperCorpusId": "59222746"
                },
                {
                    "start": 839,
                    "end": 843,
                    "matchedPaperCorpusId": "3566398"
                },
                {
                    "start": 843,
                    "end": 846,
                    "matchedPaperCorpusId": "254017755"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87841796875
        },
        {
            "corpus_id": "239016819",
            "title": "PAGnol: An Extra-Large French Generative Model",
            "text": "Successive generations of GPT models have questioned the current fine-tuning paradigm. GPT-2 (Radford et al., 2019), with 1.5 billion parameters, demonstrated that large language models could tackle entirely new tasks through few-shot learning2 . Without any fine-tuning, from just a few prompted examples, GPT-2 achieved fair performance on a number of complex downstream tasks. Furthering this endeavour, GPT-3 (Brown et al., 2020), with 175 billion parameters, achieved stateof-the-art performance on some tasks, without the need for fine-tuning. This opens new possibilities for low-resources tasks, as well as paths to more natural interactions with these models: recent research suggests the gap between few-shot learning and fine-tuning may even be bridged through socalled prompt programming/tuning (Li and Liang, 2021;Lester et al., 2021). \n\nScaling laws. More specifically to our setting, neural language models have been shown to predictably benefit from increased scale (Kaplan et al., 2020). Their training dynamics are size-invariant, allowing test loss, parameter count, and dataset size to be correlated through smooth scaling laws. This is in fact true of all GPT-like autoregressive models, even when applied to image, multimodal, or mathematics modeling (Henighan et al., 2020). Gains in autoregressive cross-entropy loss also directly translates to gains in end-task performance after fine-tuning. As they relate to compute budget, these predictions can be used to inform the training of large models. \n\nNon-English generative models. BERT-like models are now available in a broad number of languages, either as specialized models are as multilingual ones. This is less so the case for generative models, perhaps because of issues in controlling the language used at generation time. For the French language, GPT fr is an autoregressive generative model, and BARThez (Eddine et al., 2020a) targets some generative abilities. Smaller-scale efforts exist, such as BelGPT (Louis, 2020), but they are limited to small models.",
            "score": 0.3755715281714136,
            "section_title": "Related work",
            "char_start_offset": 5884,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 848
                },
                {
                    "start": 851,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1521
                },
                {
                    "start": 1524,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2041
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 115,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "269042752",
            "title": "On Training Data Influence of GPT Models",
            "text": "Practical Influence via Checkpoints Our featured simulator is adept at learning from past training dynamics. However, monitoring the training dynamics at every step can be expensive, especially  when dealing with large-sized GPTs. Therefore, we conduct experiments to choose training checkpoints at specific intervals to approximate the reality of the neighboring points with the training state of that particular point. Then, we trained our simulator on the approximate training dynamics to find the balance between the cost of collecting training dynamics and the simulator performance. \n\nResults are shown in Fig. 3. Unless otherwise specified, we instruction tuning the Pythia-410M for further analysis. In general, the performance of our simulator deteriorates as the number of checkpoint intervals increases. This is manifested by a rise in MSE and MAE at all steps and a drop in Spearman's \u03c1 when the checkpoint interval is large. However, even when the number of checkpoint intervals is equal to 10, which means that we will use the training state of one point to approximate the training state of the previous ten points and the training dynamics collection time will be shortened by almost 90%, our method still has comparable prediction error at all steps and better Spearman coefficient than Simfluence. Empirical Analysis of Markov Order Dependency Using the first-order Markov process to predict future states based on the prior step, potentially oversimplifies GPT training dynamics. Therefore, we consider the training dynamics as an n-th order Markov process (n = 2, 3, 5, 10) and experiment on both language understanding (RTE) and generative (WebNLG) tasks. \n\nThe result can be seen in Fig. 4. Overall, when considering more preceding training information, the simulation error initially increases and decreases for both datasets, as indicated by the allsteps MSE metric. It suggests that a high order n might introduce noise, leading to a degraded simulator's performance. Moreover, the final-step Spearman's \u03c1 shows a significant increase from 0.746 to 0.785 for RTE with the increase of order n, but not the same for WebNLG. We guess considering more past training information could improve the prediction accuracy for NLU tasks.",
            "score": 0.37554781946042737,
            "section_title": "Ablation Study",
            "char_start_offset": 16042,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1676
                },
                {
                    "start": 1679,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2251
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88671875
        },
        {
            "corpus_id": "257921893",
            "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
            "text": "How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\url{https://github.com/EleutherAI/pythia}.",
            "score": 0.375186923640818,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99267578125
        },
        {
            "corpus_id": "270063025",
            "title": "BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation",
            "text": "We conduct pre-training on the Simpajama and Starcoder dataset, which follows the setting from the Tinyllama codebase.Our training involves 30 billion tokens, including training the language world model and inverse dynamics model on 30B tokens and subsequently training the policy on the same 30B tokens.First, we evaluate our model as well as Tinyllama trained with the same 30B tokens on several standard benchmarks, including MMLU [49], DROP [50], BBH [10] and TruthfulQA [51].Results in Table 1 show that our method achieves a comparable performance with the Tinyllama model.We demonstrate other important properties of our model below.On Action Controllability.In order to evaluate the controllability of our constructed action space, our objective is to confirm whether the action can genuinely control distinct distributions.Initially, we randomly sample actions and obtain the next token with the maximum probability to observe if the generated sentences are distinct.Specifically, we use \"I like\" as a prompt and randomly sample actions from the action space.Our findings indicate that distinct actions can indeed result in disparate generation outcomes: \u2022 Example 1: I like to work with a lot of different people.I'm a big fan of the creative process.\n\n\u2022 Example 2: I like them, especially with the cats.\n\n\u2022 Example 3: I like this car, as well as the newness of the interior and features.\n\n\u2022 Example 4: I like having someone look out for them whenever I go out to a movie.\n\n\u2022 Example 5: I like my food best) and sometimes love seeing new restaurants ... We find that despite the random sampling of actions, the resulting sentences remain coherent.This suggests that our action space holds a more meaningful structure than a raw token space, showing its efficacy in guiding the token generation.We also provide other results in Appendix A.3.4.\n\nOn Reducing Predictive Variance.One benefit of introducing latent actions is to reduce the predictive variance of tokens, which is further reflected in the associated entropy.",
            "score": 0.3749633179805642,
            "section_title": "Pre-training and Supervised Fine-Tuning",
            "char_start_offset": 19278,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 118,
                    "end": 304
                },
                {
                    "start": 304,
                    "end": 480
                },
                {
                    "start": 480,
                    "end": 579
                },
                {
                    "start": 579,
                    "end": 640
                },
                {
                    "start": 640,
                    "end": 666
                },
                {
                    "start": 666,
                    "end": 832
                },
                {
                    "start": 832,
                    "end": 976
                },
                {
                    "start": 976,
                    "end": 1068
                },
                {
                    "start": 1068,
                    "end": 1223
                },
                {
                    "start": 1223,
                    "end": 1261
                },
                {
                    "start": 1263,
                    "end": 1314
                },
                {
                    "start": 1316,
                    "end": 1398
                },
                {
                    "start": 1400,
                    "end": 1482
                },
                {
                    "start": 1484,
                    "end": 1657
                },
                {
                    "start": 1657,
                    "end": 1804
                },
                {
                    "start": 1804,
                    "end": 1852
                },
                {
                    "start": 1854,
                    "end": 1886
                },
                {
                    "start": 1886,
                    "end": 2029
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83544921875
        },
        {
            "corpus_id": "270199736",
            "title": "An iterated learning model of language change that mixes supervised and unsupervised learning",
            "text": "As a simple demonstration of this, we can consider how well the neural networks inside the Semi-Supervised ILM's agents learn as the initially arbitrary language matures into an XCS language. Of course, the learning implemented here, using stochastic gradient descent, is very different from anything that plausibly happens in the brain, so it would be wrong to suggest that the Semi-Supervised ILM's learning dynamics align with the learning dynamics of a human language learner. However, the ease with which the neural networks An iterated learning model of language change. 17/33 \n\nreduce their loss function is likely related in some general way to how easy the language is to learn. In Fig. 9 the loss achieved by each of the pupil's neural networks is plotted against training epoch for each generation. Since each pupil always starts in a na\u00efve state, the initial loss is always the same at epoch one, but for higher generations, and hence more mature languages, the loss falls more quickly. \n\nMore broadly though, the issue of how well a language supports communication goes well beyond our approach here. If languages are subject to selective pressures to minimize ambiguity or reduce cognitive load on language users, these pressures might be seen to shape language change within an iterated learning model. Given its lower computational demands, the Semi-Supervised ILM could be extended to incorporate such additional pressures on language, for example related to the cognitive load experienced by a speaker while forming sentences or the ease with which these sentences can be processed or unambiguously understood by a listener.",
            "score": 0.374426546493119,
            "section_title": "The rate of learning",
            "char_start_offset": 29083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1642
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "222142224",
            "title": "On the Sparsity of Neural Machine Translation Models",
            "text": "In this paper, we prove that existing NMT systems are over-parameterized and propose to improve the utilization efficiency of parameters in NMT models by introducing a rejuvenation approach. Empirical results on a variety of language pairs and architectures demonstrate the effectiveness and universality of the presented method. We also analyze the gains from perspectives of learning dynamics and linguistic probing, which give insightful research directions for future work. Future directions include continuing the exploration of this research topic for large sequenceto-sequence pre-training models (Liu et al., 2020) and multi-domain translation models (Wang et al., 2019b). We will employ recent analysis methods to better understand the behaviors of rejuvenated models (He et al., 2019;Yang et al., 2020).",
            "score": 0.3738706907678653,
            "section_title": "Conclusion",
            "char_start_offset": 10266,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 813
                }
            ],
            "ref_mentions": [
                {
                    "start": 659,
                    "end": 679,
                    "matchedPaperCorpusId": "208248157"
                },
                {
                    "start": 777,
                    "end": 794,
                    "matchedPaperCorpusId": "202539954"
                },
                {
                    "start": 794,
                    "end": 812,
                    "matchedPaperCorpusId": "222141008"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74951171875
        },
        {
            "corpus_id": "277786647",
            "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality through the Lens of Layer-wise Gradients",
            "text": "As the post-training of large language models (LLMs) advances from instruction-following to complex reasoning tasks, understanding how different data affect finetuning dynamics remains largely unexplored. In this paper, we present a spectral analysis of layer-wise gradients induced by low/high-quality instruction and reasoning data for LLM post-training. Our analysis reveals that widely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and Reward, can be explained and unified by spectral properties computed from gradients' singular value decomposition (SVD). Specifically, higher-quality data are usually associated with lower nuclear norms and higher effective ranks. Notably, effective rank exhibits better robustness and resolution than nuclear norm in capturing subtle quality differences. For example, reasoning data achieves substantially higher effective ranks than instruction data, implying richer gradient structures on more complex tasks. Our experiments also highlight that models within the same family share similar gradient patterns regardless of their sizes, whereas different model families diverge significantly. Providing a unified view on the effects of data quality across instruction and reasoning data, this work illuminates the interplay between data quality and training stability, shedding novel insights into developing better data exploration strategies for post-training.",
            "score": 0.3736796229729873,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.966796875
        },
        {
            "corpus_id": "276409388",
            "title": "AdaGC: Improving Training Stability for Large Language Model Pretraining",
            "text": "Loss spikes present a significant challenge when scaling models, potentially impeding learning or destabilizing the training process. Various strategies have been proposed to enhance the stability of pre-training in large language models. \n\nTraining Strategies: Effective training strategies are pivotal for achieving stability. Adopting the bfloat16 format (Wang & Kanwar, 2019) is a cornerstone for stable training. Using smaller learning rates (Wortsman et al., 2023b;Zhang et al., 2022) also promotes stable training dynamics. To mitigate loss spikes, PaLM (Chowdhery et al., 2023) resumes training from a checkpoint approximately 100 steps before the spike and bypasses 200-500 data batches to avoid those preceding and during the spike. Varying the sequence length (Li et al., 2022) is also effective for stabilizing the pre-training of large language models. \n\nModel Architecture: Advanced architectures are crucial for stability in training large-scale models. Research shows that Pre-LN Transformers offer enhanced stability over Post-LN Transformers (Xiong et al., 2020;Vaswani et al., 2017;Liu et al., 2020), both theoretically and practically. Thus, contemporary studies predominantly employ Pre-LN Transformers for building large-scale models. Techniques such as NormHead (Yang et al., 2023), which normalizes output embeddings, and RMSNorm (Zhang & Sennrich, 2019), as reported in Llama (Touvron et al., 2023a), contribute to stable training. Applying layer normalization to the embedding layer (Scao et al., 2022), normalizing the input vector before the standard softmax function (NormSoftmax) (Jiang et al., 2023), and the shrink embedding gradient technique (Zeng et al., 2022) are also beneficial. \n\nModel Initialization: Proper model initialization significantly enhances training stability. Scaling the embedding layer with an appropriate constant (Takase et al., 2023) reinforces LLM pre-training stability. Initializing Transformers with small parameter values (Nguyen & Salazar, 2019) also improves pre-training stability.",
            "score": 0.37343414208504194,
            "section_title": "Related Work",
            "char_start_offset": 4009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 238
                },
                {
                    "start": 241,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 561,
                    "end": 585,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 771,
                    "end": 788,
                    "matchedPaperCorpusId": "252917575"
                },
                {
                    "start": 1060,
                    "end": 1080,
                    "matchedPaperCorpusId": "211082816"
                },
                {
                    "start": 1354,
                    "end": 1378,
                    "matchedPaperCorpusId": "113405151"
                },
                {
                    "start": 1610,
                    "end": 1630,
                    "matchedPaperCorpusId": "260255061"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95947265625
        },
        {
            "paperId": "42794b0dc76e077f961be4ee26adcbc708f3f226",
            "corpusId": 263909426,
            "title": "Crosslingual Structural Priming and the Pre-Training Dynamics of Bilingual Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 20,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.07929",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.07929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257347764",
                    "name": "Catherine Arnett"
                },
                {
                    "authorId": "2087001989",
                    "name": "Tyler A. Chang"
                },
                {
                    "authorId": "118683123",
                    "name": "J. Michaelov"
                },
                {
                    "authorId": "2257347492",
                    "name": "Benjamin Bergen"
                }
            ],
            "abstract": "Do multilingual language models share abstract grammatical representations across languages, and if so, when do these develop? Following Sinclair et al. (2022), we use structural priming to test for abstract grammatical representations with causal effects on model outputs. We extend the approach to a Dutch-English bilingual setting, and we evaluate a Dutch-English language model during pre-training. We find that crosslingual structural priming effects emerge early after exposure to the second language, with less than 1M tokens of data in that language. We discuss implications for data contamination, low-resource transfer, and how abstract grammatical representations emerge in multilingual models.",
            "corpus_id": "263909426",
            "text": "Do multilingual language models share abstract grammatical representations across languages, and if so, when do these develop? Following Sinclair et al. (2022), we use structural priming to test for abstract grammatical representations with causal effects on model outputs. We extend the approach to a Dutch-English bilingual setting, and we evaluate a Dutch-English language model during pre-training. We find that crosslingual structural priming effects emerge early after exposure to the second language, with less than 1M tokens of data in that language. We discuss implications for data contamination, low-resource transfer, and how abstract grammatical representations emerge in multilingual models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.91015625
        },
        {
            "paperId": "49873ee415619efd9e1e4c16f73ee066ff008c1f",
            "corpusId": 269009975,
            "title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 73,
            "citationCount": 345,
            "influentialCitationCount": 38,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.06395, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1576223501",
                    "name": "Shengding Hu"
                },
                {
                    "authorId": "2295757664",
                    "name": "Yuge Tu"
                },
                {
                    "authorId": "48506411",
                    "name": "Xu Han"
                },
                {
                    "authorId": "2256951801",
                    "name": "Chaoqun He"
                },
                {
                    "authorId": "52297757",
                    "name": "Ganqu Cui"
                },
                {
                    "authorId": "2295733503",
                    "name": "Xiang Long"
                },
                {
                    "authorId": "2295929465",
                    "name": "Zhi Zheng"
                },
                {
                    "authorId": "2295846809",
                    "name": "Yewei Fang"
                },
                {
                    "authorId": "2214586078",
                    "name": "Yuxiang Huang"
                },
                {
                    "authorId": "2150606888",
                    "name": "Weilin Zhao"
                },
                {
                    "authorId": "2254576790",
                    "name": "Xinrong Zhang"
                },
                {
                    "authorId": "2284862784",
                    "name": "Z. Thai"
                },
                {
                    "authorId": "2295817905",
                    "name": "Kaihuo Zhang"
                },
                {
                    "authorId": "2249899670",
                    "name": "Chongyi Wang"
                },
                {
                    "authorId": "1390925224",
                    "name": "Yuan Yao"
                },
                {
                    "authorId": "2267866399",
                    "name": "Chenyang Zhao"
                },
                {
                    "authorId": "2295789325",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "2295809950",
                    "name": "Jie Cai"
                },
                {
                    "authorId": "2295733180",
                    "name": "Zhongwu Zhai"
                },
                {
                    "authorId": "46649145",
                    "name": "Ning Ding"
                },
                {
                    "authorId": "2279190843",
                    "name": "Chaochao Jia"
                },
                {
                    "authorId": "1398454307",
                    "name": "Guoyang Zeng"
                },
                {
                    "authorId": "2144118403",
                    "name": "Dahai Li"
                },
                {
                    "authorId": "2269703458",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2273551430",
                    "name": "Maosong Sun"
                }
            ],
            "abstract": "The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .",
            "corpus_id": "269009975",
            "text": "The burgeoning interest in developing Large Language Models (LLMs) with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future LLM research. Regarding model scaling, we employ extensive model wind tunnel experiments for stable and optimal scaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and domain adaptation. We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model scaling law without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM's foundation in diverse SLM applications. MiniCPM models are available publicly at https://github.com/OpenBMB/MiniCPM .",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.943359375
        },
        {
            "paperId": "c7137aa84b5a48bda9a96432524f9948e8c823e0",
            "corpusId": 268091264,
            "title": "Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 101,
            "citationCount": 12,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.19465, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2288537472",
                    "name": "Chen Qian"
                },
                {
                    "authorId": "2288673056",
                    "name": "Jie Zhang"
                },
                {
                    "authorId": "2279748096",
                    "name": "Wei Yao"
                },
                {
                    "authorId": "2289139082",
                    "name": "Dongrui Liu"
                },
                {
                    "authorId": "13050405",
                    "name": "Zhen-fei Yin"
                },
                {
                    "authorId": "2265493981",
                    "name": "Yu Qiao"
                },
                {
                    "authorId": "2288941451",
                    "name": "Yong Liu"
                },
                {
                    "authorId": "2254280929",
                    "name": "Jing Shao"
                }
            ],
            "abstract": "Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness. To begin with, we apply linear probing to LLMs. The high probing accuracy suggests that \\textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM's pre-training checkpoints to enhance the LLM's trustworthiness. Finally, inspired by~\\citet{choi2023understanding} that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to investigate the dynamics of trustworthiness during pre-training. We are the first to observe a similar two-phase phenomenon: fitting and compression~\\citep{shwartz2017opening}. This research provides an initial exploration of trustworthiness modeling during LLM pre-training, seeking to unveil new insights and spur further developments in the field. We will make our code publicly accessible at \\url{https://github.com/ChnQ/TracingLLM}.",
            "corpus_id": "268091264",
            "text": "Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness. To begin with, we apply linear probing to LLMs. The high probing accuracy suggests that \\textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM's pre-training checkpoints to enhance the LLM's trustworthiness. Finally, inspired by~\\citet{choi2023understanding} that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to investigate the dynamics of trustworthiness during pre-training. We are the first to observe a similar two-phase phenomenon: fitting and compression~\\citep{shwartz2017opening}. This research provides an initial exploration of trustworthiness modeling during LLM pre-training, seeking to unveil new insights and spur further developments in the field. We will make our code publicly accessible at \\url{https://github.com/ChnQ/TracingLLM}.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.759765625
        },
        {
            "paperId": "00154b6016b1c8e86e678a4da53804a4c89800f0",
            "corpusId": 274060474,
            "title": "Hop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "referenceCount": 20,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2024.findings-emnlp.954?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2024.findings-emnlp.954, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237161748",
                    "name": "Shreyas Vathul Subramanian"
                },
                {
                    "authorId": "7194698",
                    "name": "Vignesh Ganapathiraman"
                },
                {
                    "authorId": "2258574389",
                    "name": "Corey Barrett"
                }
            ],
            "abstract": "Various types of learning rate (LR) schedulers are being used for training or fine tuning of Large Language Models today. In practice, several mid-flight changes are required in the LR schedule either manually, or with careful choices around warmup steps, peak LR, type of decay and restarts. To study this further, we consider the effect of switching the learning rate at a predetermined time during training, which we refer to as \u201cSkipLR\u201d. We model SGD as a stochastic gradient flow and show that when starting from the same initial parameters, switching the learning rate causes the loss curves to contract towards each other. We demonstrate this theoretically for some simple cases, and empirically on large language models. Our analysis provides insight into how learning rate schedules affect the training dynamics, and could inform the design of new schedules to accelerate convergence.",
            "corpus_id": "274060474",
            "text": "Various types of learning rate (LR) schedulers are being used for training or fine tuning of Large Language Models today. In practice, several mid-flight changes are required in the LR schedule either manually, or with careful choices around warmup steps, peak LR, type of decay and restarts. To study this further, we consider the effect of switching the learning rate at a predetermined time during training, which we refer to as \u201cSkipLR\u201d. We model SGD as a stochastic gradient flow and show that when starting from the same initial parameters, switching the learning rate causes the loss curves to contract towards each other. We demonstrate this theoretically for some simple cases, and empirically on large language models. Our analysis provides insight into how learning rate schedules affect the training dynamics, and could inform the design of new schedules to accelerate convergence.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.99658203125
        },
        {
            "paperId": "c31044506dbb22b5546342605913eb9f40b1d166",
            "corpusId": 267413022,
            "title": "Frequency Explains the Inverse Correlation of Large Language Models\u2019 Size, Training Data Amount, and Surprisal\u2019s Fit to Reading Times",
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 38,
            "citationCount": 20,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.02255, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "71829539",
                    "name": "Byung-Doh Oh"
                },
                {
                    "authorId": "2282541251",
                    "name": "Shisen Yue"
                },
                {
                    "authorId": "2282540236",
                    "name": "William Schuler"
                }
            ],
            "abstract": "Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades. The current work presents a series of analyses showing that word frequency is a key explanatory factor underlying these two trends. First, residual errors from four language model families on four corpora show that the inverse correlation between model size and fit to reading times is the strongest on the subset of least frequent words, which is driven by excessively accurate predictions of larger model variants. Additionally, training dynamics reveal that during later training steps, all model variants learn to predict rare words and that larger model variants do so more accurately, which explains the detrimental effect of both training data amount and model size on fit to reading times. Finally, a feature attribution analysis demonstrates that larger model variants are able to accurately predict rare words based on both an effectively longer context window size as well as stronger local associations compared to smaller model variants. Taken together, these results indicate that Transformer-based language models\u2019 surprisal estimates diverge from human-like expectations due to the superhumanly complex associations they learn for predicting rare words.",
            "corpus_id": "267413022",
            "text": "Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades. The current work presents a series of analyses showing that word frequency is a key explanatory factor underlying these two trends. First, residual errors from four language model families on four corpora show that the inverse correlation between model size and fit to reading times is the strongest on the subset of least frequent words, which is driven by excessively accurate predictions of larger model variants. Additionally, training dynamics reveal that during later training steps, all model variants learn to predict rare words and that larger model variants do so more accurately, which explains the detrimental effect of both training data amount and model size on fit to reading times. Finally, a feature attribution analysis demonstrates that larger model variants are able to accurately predict rare words based on both an effectively longer context window size as well as stronger local associations compared to smaller model variants. Taken together, these results indicate that Transformer-based language models\u2019 surprisal estimates diverge from human-like expectations due to the superhumanly complex associations they learn for predicting rare words.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9443359375
        },
        {
            "paperId": "55490520ef6b724f5235180d6686121d8cd0332d",
            "corpusId": 274370884,
            "title": "Language Models for Molecular Dynamics",
            "venue": "bioRxiv",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.biorxiv.org/content/biorxiv/early/2024/11/28/2024.11.25.625337.full.pdf",
                "status": "GREEN",
                "license": "CCBYNC",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2024.11.25.625337?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2024.11.25.625337, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2209385989",
                    "name": "Mhd Hussein Murtada"
                },
                {
                    "authorId": "12769846",
                    "name": "Z. F. Brotzakis"
                },
                {
                    "authorId": "2243238956",
                    "name": "M. Vendruscolo"
                }
            ],
            "abstract": "Molecular Dynamics (MD) simulations provide accurate descriptions of the motions of molecular systems, yet their computational demands pose significant challenges in applications in molecular biology and materials science. Given the success of deep learning methods in a wide range of fields, a timely question concerns whether these methods could be leveraged to improve the efficiency of MD simulations. To investigate this possibility, we introduce Molecular Dynamics Language Models (MDLMs), to enable the generation of MD trajectories. In the present implementation, an MDLM is trained on a short classical MD trajectory of a protein, where structural accuracy is maintained through kernel density estimations derived from extensive MD datasets. We illustrate the application of this MDLM in the case of the determination of the free energy landscape a small protein, showing that this approach makes it possible to discover conformational states undersampled in the training data. These results provide initial evidence for the use of language models for the efficient implementation of molecular dynamics.",
            "corpus_id": "274370884",
            "text": "Molecular Dynamics (MD) simulations provide accurate descriptions of the motions of molecular systems, yet their computational demands pose significant challenges in applications in molecular biology and materials science. Given the success of deep learning methods in a wide range of fields, a timely question concerns whether these methods could be leveraged to improve the efficiency of MD simulations. To investigate this possibility, we introduce Molecular Dynamics Language Models (MDLMs), to enable the generation of MD trajectories. In the present implementation, an MDLM is trained on a short classical MD trajectory of a protein, where structural accuracy is maintained through kernel density estimations derived from extensive MD datasets. We illustrate the application of this MDLM in the case of the determination of the free energy landscape a small protein, showing that this approach makes it possible to discover conformational states undersampled in the training data. These results provide initial evidence for the use of language models for the efficient implementation of molecular dynamics.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.77392578125
        },
        {
            "paperId": "db13b49ede9b44ccd95ace924a1a7ef81bbd4a75",
            "corpusId": 275134360,
            "title": "Adaptive Batch Size Schedules for Distributed Training of Language Models with Data and Model Parallelism",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 81,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.21124, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "40897063",
                    "name": "Tim Tsz-Kit Lau"
                },
                {
                    "authorId": "2277230783",
                    "name": "Weijian Li"
                },
                {
                    "authorId": "2266835814",
                    "name": "Chenwei Xu"
                },
                {
                    "authorId": "2284737358",
                    "name": "Han Liu"
                },
                {
                    "authorId": "2284678651",
                    "name": "Mladen Kolar"
                }
            ],
            "abstract": "An appropriate choice of batch sizes in large-scale model training is crucial, yet it involves an intrinsic yet inevitable dilemma: large-batch training improves training efficiency in terms of memory utilization, while generalization performance often deteriorates due to small amounts of gradient noise. Despite this dilemma, the common practice of choosing batch sizes in language model training often prioritizes training efficiency -- employing either constant large sizes with data parallelism or implementing batch size warmup schedules. However, such batch size schedule designs remain heuristic and often fail to adapt to training dynamics, presenting the challenge of designing adaptive batch size schedules. Given the abundance of available datasets and the data-hungry nature of language models, data parallelism has become an indispensable distributed training paradigm, enabling the use of larger batch sizes for gradient computation. However, vanilla data parallelism requires replicas of model parameters, gradients, and optimizer states at each worker, which prohibits training larger models with billions of parameters. To optimize memory usage, more advanced parallelism strategies must be employed. In this work, we propose general-purpose and theoretically principled adaptive batch size schedules compatible with data parallelism and model parallelism. We develop a practical implementation with PyTorch Fully Sharded Data Parallel, facilitating the pretraining of language models of different sizes. We empirically demonstrate that our proposed approaches outperform constant batch sizes and heuristic batch size warmup schedules in the pretraining of models in the Llama 2 family, with particular focus on smaller models with up to 3 billion parameters. We also establish theoretical convergence guarantees for such adaptive batch size schedules with Adam for general smooth nonconvex objectives.",
            "corpus_id": "275134360",
            "text": "An appropriate choice of batch sizes in large-scale model training is crucial, yet it involves an intrinsic yet inevitable dilemma: large-batch training improves training efficiency in terms of memory utilization, while generalization performance often deteriorates due to small amounts of gradient noise. Despite this dilemma, the common practice of choosing batch sizes in language model training often prioritizes training efficiency -- employing either constant large sizes with data parallelism or implementing batch size warmup schedules. However, such batch size schedule designs remain heuristic and often fail to adapt to training dynamics, presenting the challenge of designing adaptive batch size schedules. Given the abundance of available datasets and the data-hungry nature of language models, data parallelism has become an indispensable distributed training paradigm, enabling the use of larger batch sizes for gradient computation. However, vanilla data parallelism requires replicas of model parameters, gradients, and optimizer states at each worker, which prohibits training larger models with billions of parameters. To optimize memory usage, more advanced parallelism strategies must be employed. In this work, we propose general-purpose and theoretically principled adaptive batch size schedules compatible with data parallelism and model parallelism. We develop a practical implementation with PyTorch Fully Sharded Data Parallel, facilitating the pretraining of language models of different sizes. We empirically demonstrate that our proposed approaches outperform constant batch sizes and heuristic batch size warmup schedules in the pretraining of models in the Llama 2 family, with particular focus on smaller models with up to 3 billion parameters. We also establish theoretical convergence guarantees for such adaptive batch size schedules with Adam for general smooth nonconvex objectives.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9736328125
        },
        {
            "paperId": "8144e35c7c245b1b7c630d12fb2dfca623df7f26",
            "corpusId": 274166534,
            "title": "Domain Dynamics: Evaluating Large Language Models in English-Hindi Translation",
            "venue": "ICON",
            "year": 2024,
            "referenceCount": 55,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.icon-1.19, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2331624389",
                    "name": "Soham Bhattacharjee"
                },
                {
                    "authorId": "146259959",
                    "name": "Baban Gain"
                },
                {
                    "authorId": "2273344233",
                    "name": "Asif Ekbal"
                }
            ],
            "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in machine translation, leveraging extensive pre-training on vast amounts of data. However, this generalist training often overlooks domain-specific nuances, leading to potential difficulties when translating specialized texts. In this study, we present a multi-domain test suite, collated from previously published datasets, designed to challenge and evaluate the translation abilities of LLMs. The test suite encompasses diverse domains such as judicial, education, literature (specifically religious texts), and noisy user-generated content from online product reviews and forums like Reddit. Each domain consists of approximately 250-300 sentences, carefully curated and randomized in the final compilation. This English-to-Hindi dataset aims to evaluate and expose the limitations of LLM-based translation systems, offering valuable insights into areas requiring further research and development. We have submitted the dataset to WMT24 Break the LLM",
            "corpus_id": "274166534",
            "text": "Large Language Models (LLMs) have demonstrated impressive capabilities in machine translation, leveraging extensive pre-training on vast amounts of data. However, this generalist training often overlooks domain-specific nuances, leading to potential difficulties when translating specialized texts. In this study, we present a multi-domain test suite, collated from previously published datasets, designed to challenge and evaluate the translation abilities of LLMs. The test suite encompasses diverse domains such as judicial, education, literature (specifically religious texts), and noisy user-generated content from online product reviews and forums like Reddit. Each domain consists of approximately 250-300 sentences, carefully curated and randomized in the final compilation. This English-to-Hindi dataset aims to evaluate and expose the limitations of LLM-based translation systems, offering valuable insights into areas requiring further research and development. We have submitted the dataset to WMT24 Break the LLM",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.443359375
        },
        {
            "paperId": "c96297261467b5daa2d01227496a70d444602434",
            "corpusId": 261951743,
            "title": "Baichuan 2: Open Large-scale Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 83,
            "citationCount": 749,
            "influentialCitationCount": 88,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.10305",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.10305, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2208290330",
                    "name": "Ai Ming Yang"
                },
                {
                    "authorId": "2264098855",
                    "name": "Bin Xiao"
                },
                {
                    "authorId": "2242351985",
                    "name": "Bingning Wang"
                },
                {
                    "authorId": "152705071",
                    "name": "Borong Zhang"
                },
                {
                    "authorId": "2221566410",
                    "name": "Ce Bian"
                },
                {
                    "authorId": "2242126323",
                    "name": "Chao Yin"
                },
                {
                    "authorId": "2242124793",
                    "name": "Chenxu Lv"
                },
                {
                    "authorId": "2242124866",
                    "name": "Da Pan"
                },
                {
                    "authorId": "2243393517",
                    "name": "Dian Wang"
                },
                {
                    "authorId": "2284219972",
                    "name": "Dong Yan"
                },
                {
                    "authorId": "47829900",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "2242124540",
                    "name": "Fei Deng"
                },
                {
                    "authorId": "2242730297",
                    "name": "Feng Wang"
                },
                {
                    "authorId": "2242207666",
                    "name": "Feng Liu"
                },
                {
                    "authorId": "2242123567",
                    "name": "Guangwei Ai"
                },
                {
                    "authorId": "2242124536",
                    "name": "Guosheng Dong"
                },
                {
                    "authorId": "47941144",
                    "name": "Hai Zhao"
                },
                {
                    "authorId": "2116309368",
                    "name": "Hang Xu"
                },
                {
                    "authorId": "2118181226",
                    "name": "Hao-Lun Sun"
                },
                {
                    "authorId": "2242180178",
                    "name": "Hongda Zhang"
                },
                {
                    "authorId": "2242276762",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "2154630502",
                    "name": "Jiaming Ji"
                },
                {
                    "authorId": "2242192116",
                    "name": "Jian Xie"
                },
                {
                    "authorId": "14548852",
                    "name": "Juntao Dai"
                },
                {
                    "authorId": "144669461",
                    "name": "Kuncheng Fang"
                },
                {
                    "authorId": "2242130652",
                    "name": "Lei Su"
                },
                {
                    "authorId": "2238154661",
                    "name": "Liang Song"
                },
                {
                    "authorId": "2242309097",
                    "name": "Lifeng Liu"
                },
                {
                    "authorId": "2242124220",
                    "name": "Liyun Ru"
                },
                {
                    "authorId": "2242280004",
                    "name": "Luyao Ma"
                },
                {
                    "authorId": "2242171637",
                    "name": "Mang Wang"
                },
                {
                    "authorId": "2210950163",
                    "name": "Mickel Liu"
                },
                {
                    "authorId": "2244046109",
                    "name": "Mingan Lin"
                },
                {
                    "authorId": "2242123471",
                    "name": "Nuolan Nie"
                },
                {
                    "authorId": "2197287293",
                    "name": "Pei Guo"
                },
                {
                    "authorId": "2217316509",
                    "name": "Ruiyang Sun"
                },
                {
                    "authorId": "1689115",
                    "name": "Zhang Tao"
                },
                {
                    "authorId": "2299324291",
                    "name": "Tianpeng Li"
                },
                {
                    "authorId": null,
                    "name": "Tianyu Li"
                },
                {
                    "authorId": "145859270",
                    "name": "Wei Cheng"
                },
                {
                    "authorId": "2144305331",
                    "name": "Weipeng Chen"
                },
                {
                    "authorId": "2441459",
                    "name": "Xiangrong Zeng"
                },
                {
                    "authorId": "2242180212",
                    "name": "Xiaochuan Wang"
                },
                {
                    "authorId": "2242278442",
                    "name": "Xiaoxi Chen"
                },
                {
                    "authorId": "2242124145",
                    "name": "Xin Men"
                },
                {
                    "authorId": "2242135086",
                    "name": "Xin Yu"
                },
                {
                    "authorId": "2190800297",
                    "name": "Xuehai Pan"
                },
                {
                    "authorId": "2178403464",
                    "name": "Yan-Bin Shen"
                },
                {
                    "authorId": "2242155514",
                    "name": "Yiding Wang"
                },
                {
                    "authorId": "2242175361",
                    "name": "Yiyu Li"
                },
                {
                    "authorId": "2242171277",
                    "name": "Youxin Jiang"
                },
                {
                    "authorId": "2244143098",
                    "name": "Yuchen Gao"
                },
                {
                    "authorId": "2242155612",
                    "name": "Yupeng Zhang"
                },
                {
                    "authorId": "2183768304",
                    "name": "Zenan Zhou"
                },
                {
                    "authorId": "2242279152",
                    "name": "Zhiying Wu"
                }
            ],
            "abstract": "Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2.",
            "corpus_id": "261951743",
            "text": "Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.72021484375
        },
        {
            "paperId": "370ece25c851b4eefb8a1b81f3110958e8ca0b20",
            "corpusId": 263310873,
            "title": "Training and inference of large language models using 8-bit floating point",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 37,
            "citationCount": 19,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.17224",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.17224, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2249718952",
                    "name": "Sergio P. Perez"
                },
                {
                    "authorId": "2256617265",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2249532648",
                    "name": "James Briggs"
                },
                {
                    "authorId": "2249530912",
                    "name": "Charlie Blake"
                },
                {
                    "authorId": "2249532446",
                    "name": "Josh Levy-Kramer"
                },
                {
                    "authorId": "2249532512",
                    "name": "Paul Balanca"
                },
                {
                    "authorId": "2249537360",
                    "name": "Carlo Luschi"
                },
                {
                    "authorId": "2249530931",
                    "name": "Stephen Barlow"
                },
                {
                    "authorId": "2241176275",
                    "name": "Andrew W. Fitzgibbon"
                }
            ],
            "abstract": "FP8 formats are gaining popularity to boost the computational efficiency for training and inference of large deep learning models. Their main challenge is that a careful choice of scaling is needed to prevent degradation due to the reduced dynamic range compared to higher-precision formats. Although there exists ample literature about selecting such scalings for INT formats, this critical aspect has yet to be addressed for FP8. This paper presents a methodology to select the scalings for FP8 linear layers, based on dynamically updating per-tensor scales for the weights, gradients and activations. We apply this methodology to train and validate large language models of the type of GPT and Llama 2 using FP8, for model sizes ranging from 111M to 70B. To facilitate the understanding of the FP8 dynamics, our results are accompanied by plots of the per-tensor scale distribution for weights, activations and gradients during both training and inference.",
            "corpus_id": "263310873",
            "text": "FP8 formats are gaining popularity to boost the computational efficiency for training and inference of large deep learning models. Their main challenge is that a careful choice of scaling is needed to prevent degradation due to the reduced dynamic range compared to higher-precision formats. Although there exists ample literature about selecting such scalings for INT formats, this critical aspect has yet to be addressed for FP8. This paper presents a methodology to select the scalings for FP8 linear layers, based on dynamically updating per-tensor scales for the weights, gradients and activations. We apply this methodology to train and validate large language models of the type of GPT and Llama 2 using FP8, for model sizes ranging from 111M to 70B. To facilitate the understanding of the FP8 dynamics, our results are accompanied by plots of the per-tensor scale distribution for weights, activations and gradients during both training and inference.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8486328125
        },
        {
            "paperId": "ca997f1a733e53ad0fa29041246ff655243e8c1b",
            "corpusId": 273850118,
            "title": "Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "referenceCount": 63,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.03884, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2210797120",
                    "name": "Zhijian Zhuo"
                },
                {
                    "authorId": "2125061111",
                    "name": "Ya Wang"
                },
                {
                    "authorId": "122290781",
                    "name": "Yutao Zeng"
                },
                {
                    "authorId": "2108764287",
                    "name": "Xiaoqing Li"
                },
                {
                    "authorId": "2323563576",
                    "name": "Xun Zhou"
                },
                {
                    "authorId": "2329571633",
                    "name": "Jinwen Ma"
                }
            ],
            "abstract": "Transformers have found extensive applications across various domains due to the powerful fitting capabilities. This success can be partially attributed to their inherent nonlinearity. Thus, in addition to the ReLU function employed in the original transformer architecture, researchers have explored alternative modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment representational capacity. In this paper, we propose a novel category of polynomial composition activations (PolyCom), designed to optimize the dynamics of transformers. Theoretically, we provide a comprehensive mathematical analysis of PolyCom, highlighting its enhanced expressivity and efficacy relative to other activation functions. Notably, we demonstrate that networks incorporating PolyCom achieve the $\\textbf{optimal approximation rate}$, indicating that PolyCom networks require minimal parameters to approximate general smooth functions in Sobolev spaces. We conduct empirical experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. By substituting conventional activation functions with PolyCom, we enable LLMs to capture higher-order interactions within the data, thus improving performance metrics in terms of accuracy and convergence rates. Extensive experimental results demonstrate the effectiveness of our method, showing substantial improvements over other activation functions. Code is available at https://github.com/BryceZhuo/PolyCom.",
            "corpus_id": "273850118",
            "text": "Transformers have found extensive applications across various domains due to the powerful fitting capabilities. This success can be partially attributed to their inherent nonlinearity. Thus, in addition to the ReLU function employed in the original transformer architecture, researchers have explored alternative modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment representational capacity. In this paper, we propose a novel category of polynomial composition activations (PolyCom), designed to optimize the dynamics of transformers. Theoretically, we provide a comprehensive mathematical analysis of PolyCom, highlighting its enhanced expressivity and efficacy relative to other activation functions. Notably, we demonstrate that networks incorporating PolyCom achieve the $\\textbf{optimal approximation rate}$, indicating that PolyCom networks require minimal parameters to approximate general smooth functions in Sobolev spaces. We conduct empirical experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. By substituting conventional activation functions with PolyCom, we enable LLMs to capture higher-order interactions within the data, thus improving performance metrics in terms of accuracy and convergence rates. Extensive experimental results demonstrate the effectiveness of our method, showing substantial improvements over other activation functions. Code is available at https://github.com/BryceZhuo/PolyCom.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.93994140625
        },
        {
            "paperId": "b2ee0f84af3c1aff059bf67b9b41c836d67db137",
            "corpusId": 269626444,
            "title": "Towards a Theoretical Understanding of the 'Reversal Curse' via Training Dynamics",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "referenceCount": 68,
            "citationCount": 18,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.04669, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2254086052",
                    "name": "Hanlin Zhu"
                },
                {
                    "authorId": "2253949316",
                    "name": "Baihe Huang"
                },
                {
                    "authorId": "2300355734",
                    "name": "Shaolun Zhang"
                },
                {
                    "authorId": "2292409976",
                    "name": "Michael Jordan"
                },
                {
                    "authorId": "2258657022",
                    "name": "Jiantao Jiao"
                },
                {
                    "authorId": "2254299987",
                    "name": "Yuandong Tian"
                },
                {
                    "authorId": "2253435878",
                    "name": "Stuart Russell"
                }
            ],
            "abstract": "Auto-regressive large language models (LLMs) show impressive capacities to solve many complex reasoning tasks while struggling with some simple logical reasoning tasks such as inverse search: when trained on '$A \\to B$' (e.g., 'Tom is the parent of John'), LLM fails to directly conclude '$B \\gets A$' (e.g., 'John is the child of Tom') during inference even if the two sentences are semantically identical, which is known as the 'reversal curse'. In this paper, we theoretically analyze the reversal curse via the training dynamics of (stochastic) gradient descent for two auto-regressive models: (1) a bilinear model that can be viewed as a simplification of a one-layer transformer; (2) one-layer transformers under certain assumptions. Our analysis reveals that for both models, the reversal curse is a consequence of the (effective) model weights 'asymmetry', i.e., the increase of weights from a token $A$ to token $B$ during training does not necessarily cause the increase of the weights from $B$ to $A$, which is caused by the training dynamics under certain choice of loss function and the optimization space of model parameters. Moreover, our analysis can be naturally applied to other logical reasoning tasks such as chain-of-thought (COT), which provides a new perspective different from previous work that focuses on expressivity. Finally, we conduct experiments to validate our theory on multi-layer transformers under different settings. Our code is available at https://github.com/marlo-z/reversal_curse_analysis/.",
            "corpus_id": "269626444",
            "text": "Auto-regressive large language models (LLMs) show impressive capacities to solve many complex reasoning tasks while struggling with some simple logical reasoning tasks such as inverse search: when trained on '$A \\to B$' (e.g., 'Tom is the parent of John'), LLM fails to directly conclude '$B \\gets A$' (e.g., 'John is the child of Tom') during inference even if the two sentences are semantically identical, which is known as the 'reversal curse'. In this paper, we theoretically analyze the reversal curse via the training dynamics of (stochastic) gradient descent for two auto-regressive models: (1) a bilinear model that can be viewed as a simplification of a one-layer transformer; (2) one-layer transformers under certain assumptions. Our analysis reveals that for both models, the reversal curse is a consequence of the (effective) model weights 'asymmetry', i.e., the increase of weights from a token $A$ to token $B$ during training does not necessarily cause the increase of the weights from $B$ to $A$, which is caused by the training dynamics under certain choice of loss function and the optimization space of model parameters. Moreover, our analysis can be naturally applied to other logical reasoning tasks such as chain-of-thought (COT), which provides a new perspective different from previous work that focuses on expressivity. Finally, we conduct experiments to validate our theory on multi-layer transformers under different settings. Our code is available at https://github.com/marlo-z/reversal_curse_analysis/.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.97216796875
        },
        {
            "paperId": "8273a4b1930e6cedf427f37022d4f499f5374420",
            "corpusId": 258967920,
            "title": "SlimFit: Memory-Efficient Fine-Tuning of Transformer-based Models Using Training Dynamics",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2023,
            "referenceCount": 42,
            "citationCount": 4,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.18513",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.18513, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "143833550",
                    "name": "A. Ardakani"
                },
                {
                    "authorId": "1751623947",
                    "name": "Altan Haan"
                },
                {
                    "authorId": "2028613058",
                    "name": "Shangyin Tan"
                },
                {
                    "authorId": "2319155",
                    "name": "Doru-Thom Popovici"
                },
                {
                    "authorId": "2172244319",
                    "name": "Alvin Cheung"
                },
                {
                    "authorId": "1702354",
                    "name": "Costin Iancu"
                },
                {
                    "authorId": "145741786",
                    "name": "Koushik Sen"
                }
            ],
            "abstract": "Transformer-based models, such as BERT and ViT, have achieved state-of-the-art results across different natural language processing (NLP) and computer vision (CV) tasks. However, these models are extremely memory intensive during their fine-tuning process, making them difficult to deploy on GPUs with limited memory resources. To address this issue, we introduce a new tool called SlimFit that reduces the memory requirements of these models by dynamically analyzing their training dynamics and freezing less-contributory layers during fine-tuning. The layers to freeze are chosen using a runtime inter-layer scheduling algorithm. This allows SlimFit to freeze up to 95% of layers and reduce the overall on-device GPU memory usage of transformer-based models such as ViT and BERT by an average of 2.2x, across different NLP and CV benchmarks/datasets such as GLUE, SQuAD 2.0, CIFAR-10, CIFAR-100 and ImageNet with an average degradation of 0.2% in accuracy. For such NLP and CV tasks, SlimFit can reduce up to 3.1x the total on-device memory usage with an accuracy degradation of only up to 0.4%. As a result, while fine-tuning of ViT on ImageNet and BERT on SQuAD 2.0 with a batch size of 128 requires 3 and 2 32GB GPUs, respectively, SlimFit enables fine-tuning them on a single 32GB GPU without any significant accuracy degradation. The code of SlimFit is available at https://github.com/arashardakani/SlimFit.",
            "corpus_id": "258967920",
            "text": "Transformer-based models, such as BERT and ViT, have achieved state-of-the-art results across different natural language processing (NLP) and computer vision (CV) tasks. However, these models are extremely memory intensive during their fine-tuning process, making them difficult to deploy on GPUs with limited memory resources. To address this issue, we introduce a new tool called SlimFit that reduces the memory requirements of these models by dynamically analyzing their training dynamics and freezing less-contributory layers during fine-tuning. The layers to freeze are chosen using a runtime inter-layer scheduling algorithm. This allows SlimFit to freeze up to 95% of layers and reduce the overall on-device GPU memory usage of transformer-based models such as ViT and BERT by an average of 2.2x, across different NLP and CV benchmarks/datasets such as GLUE, SQuAD 2.0, CIFAR-10, CIFAR-100 and ImageNet with an average degradation of 0.2% in accuracy. For such NLP and CV tasks, SlimFit can reduce up to 3.1x the total on-device memory usage with an accuracy degradation of only up to 0.4%. As a result, while fine-tuning of ViT on ImageNet and BERT on SQuAD 2.0 with a batch size of 128 requires 3 and 2 32GB GPUs, respectively, SlimFit enables fine-tuning them on a single 32GB GPU without any significant accuracy degradation. The code of SlimFit is available at https://github.com/arashardakani/SlimFit.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.830078125
        }
    ],
    "quotes": {
        "cost": 0.22381200000000004,
        "quotes": [
            {
                "idx": 0,
                "key": "[237491997 | Choshen et al. | 2021 | Citations: 29]",
                "snippets": "We begin ( \u00a73) by establishing that NLMs learn grammatical phenomena in a consistent order. We evaluate NLMs at different time points along their training, showing that the performance on linguistic phenomena across initializations is highly correlated. We further find many similarities in the set of examples that they correctly classify.\n\nStill, models of different architectures learn at a different pace, and hence cannot be directly compared at identical time points. In \u00a73.3, we overcome this by re-scaling the timeline. We then show that despite architectural differences, NLMs present highly correlated performance trajectories. In \u00a73.4, we further demonstrate that even the choice of training data has minor influence on the results. Finally, in \u00a73.5 we show that the learning dynamics essentially follows a single dimension. Namely, where the average performance is similar, success on linguistic phenomena is also similar.\n\nWe proceed by analyzing the early stages of learning in \u00a74. We find that, at first, NLMs rely mostly on local cues and not on word order. They thus resemble bag-of-words models over a window of the preceding tokens. Later stages seem to drift further away from bag-of-words models toward n-gram models, and with time seem to be more sensitive to structural cues. We also find evidence that some latent features that the model learns may not be related to linguistic phenomena.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1412,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We begin ( \u00a73) by establishing that NLMs learn grammatical phenomena in a consistent order. We evaluate NLMs at different time points along their training, showing that the performance on linguistic phenomena across initializations is highly correlated. We further find many similarities in the set of examples that they correctly classify.\n\nStill, models of different architectures learn at a different pace, and hence cannot be directly compared at identical time points. In \u00a73.3, we overcome this by re-scaling the timeline. We then show that despite architectural differences, NLMs present highly correlated performance trajectories. In \u00a73.4, we further demonstrate that even the choice of training data has minor influence on the results. Finally, in \u00a73.5 we show that the learning dynamics essentially follows a single dimension. Namely, where the average performance is similar, success on linguistic phenomena is also similar.\n\nWe proceed by analyzing the early stages of learning in \u00a74. We find that, at first, NLMs rely mostly on local cues and not on word order. They thus resemble bag-of-words models over a window of the preceding tokens. Later stages seem to drift further away from bag-of-words models toward n-gram models, and with time seem to be more sensitive to structural cues. We also find evidence that some latent features that the model learns may not be related to linguistic phenomena."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[247656607 | Teehan et al. | 2022 | Citations: 11]",
                "snippets": "Training dynamics is an emerging field of research, promising to improve our understanding of knowledge acquisition in neural networks and offering insights into the utility of pre-trained models and embedded representations for downstream tasks. Most studies of Transformers (e.g. RoBERTa (Liu et al., 2021)) and LSTMs (Hochreiter et al., 1997) agree that models acquire linguistic knowledge early in the learning process.\n\nLocal syntactic information, such as parts of speech, is learned earlier than information encoding long-distance dependencies (e.g. topic) Saphra, 2021). Exploration of AL-BERT (Lan et al., 2019) and LSTM-based networks reveals different learning patterns for function and content words with more fine-grained distinctions within these categories including part of speech and verb form (Saphra, 2021;(Chiang et al., 2020).\n\nDifferences in learning trajectory were also observed between layers. In LSTMs, recurrent layers become more task-independent over the course of training, while embeddings become more taskspecific (Saphra, 2021). In Transformer-based architectures, i.e.: ALBERT and ELECTRA, (Chiang et al., 2020) observe differences in performance patterns between the top and last layers.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[222140842 | Chiang et al. | 2020 | Citations: 42]": "While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL."
                },
                "metadata": [
                    {
                        "section_title": "Training Dynamics of Internal Representation Development",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1232,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "237101158",
                            "1915014",
                            "222140842",
                            "222140842"
                        ],
                        "quote": "Training dynamics is an emerging field of research, promising to improve our understanding of knowledge acquisition in neural networks and offering insights into the utility of pre-trained models and embedded representations for downstream tasks. Most studies of Transformers (e.g. RoBERTa (Liu et al., 2021)) and LSTMs (Hochreiter et al., 1997) agree that models acquire linguistic knowledge early in the learning process.\n\nLocal syntactic information, such as parts of speech, is learned earlier than information encoding long-distance dependencies (e.g. topic) Saphra, 2021). Exploration of AL-BERT (Lan et al., 2019) and LSTM-based networks reveals different learning patterns for function and content words with more fine-grained distinctions within these categories including part of speech and verb form (Saphra, 2021;(Chiang et al., 2020).\n\nDifferences in learning trajectory were also observed between layers. In LSTMs, recurrent layers become more task-independent over the course of training, while embeddings become more taskspecific (Saphra, 2021). In Transformer-based architectures, i.e.: ALBERT and ELECTRA, (Chiang et al., 2020) observe differences in performance patterns between the top and last layers."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[248986465 | Tirumala et al. | 2022 | Citations: 197]",
                "snippets": "We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process. We measure the effects of dataset size, learning rate, and model size on memorization, finding that larger language models memorize training data faster across all settings. Surprisingly, we show that larger models can memorize a larger portion of the data before over-fitting and tend to forget less throughout the training process. We also analyze the memorization dynamics of different parts of speech and find that models memorize nouns and numbers first; we hypothesize and provide empirical evidence that nouns and numbers act as a unique identifier for memorizing individual training examples.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process. We measure the effects of dataset size, learning rate, and model size on memorization, finding that larger language models memorize training data faster across all settings. Surprisingly, we show that larger models can memorize a larger portion of the data before over-fitting and tend to forget less throughout the training process. We also analyze the memorization dynamics of different parts of speech and find that models memorize nouns and numbers first; we hypothesize and provide empirical evidence that nouns and numbers act as a unique identifier for memorizing individual training examples.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[254877112 | Xia et al. | 2022 | Citations: 64]",
                "snippets": "We attempt to make progress to answer these questions by studying the training trajectories of differently-sized OPT models (Zhang et al., 2022) through analyzing their intermediate checkpoints.\n\nIn contrast to prior work, which studies the trajectories of small models with up to 300M parameters (Liu et al., 2021;Choshen et al., 2022;Blevins et al., 2022) or focuses on the language modeling objective alone (Kaplan et al., 2020;Hernandez et al., 2021Hernandez et al., , 2022, we are the first to comprehensively study the training trajectories of large-scale autoregressive language models with up to 175B parameters across a wide range of settings.\n\nRepeatedly across training and different model scales, we analyze three aspects of model performance: (i) next-token prediction on subsets of tokens (ii) sequence-level generation and (iii) downstream task performance. We use perplexity, which is closely tied to language model evaluation, as the major metric throughout the study.\n\nFor next-token prediction ( \u00a73), we study the trajectory by categorizing each token's prediction as stagnated, upward or downward according to its perplexity trend as training progresses. We find each category comprising a significant number of tokens: while a significant number of tokens' perplexity stagnate, a subset of tokens with an increasing perplexity in smaller models exhibit a doubledescent trend (Nakkiran et al., 2020) where perplexity increases and then decreases in larger models. These behaviors primarily emerge at a similar validation perplexity across model scales",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 513,
                        "end": 2084,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We attempt to make progress to answer these questions by studying the training trajectories of differently-sized OPT models (Zhang et al., 2022) through analyzing their intermediate checkpoints.\n\nIn contrast to prior work, which studies the trajectories of small models with up to 300M parameters (Liu et al., 2021;Choshen et al., 2022;Blevins et al., 2022) or focuses on the language modeling objective alone (Kaplan et al., 2020;Hernandez et al., 2021Hernandez et al., , 2022, we are the first to comprehensively study the training trajectories of large-scale autoregressive language models with up to 175B parameters across a wide range of settings.\n\nRepeatedly across training and different model scales, we analyze three aspects of model performance: (i) next-token prediction on subsets of tokens (ii) sequence-level generation and (iii) downstream task performance. We use perplexity, which is closely tied to language model evaluation, as the major metric throughout the study.\n\nFor next-token prediction ( \u00a73), we study the trajectory by categorizing each token's prediction as stagnated, upward or downward according to its perplexity trend as training progresses. We find each category comprising a significant number of tokens: while a significant number of tokens' perplexity stagnate, a subset of tokens with an increasing perplexity in smaller models exhibit a doubledescent trend (Nakkiran et al., 2020) where perplexity increases and then decreases in larger models. These behaviors primarily emerge at a similar validation perplexity across model scales"
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[257921893 | Biderman et al. | 2023 | Citations: 1306]",
                "snippets": "How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 821,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[261277016 | Chang et al. | 2023 | Citations: 13]",
                "snippets": "Previous work has studied the pre-training dynamics of language models (Saphra, 0). Choshen et al. (2022) and (Evanson et al., 2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021). Olsson et al. (2022) find that copy mechanisms (\"induction heads\" for in-context learning) appear at an inflection point during pre-training. These results establish when a variety of abilities emerge in language models. Our work studies more fine-grained learning trajectories by evaluating individual tokens in context...Previous work has demonstrated that language models exhibit fine-grained learning patterns that are not captured by the corpus-level loss curve (related work in \u00a72). In particular, sudden increases and decreases in example loss ( \u00a75 and (Xia et al., 2022)) may be somewhat surprising given that the pre-training text is i.i.d. for all pre-training steps. By demonstrating that many of these sudden changes are consistent regardless of random initialization and data shuffling ( \u00a75.2), our work indicates that some instances of sudden learning and \"forgetting\" are not due to random chance or the specific examples observed in a given step.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[254877112 | Xia et al. | 2022 | Citations: 64]": "Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al., 2022)\u2014from 125M to 175B parameters\u2014on next-token prediction, sequence-level generation and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior (Nakkiran et al., 2020); 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; and 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation.",
                    "[222140842 | Chiang et al. | 2020 | Citations: 42]": "While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL.",
                    "[233289478 | Liu et al. | 2021 | Citations: 87]": "Models of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers\"probing\"the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster.",
                    "[259089351 | Evanson et al. | 2023 | Citations: 29]": "During language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning trajectory remain largely unknown. To investigate this, we here compare the learning trajectories of deep language models to those of children. Specifically, we test whether, during its training, GPT-2 exhibits stages of language acquisition comparable to those observed in children aged between 18 months and 6 years. For this, we train 48 GPT-2 models from scratch and evaluate their syntactic and semantic abilities at each training step, using 96 probes curated from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these evaluations with the behavior of 54 children during language production. Our analyses reveal three main findings. First, similarly to children, the language models tend to learn linguistic skills in a systematic order. Second, this learning scheme is parallel: the language tasks that are learned last improve from the very first training steps. Third, some - but not all - learning stages are shared between children and these language models. Overall, these results shed new light on the principles of language acquisition, and highlight important divergences in how humans and modern algorithms learn to process natural language."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 782,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 96
                            },
                            {
                                "start": 97,
                                "end": 291
                            },
                            {
                                "start": 292,
                                "end": 461
                            },
                            {
                                "start": 462,
                                "end": 603
                            },
                            {
                                "start": 604,
                                "end": 682
                            },
                            {
                                "start": 683,
                                "end": 783
                            }
                        ],
                        "ref_mentions": [
                            "267903883",
                            "259089351",
                            "222140842",
                            "233289478"
                        ],
                        "quote": "Previous work has studied the pre-training dynamics of language models (Saphra, 0). Choshen et al. (2022) and (Evanson et al., 2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021). Olsson et al. (2022) find that copy mechanisms (\"induction heads\" for in-context learning) appear at an inflection point during pre-training. These results establish when a variety of abilities emerge in language models. Our work studies more fine-grained learning trajectories by evaluating individual tokens in context"
                    },
                    {
                        "section_title": "Discussion",
                        "pdf_hash": "",
                        "start": 421,
                        "end": 1059,
                        "sentence_offsets": [
                            {
                                "start": 421,
                                "end": 586
                            },
                            {
                                "start": 587,
                                "end": 773
                            },
                            {
                                "start": 774,
                                "end": 1061
                            }
                        ],
                        "ref_mentions": [
                            "254877112"
                        ],
                        "quote": "Previous work has demonstrated that language models exhibit fine-grained learning patterns that are not captured by the corpus-level loss curve (related work in \u00a72). In particular, sudden increases and decreases in example loss ( \u00a75 and (Xia et al., 2022)) may be somewhat surprising given that the pre-training text is i.i.d. for all pre-training steps. By demonstrating that many of these sudden changes are consistent regardless of random initialization and data shuffling ( \u00a75.2), our work indicates that some instances of sudden learning and \"forgetting\" are not due to random chance or the specific examples observed in a given step."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[261822542 | Chen et al. | 2023 | Citations: 74]",
                "snippets": "While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2022), or phase transitions (Olsson et al., 2022). The interpretability literature rarely illuminates how these capabilities emerge, in part because most analyses only examine the final trained model. Instead, we consider developmental analysis as a complementary explanatory lens.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[253117181 | Caballero et al. | 2022 | Citations: 76]": "We present a smoothly broken power law functional form (that we refer to as a Broken Neural Scaling Law (BNSL)) that accurately models&extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as amount of compute used for training (or inference), number of model parameters, training dataset size, model input size, number of training steps, or upstream performance varies) for various architectures&for each of various tasks within a large&diverse set of upstream&downstream tasks, in zero-shot, prompted,&finetuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, AI capabilities, robotics, out-of-distribution (OOD) generalization, continual learning, transfer learning, uncertainty estimation / calibration, OOD detection, adversarial robustness, distillation, sparsity, retrieval, quantization, pruning, fairness, molecules, computer programming/coding, math word problems,\"emergent phase transitions\", arithmetic, supervised learning, unsupervised/self-supervised learning,&reinforcement learning (single agent&multi-agent). When compared to other functional forms for neural scaling, this functional form yields extrapolations of scaling behavior that are considerably more accurate on this set. Moreover, this functional form accurately models&extrapolates scaling behavior that other functional forms are incapable of expressing such as the nonmonotonic transitions present in the scaling behavior of phenomena such as double descent&the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Lastly, we use this functional form to glean insights about the limit of the predictability of scaling behavior. Code is available at https://github.com/ethancaballero/broken_neural_scaling_laws"
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 888,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 144
                            },
                            {
                                "start": 145,
                                "end": 233
                            },
                            {
                                "start": 234,
                                "end": 381
                            },
                            {
                                "start": 382,
                                "end": 657
                            },
                            {
                                "start": 658,
                                "end": 807
                            },
                            {
                                "start": 808,
                                "end": 888
                            }
                        ],
                        "ref_mentions": [
                            "253117181"
                        ],
                        "quote": "While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2022), or phase transitions (Olsson et al., 2022). The interpretability literature rarely illuminates how these capabilities emerge, in part because most analyses only examine the final trained model. Instead, we consider developmental analysis as a complementary explanatory lens."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[262217060 | Long et al. | 2023 | Citations: 10]",
                "snippets": "Next, we plot the same quantities when the network is trained with SAM, with \u03c1 = 0.3, in Figure 13. Here, the operator norm of the Hessian is significantly less than when SGD is used, and we see evidence that training in SAM operates at the edge of stability analyzed in Section 2. In Figure 14, we zoom in on the lower part of the curve, and plot the operator norm of the Hessian, to examine the relationship between this quantity and the SAM edge in more detail...Figure 15 contains plots of the training loss, once again estimated per-minibatch. We included these mainly to motivate the combinations of hyperparameters where we examined other aspects of the dynamics of SAM. As expected, while SAM does take longer to achieve a certain loss, it ultimately achieves training error similar to SGD, but with less sharpness...Figure 16 contains plots of the alignment, once again estimated per-minibatch. For the large learning rates, late in training, despite the sampling noise arising from the use of minibatches, we see a systematic tendency for the SAM gradients align more closely with the principal eigenvector of the Hessian than the gradients at the initial solution. However, for the smallest learning rates, the opposite holds.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Language modeling",
                        "pdf_hash": "",
                        "start": 109,
                        "end": 572,
                        "sentence_offsets": [
                            {
                                "start": 109,
                                "end": 208
                            },
                            {
                                "start": 209,
                                "end": 573
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Next, we plot the same quantities when the network is trained with SAM, with \u03c1 = 0.3, in Figure 13. Here, the operator norm of the Hessian is significantly less than when SGD is used, and we see evidence that training in SAM operates at the edge of stability analyzed in Section 2. In Figure 14, we zoom in on the lower part of the curve, and plot the operator norm of the Hessian, to examine the relationship between this quantity and the SAM edge in more detail"
                    },
                    {
                        "section_title": "Language modeling",
                        "pdf_hash": "",
                        "start": 576,
                        "end": 933,
                        "sentence_offsets": [
                            {
                                "start": 576,
                                "end": 658
                            },
                            {
                                "start": 659,
                                "end": 787
                            },
                            {
                                "start": 788,
                                "end": 933
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Figure 15 contains plots of the training loss, once again estimated per-minibatch. We included these mainly to motivate the combinations of hyperparameters where we examined other aspects of the dynamics of SAM. As expected, while SAM does take longer to achieve a certain loss, it ultimately achieves training error similar to SGD, but with less sharpness"
                    },
                    {
                        "section_title": "Language modeling",
                        "pdf_hash": "",
                        "start": 936,
                        "end": 1349,
                        "sentence_offsets": [
                            {
                                "start": 936,
                                "end": 1014
                            },
                            {
                                "start": 1015,
                                "end": 1286
                            },
                            {
                                "start": 1287,
                                "end": 1348
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Figure 16 contains plots of the alignment, once again estimated per-minibatch. For the large learning rates, late in training, despite the sampling noise arising from the use of minibatches, we see a systematic tendency for the SAM gradients align more closely with the principal eigenvector of the Hessian than the gradients at the initial solution. However, for the smallest learning rates, the opposite holds."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[264935245 | Quirke et al. | 2023 | Citations: 5]",
                "snippets": "Training dynamics Several works (Olsson et al., 2022;Nanda et al., 2023) discovered circuits in language models that seem causally relevant to emergent capabilities and can be ablated to reduce the capabilities. Michaud et al. (2023) postulated that many capabilities in language models emerge rapidly in phase transitions and proposed a model in which any capabilities which appear to instead form gradually can be decomposed into discrete, additive capabilities which emerge in independent phase transitions. Finally, several works (Varma et al., 2023;Nanda et al., 2023) observed the phenomenon of grokking, where a model that has already achieved near-perfect performance on the training data transitions to a mechanism that generalizes. \n\nWhile only a minority of circuits-style analysis has focused on model development, a wealth of results in the field of developmental interpretability have characterized the training dynamics of language models using other units of analysis. Behavioral analysis has shown that many model capabilities, such as predicting parts of speech and individual tokens, are learned during independent phase transitions characterized by varying slopes and points of maximum curvature (Chiang et al., 2020)Wei et al., 2022). Raghu et al. (2017) proposed singular vector canonical correlation analysis, a tool which allows different model layers and networks to be compared, and showed that early network layers converge earlier in training than late layers. Saphra and Lopez (2020b,a) found that LSTM models learn hierarchical sequential features in natural language bottom up -shorter sequences are learned first and form the basis for the representation of longer sequences that contain the shorter ones. Finally, Saxe et al. (2019) found that the features are learned in an order determined by the magnitude of the corresponding singular value in the model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[222140842 | Chiang et al. | 2020 | Citations: 42]": "While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1891,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 211
                            },
                            {
                                "start": 212,
                                "end": 510
                            },
                            {
                                "start": 511,
                                "end": 741
                            },
                            {
                                "start": 744,
                                "end": 984
                            },
                            {
                                "start": 985,
                                "end": 1255
                            },
                            {
                                "start": 1256,
                                "end": 1488
                            },
                            {
                                "start": 1489,
                                "end": 1737
                            },
                            {
                                "start": 1738,
                                "end": 1891
                            }
                        ],
                        "ref_mentions": [
                            "222140842"
                        ],
                        "quote": "Training dynamics Several works (Olsson et al., 2022;Nanda et al., 2023) discovered circuits in language models that seem causally relevant to emergent capabilities and can be ablated to reduce the capabilities. Michaud et al. (2023) postulated that many capabilities in language models emerge rapidly in phase transitions and proposed a model in which any capabilities which appear to instead form gradually can be decomposed into discrete, additive capabilities which emerge in independent phase transitions. Finally, several works (Varma et al., 2023;Nanda et al., 2023) observed the phenomenon of grokking, where a model that has already achieved near-perfect performance on the training data transitions to a mechanism that generalizes. \n\nWhile only a minority of circuits-style analysis has focused on model development, a wealth of results in the field of developmental interpretability have characterized the training dynamics of language models using other units of analysis. Behavioral analysis has shown that many model capabilities, such as predicting parts of speech and individual tokens, are learned during independent phase transitions characterized by varying slopes and points of maximum curvature (Chiang et al., 2020)Wei et al., 2022). Raghu et al. (2017) proposed singular vector canonical correlation analysis, a tool which allows different model layers and networks to be compared, and showed that early network layers converge earlier in training than late layers. Saphra and Lopez (2020b,a) found that LSTM models learn hierarchical sequential features in natural language bottom up -shorter sequences are learned first and form the basis for the representation of longer sequences that contain the shorter ones. Finally, Saxe et al. (2019) found that the features are learned in an order determined by the magnitude of the corresponding singular value in the model."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[267061159 | Zhu et al. | 2024 | Citations: 18]",
                "snippets": "We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 913,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[268041376 | Gu et al. | 2024 | Citations: 7]",
                "snippets": "This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an\"LM-training-as-lossless-compression\"view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 647,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an\"LM-training-as-lossless-compression\"view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[268379408 | Nicolini et al. | 2024 | Citations: 1]",
                "snippets": "In this study we have analyzed both the temporal and spatial dimensions of training a large language model.As discussed above, our work is the first one dealing with distribution of network weights as a whole, by means of computational methods borrowed from statistical mechanics.More specifically, this work shows that a bifurcation occurs in the dynamics of the weights during the training process.Such transitions are observed across various models of different sizes trained with distinct datasets.We have conducted a thorough and meticulous analysis of this aspect and concluded that this bifurcation marks a transition to a stationary state, indicating that further training is unlikely to significantly alter the weight values.Thus, training can be efficiently terminated upon reaching such a stationary state.Moreover, our study has offered a possible interpretation of the bifurcation phenomenon in terms of model perplexity.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusions",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 934,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 107
                            },
                            {
                                "start": 107,
                                "end": 280
                            },
                            {
                                "start": 280,
                                "end": 400
                            },
                            {
                                "start": 400,
                                "end": 502
                            },
                            {
                                "start": 502,
                                "end": 734
                            },
                            {
                                "start": 734,
                                "end": 817
                            },
                            {
                                "start": 817,
                                "end": 934
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this study we have analyzed both the temporal and spatial dimensions of training a large language model.As discussed above, our work is the first one dealing with distribution of network weights as a whole, by means of computational methods borrowed from statistical mechanics.More specifically, this work shows that a bifurcation occurs in the dynamics of the weights during the training process.Such transitions are observed across various models of different sizes trained with distinct datasets.We have conducted a thorough and meticulous analysis of this aspect and concluded that this bifurcation marks a transition to a stationary state, indicating that further training is unlikely to significantly alter the weight values.Thus, training can be efficiently terminated upon reaching such a stationary state.Moreover, our study has offered a possible interpretation of the bifurcation phenomenon in terms of model perplexity."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[268820276 | Yang et al. | 2024 | Citations: 2]",
                "snippets": "In this work, we first investigate the dynamics of several open-sourced large language models (i.e., Baichuan-7B (Yang et al., 2023), DeepSeek-7B (DeepSeek-AI et al., 2024), Amber-7B (Liu et al., 2023), OpenLLaMA-7B (Geng & Liu, 2023;Touvron et al., 2023;Computer, 2023), Yi-34B (AI et al., 2024) and DeepSeek-67B), and analyze their performance results across diverse tasks using the corresponding intermediate checkpoints based on the number of pre-trained tokens. Then, based on the theoretical framework of the Scaling Law (Kaplan et al., 2020), we analyze the performance patterns of different models across various downstream tasks, and provide findings to facilitate further research on the training dynamics of the LLMs, where the findings are shown as follows: \n\n\u2022 Findings on task dynamic prediction: Within the training process of LLMs, we observe that the dynamics of existing downstream tasks within a domain can predict the dynamics of unseen tasks. This suggests that a model's performance on known tasks can inform us about how it might perform on similar, yet unseen tasks within the same domain. (Section 4.1.1) \n\n\u2022 Findings on cross-domain promotion: Similar to the human cognitive process, the enhancement of different abilities across various domains progresses from basic to advanced levels following curriculum learning. The curriculum between crossdomain tasks can guide the training direction of models and the insights gained from one domain can potentially promote the learning process of other domains. (Section 4.1.2) \n\n\u2022 Findings on the effect of training strategies, model architecture, etc. : Based on the results of several 7b-scale models (Baichuan2-7b, DeepSeek-7b, OpenLLaMA-7b, and Amber-7b), we comprehensively analyze the effect of training strategies, dataset quality, and model architecture in training LLMs. For example, we observe that the training dataset, learning rate adjustments, batch size, and regularization techniques play a significant role in the learning efficiency in the early training stage (Section 4.2.1).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Preprint",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2065,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 466
                            },
                            {
                                "start": 467,
                                "end": 769
                            },
                            {
                                "start": 772,
                                "end": 963
                            },
                            {
                                "start": 964,
                                "end": 1113
                            },
                            {
                                "start": 1114,
                                "end": 1129
                            },
                            {
                                "start": 1132,
                                "end": 1343
                            },
                            {
                                "start": 1344,
                                "end": 1530
                            },
                            {
                                "start": 1531,
                                "end": 1546
                            },
                            {
                                "start": 1549,
                                "end": 1849
                            },
                            {
                                "start": 1850,
                                "end": 2065
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this work, we first investigate the dynamics of several open-sourced large language models (i.e., Baichuan-7B (Yang et al., 2023), DeepSeek-7B (DeepSeek-AI et al., 2024), Amber-7B (Liu et al., 2023), OpenLLaMA-7B (Geng & Liu, 2023;Touvron et al., 2023;Computer, 2023), Yi-34B (AI et al., 2024) and DeepSeek-67B), and analyze their performance results across diverse tasks using the corresponding intermediate checkpoints based on the number of pre-trained tokens. Then, based on the theoretical framework of the Scaling Law (Kaplan et al., 2020), we analyze the performance patterns of different models across various downstream tasks, and provide findings to facilitate further research on the training dynamics of the LLMs, where the findings are shown as follows: \n\n\u2022 Findings on task dynamic prediction: Within the training process of LLMs, we observe that the dynamics of existing downstream tasks within a domain can predict the dynamics of unseen tasks. This suggests that a model's performance on known tasks can inform us about how it might perform on similar, yet unseen tasks within the same domain. (Section 4.1.1) \n\n\u2022 Findings on cross-domain promotion: Similar to the human cognitive process, the enhancement of different abilities across various domains progresses from basic to advanced levels following curriculum learning. The curriculum between crossdomain tasks can guide the training direction of models and the insights gained from one domain can potentially promote the learning process of other domains. (Section 4.1.2) \n\n\u2022 Findings on the effect of training strategies, model architecture, etc. : Based on the results of several 7b-scale models (Baichuan2-7b, DeepSeek-7b, OpenLLaMA-7b, and Amber-7b), we comprehensively analyze the effect of training strategies, dataset quality, and model architecture in training LLMs. For example, we observe that the training dataset, learning rate adjustments, batch size, and regularization techniques play a significant role in the learning efficiency in the early training stage (Section 4.2.1)."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[269042762 | Lin et al. | 2024 | Citations: 75]",
                "snippets": "To explore how language models learn at the token level, we initially examined training dynamics, particularly how the token-level loss evolves during usual pretraining. In \u00a72.1, we evaluated the model's token perplexity at different checkpoints and categorized tokens into different types. Our findings reveal that significant loss reduction is limited to a select group of tokens. Many tokens are \"easy tokens\" that are already learned, and some are \"hard tokens\" that exhibit variable losses and resist convergence. These tokens can lead to numerous ineffective gradient updates....Investigating the training dynamics of language models is essential for understanding their behavior throughout the training process. This research includes studying internal representations [Saphra and Lopez, 2018], the acquisition of linguistic knowledge [Choshen et al., 2021, Liu et al., 2021], and the phenomenon of grokking [Power et al., 2022]. The analysis by Xia et al. [2022] is the most related to ours, which examines token-level training trajectories in models of varying sizes. Our findings, however, diverge from those of Xia et al. [2022], who posit that tokens with little change in perplexity are \"already learned\". We identify a spectrum of token patterns, including \"easy tokens\" and \"hard tokens\" that resist convergence.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "body",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 584,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 169
                            },
                            {
                                "start": 170,
                                "end": 290
                            },
                            {
                                "start": 291,
                                "end": 382
                            },
                            {
                                "start": 383,
                                "end": 518
                            },
                            {
                                "start": 519,
                                "end": 582
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To explore how language models learn at the token level, we initially examined training dynamics, particularly how the token-level loss evolves during usual pretraining. In \u00a72.1, we evaluated the model's token perplexity at different checkpoints and categorized tokens into different types. Our findings reveal that significant loss reduction is limited to a select group of tokens. Many tokens are \"easy tokens\" that are already learned, and some are \"hard tokens\" that exhibit variable losses and resist convergence. These tokens can lead to numerous ineffective gradient updates"
                    },
                    {
                        "section_title": "B.3 Language Model Training Dynamics",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 743,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 133
                            },
                            {
                                "start": 134,
                                "end": 351
                            },
                            {
                                "start": 352,
                                "end": 491
                            },
                            {
                                "start": 492,
                                "end": 633
                            },
                            {
                                "start": 634,
                                "end": 742
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".Investigating the training dynamics of language models is essential for understanding their behavior throughout the training process. This research includes studying internal representations [Saphra and Lopez, 2018], the acquisition of linguistic knowledge [Choshen et al., 2021, Liu et al., 2021], and the phenomenon of grokking [Power et al., 2022]. The analysis by Xia et al. [2022] is the most related to ours, which examines token-level training trajectories in models of varying sizes. Our findings, however, diverge from those of Xia et al. [2022], who posit that tokens with little change in perplexity are \"already learned\". We identify a spectrum of token patterns, including \"easy tokens\" and \"hard tokens\" that resist convergence."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[269449894 | Xiong et al. | 2024 | Citations: 10]",
                "snippets": "Recently, Large Language Models (LLMs) have been widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed Scaling Laws, have discovered that the final test loss of LLMs scales as power-laws with model size, computational budget, and dataset size. However, the temporal change of the test loss of an LLM throughout its pre-training process remains unexplored, though it is valuable in many aspects, such as selecting better hyperparameters \\textit{directly} on the target LLM. In this paper, we propose the novel concept of Temporal Scaling Law, studying how the test loss of an LLM evolves as the training steps scale up. In contrast to modeling the test loss as a whole in a coarse-grained manner, we break it down and dive into the fine-grained test loss of each token position, and further develop a dynamic hyperbolic-law. Afterwards, we derive the much more precise temporal scaling law by studying the temporal patterns of the parameters in the dynamic hyperbolic-law. Results on both in-distribution (ID) and out-of-distribution (OOD) validation datasets demonstrate that our temporal scaling law accurately predicts the test loss of LLMs across training steps.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Recently, Large Language Models (LLMs) have been widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed Scaling Laws, have discovered that the final test loss of LLMs scales as power-laws with model size, computational budget, and dataset size. However, the temporal change of the test loss of an LLM throughout its pre-training process remains unexplored, though it is valuable in many aspects, such as selecting better hyperparameters \\textit{directly} on the target LLM. In this paper, we propose the novel concept of Temporal Scaling Law, studying how the test loss of an LLM evolves as the training steps scale up. In contrast to modeling the test loss as a whole in a coarse-grained manner, we break it down and dive into the fine-grained test loss of each token position, and further develop a dynamic hyperbolic-law. Afterwards, we derive the much more precise temporal scaling law by studying the temporal patterns of the parameters in the dynamic hyperbolic-law. Results on both in-distribution (ID) and out-of-distribution (OOD) validation datasets demonstrate that our temporal scaling law accurately predicts the test loss of LLMs across training steps.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[269921716 | Jung et al. | 2024 | Citations: 0]",
                "snippets": "To explore linguistic aspects, we followed the methodology of Conneau et al. (2018) and utilized SentEval toolkit \u2021 to investigate training dynamics of ten different linguistic abilities.Based on the observed trends in training dynamics, we grouped the ten probing tasks into three categories, as presented in Figure 4.\n\nThe first group (Figure 4a) consists of three tasks that exhibit deterioration in Phase 1 followed by recovery in Phase 2. They are Length (number of tokens), Depth (depth of sentence structure trees), and TopConstituents (the grammatical structure of sentences), and all three are closely related to the performance of sentence embedding.We emphasize that the worst performance occurs at or near the boundary between Phase 1 and Phase 2 for the three linguistic abilities, indicating a strong correlation with representation rank.The trend of deterioration followed by recovery was also observed for alignment, where the deterioration occurs while uniformity sharply improves.\n\nThe second group (Figure 4b) consists of three tasks that exhibit an upward performance trend in both Phase 1 and Phase 2. They are WordContent (deducing words from sentence representations), \u2021 https://github.com/facebookresearch/SentEval/tree/main/data/probing  SubjNumber, and ObjNumber (matching the number of subjects and objects in sentence clauses, respectively), and all three are intimately related to the sentence embedding task.These three linguistic abilities do not deteriorate in Phase 1 despite uniformity's sharp improvement, suggesting that the three do not form a trade-off relationship with uniformity.\n\nThe third group (Figure 4c) consists of the four remaining tasks.Their performance either deteriorates (BigramShift and CoordinationInversion) or oscillates (Tense and OddManOut) throughout the fine-tuning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Linguistic Abilities",
                        "pdf_hash": "",
                        "start": 410,
                        "end": 2238,
                        "sentence_offsets": [
                            {
                                "start": 410,
                                "end": 597
                            },
                            {
                                "start": 597,
                                "end": 729
                            },
                            {
                                "start": 731,
                                "end": 1070
                            },
                            {
                                "start": 1070,
                                "end": 1262
                            },
                            {
                                "start": 1262,
                                "end": 1408
                            },
                            {
                                "start": 1410,
                                "end": 1649
                            },
                            {
                                "start": 1649,
                                "end": 1848
                            },
                            {
                                "start": 1848,
                                "end": 2030
                            },
                            {
                                "start": 2032,
                                "end": 2097
                            },
                            {
                                "start": 2097,
                                "end": 2238
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To explore linguistic aspects, we followed the methodology of Conneau et al. (2018) and utilized SentEval toolkit \u2021 to investigate training dynamics of ten different linguistic abilities.Based on the observed trends in training dynamics, we grouped the ten probing tasks into three categories, as presented in Figure 4.\n\nThe first group (Figure 4a) consists of three tasks that exhibit deterioration in Phase 1 followed by recovery in Phase 2. They are Length (number of tokens), Depth (depth of sentence structure trees), and TopConstituents (the grammatical structure of sentences), and all three are closely related to the performance of sentence embedding.We emphasize that the worst performance occurs at or near the boundary between Phase 1 and Phase 2 for the three linguistic abilities, indicating a strong correlation with representation rank.The trend of deterioration followed by recovery was also observed for alignment, where the deterioration occurs while uniformity sharply improves.\n\nThe second group (Figure 4b) consists of three tasks that exhibit an upward performance trend in both Phase 1 and Phase 2. They are WordContent (deducing words from sentence representations), \u2021 https://github.com/facebookresearch/SentEval/tree/main/data/probing  SubjNumber, and ObjNumber (matching the number of subjects and objects in sentence clauses, respectively), and all three are intimately related to the sentence embedding task.These three linguistic abilities do not deteriorate in Phase 1 despite uniformity's sharp improvement, suggesting that the three do not form a trade-off relationship with uniformity.\n\nThe third group (Figure 4c) consists of the four remaining tasks.Their performance either deteriorates (BigramShift and CoordinationInversion) or oscillates (Tense and OddManOut) throughout the fine-tuning."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[270062468 | Tomihari et al. | 2024 | Citations: 4]",
                "snippets": "In this paper, we analyze the training dynamics of LP-FT for classification tasks on the basis of the neural tangent kernel (NTK) theory. Our analysis decomposes the NTK matrix into two components. This decomposition highlights the importance of the linear head norm alongside the prediction accuracy at the start of the FT stage. We also observe a significant increase in the linear head norm during LP, which stems from training with the cross-entropy (CE) loss. This increase in the linear head norm effectively reduces changes in learned features.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this paper, we analyze the training dynamics of LP-FT for classification tasks on the basis of the neural tangent kernel (NTK) theory. Our analysis decomposes the NTK matrix into two components. This decomposition highlights the importance of the linear head norm alongside the prediction accuracy at the start of the FT stage. We also observe a significant increase in the linear head norm during LP, which stems from training with the cross-entropy (CE) loss. This increase in the linear head norm effectively reduces changes in learned features.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[271213641 | Ren et al. | 2024 | Citations: 21]",
                "snippets": "Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, gives us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during different types of finetuning, by analyzing the step-wise decomposition of how influence accumulates among different potential responses.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 418,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, gives us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during different types of finetuning, by analyzing the step-wise decomposition of how influence accumulates among different potential responses."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[271860164 | Sun et al. | 2024 | Citations: 2]",
                "snippets": "Recent studies identify phase transition of model training (Olsson et al., 2022;Wei et al., 2022), where new capabilities or behaviors suddenly emerge when certain thresholds of model complexity are reached. The aspects of complexity often include model size, amount of training by FLOPs or tokens, and model architecture. Several prior works studied the training dynamics of language models by analyzing the internals of train-fromscratch models (Tirumala et al., 2022)Chen et al., 2023;(Tian et al., 2023)Chen et al., 2024;(Chang et al., 2023). The results of these works suggest that the behaviors that are often overlooked after training could be valuable signals for model analysis. In addition to train-from-scratch models, Ren and Sutherland (2024) studied the fine-tuning dynamics of language models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[248986465 | Tirumala et al. | 2022 | Citations: 197]": "Despite their wide adoption, the underlying training and memorization dynamics of very large language models is not well understood. We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process. We measure the effects of dataset size, learning rate, and model size on memorization, finding that larger language models memorize training data faster across all settings. Surprisingly, we show that larger models can memorize a larger portion of the data before over-fitting and tend to forget less throughout the training process. We also analyze the memorization dynamics of different parts of speech and find that models memorize nouns and numbers first; we hypothesize and provide empirical evidence that nouns and numbers act as a unique identifier for memorizing individual training examples. Together, these findings present another piece of the broader puzzle of trying to understand what actually improves as models get bigger.",
                    "[261277016 | Chang et al. | 2023 | Citations: 13]": "Abstract How do language models learn to make predictions during pre-training? To study this, we extract learning curves from five autoregressive English language model pre-training runs, for 1M unseen tokens in context. We observe that the language models generate short repetitive phrases before learning to generate longer and more coherent text. We also find that individual tokens often exhibit sudden increases or decreases in loss that are surprisingly consistent across pre-training runs. To better understand these fluctuations, we quantify the final surprisal, within-run variability, age of acquisition, forgettability, and cross-run variability of learning curves for individual tokens in context. More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be \u201cforgotten\u201d during pre-training. Higher n-gram probabilities further accentuate these effects. Independent of the target token, shorter and more frequent contexts correlate with marginally more stable and quickly acquired predictions. Based on our results, we argue for the existence of sequential learning dependencies between different model capabilities, and we characterize language model learning as early n-gram learning before gradual refinement of tail n-gram predictions.",
                    "[258947127 | Tian et al. | 2023 | Citations: 79]": "Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \\emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \\emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends more to distinct key tokens for a specific next token to be predicted, and pays less attention to common key tokens that occur across different next tokens. Among distinct tokens, it progressively drops attention weights, following the order of low to high co-occurrence between the key and the query token in the training set. Interestingly, this procedure does not lead to winner-takes-all, but decelerates due to a \\emph{phase transition} that is controllable by the learning rates of the two layers, leaving (almost) fixed token combination. We verify this \\textbf{\\emph{scan and snap}} dynamics on synthetic and real-world data (WikiText)."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 806,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 207
                            },
                            {
                                "start": 208,
                                "end": 322
                            },
                            {
                                "start": 323,
                                "end": 544
                            },
                            {
                                "start": 545,
                                "end": 685
                            },
                            {
                                "start": 686,
                                "end": 806
                            }
                        ],
                        "ref_mentions": [
                            "248986465",
                            "258947127",
                            "261277016"
                        ],
                        "quote": "Recent studies identify phase transition of model training (Olsson et al., 2022;Wei et al., 2022), where new capabilities or behaviors suddenly emerge when certain thresholds of model complexity are reached. The aspects of complexity often include model size, amount of training by FLOPs or tokens, and model architecture. Several prior works studied the training dynamics of language models by analyzing the internals of train-fromscratch models (Tirumala et al., 2022)Chen et al., 2023;(Tian et al., 2023)Chen et al., 2024;(Chang et al., 2023). The results of these works suggest that the behaviors that are often overlooked after training could be valuable signals for model analysis. In addition to train-from-scratch models, Ren and Sutherland (2024) studied the fine-tuning dynamics of language models."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[271909320 | Tissue et al. | 2024 | Citations: 9]",
                "snippets": "We find that the cross-entropy loss curves of neural language models empirically adhere to a scaling law with learning rate (LR) annealing over training steps: $$L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C\\cdot S_2,$$ where $L(s)$ is the validation loss at step $s$, $S_1$ is the area under the LR curve, $S_2$ is the LR annealing area, and $L_0$, $A$, $C$, $\\alpha$ are constant parameters. This formulation takes into account two factors: (1) power-law scaling over data size, and (2) the additional loss reduction during LR annealing. Therefore, this formulation can describe the full loss curve at each step, rather than the single loss point at the end of training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We find that the cross-entropy loss curves of neural language models empirically adhere to a scaling law with learning rate (LR) annealing over training steps: $$L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C\\cdot S_2,$$ where $L(s)$ is the validation loss at step $s$, $S_1$ is the area under the LR curve, $S_2$ is the LR annealing area, and $L_0$, $A$, $C$, $\\alpha$ are constant parameters. This formulation takes into account two factors: (1) power-law scaling over data size, and (2) the additional loss reduction during LR annealing. Therefore, this formulation can describe the full loss curve at each step, rather than the single loss point at the end of training.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[272826680 | M'esz'aros et al. | 2024 | Citations: 0]",
                "snippets": "We analize the dynamics of learning rule extrapolation. We report results on the Transformer (training dynamics of Mamba and the LSTM are in Appx. A), trained on the a n b n language-where high rule extrapolation ability is achieved. Fig. 3 shows that first, the model learns the sequences obeying (R2), then it learns the language (R1) \u2229 (R2) as its subset. We argue that the order in which rules are learnt is governed by the relative simplicity of the rules, quantified by Kolmogorov complexity. Given a formal language with rules (R1) and (R2), let p 1 , p 2 and p 1,2 be distributions defined by LMs that generate sequences that satisfy (R1), (R2) and (R1) \u2229 (R2), respectively. If, e.g., K(p 2 ) \u226a K(p 1,2 ), our normative algorithm will first learn (R2), and then learn the (R1) \u2229 (R2) as its subset. In the a n b n language, (R2) (a's before b's), is, on average, simpler to generate than (R1) (#a=#b) and (R1) \u2229 (R2). Therefore, we expect our normative algorithm to first learn (R2), and then learn (R1) \u2229 (R2) as its subset. Remarkably, our Transformer employs the same strategy ( Fig. 3), verifying the presence of simplicity bias. This result is matches past observations that Transformers are biased towards low Kolmogorov complexity [Goldblum et al., 2023].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Towards explaining training dynamics and rule extrapolation",
                        "pdf_hash": "",
                        "start": 413,
                        "end": 1684,
                        "sentence_offsets": [
                            {
                                "start": 413,
                                "end": 468
                            },
                            {
                                "start": 469,
                                "end": 559
                            },
                            {
                                "start": 560,
                                "end": 646
                            },
                            {
                                "start": 647,
                                "end": 771
                            },
                            {
                                "start": 772,
                                "end": 911
                            },
                            {
                                "start": 912,
                                "end": 1096
                            },
                            {
                                "start": 1097,
                                "end": 1220
                            },
                            {
                                "start": 1221,
                                "end": 1339
                            },
                            {
                                "start": 1340,
                                "end": 1447
                            },
                            {
                                "start": 1448,
                                "end": 1555
                            },
                            {
                                "start": 1556,
                                "end": 1684
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We analize the dynamics of learning rule extrapolation. We report results on the Transformer (training dynamics of Mamba and the LSTM are in Appx. A), trained on the a n b n language-where high rule extrapolation ability is achieved. Fig. 3 shows that first, the model learns the sequences obeying (R2), then it learns the language (R1) \u2229 (R2) as its subset. We argue that the order in which rules are learnt is governed by the relative simplicity of the rules, quantified by Kolmogorov complexity. Given a formal language with rules (R1) and (R2), let p 1 , p 2 and p 1,2 be distributions defined by LMs that generate sequences that satisfy (R1), (R2) and (R1) \u2229 (R2), respectively. If, e.g., K(p 2 ) \u226a K(p 1,2 ), our normative algorithm will first learn (R2), and then learn the (R1) \u2229 (R2) as its subset. In the a n b n language, (R2) (a's before b's), is, on average, simpler to generate than (R1) (#a=#b) and (R1) \u2229 (R2). Therefore, we expect our normative algorithm to first learn (R2), and then learn (R1) \u2229 (R2) as its subset. Remarkably, our Transformer employs the same strategy ( Fig. 3), verifying the presence of simplicity bias. This result is matches past observations that Transformers are biased towards low Kolmogorov complexity [Goldblum et al., 2023]."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[272910946 | Huang et al. | 2024 | Citations: 7]",
                "snippets": "The transformer architecture (Vaswani et al., 2017) has revolutionized the field of machine learning, establishing itself as a foundation model for numerous applications, including natural language processing (NLP) (Devlin et al., 2018), computer vision (Dosovitskiy et al., 2020), and multi-modal signal processing (Tsai et al., 2019). In particular, transformers achieve tremendous empirical success in large language models (LLMs) such as GPT-3 (Brown et al., 2020). Despite the empirical success, limited theoretical understanding of transformers have caused a series of critical concerns about their robustness, interpretability, and bias issues (Bommasani et al., 2021;(Belkin, 2024). \n\nTo overcome these issues, recent advances in transformer theory have investigated the convergence of training transformers under theoretically amenable setting such as linear regression (Mahankali et al., 2023;Zhang et al., 2023;Huang et al., 2023) and binary classification (Tarzanagh et al., 2023b,a;Vasudeva et al., 2024;Li et al., 2023). Nevertheless, one of the fundamental task in LLMs and other generative models is next-token prediction (NTP), which involves predicting the next word or token in a sequence, given the previous tokens. In NTP, a few recent theoretical studies have started to investigate the training dynamics of transformers (Tian et al., 2023)(Li et al., 2024). However, those works lack of fine-grained non-asymptotic convergence analysis of the training process, posing the following open questions for further investigation: \n\nHow fast does the training of a transformer converge in NTP?",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[13756489 | Vaswani et al. | 2017 | Citations: 132444]": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
                    "[173990158 | Tsai et al. | 2019 | Citations: 1308]": "Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.",
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                    "[258947127 | Tian et al. | 2023 | Citations: 79]": "Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \\emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \\emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends more to distinct key tokens for a specific next token to be predicted, and pays less attention to common key tokens that occur across different next tokens. Among distinct tokens, it progressively drops attention weights, following the order of low to high co-occurrence between the key and the query token in the training set. Interestingly, this procedure does not lead to winner-takes-all, but decelerates due to a \\emph{phase transition} that is controllable by the learning rates of the two layers, leaving (almost) fixed token combination. We verify this \\textbf{\\emph{scan and snap}} dynamics on synthetic and real-world data (WikiText).",
                    "[268379753 | Li et al. | 2024 | Citations: 30]": "Transformer-based language models are trained on large datasets to predict the next token given an input sequence. Despite this simple training objective, they have led to revolutionary advances in natural language processing. Underlying this success is the self-attention mechanism. In this work, we ask: $\\textit{What}$ $\\textit{does}$ $\\textit{a}$ $\\textit{single}$ $\\textit{self-attention}$ $\\textit{layer}$ $\\textit{learn}$ $\\textit{from}$ $\\textit{next-token}$ $\\textit{prediction?}$ We show that training self-attention with gradient descent learns an automaton which generates the next token in two distinct steps: $\\textbf{(1)}$ $\\textbf{Hard}$ $\\textbf{retrieval:}$ Given input sequence, self-attention precisely selects the $\\textit{high-priority}$ $\\textit{input}$ $\\textit{tokens}$ associated with the last input token. $\\textbf{(2)}$ $\\textbf{Soft}$ $\\textbf{composition:}$ It then creates a convex combination of the high-priority tokens from which the next token can be sampled. Under suitable conditions, we rigorously characterize these mechanics through a directed graph over tokens extracted from the training data. We prove that gradient descent implicitly discovers the strongly-connected components (SCC) of this graph and self-attention learns to retrieve the tokens that belong to the highest-priority SCC available in the context window. Our theory relies on decomposing the model weights into a directional component and a finite component that correspond to hard retrieval and soft composition steps respectively. This also formalizes a related implicit bias formula conjectured in [Tarzanagh et al. 2023]. We hope that these findings shed light on how self-attention processes sequential data and pave the path toward demystifying more complex architectures."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1608,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 336
                            },
                            {
                                "start": 337,
                                "end": 469
                            },
                            {
                                "start": 470,
                                "end": 689
                            },
                            {
                                "start": 692,
                                "end": 1033
                            },
                            {
                                "start": 1034,
                                "end": 1234
                            },
                            {
                                "start": 1235,
                                "end": 1379
                            },
                            {
                                "start": 1380,
                                "end": 1545
                            },
                            {
                                "start": 1548,
                                "end": 1608
                            }
                        ],
                        "ref_mentions": [
                            "13756489",
                            "173990158",
                            "218971783",
                            "267303083",
                            "258947127",
                            "268379753"
                        ],
                        "quote": "The transformer architecture (Vaswani et al., 2017) has revolutionized the field of machine learning, establishing itself as a foundation model for numerous applications, including natural language processing (NLP) (Devlin et al., 2018), computer vision (Dosovitskiy et al., 2020), and multi-modal signal processing (Tsai et al., 2019). In particular, transformers achieve tremendous empirical success in large language models (LLMs) such as GPT-3 (Brown et al., 2020). Despite the empirical success, limited theoretical understanding of transformers have caused a series of critical concerns about their robustness, interpretability, and bias issues (Bommasani et al., 2021;(Belkin, 2024). \n\nTo overcome these issues, recent advances in transformer theory have investigated the convergence of training transformers under theoretically amenable setting such as linear regression (Mahankali et al., 2023;Zhang et al., 2023;Huang et al., 2023) and binary classification (Tarzanagh et al., 2023b,a;Vasudeva et al., 2024;Li et al., 2023). Nevertheless, one of the fundamental task in LLMs and other generative models is next-token prediction (NTP), which involves predicting the next word or token in a sequence, given the previous tokens. In NTP, a few recent theoretical studies have started to investigate the training dynamics of transformers (Tian et al., 2023)(Li et al., 2024). However, those works lack of fine-grained non-asymptotic convergence analysis of the training process, posing the following open questions for further investigation: \n\nHow fast does the training of a transformer converge in NTP?"
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[273025704 | Lee et al. | 2024 | Citations: 4]",
                "snippets": "Most research on LMs focuses on the final configuration of the model at the end of pre-training. Yet, recent work shows that learning dynamics can elucidate the behavior and computational mechanisms of LMs (Chen et al., 2023)(Singh et al., 2023)Tigges et al., 2024). It has been found that, over training, LMs' weight matrices become higher-rank (Boix-Adser\u00e0 et al., 2023), their representations higher dimensional (Cheng et al., 2024), and their gradients increasingly diffuse (Weber et al., 2024). Over finetuning, representational dimensionality has been found to change in concert with geometric properties like cluster reorganization (Doimo et al., 2024).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[261822542 | Chen et al. | 2023 | Citations: 74]": "Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by manipulating SAS during training, and demonstrate that SAS is necessary for the development of grammatical capabilities. We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality. These findings offer an interpretation of a real-world example of both simplicity bias and breakthrough training dynamics.",
                    "[259138551 | Boix-Adsera et al. | 2023 | Citations: 37]": "We identify incremental learning dynamics in transformers, where the difference between trained and initial weights progressively increases in rank. We rigorously prove this occurs under the simplifying assumptions of diagonal weight matrices and small initialization. Our experiments support the theory and also show that phenomenon can occur in practice without the simplifying assumptions.",
                    "[265157721 | Singh et al. | 2023 | Citations: 44]": "Transformer neural networks can exhibit a surprising capacity for in-context learning (ICL) despite not being explicitly trained for it. Prior work has provided a deeper understanding of how ICL emerges in transformers, e.g. through the lens of mechanistic interpretability, Bayesian inference, or by examining the distributional properties of training data. However, in each of these cases, ICL is treated largely as a persistent phenomenon; namely, once ICL emerges, it is assumed to persist asymptotically. Here, we show that the emergence of ICL during transformer training is, in fact, often transient. We train transformers on synthetic data designed so that both ICL and in-weights learning (IWL) strategies can lead to correct predictions. We find that ICL first emerges, then disappears and gives way to IWL, all while the training loss decreases, indicating an asymptotic preference for IWL. The transient nature of ICL is observed in transformers across a range of model sizes and datasets, raising the question of how much to\"overtrain\"transformers when seeking compact, cheaper-to-run models. We find that L2 regularization may offer a path to more persistent ICL that removes the need for early stopping based on ICL-style validation tasks. Finally, we present initial evidence that ICL transience may be caused by competition between ICL and IWL circuits.",
                    "[270370801 | Weber et al. | 2024 | Citations: 4]": "The usual way to interpret language models (LMs) is to test their performance on different benchmarks and subsequently infer their internal processes. In this paper, we present an alternative approach, concentrating on the quality of LM processing, with a focus on their language abilities. To this end, we construct 'linguistic task spaces' -- representations of an LM's language conceptualisation -- that shed light on the connections LMs draw between language phenomena. Task spaces are based on the interactions of the learning signals from different linguistic phenomena, which we assess via a method we call 'similarity probing'. To disentangle the learning signals of linguistic phenomena, we further introduce a method called 'fine-tuning via gradient differentials' (FTGD). We apply our methods to language models of three different scales and find that larger models generalise better to overarching general concepts for linguistic tasks, making better use of their shared structure. Further, the distributedness of linguistic processing increases with pre-training through increased parameter sharing between related linguistic tasks. The overall generalisation patterns are mostly stable throughout training and not marked by incisive stages, potentially explaining the lack of successful curriculum strategies for LMs."
                },
                "metadata": [
                    {
                        "section_title": "Language model training dynamics",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 652,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 96
                            },
                            {
                                "start": 97,
                                "end": 265
                            },
                            {
                                "start": 266,
                                "end": 491
                            },
                            {
                                "start": 492,
                                "end": 652
                            }
                        ],
                        "ref_mentions": [
                            "261822542",
                            "265157721",
                            "259138551",
                            "270370801"
                        ],
                        "quote": "Most research on LMs focuses on the final configuration of the model at the end of pre-training. Yet, recent work shows that learning dynamics can elucidate the behavior and computational mechanisms of LMs (Chen et al., 2023)(Singh et al., 2023)Tigges et al., 2024). It has been found that, over training, LMs' weight matrices become higher-rank (Boix-Adser\u00e0 et al., 2023), their representations higher dimensional (Cheng et al., 2024), and their gradients increasingly diffuse (Weber et al., 2024). Over finetuning, representational dimensionality has been found to change in concert with geometric properties like cluster reorganization (Doimo et al., 2024)."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[273346324 | Yang et al. | 2024 | Citations: 3]",
                "snippets": "In this work, we study the dynamics of training a shallow transformer on a task of recognizing co-occurrence of two designated words. In the literature of studying training dynamics of transformers, several simplifications are commonly adopted such as weight reparameterization, attention linearization, special initialization, and lazy regime. In contrast, we analyze the gradient flow dynamics of simultaneously training three attention matrices and a linear MLP layer from random initialization, and provide a framework of analyzing such dynamics via a coupled dynamical system...We discover that gradient flow serves as an inherent mechanism that naturally divide the training process into two phases. In Phase 1, the linear MLP quickly aligns with the two target signals for correct classification, whereas the softmax attention remains almost unchanged. In Phase 2, the attention matrices and the MLP evolve jointly to enlarge the classification margin and reduce the loss to a near minimum value. Technically, we prove a novel property of the gradient flow, termed \\textit{automatic balancing of gradients}, which enables the loss values of different samples to decrease almost at the same rate and further facilitates the proof of near minimum training loss.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 134,
                        "end": 714,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In this work, we study the dynamics of training a shallow transformer on a task of recognizing co-occurrence of two designated words. In the literature of studying training dynamics of transformers, several simplifications are commonly adopted such as weight reparameterization, attention linearization, special initialization, and lazy regime. In contrast, we analyze the gradient flow dynamics of simultaneously training three attention matrices and a linear MLP layer from random initialization, and provide a framework of analyzing such dynamics via a coupled dynamical system"
                    },
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 800,
                        "end": 1484,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We discover that gradient flow serves as an inherent mechanism that naturally divide the training process into two phases. In Phase 1, the linear MLP quickly aligns with the two target signals for correct classification, whereas the softmax attention remains almost unchanged. In Phase 2, the attention matrices and the MLP evolve jointly to enlarge the classification margin and reduce the loss to a near minimum value. Technically, we prove a novel property of the gradient flow, termed \\textit{automatic balancing of gradients}, which enables the loss values of different samples to decrease almost at the same rate and further facilitates the proof of near minimum training loss."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[273351173 | Martinez et al. | 2024 | Citations: 4]",
                "snippets": "We use the Pythia model suite to analyse the training dynamics that underlie this phenomenon. Across different model sizes, we investigate the convergence of the Attention and MLP activations to their final state and examine how the effective rank of their parameters influences this process. We find that nearly all layers in larger models stabilise early in training - within the first 20% - whereas layers in smaller models exhibit slower and less stable convergence, especially when their parameters have lower effective rank. By linking the convergence of layers' activations to their parameters' effective rank, our analyses can guide future work to address inefficiencies in the learning dynamics of small models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 591,
                        "end": 1311,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We use the Pythia model suite to analyse the training dynamics that underlie this phenomenon. Across different model sizes, we investigate the convergence of the Attention and MLP activations to their final state and examine how the effective rank of their parameters influences this process. We find that nearly all layers in larger models stabilise early in training - within the first 20% - whereas layers in smaller models exhibit slower and less stable convergence, especially when their parameters have lower effective rank. By linking the convergence of layers' activations to their parameters' effective rank, our analyses can guide future work to address inefficiencies in the learning dynamics of small models."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[274060474 | Subramanian et al. | 2024 | Citations: 0]",
                "snippets": "We consider the effect of switching the learning rate at a predetermined time during training, which we refer to as \"SkipLR\". We model SGD as a stochastic gradient flow and show that when starting from the same initial parameters, switching the learning rate causes the loss curves to contract towards each other. We demonstrate this theoretically for some simple cases, and empirically on large language models. Our analysis provides insight into how learning rate schedules affect the training dynamics, and could inform the design of new schedules to accelerate convergence.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We consider the effect of switching the learning rate at a predetermined time during training, which we refer to as \"SkipLR\". We model SGD as a stochastic gradient flow and show that when starting from the same initial parameters, switching the learning rate causes the loss curves to contract towards each other. We demonstrate this theoretically for some simple cases, and empirically on large language models. Our analysis provides insight into how learning rate schedules affect the training dynamics, and could inform the design of new schedules to accelerate convergence.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[274823085 | Li et al. | 2024 | Citations: 0]",
                "snippets": "Training Dynamics: We analyze how validation loss and downstream benchmarks (e.g., Hellaswag, ARC) evolve over 40,000+ training steps and correlate these metrics with improvements in text fluency, coherence, and factual accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1332,
                        "end": 1561,
                        "sentence_offsets": [
                            {
                                "start": 1329,
                                "end": 1561
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Training Dynamics: We analyze how validation loss and downstream benchmarks (e.g., Hellaswag, ARC) evolve over 40,000+ training steps and correlate these metrics with improvements in text fluency, coherence, and factual accuracy."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[274981612 | Xu et al. | 2024 | Citations: 4]",
                "snippets": "Understanding training dynamics and feature evolution is crucial for the mechanistic interpretability of large language models (LLMs). Although sparse autoencoders (SAEs) have been used to identify features within LLMs, a clear picture of how these features evolve during training remains elusive. In this study, we (1) introduce SAE-Track, a novel method for efficiently obtaining a continual series of SAEs, providing the foundation for a mechanistic study that covers (2) the semantic evolution of features, (3) the underlying processes of feature formation, and (4) the directional drift of feature vectors. Our work provides new insights into the dynamics of features in LLMs, enhancing our understanding of training mechanisms and feature evolution.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Understanding training dynamics and feature evolution is crucial for the mechanistic interpretability of large language models (LLMs). Although sparse autoencoders (SAEs) have been used to identify features within LLMs, a clear picture of how these features evolve during training remains elusive. In this study, we (1) introduce SAE-Track, a novel method for efficiently obtaining a continual series of SAEs, providing the foundation for a mechanistic study that covers (2) the semantic evolution of features, (3) the underlying processes of feature formation, and (4) the directional drift of feature vectors. Our work provides new insights into the dynamics of features in LLMs, enhancing our understanding of training mechanisms and feature evolution.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[276235362 | Yao et al. | 2025 | Citations: 2]",
                "snippets": "Further analysis of initial training dynamics suggests that specific model components, particularly the embedding space and self-attention mechanisms, play pivotal roles in shaping these learning biases. We provide a theoretical framework from the perspective of model training dynamics to explain these phenomena.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 552,
                        "end": 866,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Further analysis of initial training dynamics suggests that specific model components, particularly the embedding space and self-attention mechanisms, play pivotal roles in shaping these learning biases. We provide a theoretical framework from the perspective of model training dynamics to explain these phenomena."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[276937763 | Wal et al. | 2025 | Citations: 2]",
                "snippets": "In this work, we define stability as the change in a metric of interest (e.g., validation loss) caused by changes in randomness factors and quantify it using the standard deviation of that metric (see (Du et al., 2023) for other approaches to quantify stability)...using training maps constructed from statistics of the model parameters, we identify the characteristics of stable training runs and the early signals of instability.\n\nLanguage modelling is largely stable. Generally, we observe LM pre-training dynamics to follow consistent trajectories. Across seeds and model sizes, downstream performance and representational efficiency consistently increase during pre-training, and training maps are linear (except for the outlier seeds).\n\nLinguistic information is encoded in the initial learning phase (10^3-10^4 steps). Across all experiments, metrics start moving away from the initial random baseline around step 10^3 (2B tokens circa) and reach their convergence level around step 10^4 (20B tokens circa). In this initial phase, representational shift peaks and linguistic information begins to be encoded into the models' latent representations.\n\nMost improvements happen in the \"critical\" learning phase (10^4-10^5 steps). In the range of 10^3 to 10^4 steps, most learning occurs, as measured by all of our metrics.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[256868814 | Du et al. | 2023 | Citations: 4]": "Fine-tuning pre-trained language models on downstream tasks with varying random seeds has been shown to be unstable, especially on small datasets. Many previous studies have investigated this instability and proposed methods to mitigate it. However, most of these studies only used the standard deviation of performance scores (SD) as their measure, which is a narrow characterization of instability. In this paper, we analyze SD and six other measures quantifying instability of different granularity levels. Moreover, we propose a systematic evaluation framework of these measures\u2019 validity. Finally, we analyze the consistency and difference between different measures by reassessing existing instability mitigation methods. We hope our results will inform better measurements of the fine-tuning instability."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 264,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 265
                            }
                        ],
                        "ref_mentions": [
                            "256868814"
                        ],
                        "quote": "In this work, we define stability as the change in a metric of interest (e.g., validation loss) caused by changes in randomness factors and quantify it using the standard deviation of that metric (see (Du et al., 2023) for other approaches to quantify stability)"
                    },
                    {
                        "quote": "using training maps constructed from statistics of the model parameters, we identify the characteristics of stable training runs and the early signals of instability.\n\nLanguage modelling is largely stable. Generally, we observe LM pre-training dynamics to follow consistent trajectories. Across seeds and model sizes, downstream performance and representational efficiency consistently increase during pre-training, and training maps are linear (except for the outlier seeds).\n\nLinguistic information is encoded in the initial learning phase (10^3-10^4 steps). Across all experiments, metrics start moving away from the initial random baseline around step 10^3 (2B tokens circa) and reach their convergence level around step 10^4 (20B tokens circa). In this initial phase, representational shift peaks and linguistic information begins to be encoded into the models' latent representations.\n\nMost improvements happen in the \"critical\" learning phase (10^4-10^5 steps). In the range of 10^3 to 10^4 steps, most learning occurs, as measured by all of our metrics.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[277349236 | Zucchet et al. | 2025 | Citations: 7]",
                "snippets": "This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distributions lead to shorter plateaus. Finally, hallucinations emerge simultaneously with knowledge, and integrating new knowledge into the model through fine-tuning is challenging, as it quickly corrupts its existing parametric memories.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 139,
                        "end": 834,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distributions lead to shorter plateaus. Finally, hallucinations emerge simultaneously with knowledge, and integrating new knowledge into the model through fine-tuning is challenging, as it quickly corrupts its existing parametric memories."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[278534923 | Wang et al. | 2025 | Citations: 0]",
                "snippets": "In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.037758,
        "cot": "To answer the user's query about analyzing training dynamics of language models, I need to organize the quotes into meaningful dimensions that will help create a comprehensive summary. After reviewing the quotes, I can identify several key themes:\n\n1. Introduction/Background: This section will provide a foundation for understanding what training dynamics are in the context of language models and why they matter. This is essential for framing the subsequent dimensions.\n\n2. Stages and Phases of Learning: Many quotes discuss how language models progress through distinct phases or stages during training, with different capabilities emerging at different times. This should be a synthesis section to explain the overall pattern of how models learn.\n\n3. Architectural Influence on Training Dynamics: Several quotes discuss how different model architectures (Transformers, LSTMs, etc.) affect training dynamics. This should be a synthesis section to compare and contrast.\n\n4. Factors Affecting Training Dynamics: Many quotes mention various factors that influence how models learn, including dataset size, learning rate, model size, etc. This should be a synthesis section to explain these relationships.\n\n5. Metrics and Methods for Analyzing Training Dynamics: The quotes reference various ways researchers measure and analyze training dynamics. This is best as a list format to clearly enumerate the different approaches.\n\n6. Emergent Capabilities and Phenomena: Several quotes discuss specific phenomena that emerge during training, like grokking, memorization patterns, and phase transitions. This should be a synthesis section to explain these connected concepts.\n\n7. Applications and Implications: Some quotes discuss how understanding training dynamics can be applied to improve training efficiency or model performance. This should be a synthesis section to explain the practical relevance.\n\nEach of these dimensions offers a distinct lens through which to understand the training dynamics of language models, and together they will provide a comprehensive analysis of the topic.",
        "plan": {
            "Introduction: What are Training Dynamics? (synthesis)": [],
            "Stages and Phases of Learning (synthesis)": [
                0,
                1,
                3,
                5,
                6,
                9,
                11,
                15,
                18,
                20,
                22,
                23,
                29,
                30,
                31
            ],
            "Architectural Influence on Training Dynamics (synthesis)": [
                0,
                1,
                8,
                21,
                23,
                24
            ],
            "Factors Affecting Training Dynamics (synthesis)": [
                2,
                3,
                7,
                9,
                12,
                14,
                16,
                19,
                25,
                28
            ],
            "Metrics and Methods for Analyzing Training Dynamics (list)": [
                4,
                5,
                8,
                10,
                13,
                14,
                17,
                24,
                26,
                27,
                29
            ],
            "Emergent Capabilities and Phenomena (synthesis)": [
                2,
                6,
                8,
                9,
                13,
                18,
                20,
                30
            ],
            "Applications and Implications (synthesis)": [
                10,
                11,
                12,
                16,
                19,
                25,
                31
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction: What are Training Dynamics?",
                "tldr": "Training dynamics refer to the complex patterns of how language models learn and evolve during the training process. Understanding these dynamics reveals how models acquire knowledge progressively, develop capabilities, and potentially encounter challenges like memorization or forgetting. (LLM Memory)",
                "text": "\nTraining dynamics encompass the entire spectrum of behaviors, patterns, and phenomena that occur as language models learn from data over time. At its core, training dynamics analyze how neural networks evolve through their learning trajectory - from random initialization to a fully trained model capable of sophisticated language understanding and generation. This includes examining how loss functions decrease, how representations form within the network, and how performance on various tasks develops at different stages of training.\n\nThe study of training dynamics reveals critical insights about the learning process itself. For example, language models don't learn all skills simultaneously but instead develop capabilities in a systematic progression - often mastering simple syntactic patterns before complex semantic relationships. This progressive acquisition of knowledge follows distinct phases where different types of information are internalized at varying rates.\n\nAnalyzing training dynamics also illuminates important practical concerns such as optimization challenges, the emergence of capabilities at specific training thresholds, and potential issues like catastrophic forgetting or overfitting. Researchers track metrics like gradient norms, attention patterns, and activation distributions to gain visibility into these internal processes. Such analysis can help determine optimal training schedules, architecture choices, and data requirements.\n\nFor large language models specifically, training dynamics are particularly complex due to the billions of parameters and massive datasets involved. The scale introduces emergent behaviors where certain capabilities appear only after specific training thresholds are crossed. Understanding these dynamics helps explain why some models exhibit surprising capabilities that weren't explicitly programmed but emerge organically through the training process. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Stages and Phases of Learning",
                "tldr": "Language models follow consistent developmental trajectories with distinct learning phases, progressing from simple pattern recognition to complex linguistic understanding. These phases include early acquisition of syntactic knowledge, development of intermediate capabilities like attention mechanisms, and later refinement of semantic and world knowledge. (16 sources)",
                "text": "\nResearch has consistently demonstrated that language models progress through distinct stages during training, with certain linguistic capabilities emerging in a remarkably consistent order. Studies have found that language models learn grammatical phenomena in a predictable sequence regardless of architecture, initialization, or data shuffling <Paper corpusId=\"237491997\" paperTitle=\"(Choshen et al., 2021)\" isShortName></Paper> <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> . This consistency suggests that training dynamics follow fundamental learning principles rather than random pathways.\n\nThe early stages of learning are characterized by the acquisition of basic syntactic patterns. Models initially rely on local cues rather than word order, resembling bag-of-words models before gradually developing sensitivity to structural relationships <Paper corpusId=\"237491997\" paperTitle=\"(Choshen et al., 2021)\" isShortName></Paper>. Research on both Transformer and LSTM architectures shows that local syntactic information, such as parts of speech, is learned earlier than information encoding long-distance dependencies <Paper corpusId=\"247656607\" paperTitle=\"(Teehan et al., 2022)\" isShortName></Paper>. Function and content words also show different learning patterns, with finer distinctions appearing for specific parts of speech and verb forms <Paper corpusId=\"247656607\" paperTitle=\"(Teehan et al., 2022)\" isShortName></Paper> <Paper corpusId=\"222140842\" paperTitle=\"(Chiang et al., 2020)\" isShortName></Paper>.\n\nThe intermediate training phase often features a critical transition where models develop key computational mechanisms. For instance, in a synthetic factual recall task, researchers identified a performance plateau that coincides with the formation of attention-based circuits supporting recall <Paper corpusId=\"277349236\" paperTitle=\"(Zucchet et al., 2025)\" isShortName></Paper>. Similarly, \"induction heads\" for in-context learning have been observed to appear at specific inflection points during pre-training <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper>. These transitions can be abrupt and dramatic - often characterized as breakthroughs, emergence, or phase transitions in the literature <Paper corpusId=\"261822542\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper> <Paper corpusId=\"253117181\" paperTitle=\"(Caballero et al., 2022)\" isShortName></Paper>.\n\nAs training progresses, language models enter later stages where they refine their capabilities and develop more complex understanding. Studies of ALBERT show that reconstruction and prediction of different parts of speech occur at different learning speeds during pretraining <Paper corpusId=\"222140842\" paperTitle=\"(Chiang et al., 2020)\" isShortName></Paper>. World knowledge and reasoning abilities are typically learned later and less stably than syntactic rules <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. This progression from syntax to semantics reflects a developmental trajectory somewhat similar to human language acquisition .\n\nThe learning trajectory can also be divided into specific temporal phases based on training steps. Research on language model pretraining has identified that linguistic information begins to be encoded between 10\u00b3-10\u2074 steps (approximately 2-20 billion tokens), followed by a \"critical\" learning phase between 10\u2074-10\u2075 steps where most improvements occur across various metrics <Paper corpusId=\"276937763\" paperTitle=\"(Wal et al., 2025)\" isShortName></Paper>. This pattern remains consistent across model sizes.\n\nInterestingly, learning dynamics don't always follow a simple monotonic improvement pattern. Some capabilities show a \"double-descent\" trend where performance initially degrades before improving <Paper corpusId=\"254877112\" paperTitle=\"(Xia et al., 2022)\" isShortName></Paper>. Other linguistic abilities exhibit more complex patterns, with some deteriorating in early phases before recovering, and others showing continuous improvement or oscillation throughout training <Paper corpusId=\"269921716\" paperTitle=\"(Jung et al., 2024)\" isShortName></Paper>. These non-monotonic learning trajectories suggest complex interactions between different capabilities during training.\n\nScale plays an important role in determining learning phases. Larger models often require more data to reach critical learning thresholds <Paper corpusId=\"267061159\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>. However, research comparing models of different sizes (from 125M to 175B parameters) has found that perplexity is a stronger predictor of model behaviors than model size or training computation alone <Paper corpusId=\"254877112\" paperTitle=\"(Xia et al., 2022)\" isShortName></Paper>.\n\nTraining dynamics can also reveal when models transition from memorization to generalization. Recent work has formalized the concept of a \"critical data size\" that marks a shift from quick memorization to slow generalization, identifying data insufficiency, sufficiency, and surplus regimes in language model training <Paper corpusId=\"267061159\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>. Similarly, researchers have observed that the order in which rules are learned appears to be governed by their relative simplicity, with models first learning simpler patterns before more complex ones <Paper corpusId=\"272826680\" paperTitle=\"(M'esz'aros et al., 2024)\" isShortName></Paper>.\n\nUnderstanding these developmental stages has practical implications for training and fine-tuning language models. Some studies have identified bifurcations in weight dynamics that mark transitions to stationary states, suggesting optimal points to terminate training <Paper corpusId=\"268379408\" paperTitle=\"(Nicolini et al., 2024)\" isShortName></Paper>. Others have found that model capabilities can be transient, with some abilities emerging and then disappearing during training despite continuous improvement in loss metrics <Paper corpusId=\"273025704\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265157721\" paperTitle=\"(Singh et al., 2023)\" isShortName></Paper>. These insights highlight the importance of monitoring multiple metrics beyond just loss when evaluating model development.",
                "citations": [
                    {
                        "id": "(Choshen et al., 2021)",
                        "snippets": [
                            "We begin ( \u00a73) by establishing that NLMs learn grammatical phenomena in a consistent order. We evaluate NLMs at different time points along their training, showing that the performance on linguistic phenomena across initializations is highly correlated. We further find many similarities in the set of examples that they correctly classify.\n\nStill, models of different architectures learn at a different pace, and hence cannot be directly compared at identical time points. In \u00a73.3, we overcome this by re-scaling the timeline. We then show that despite architectural differences, NLMs present highly correlated performance trajectories. In \u00a73.4, we further demonstrate that even the choice of training data has minor influence on the results. Finally, in \u00a73.5 we show that the learning dynamics essentially follows a single dimension. Namely, where the average performance is similar, success on linguistic phenomena is also similar.\n\nWe proceed by analyzing the early stages of learning in \u00a74. We find that, at first, NLMs rely mostly on local cues and not on word order. They thus resemble bag-of-words models over a window of the preceding tokens. Later stages seem to drift further away from bag-of-words models toward n-gram models, and with time seem to be more sensitive to structural cues. We also find evidence that some latent features that the model learns may not be related to linguistic phenomena."
                        ],
                        "paper": {
                            "corpus_id": 237491997,
                            "title": "The Grammar-Learning Trajectories of Neural Language Models",
                            "authors": [
                                {
                                    "authorId": "41019330",
                                    "name": "Leshem Choshen"
                                },
                                {
                                    "authorId": "94064232",
                                    "name": "Guy Hacohen"
                                },
                                {
                                    "authorId": "1789171",
                                    "name": "D. Weinshall"
                                },
                                {
                                    "authorId": "2769805",
                                    "name": "Omri Abend"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 29
                        },
                        "score": 0.99267578125
                    },
                    {
                        "id": "(Chang et al., 2023)",
                        "snippets": [
                            "Previous work has studied the pre-training dynamics of language models (Saphra, 0). Choshen et al. (2022) and (Evanson et al., 2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021). Olsson et al. (2022) find that copy mechanisms (\"induction heads\" for in-context learning) appear at an inflection point during pre-training. These results establish when a variety of abilities emerge in language models. Our work studies more fine-grained learning trajectories by evaluating individual tokens in context",
                            "Previous work has demonstrated that language models exhibit fine-grained learning patterns that are not captured by the corpus-level loss curve (related work in \u00a72). In particular, sudden increases and decreases in example loss ( \u00a75 and (Xia et al., 2022)) may be somewhat surprising given that the pre-training text is i.i.d. for all pre-training steps. By demonstrating that many of these sudden changes are consistent regardless of random initialization and data shuffling ( \u00a75.2), our work indicates that some instances of sudden learning and \"forgetting\" are not due to random chance or the specific examples observed in a given step."
                        ],
                        "paper": {
                            "corpus_id": 261277016,
                            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
                            "authors": [
                                {
                                    "authorId": "2087001989",
                                    "name": "Tyler A. Chang"
                                },
                                {
                                    "authorId": "144035504",
                                    "name": "Z. Tu"
                                },
                                {
                                    "authorId": "24316216",
                                    "name": "B. Bergen"
                                }
                            ],
                            "year": 2023,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 13
                        },
                        "score": 0.994140625
                    },
                    {
                        "id": "(Teehan et al., 2022)",
                        "snippets": [
                            "Training dynamics is an emerging field of research, promising to improve our understanding of knowledge acquisition in neural networks and offering insights into the utility of pre-trained models and embedded representations for downstream tasks. Most studies of Transformers (e.g. RoBERTa (Liu et al., 2021)) and LSTMs (Hochreiter et al., 1997) agree that models acquire linguistic knowledge early in the learning process.\n\nLocal syntactic information, such as parts of speech, is learned earlier than information encoding long-distance dependencies (e.g. topic) Saphra, 2021). Exploration of AL-BERT (Lan et al., 2019) and LSTM-based networks reveals different learning patterns for function and content words with more fine-grained distinctions within these categories including part of speech and verb form (Saphra, 2021;(Chiang et al., 2020).\n\nDifferences in learning trajectory were also observed between layers. In LSTMs, recurrent layers become more task-independent over the course of training, while embeddings become more taskspecific (Saphra, 2021). In Transformer-based architectures, i.e.: ALBERT and ELECTRA, (Chiang et al., 2020) observe differences in performance patterns between the top and last layers."
                        ],
                        "paper": {
                            "corpus_id": 247656607,
                            "title": "Emergent Structures and Training Dynamics in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2131107966",
                                    "name": "Ryan Teehan"
                                },
                                {
                                    "authorId": "2029314697",
                                    "name": "Miruna Clinciu"
                                },
                                {
                                    "authorId": "1799401599",
                                    "name": "Oleg Serikov"
                                },
                                {
                                    "authorId": "50812522",
                                    "name": "Eliza Szczechla"
                                },
                                {
                                    "authorId": "12046785",
                                    "name": "Natasha Seelam"
                                },
                                {
                                    "authorId": "8963527",
                                    "name": "Shachar Mirkin"
                                },
                                {
                                    "authorId": "2273789852",
                                    "name": "Aaron Gokaslan"
                                }
                            ],
                            "year": 2022,
                            "venue": "BIGSCIENCE",
                            "n_citations": 11
                        },
                        "score": 0.99267578125
                    },
                    {
                        "id": "(Chiang et al., 2020)",
                        "snippets": [
                            "While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL."
                        ],
                        "paper": {
                            "corpus_id": 222140842,
                            "title": "Pretrained Language Model Embryology: The Birth of ALBERT",
                            "authors": [
                                {
                                    "authorId": "1992777064",
                                    "name": "Cheng-Han Chiang"
                                },
                                {
                                    "authorId": "2210669195",
                                    "name": "Sung-Feng Huang"
                                },
                                {
                                    "authorId": "1706104",
                                    "name": "Hung-yi Lee"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 42
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zucchet et al., 2025)",
                        "snippets": [
                            "This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distributions lead to shorter plateaus. Finally, hallucinations emerge simultaneously with knowledge, and integrating new knowledge into the model through fine-tuning is challenging, as it quickly corrupts its existing parametric memories."
                        ],
                        "paper": {
                            "corpus_id": 277349236,
                            "title": "How do language models learn facts? Dynamics, curricula and hallucinations",
                            "authors": [
                                {
                                    "authorId": "1729494470",
                                    "name": "Nicolas Zucchet"
                                },
                                {
                                    "authorId": "2320771936",
                                    "name": "J\u00f6rg Bornschein"
                                },
                                {
                                    "authorId": "2316336431",
                                    "name": "Stephanie Chan"
                                },
                                {
                                    "authorId": "2270676283",
                                    "name": "Andrew K. Lampinen"
                                },
                                {
                                    "authorId": "1996134",
                                    "name": "Razvan Pascanu"
                                },
                                {
                                    "authorId": "2289159449",
                                    "name": "Soham De"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2022), or phase transitions (Olsson et al., 2022). The interpretability literature rarely illuminates how these capabilities emerge, in part because most analyses only examine the final trained model. Instead, we consider developmental analysis as a complementary explanatory lens."
                        ],
                        "paper": {
                            "corpus_id": 261822542,
                            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
                            "authors": [
                                {
                                    "authorId": "13336152",
                                    "name": "Angelica Chen"
                                },
                                {
                                    "authorId": "2240524527",
                                    "name": "Ravid Schwartz-Ziv"
                                },
                                {
                                    "authorId": "1979489",
                                    "name": "Kyunghyun Cho"
                                },
                                {
                                    "authorId": "2240527814",
                                    "name": "Matthew L. Leavitt"
                                },
                                {
                                    "authorId": "2362960",
                                    "name": "Naomi Saphra"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 74
                        },
                        "score": 0.99169921875
                    },
                    {
                        "id": "(Caballero et al., 2022)",
                        "snippets": [
                            "We present a smoothly broken power law functional form (that we refer to as a Broken Neural Scaling Law (BNSL)) that accurately models&extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as amount of compute used for training (or inference), number of model parameters, training dataset size, model input size, number of training steps, or upstream performance varies) for various architectures&for each of various tasks within a large&diverse set of upstream&downstream tasks, in zero-shot, prompted,&finetuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, AI capabilities, robotics, out-of-distribution (OOD) generalization, continual learning, transfer learning, uncertainty estimation / calibration, OOD detection, adversarial robustness, distillation, sparsity, retrieval, quantization, pruning, fairness, molecules, computer programming/coding, math word problems,\"emergent phase transitions\", arithmetic, supervised learning, unsupervised/self-supervised learning,&reinforcement learning (single agent&multi-agent). When compared to other functional forms for neural scaling, this functional form yields extrapolations of scaling behavior that are considerably more accurate on this set. Moreover, this functional form accurately models&extrapolates scaling behavior that other functional forms are incapable of expressing such as the nonmonotonic transitions present in the scaling behavior of phenomena such as double descent&the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Lastly, we use this functional form to glean insights about the limit of the predictability of scaling behavior. Code is available at https://github.com/ethancaballero/broken_neural_scaling_laws"
                        ],
                        "paper": {
                            "corpus_id": 253117181,
                            "title": "Broken Neural Scaling Laws",
                            "authors": [
                                {
                                    "authorId": "24130593",
                                    "name": "Ethan Caballero"
                                },
                                {
                                    "authorId": "2066789106",
                                    "name": "Kshitij Gupta"
                                },
                                {
                                    "authorId": "2109771",
                                    "name": "I. Rish"
                                },
                                {
                                    "authorId": "145055042",
                                    "name": "David Krueger"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 76
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2021)",
                        "snippets": [
                            "Models of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers\"probing\"the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster."
                        ],
                        "paper": {
                            "corpus_id": 233289478,
                            "title": "Probing Across Time: What Does RoBERTa Know and When?",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Leo Z. Liu"
                                },
                                {
                                    "authorId": "1705260",
                                    "name": "Yizhong Wang"
                                },
                                {
                                    "authorId": "11348687",
                                    "name": "Jungo Kasai"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                },
                                {
                                    "authorId": "144365875",
                                    "name": "Noah A. Smith"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 87
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wal et al., 2025)",
                        "snippets": [
                            "In this work, we define stability as the change in a metric of interest (e.g., validation loss) caused by changes in randomness factors and quantify it using the standard deviation of that metric (see (Du et al., 2023) for other approaches to quantify stability)",
                            "using training maps constructed from statistics of the model parameters, we identify the characteristics of stable training runs and the early signals of instability.\n\nLanguage modelling is largely stable. Generally, we observe LM pre-training dynamics to follow consistent trajectories. Across seeds and model sizes, downstream performance and representational efficiency consistently increase during pre-training, and training maps are linear (except for the outlier seeds).\n\nLinguistic information is encoded in the initial learning phase (10^3-10^4 steps). Across all experiments, metrics start moving away from the initial random baseline around step 10^3 (2B tokens circa) and reach their convergence level around step 10^4 (20B tokens circa). In this initial phase, representational shift peaks and linguistic information begins to be encoded into the models' latent representations.\n\nMost improvements happen in the \"critical\" learning phase (10^4-10^5 steps). In the range of 10^3 to 10^4 steps, most learning occurs, as measured by all of our metrics."
                        ],
                        "paper": {
                            "corpus_id": 276937763,
                            "title": "PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs",
                            "authors": [
                                {
                                    "authorId": "1986356851",
                                    "name": "Oskar van der Wal"
                                },
                                {
                                    "authorId": "2325954375",
                                    "name": "Pietro Lesci"
                                },
                                {
                                    "authorId": "1416353805",
                                    "name": "Max M\u00fcller-Eberstein"
                                },
                                {
                                    "authorId": "2308101135",
                                    "name": "Naomi Saphra"
                                },
                                {
                                    "authorId": "2184031883",
                                    "name": "Hailey Schoelkopf"
                                },
                                {
                                    "authorId": "2254288138",
                                    "name": "Willem Zuidema"
                                },
                                {
                                    "authorId": "2273535086",
                                    "name": "Stella Biderman"
                                }
                            ],
                            "year": 2025,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 2
                        },
                        "score": 0.99609375
                    },
                    {
                        "id": "(Xia et al., 2022)",
                        "snippets": [
                            "We attempt to make progress to answer these questions by studying the training trajectories of differently-sized OPT models (Zhang et al., 2022) through analyzing their intermediate checkpoints.\n\nIn contrast to prior work, which studies the trajectories of small models with up to 300M parameters (Liu et al., 2021;Choshen et al., 2022;Blevins et al., 2022) or focuses on the language modeling objective alone (Kaplan et al., 2020;Hernandez et al., 2021Hernandez et al., , 2022, we are the first to comprehensively study the training trajectories of large-scale autoregressive language models with up to 175B parameters across a wide range of settings.\n\nRepeatedly across training and different model scales, we analyze three aspects of model performance: (i) next-token prediction on subsets of tokens (ii) sequence-level generation and (iii) downstream task performance. We use perplexity, which is closely tied to language model evaluation, as the major metric throughout the study.\n\nFor next-token prediction ( \u00a73), we study the trajectory by categorizing each token's prediction as stagnated, upward or downward according to its perplexity trend as training progresses. We find each category comprising a significant number of tokens: while a significant number of tokens' perplexity stagnate, a subset of tokens with an increasing perplexity in smaller models exhibit a doubledescent trend (Nakkiran et al., 2020) where perplexity increases and then decreases in larger models. These behaviors primarily emerge at a similar validation perplexity across model scales"
                        ],
                        "paper": {
                            "corpus_id": 254877112,
                            "title": "Training Trajectories of Language Models Across Scales",
                            "authors": [
                                {
                                    "authorId": "67284811",
                                    "name": "Mengzhou Xia"
                                },
                                {
                                    "authorId": "2347956",
                                    "name": "Mikel Artetxe"
                                },
                                {
                                    "authorId": "2384711",
                                    "name": "Chunting Zhou"
                                },
                                {
                                    "authorId": "143724481",
                                    "name": "Xi Victoria Lin"
                                },
                                {
                                    "authorId": "10721120",
                                    "name": "Ramakanth Pasunuru"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "1389924486",
                                    "name": "Ves Stoyanov"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 64
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Jung et al., 2024)",
                        "snippets": [
                            "To explore linguistic aspects, we followed the methodology of Conneau et al. (2018) and utilized SentEval toolkit \u2021 to investigate training dynamics of ten different linguistic abilities.Based on the observed trends in training dynamics, we grouped the ten probing tasks into three categories, as presented in Figure 4.\n\nThe first group (Figure 4a) consists of three tasks that exhibit deterioration in Phase 1 followed by recovery in Phase 2. They are Length (number of tokens), Depth (depth of sentence structure trees), and TopConstituents (the grammatical structure of sentences), and all three are closely related to the performance of sentence embedding.We emphasize that the worst performance occurs at or near the boundary between Phase 1 and Phase 2 for the three linguistic abilities, indicating a strong correlation with representation rank.The trend of deterioration followed by recovery was also observed for alignment, where the deterioration occurs while uniformity sharply improves.\n\nThe second group (Figure 4b) consists of three tasks that exhibit an upward performance trend in both Phase 1 and Phase 2. They are WordContent (deducing words from sentence representations), \u2021 https://github.com/facebookresearch/SentEval/tree/main/data/probing  SubjNumber, and ObjNumber (matching the number of subjects and objects in sentence clauses, respectively), and all three are intimately related to the sentence embedding task.These three linguistic abilities do not deteriorate in Phase 1 despite uniformity's sharp improvement, suggesting that the three do not form a trade-off relationship with uniformity.\n\nThe third group (Figure 4c) consists of the four remaining tasks.Their performance either deteriorates (BigramShift and CoordinationInversion) or oscillates (Tense and OddManOut) throughout the fine-tuning."
                        ],
                        "paper": {
                            "corpus_id": 269921716,
                            "title": "Unveiling Key Aspects of Fine-Tuning in Sentence Embeddings: A Representation Rank Analysis",
                            "authors": [
                                {
                                    "authorId": "2053481633",
                                    "name": "Euna Jung"
                                },
                                {
                                    "authorId": "2157223778",
                                    "name": "Jaeill Kim"
                                },
                                {
                                    "authorId": "2244228799",
                                    "name": "Jungmin Ko"
                                },
                                {
                                    "authorId": "2302470951",
                                    "name": "Jinwoo Park"
                                },
                                {
                                    "authorId": "2292408696",
                                    "name": "Wonjong Rhee"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 0
                        },
                        "score": 0.9873046875
                    },
                    {
                        "id": "(Zhu et al., 2024)",
                        "snippets": [
                            "We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data."
                        ],
                        "paper": {
                            "corpus_id": 267061159,
                            "title": "Critical Data Size of Language Models from a Grokking Perspective",
                            "authors": [
                                {
                                    "authorId": "2145238612",
                                    "name": "Xuekai Zhu"
                                },
                                {
                                    "authorId": "2280103402",
                                    "name": "Yao Fu"
                                },
                                {
                                    "authorId": "2218723159",
                                    "name": "Bowen Zhou"
                                },
                                {
                                    "authorId": "2280367391",
                                    "name": "Zhouhan Lin"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 18
                        },
                        "score": 0.99560546875
                    },
                    {
                        "id": "(M'esz'aros et al., 2024)",
                        "snippets": [
                            "We analize the dynamics of learning rule extrapolation. We report results on the Transformer (training dynamics of Mamba and the LSTM are in Appx. A), trained on the a n b n language-where high rule extrapolation ability is achieved. Fig. 3 shows that first, the model learns the sequences obeying (R2), then it learns the language (R1) \u2229 (R2) as its subset. We argue that the order in which rules are learnt is governed by the relative simplicity of the rules, quantified by Kolmogorov complexity. Given a formal language with rules (R1) and (R2), let p 1 , p 2 and p 1,2 be distributions defined by LMs that generate sequences that satisfy (R1), (R2) and (R1) \u2229 (R2), respectively. If, e.g., K(p 2 ) \u226a K(p 1,2 ), our normative algorithm will first learn (R2), and then learn the (R1) \u2229 (R2) as its subset. In the a n b n language, (R2) (a's before b's), is, on average, simpler to generate than (R1) (#a=#b) and (R1) \u2229 (R2). Therefore, we expect our normative algorithm to first learn (R2), and then learn (R1) \u2229 (R2) as its subset. Remarkably, our Transformer employs the same strategy ( Fig. 3), verifying the presence of simplicity bias. This result is matches past observations that Transformers are biased towards low Kolmogorov complexity [Goldblum et al., 2023]."
                        ],
                        "paper": {
                            "corpus_id": 272826680,
                            "title": "Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts",
                            "authors": [
                                {
                                    "authorId": "2214094238",
                                    "name": "Anna M'esz'aros"
                                },
                                {
                                    "authorId": "2299943014",
                                    "name": "Szilvia Ujv'ary"
                                },
                                {
                                    "authorId": "40634590",
                                    "name": "Wieland Brendel"
                                },
                                {
                                    "authorId": "1382657853",
                                    "name": "Patrik Reizinger"
                                },
                                {
                                    "authorId": "2322443584",
                                    "name": "Ferenc Husz'ar"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.99365234375
                    },
                    {
                        "id": "(Nicolini et al., 2024)",
                        "snippets": [
                            "In this study we have analyzed both the temporal and spatial dimensions of training a large language model.As discussed above, our work is the first one dealing with distribution of network weights as a whole, by means of computational methods borrowed from statistical mechanics.More specifically, this work shows that a bifurcation occurs in the dynamics of the weights during the training process.Such transitions are observed across various models of different sizes trained with distinct datasets.We have conducted a thorough and meticulous analysis of this aspect and concluded that this bifurcation marks a transition to a stationary state, indicating that further training is unlikely to significantly alter the weight values.Thus, training can be efficiently terminated upon reaching such a stationary state.Moreover, our study has offered a possible interpretation of the bifurcation phenomenon in terms of model perplexity."
                        ],
                        "paper": {
                            "corpus_id": 268379408,
                            "title": "The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2256990455",
                                    "name": "Carlo Nicolini"
                                },
                                {
                                    "authorId": "2256994086",
                                    "name": "Jacopo Staiano"
                                },
                                {
                                    "authorId": "2291065942",
                                    "name": "Bruno Lepri"
                                },
                                {
                                    "authorId": "2291066066",
                                    "name": "Raffaele Marino"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9951171875
                    },
                    {
                        "id": "(Lee et al., 2024)",
                        "snippets": [
                            "Most research on LMs focuses on the final configuration of the model at the end of pre-training. Yet, recent work shows that learning dynamics can elucidate the behavior and computational mechanisms of LMs (Chen et al., 2023)(Singh et al., 2023)Tigges et al., 2024). It has been found that, over training, LMs' weight matrices become higher-rank (Boix-Adser\u00e0 et al., 2023), their representations higher dimensional (Cheng et al., 2024), and their gradients increasingly diffuse (Weber et al., 2024). Over finetuning, representational dimensionality has been found to change in concert with geometric properties like cluster reorganization (Doimo et al., 2024)."
                        ],
                        "paper": {
                            "corpus_id": 273025704,
                            "title": "Geometric Signatures of Compositionality Across a Language Model's Lifetime",
                            "authors": [
                                {
                                    "authorId": "2323997802",
                                    "name": "Jin Hwa Lee"
                                },
                                {
                                    "authorId": "2187058673",
                                    "name": "Thomas Jiralerspong"
                                },
                                {
                                    "authorId": "2324060912",
                                    "name": "Lei Yu"
                                },
                                {
                                    "authorId": "1865800402",
                                    "name": "Y. Bengio"
                                },
                                {
                                    "authorId": "2323783941",
                                    "name": "Emily Cheng"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.98681640625
                    },
                    {
                        "id": "(Singh et al., 2023)",
                        "snippets": [
                            "Transformer neural networks can exhibit a surprising capacity for in-context learning (ICL) despite not being explicitly trained for it. Prior work has provided a deeper understanding of how ICL emerges in transformers, e.g. through the lens of mechanistic interpretability, Bayesian inference, or by examining the distributional properties of training data. However, in each of these cases, ICL is treated largely as a persistent phenomenon; namely, once ICL emerges, it is assumed to persist asymptotically. Here, we show that the emergence of ICL during transformer training is, in fact, often transient. We train transformers on synthetic data designed so that both ICL and in-weights learning (IWL) strategies can lead to correct predictions. We find that ICL first emerges, then disappears and gives way to IWL, all while the training loss decreases, indicating an asymptotic preference for IWL. The transient nature of ICL is observed in transformers across a range of model sizes and datasets, raising the question of how much to\"overtrain\"transformers when seeking compact, cheaper-to-run models. We find that L2 regularization may offer a path to more persistent ICL that removes the need for early stopping based on ICL-style validation tasks. Finally, we present initial evidence that ICL transience may be caused by competition between ICL and IWL circuits."
                        ],
                        "paper": {
                            "corpus_id": 265157721,
                            "title": "The Transient Nature of Emergent In-Context Learning in Transformers",
                            "authors": [
                                {
                                    "authorId": "2257127712",
                                    "name": "Aaditya K. Singh"
                                },
                                {
                                    "authorId": "50328436",
                                    "name": "Stephanie C. Y. Chan"
                                },
                                {
                                    "authorId": "2238206617",
                                    "name": "Ted Moskovitz"
                                },
                                {
                                    "authorId": "2266468307",
                                    "name": "Erin Grant"
                                },
                                {
                                    "authorId": "2261233446",
                                    "name": "Andrew M. Saxe"
                                },
                                {
                                    "authorId": "2265548290",
                                    "name": "Felix Hill"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 44
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Architectural Influence on Training Dynamics",
                "tldr": "The architecture of language models significantly impacts how they learn during training, with different designs showing varied learning speeds and patterns despite following similar overall trajectories. Transformers have become the dominant architecture due to their effective attention mechanisms, though the specific interactions between components like attention layers and MLPs evolve distinctly through training phases. (10 sources)",
                "text": "\nThe architecture of a language model substantially influences its training dynamics, though research suggests that fundamental learning patterns remain consistent across different designs. Studies have found that despite architectural differences, neural language models (NLMs) present highly correlated performance trajectories when their timelines are appropriately scaled <Paper corpusId=\"237491997\" paperTitle=\"(Choshen et al., 2021)\" isShortName></Paper>. This suggests that architecture affects the pace of learning rather than the fundamental order in which linguistic phenomena are acquired.\n\nDifferent architectures show distinct learning patterns for various linguistic elements. In both Transformer and LSTM architectures, local syntactic information like parts of speech is learned earlier than information encoding long-distance dependencies <Paper corpusId=\"247656607\" paperTitle=\"(Teehan et al., 2022)\" isShortName></Paper>. However, the specific learning trajectories can vary by architecture. For instance, in LSTMs, recurrent layers become more task-independent during training while embeddings become more task-specific. In contrast, Transformer-based models like ALBERT and ELECTRA show different performance patterns between their top and bottom layers <Paper corpusId=\"247656607\" paperTitle=\"(Teehan et al., 2022)\" isShortName></Paper> <Paper corpusId=\"222140842\" paperTitle=\"(Chiang et al., 2020)\" isShortName></Paper>.\n\nThe Transformer architecture has become particularly dominant in recent years <Paper corpusId=\"272910946\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"13756489\" paperTitle=\"(Vaswani et al., 2017)\" isShortName></Paper>, revolutionizing natural language processing through its self-attention mechanism. This architecture enables models to attend to interactions between different elements in a sequence, which is crucial for capturing linguistic dependencies. Recent theoretical advances have begun to elucidate how the self-attention mechanism learns during training for next-token prediction tasks <Paper corpusId=\"272910946\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258947127\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268379753\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nAnalysis of training dynamics in Transformers has revealed a two-step process in which self-attention operates as a \"discriminative scanning algorithm.\" Starting from uniform attention, the model gradually attends more to distinct key tokens for specific next-token predictions while paying less attention to common tokens that appear across different contexts <Paper corpusId=\"258947127\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper>. This process involves what researchers call a \"scan and snap\" dynamic, where the attention mechanism first scans for relevant tokens and then snaps to a stable configuration.\n\nFurther research has characterized this process as involving two distinct steps: hard retrieval, where self-attention selects high-priority input tokens associated with the last input token, followed by soft composition, where it creates a convex combination of these tokens from which the next token can be sampled <Paper corpusId=\"268379753\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This suggests that the attention mechanism implicitly discovers the structure of the language during training.\n\nStudies of shallow transformers have shown that gradient flow serves as an inherent mechanism dividing the training process into two phases: first, the linear MLP quickly aligns with target signals while softmax attention remains almost unchanged; then, attention matrices and the MLP evolve jointly to enlarge the classification margin <Paper corpusId=\"273346324\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. This phased approach appears to be an inherent property of the architecture rather than a result of explicit design.\n\nModel size also affects training dynamics. Larger models show different stabilization patterns than smaller ones, with nearly all layers in larger models stabilizing early in training (within the first 20%), while smaller models exhibit slower and less stable convergence <Paper corpusId=\"273351173\" paperTitle=\"(Martinez et al., 2024)\" isShortName></Paper>. This difference appears to be related to the effective rank of the model parameters, with lower effective rank correlating with slower convergence.\n\nBeyond model size, research has identified other architectural elements that influence training dynamics. Circuits in language models appear causally relevant to emergent capabilities and can be ablated to reduce these capabilities <Paper corpusId=\"264935245\" paperTitle=\"(Quirke et al., 2023)\" isShortName></Paper>. Different layers also converge at different rates during training, with early network layers typically converging earlier than later layers <Paper corpusId=\"264935245\" paperTitle=\"(Quirke et al., 2023)\" isShortName></Paper>.\n\nThe overall picture that emerges is that while different architectures may learn at different rates and through somewhat different mechanisms, they tend to follow similar overall trajectories in terms of the order in which they acquire different types of linguistic knowledge. This suggests fundamental principles underlying language model training that transcend specific architectural choices.",
                "citations": [
                    {
                        "id": "(Choshen et al., 2021)",
                        "snippets": [
                            "We begin ( \u00a73) by establishing that NLMs learn grammatical phenomena in a consistent order. We evaluate NLMs at different time points along their training, showing that the performance on linguistic phenomena across initializations is highly correlated. We further find many similarities in the set of examples that they correctly classify.\n\nStill, models of different architectures learn at a different pace, and hence cannot be directly compared at identical time points. In \u00a73.3, we overcome this by re-scaling the timeline. We then show that despite architectural differences, NLMs present highly correlated performance trajectories. In \u00a73.4, we further demonstrate that even the choice of training data has minor influence on the results. Finally, in \u00a73.5 we show that the learning dynamics essentially follows a single dimension. Namely, where the average performance is similar, success on linguistic phenomena is also similar.\n\nWe proceed by analyzing the early stages of learning in \u00a74. We find that, at first, NLMs rely mostly on local cues and not on word order. They thus resemble bag-of-words models over a window of the preceding tokens. Later stages seem to drift further away from bag-of-words models toward n-gram models, and with time seem to be more sensitive to structural cues. We also find evidence that some latent features that the model learns may not be related to linguistic phenomena."
                        ],
                        "paper": {
                            "corpus_id": 237491997,
                            "title": "The Grammar-Learning Trajectories of Neural Language Models",
                            "authors": [
                                {
                                    "authorId": "41019330",
                                    "name": "Leshem Choshen"
                                },
                                {
                                    "authorId": "94064232",
                                    "name": "Guy Hacohen"
                                },
                                {
                                    "authorId": "1789171",
                                    "name": "D. Weinshall"
                                },
                                {
                                    "authorId": "2769805",
                                    "name": "Omri Abend"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 29
                        },
                        "score": 0.99267578125
                    },
                    {
                        "id": "(Teehan et al., 2022)",
                        "snippets": [
                            "Training dynamics is an emerging field of research, promising to improve our understanding of knowledge acquisition in neural networks and offering insights into the utility of pre-trained models and embedded representations for downstream tasks. Most studies of Transformers (e.g. RoBERTa (Liu et al., 2021)) and LSTMs (Hochreiter et al., 1997) agree that models acquire linguistic knowledge early in the learning process.\n\nLocal syntactic information, such as parts of speech, is learned earlier than information encoding long-distance dependencies (e.g. topic) Saphra, 2021). Exploration of AL-BERT (Lan et al., 2019) and LSTM-based networks reveals different learning patterns for function and content words with more fine-grained distinctions within these categories including part of speech and verb form (Saphra, 2021;(Chiang et al., 2020).\n\nDifferences in learning trajectory were also observed between layers. In LSTMs, recurrent layers become more task-independent over the course of training, while embeddings become more taskspecific (Saphra, 2021). In Transformer-based architectures, i.e.: ALBERT and ELECTRA, (Chiang et al., 2020) observe differences in performance patterns between the top and last layers."
                        ],
                        "paper": {
                            "corpus_id": 247656607,
                            "title": "Emergent Structures and Training Dynamics in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2131107966",
                                    "name": "Ryan Teehan"
                                },
                                {
                                    "authorId": "2029314697",
                                    "name": "Miruna Clinciu"
                                },
                                {
                                    "authorId": "1799401599",
                                    "name": "Oleg Serikov"
                                },
                                {
                                    "authorId": "50812522",
                                    "name": "Eliza Szczechla"
                                },
                                {
                                    "authorId": "12046785",
                                    "name": "Natasha Seelam"
                                },
                                {
                                    "authorId": "8963527",
                                    "name": "Shachar Mirkin"
                                },
                                {
                                    "authorId": "2273789852",
                                    "name": "Aaron Gokaslan"
                                }
                            ],
                            "year": 2022,
                            "venue": "BIGSCIENCE",
                            "n_citations": 11
                        },
                        "score": 0.99267578125
                    },
                    {
                        "id": "(Chiang et al., 2020)",
                        "snippets": [
                            "While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL."
                        ],
                        "paper": {
                            "corpus_id": 222140842,
                            "title": "Pretrained Language Model Embryology: The Birth of ALBERT",
                            "authors": [
                                {
                                    "authorId": "1992777064",
                                    "name": "Cheng-Han Chiang"
                                },
                                {
                                    "authorId": "2210669195",
                                    "name": "Sung-Feng Huang"
                                },
                                {
                                    "authorId": "1706104",
                                    "name": "Hung-yi Lee"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 42
                        },
                        "score": 0
                    },
                    {
                        "id": "(Huang et al., 2024)",
                        "snippets": [
                            "The transformer architecture (Vaswani et al., 2017) has revolutionized the field of machine learning, establishing itself as a foundation model for numerous applications, including natural language processing (NLP) (Devlin et al., 2018), computer vision (Dosovitskiy et al., 2020), and multi-modal signal processing (Tsai et al., 2019). In particular, transformers achieve tremendous empirical success in large language models (LLMs) such as GPT-3 (Brown et al., 2020). Despite the empirical success, limited theoretical understanding of transformers have caused a series of critical concerns about their robustness, interpretability, and bias issues (Bommasani et al., 2021;(Belkin, 2024). \n\nTo overcome these issues, recent advances in transformer theory have investigated the convergence of training transformers under theoretically amenable setting such as linear regression (Mahankali et al., 2023;Zhang et al., 2023;Huang et al., 2023) and binary classification (Tarzanagh et al., 2023b,a;Vasudeva et al., 2024;Li et al., 2023). Nevertheless, one of the fundamental task in LLMs and other generative models is next-token prediction (NTP), which involves predicting the next word or token in a sequence, given the previous tokens. In NTP, a few recent theoretical studies have started to investigate the training dynamics of transformers (Tian et al., 2023)(Li et al., 2024). However, those works lack of fine-grained non-asymptotic convergence analysis of the training process, posing the following open questions for further investigation: \n\nHow fast does the training of a transformer converge in NTP?"
                        ],
                        "paper": {
                            "corpus_id": 272910946,
                            "title": "Non-asymptotic Convergence of Training Transformers for Next-token Prediction",
                            "authors": [
                                {
                                    "authorId": "2140386847",
                                    "name": "Ruiquan Huang"
                                },
                                {
                                    "authorId": "2294511987",
                                    "name": "Yingbin Liang"
                                },
                                {
                                    "authorId": "2261298891",
                                    "name": "Jing Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 7
                        },
                        "score": 0.98681640625
                    },
                    {
                        "id": "(Vaswani et al., 2017)",
                        "snippets": [
                            "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
                        ],
                        "paper": {
                            "corpus_id": 13756489,
                            "title": "Attention is All you Need",
                            "authors": [
                                {
                                    "authorId": "40348417",
                                    "name": "Ashish Vaswani"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "3877127",
                                    "name": "Niki Parmar"
                                },
                                {
                                    "authorId": "39328010",
                                    "name": "Jakob Uszkoreit"
                                },
                                {
                                    "authorId": "145024664",
                                    "name": "Llion Jones"
                                },
                                {
                                    "authorId": "19177000",
                                    "name": "Aidan N. Gomez"
                                },
                                {
                                    "authorId": "40527594",
                                    "name": "Lukasz Kaiser"
                                },
                                {
                                    "authorId": "3443442",
                                    "name": "I. Polosukhin"
                                }
                            ],
                            "year": 2017,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 132444
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tian et al., 2023)",
                        "snippets": [
                            "Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \\emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \\emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends more to distinct key tokens for a specific next token to be predicted, and pays less attention to common key tokens that occur across different next tokens. Among distinct tokens, it progressively drops attention weights, following the order of low to high co-occurrence between the key and the query token in the training set. Interestingly, this procedure does not lead to winner-takes-all, but decelerates due to a \\emph{phase transition} that is controllable by the learning rates of the two layers, leaving (almost) fixed token combination. We verify this \\textbf{\\emph{scan and snap}} dynamics on synthetic and real-world data (WikiText)."
                        ],
                        "paper": {
                            "corpus_id": 258947127,
                            "title": "Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer",
                            "authors": [
                                {
                                    "authorId": "1932187449",
                                    "name": "Yuandong Tian"
                                },
                                {
                                    "authorId": "2167496459",
                                    "name": "Yiping Wang"
                                },
                                {
                                    "authorId": "4319427",
                                    "name": "Beidi Chen"
                                },
                                {
                                    "authorId": "145697585",
                                    "name": "S. Du"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 79
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Transformer-based language models are trained on large datasets to predict the next token given an input sequence. Despite this simple training objective, they have led to revolutionary advances in natural language processing. Underlying this success is the self-attention mechanism. In this work, we ask: $\\textit{What}$ $\\textit{does}$ $\\textit{a}$ $\\textit{single}$ $\\textit{self-attention}$ $\\textit{layer}$ $\\textit{learn}$ $\\textit{from}$ $\\textit{next-token}$ $\\textit{prediction?}$ We show that training self-attention with gradient descent learns an automaton which generates the next token in two distinct steps: $\\textbf{(1)}$ $\\textbf{Hard}$ $\\textbf{retrieval:}$ Given input sequence, self-attention precisely selects the $\\textit{high-priority}$ $\\textit{input}$ $\\textit{tokens}$ associated with the last input token. $\\textbf{(2)}$ $\\textbf{Soft}$ $\\textbf{composition:}$ It then creates a convex combination of the high-priority tokens from which the next token can be sampled. Under suitable conditions, we rigorously characterize these mechanics through a directed graph over tokens extracted from the training data. We prove that gradient descent implicitly discovers the strongly-connected components (SCC) of this graph and self-attention learns to retrieve the tokens that belong to the highest-priority SCC available in the context window. Our theory relies on decomposing the model weights into a directional component and a finite component that correspond to hard retrieval and soft composition steps respectively. This also formalizes a related implicit bias formula conjectured in [Tarzanagh et al. 2023]. We hope that these findings shed light on how self-attention processes sequential data and pave the path toward demystifying more complex architectures."
                        ],
                        "paper": {
                            "corpus_id": 268379753,
                            "title": "Mechanics of Next Token Prediction with Self-Attention",
                            "authors": [
                                {
                                    "authorId": "1527089987",
                                    "name": "Yingcong Li"
                                },
                                {
                                    "authorId": "2284934923",
                                    "name": "Yixiao Huang"
                                },
                                {
                                    "authorId": "46214352",
                                    "name": "M. E. Ildiz"
                                },
                                {
                                    "authorId": "2241094",
                                    "name": "A. Rawat"
                                },
                                {
                                    "authorId": "3103394",
                                    "name": "Samet Oymak"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Artificial Intelligence and Statistics",
                            "n_citations": 30
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "In this work, we study the dynamics of training a shallow transformer on a task of recognizing co-occurrence of two designated words. In the literature of studying training dynamics of transformers, several simplifications are commonly adopted such as weight reparameterization, attention linearization, special initialization, and lazy regime. In contrast, we analyze the gradient flow dynamics of simultaneously training three attention matrices and a linear MLP layer from random initialization, and provide a framework of analyzing such dynamics via a coupled dynamical system",
                            "We discover that gradient flow serves as an inherent mechanism that naturally divide the training process into two phases. In Phase 1, the linear MLP quickly aligns with the two target signals for correct classification, whereas the softmax attention remains almost unchanged. In Phase 2, the attention matrices and the MLP evolve jointly to enlarge the classification margin and reduce the loss to a near minimum value. Technically, we prove a novel property of the gradient flow, termed \\textit{automatic balancing of gradients}, which enables the loss values of different samples to decrease almost at the same rate and further facilitates the proof of near minimum training loss."
                        ],
                        "paper": {
                            "corpus_id": 273346324,
                            "title": "Training Dynamics of Transformers to Recognize Word Co-occurrence via Gradient Flow Analysis",
                            "authors": [
                                {
                                    "authorId": "2118571035",
                                    "name": "Hongru Yang"
                                },
                                {
                                    "authorId": "1749353",
                                    "name": "B. Kailkhura"
                                },
                                {
                                    "authorId": "2269758970",
                                    "name": "Zhangyang Wang"
                                },
                                {
                                    "authorId": "2269703508",
                                    "name": "Yingbin Liang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 3
                        },
                        "score": 0.99658203125
                    },
                    {
                        "id": "(Martinez et al., 2024)",
                        "snippets": [
                            "We use the Pythia model suite to analyse the training dynamics that underlie this phenomenon. Across different model sizes, we investigate the convergence of the Attention and MLP activations to their final state and examine how the effective rank of their parameters influences this process. We find that nearly all layers in larger models stabilise early in training - within the first 20% - whereas layers in smaller models exhibit slower and less stable convergence, especially when their parameters have lower effective rank. By linking the convergence of layers' activations to their parameters' effective rank, our analyses can guide future work to address inefficiencies in the learning dynamics of small models."
                        ],
                        "paper": {
                            "corpus_id": 273351173,
                            "title": "Tending Towards Stability: Convergence Challenges in Small Language Models",
                            "authors": [
                                {
                                    "authorId": "2266941716",
                                    "name": "Richard Diehl Martinez"
                                },
                                {
                                    "authorId": "2325954375",
                                    "name": "Pietro Lesci"
                                },
                                {
                                    "authorId": "33490976",
                                    "name": "P. Buttery"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 4
                        },
                        "score": 0.9931640625
                    },
                    {
                        "id": "(Quirke et al., 2023)",
                        "snippets": [
                            "Training dynamics Several works (Olsson et al., 2022;Nanda et al., 2023) discovered circuits in language models that seem causally relevant to emergent capabilities and can be ablated to reduce the capabilities. Michaud et al. (2023) postulated that many capabilities in language models emerge rapidly in phase transitions and proposed a model in which any capabilities which appear to instead form gradually can be decomposed into discrete, additive capabilities which emerge in independent phase transitions. Finally, several works (Varma et al., 2023;Nanda et al., 2023) observed the phenomenon of grokking, where a model that has already achieved near-perfect performance on the training data transitions to a mechanism that generalizes. \n\nWhile only a minority of circuits-style analysis has focused on model development, a wealth of results in the field of developmental interpretability have characterized the training dynamics of language models using other units of analysis. Behavioral analysis has shown that many model capabilities, such as predicting parts of speech and individual tokens, are learned during independent phase transitions characterized by varying slopes and points of maximum curvature (Chiang et al., 2020)Wei et al., 2022). Raghu et al. (2017) proposed singular vector canonical correlation analysis, a tool which allows different model layers and networks to be compared, and showed that early network layers converge earlier in training than late layers. Saphra and Lopez (2020b,a) found that LSTM models learn hierarchical sequential features in natural language bottom up -shorter sequences are learned first and form the basis for the representation of longer sequences that contain the shorter ones. Finally, Saxe et al. (2019) found that the features are learned in an order determined by the magnitude of the corresponding singular value in the model."
                        ],
                        "paper": {
                            "corpus_id": 264935245,
                            "title": "Training Dynamics of Contextual N-Grams in Language Models",
                            "authors": [
                                {
                                    "authorId": "2243240285",
                                    "name": "Lucia Quirke"
                                },
                                {
                                    "authorId": "2047549746",
                                    "name": "Lovis Heindrich"
                                },
                                {
                                    "authorId": "2056771333",
                                    "name": "Wes Gurnee"
                                },
                                {
                                    "authorId": "2051128902",
                                    "name": "Neel Nanda"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.99755859375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Factors Affecting Training Dynamics",
                "tldr": "Multiple factors significantly influence language model training dynamics, including dataset size, model scale, learning rate schedules, and optimization techniques. These factors interact in complex ways, creating distinct learning regimes and phase transitions that affect memorization, generalization capabilities, and convergence patterns. (10 sources)",
                "text": "\nThe size and quality of training data play a fundamental role in shaping language model training dynamics. Research has identified a \"critical data size\" that marks a significant transition from quick memorization to slow generalization in language models, with data insufficiency, sufficiency, and surplus regimes each exhibiting distinct training patterns <Paper corpusId=\"267061159\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>. As model sizes increase, this critical point also becomes larger, indicating that larger models require proportionally more data to reach their full potential <Paper corpusId=\"267061159\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>.\n\nModel scale itself directly impacts learning trajectories. Larger language models memorize training data faster across all settings and, counterintuitively, can memorize a larger portion of data before overfitting compared to smaller models <Paper corpusId=\"248986465\" paperTitle=\"(Tirumala et al., 2022)\" isShortName></Paper>. They also tend to forget less throughout the training process, suggesting that scale provides inherent advantages beyond just raw computational power <Paper corpusId=\"248986465\" paperTitle=\"(Tirumala et al., 2022)\" isShortName></Paper>. When analyzing different model scales (up to 175B parameters), researchers have observed that token prediction patterns vary across sizes, with some tokens showing a \"double-descent\" trend in larger models where perplexity initially increases before decreasing <Paper corpusId=\"254877112\" paperTitle=\"(Xia et al., 2022)\" isShortName></Paper>.\n\nLearning rate schedules significantly impact training dynamics and convergence patterns. Recent research has established that the cross-entropy loss curves of neural language models follow a scaling law with learning rate annealing over training steps, accounting for both power-law scaling over data size and additional loss reduction during learning rate annealing phases <Paper corpusId=\"271909320\" paperTitle=\"(Tissue et al., 2024)\" isShortName></Paper>. The technique of \"SkipLR\" - switching learning rates at predetermined times during training - has been shown to cause loss curves to contract toward each other, providing insights into how learning rate schedules affect overall training dynamics <Paper corpusId=\"274060474\" paperTitle=\"(Subramanian et al., 2024)\" isShortName></Paper>.\n\nOptimization techniques also play a crucial role in training dynamics. Studies comparing Sharpness-Aware Minimization (SAM) with standard Stochastic Gradient Descent (SGD) have shown that SAM significantly reduces the operator norm of the Hessian and produces different alignment patterns between gradients and the principal eigenvector of the Hessian <Paper corpusId=\"262217060\" paperTitle=\"(Long et al., 2023)\" isShortName></Paper>. While SAM takes longer to achieve certain loss values, it ultimately achieves training error similar to SGD but with less sharpness, suggesting different convergence properties <Paper corpusId=\"262217060\" paperTitle=\"(Long et al., 2023)\" isShortName></Paper>.\n\nFine-tuning approaches also demonstrate distinct training dynamics. Linear Probing followed by Fine-Tuning (LP-FT) for classification tasks has been analyzed using neural tangent kernel theory, revealing that the linear head norm alongside initial prediction accuracy significantly influences fine-tuning outcomes <Paper corpusId=\"270062468\" paperTitle=\"(Tomihari et al., 2024)\" isShortName></Paper>. The increase in linear head norm during linear probing reduces changes in learned features during subsequent fine-tuning <Paper corpusId=\"270062468\" paperTitle=\"(Tomihari et al., 2024)\" isShortName></Paper>.\n\nThe content of training data and model architecture components also affect how learning progresses. Analysis of memorization patterns has revealed that models memorize nouns and numbers first, as these may act as unique identifiers for individual training examples <Paper corpusId=\"248986465\" paperTitle=\"(Tirumala et al., 2022)\" isShortName></Paper>. Further research suggests that specific model components, particularly the embedding space and self-attention mechanisms, play pivotal roles in shaping learning biases early in training <Paper corpusId=\"276235362\" paperTitle=\"(Yao et al., 2025)\" isShortName></Paper>.\n\nRecent studies have proposed a \"Temporal Scaling Law\" to model how test loss evolves as training steps increase, breaking down the overall loss into fine-grained token position loss and developing a dynamic hyperbolic law that accurately predicts test loss across training steps for both in-distribution and out-of-distribution validation datasets <Paper corpusId=\"269449894\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper>. This approach provides a more nuanced understanding of loss progression than traditional scaling laws that focus only on final test loss.\n\nComprehensive analysis of various 7B-scale models has highlighted that training strategies significantly impact learning efficiency, especially in early training stages. Factors including dataset quality, learning rate adjustments, batch size, and regularization techniques all contribute to different training trajectories <Paper corpusId=\"268820276\" paperTitle=\"(Yang et al._1, 2024)\" isShortName></Paper>. Additionally, research has found that task dynamics within a domain can predict dynamics of unseen tasks, and that enhancement of abilities across domains progresses from basic to advanced levels, similar to curriculum learning in humans <Paper corpusId=\"268820276\" paperTitle=\"(Yang et al._1, 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Zhu et al., 2024)",
                        "snippets": [
                            "We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data."
                        ],
                        "paper": {
                            "corpus_id": 267061159,
                            "title": "Critical Data Size of Language Models from a Grokking Perspective",
                            "authors": [
                                {
                                    "authorId": "2145238612",
                                    "name": "Xuekai Zhu"
                                },
                                {
                                    "authorId": "2280103402",
                                    "name": "Yao Fu"
                                },
                                {
                                    "authorId": "2218723159",
                                    "name": "Bowen Zhou"
                                },
                                {
                                    "authorId": "2280367391",
                                    "name": "Zhouhan Lin"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 18
                        },
                        "score": 0.99560546875
                    },
                    {
                        "id": "(Tirumala et al., 2022)",
                        "snippets": [
                            "We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process. We measure the effects of dataset size, learning rate, and model size on memorization, finding that larger language models memorize training data faster across all settings. Surprisingly, we show that larger models can memorize a larger portion of the data before over-fitting and tend to forget less throughout the training process. We also analyze the memorization dynamics of different parts of speech and find that models memorize nouns and numbers first; we hypothesize and provide empirical evidence that nouns and numbers act as a unique identifier for memorizing individual training examples."
                        ],
                        "paper": {
                            "corpus_id": 248986465,
                            "title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2551387",
                                    "name": "Kushal Tirumala"
                                },
                                {
                                    "authorId": "153608000",
                                    "name": "Aram H. Markosyan"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "2201435",
                                    "name": "Armen Aghajanyan"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 197
                        },
                        "score": 0.9990234375
                    },
                    {
                        "id": "(Xia et al., 2022)",
                        "snippets": [
                            "We attempt to make progress to answer these questions by studying the training trajectories of differently-sized OPT models (Zhang et al., 2022) through analyzing their intermediate checkpoints.\n\nIn contrast to prior work, which studies the trajectories of small models with up to 300M parameters (Liu et al., 2021;Choshen et al., 2022;Blevins et al., 2022) or focuses on the language modeling objective alone (Kaplan et al., 2020;Hernandez et al., 2021Hernandez et al., , 2022, we are the first to comprehensively study the training trajectories of large-scale autoregressive language models with up to 175B parameters across a wide range of settings.\n\nRepeatedly across training and different model scales, we analyze three aspects of model performance: (i) next-token prediction on subsets of tokens (ii) sequence-level generation and (iii) downstream task performance. We use perplexity, which is closely tied to language model evaluation, as the major metric throughout the study.\n\nFor next-token prediction ( \u00a73), we study the trajectory by categorizing each token's prediction as stagnated, upward or downward according to its perplexity trend as training progresses. We find each category comprising a significant number of tokens: while a significant number of tokens' perplexity stagnate, a subset of tokens with an increasing perplexity in smaller models exhibit a doubledescent trend (Nakkiran et al., 2020) where perplexity increases and then decreases in larger models. These behaviors primarily emerge at a similar validation perplexity across model scales"
                        ],
                        "paper": {
                            "corpus_id": 254877112,
                            "title": "Training Trajectories of Language Models Across Scales",
                            "authors": [
                                {
                                    "authorId": "67284811",
                                    "name": "Mengzhou Xia"
                                },
                                {
                                    "authorId": "2347956",
                                    "name": "Mikel Artetxe"
                                },
                                {
                                    "authorId": "2384711",
                                    "name": "Chunting Zhou"
                                },
                                {
                                    "authorId": "143724481",
                                    "name": "Xi Victoria Lin"
                                },
                                {
                                    "authorId": "10721120",
                                    "name": "Ramakanth Pasunuru"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "1389924486",
                                    "name": "Ves Stoyanov"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 64
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Tissue et al., 2024)",
                        "snippets": [
                            "We find that the cross-entropy loss curves of neural language models empirically adhere to a scaling law with learning rate (LR) annealing over training steps: $$L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C\\cdot S_2,$$ where $L(s)$ is the validation loss at step $s$, $S_1$ is the area under the LR curve, $S_2$ is the LR annealing area, and $L_0$, $A$, $C$, $\\alpha$ are constant parameters. This formulation takes into account two factors: (1) power-law scaling over data size, and (2) the additional loss reduction during LR annealing. Therefore, this formulation can describe the full loss curve at each step, rather than the single loss point at the end of training."
                        ],
                        "paper": {
                            "corpus_id": 271909320,
                            "title": "Scaling Law with Learning Rate Annealing",
                            "authors": [
                                {
                                    "authorId": "2316486693",
                                    "name": "Howe Tissue"
                                },
                                {
                                    "authorId": "2316485233",
                                    "name": "Venus Wang"
                                },
                                {
                                    "authorId": "2316501479",
                                    "name": "Lu Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.98828125
                    },
                    {
                        "id": "(Subramanian et al., 2024)",
                        "snippets": [
                            "We consider the effect of switching the learning rate at a predetermined time during training, which we refer to as \"SkipLR\". We model SGD as a stochastic gradient flow and show that when starting from the same initial parameters, switching the learning rate causes the loss curves to contract towards each other. We demonstrate this theoretically for some simple cases, and empirically on large language models. Our analysis provides insight into how learning rate schedules affect the training dynamics, and could inform the design of new schedules to accelerate convergence."
                        ],
                        "paper": {
                            "corpus_id": 274060474,
                            "title": "Hop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2237161748",
                                    "name": "Shreyas Vathul Subramanian"
                                },
                                {
                                    "authorId": "7194698",
                                    "name": "Vignesh Ganapathiraman"
                                },
                                {
                                    "authorId": "2258574389",
                                    "name": "Corey Barrett"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 0
                        },
                        "score": 0.99658203125
                    },
                    {
                        "id": "(Long et al., 2023)",
                        "snippets": [
                            "Next, we plot the same quantities when the network is trained with SAM, with \u03c1 = 0.3, in Figure 13. Here, the operator norm of the Hessian is significantly less than when SGD is used, and we see evidence that training in SAM operates at the edge of stability analyzed in Section 2. In Figure 14, we zoom in on the lower part of the curve, and plot the operator norm of the Hessian, to examine the relationship between this quantity and the SAM edge in more detail",
                            "Figure 15 contains plots of the training loss, once again estimated per-minibatch. We included these mainly to motivate the combinations of hyperparameters where we examined other aspects of the dynamics of SAM. As expected, while SAM does take longer to achieve a certain loss, it ultimately achieves training error similar to SGD, but with less sharpness",
                            "Figure 16 contains plots of the alignment, once again estimated per-minibatch. For the large learning rates, late in training, despite the sampling noise arising from the use of minibatches, we see a systematic tendency for the SAM gradients align more closely with the principal eigenvector of the Hessian than the gradients at the initial solution. However, for the smallest learning rates, the opposite holds."
                        ],
                        "paper": {
                            "corpus_id": 262217060,
                            "title": "Sharpness-Aware Minimization and the Edge of Stability",
                            "authors": [
                                {
                                    "authorId": "144007105",
                                    "name": "Philip M. Long"
                                },
                                {
                                    "authorId": "2244620501",
                                    "name": "Peter L. Bartlett"
                                }
                            ],
                            "year": 2023,
                            "venue": "Journal of machine learning research",
                            "n_citations": 10
                        },
                        "score": 0.98291015625
                    },
                    {
                        "id": "(Tomihari et al., 2024)",
                        "snippets": [
                            "In this paper, we analyze the training dynamics of LP-FT for classification tasks on the basis of the neural tangent kernel (NTK) theory. Our analysis decomposes the NTK matrix into two components. This decomposition highlights the importance of the linear head norm alongside the prediction accuracy at the start of the FT stage. We also observe a significant increase in the linear head norm during LP, which stems from training with the cross-entropy (CE) loss. This increase in the linear head norm effectively reduces changes in learned features."
                        ],
                        "paper": {
                            "corpus_id": 270062468,
                            "title": "Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective",
                            "authors": [
                                {
                                    "authorId": "2303396796",
                                    "name": "Akiyoshi Tomihari"
                                },
                                {
                                    "authorId": "2303397982",
                                    "name": "Issei Sato"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 4
                        },
                        "score": 0.99560546875
                    },
                    {
                        "id": "(Yao et al., 2025)",
                        "snippets": [
                            "Further analysis of initial training dynamics suggests that specific model components, particularly the embedding space and self-attention mechanisms, play pivotal roles in shaping these learning biases. We provide a theoretical framework from the perspective of model training dynamics to explain these phenomena."
                        ],
                        "paper": {
                            "corpus_id": 276235362,
                            "title": "An Analysis for Reasoning Bias of Language Models with Small Initialization",
                            "authors": [
                                {
                                    "authorId": "2277847328",
                                    "name": "Junjie Yao"
                                },
                                {
                                    "authorId": "1953073920",
                                    "name": "Zhongwang Zhang"
                                },
                                {
                                    "authorId": "2136355451",
                                    "name": "Z. Xu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.99072265625
                    },
                    {
                        "id": "(Xiong et al., 2024)",
                        "snippets": [
                            "Recently, Large Language Models (LLMs) have been widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed Scaling Laws, have discovered that the final test loss of LLMs scales as power-laws with model size, computational budget, and dataset size. However, the temporal change of the test loss of an LLM throughout its pre-training process remains unexplored, though it is valuable in many aspects, such as selecting better hyperparameters \\textit{directly} on the target LLM. In this paper, we propose the novel concept of Temporal Scaling Law, studying how the test loss of an LLM evolves as the training steps scale up. In contrast to modeling the test loss as a whole in a coarse-grained manner, we break it down and dive into the fine-grained test loss of each token position, and further develop a dynamic hyperbolic-law. Afterwards, we derive the much more precise temporal scaling law by studying the temporal patterns of the parameters in the dynamic hyperbolic-law. Results on both in-distribution (ID) and out-of-distribution (OOD) validation datasets demonstrate that our temporal scaling law accurately predicts the test loss of LLMs across training steps."
                        ],
                        "paper": {
                            "corpus_id": 269449894,
                            "title": "Temporal Scaling Law for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2249971338",
                                    "name": "Yizhe Xiong"
                                },
                                {
                                    "authorId": "2298904872",
                                    "name": "Xiansheng Chen"
                                },
                                {
                                    "authorId": "2299108794",
                                    "name": "Xin Ye"
                                },
                                {
                                    "authorId": "2298921971",
                                    "name": "Hui Chen"
                                },
                                {
                                    "authorId": "1818920",
                                    "name": "Zijia Lin"
                                },
                                {
                                    "authorId": "2298903058",
                                    "name": "Haoran Lian"
                                },
                                {
                                    "authorId": "2293626051",
                                    "name": "Jianwei Niu"
                                },
                                {
                                    "authorId": "2242661989",
                                    "name": "Guiguang Ding"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.99365234375
                    },
                    {
                        "id": "(Yang et al._1, 2024)",
                        "snippets": [
                            "In this work, we first investigate the dynamics of several open-sourced large language models (i.e., Baichuan-7B (Yang et al., 2023), DeepSeek-7B (DeepSeek-AI et al., 2024), Amber-7B (Liu et al., 2023), OpenLLaMA-7B (Geng & Liu, 2023;Touvron et al., 2023;Computer, 2023), Yi-34B (AI et al., 2024) and DeepSeek-67B), and analyze their performance results across diverse tasks using the corresponding intermediate checkpoints based on the number of pre-trained tokens. Then, based on the theoretical framework of the Scaling Law (Kaplan et al., 2020), we analyze the performance patterns of different models across various downstream tasks, and provide findings to facilitate further research on the training dynamics of the LLMs, where the findings are shown as follows: \n\n\u2022 Findings on task dynamic prediction: Within the training process of LLMs, we observe that the dynamics of existing downstream tasks within a domain can predict the dynamics of unseen tasks. This suggests that a model's performance on known tasks can inform us about how it might perform on similar, yet unseen tasks within the same domain. (Section 4.1.1) \n\n\u2022 Findings on cross-domain promotion: Similar to the human cognitive process, the enhancement of different abilities across various domains progresses from basic to advanced levels following curriculum learning. The curriculum between crossdomain tasks can guide the training direction of models and the insights gained from one domain can potentially promote the learning process of other domains. (Section 4.1.2) \n\n\u2022 Findings on the effect of training strategies, model architecture, etc. : Based on the results of several 7b-scale models (Baichuan2-7b, DeepSeek-7b, OpenLLaMA-7b, and Amber-7b), we comprehensively analyze the effect of training strategies, dataset quality, and model architecture in training LLMs. For example, we observe that the training dataset, learning rate adjustments, batch size, and regularization techniques play a significant role in the learning efficiency in the early training stage (Section 4.2.1)."
                        ],
                        "paper": {
                            "corpus_id": 268820276,
                            "title": "The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis",
                            "authors": [
                                {
                                    "authorId": "2345264293",
                                    "name": "Chenghao Yang"
                                },
                                {
                                    "authorId": "2294388803",
                                    "name": "Junzhuo Li"
                                },
                                {
                                    "authorId": "2290184043",
                                    "name": "Xinyao Niu"
                                },
                                {
                                    "authorId": "2279346001",
                                    "name": "Xinrun Du"
                                },
                                {
                                    "authorId": "2294382707",
                                    "name": "Songyang Gao"
                                },
                                {
                                    "authorId": "2281020035",
                                    "name": "Haoran Zhang"
                                },
                                {
                                    "authorId": "2294810112",
                                    "name": "Zhaoliang Chen"
                                },
                                {
                                    "authorId": "2239104064",
                                    "name": "Xingwei Qu"
                                },
                                {
                                    "authorId": "2032236274",
                                    "name": "Ruibin Yuan"
                                },
                                {
                                    "authorId": "2129449392",
                                    "name": "Yizhi Li"
                                },
                                {
                                    "authorId": "2294523552",
                                    "name": "Jiaheng Liu"
                                },
                                {
                                    "authorId": "2283188391",
                                    "name": "Stephen W. Huang"
                                },
                                {
                                    "authorId": "2282979169",
                                    "name": "Shawn Yue"
                                },
                                {
                                    "authorId": "2249847177",
                                    "name": "Wenhu Chen"
                                },
                                {
                                    "authorId": "2265967208",
                                    "name": "Jie Fu"
                                },
                                {
                                    "authorId": "2143853895",
                                    "name": "Ge Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.99169921875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Metrics and Methods for Analyzing Training Dynamics",
                "tldr": "Researchers employ a diverse array of metrics and methodologies to analyze language model training dynamics, from specialized model suites to mathematical frameworks that characterize learning patterns. These approaches reveal critical insights about how models learn, including phase transitions in capability acquisition, token-level learning patterns, and the stabilization of model components. (12 sources)",
                "text": "\n* **Model Suite Benchmarks**: The Pythia suite provides 16 language models of varying sizes (70M to 12B parameters) with 154 checkpoints each, allowing researchers to analyze how models develop across training while controlling for data ordering and other variables <Paper corpusId=\"257921893\" paperTitle=\"(Biderman et al., 2023)\" isShortName></Paper>.\n\n* **Token-Level Analysis**: Fine-grained examination of how individual tokens' losses evolve during training reveals that significant loss reduction occurs only for a select group of tokens, with many being \"easy tokens\" (already learned) or \"hard tokens\" that resist convergence <Paper corpusId=\"269042762\" paperTitle=\"(Lin et al., 2024)\" isShortName></Paper>.\n\n* **Temporal Scaling Laws**: This approach models how test loss evolves over training steps by breaking down the overall loss into fine-grained token position loss and developing a dynamic hyperbolic law that accurately predicts test loss across training steps for both in-distribution and out-of-distribution validation datasets <Paper corpusId=\"269449894\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper>.\n\n* **Singular Vector Canonical Correlation Analysis (SVCCA)**: This tool allows different model layers and networks to be compared, demonstrating that early network layers typically converge earlier in training than later layers <Paper corpusId=\"264935245\" paperTitle=\"(Quirke et al., 2023)\" isShortName></Paper>.\n\n* **Learning Dynamics Framework**: The \"Learning Law\" theoretical framework optimizes language model learning by maximizing the data compression ratio in an \"LM-training-as-lossless-compression\" perspective, revealing properties of the dynamics in the optimal learning process <Paper corpusId=\"268041376\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper>.\n\n* **Stability Metrics**: Researchers quantify stability during training using standard deviation of metrics like validation loss, and identify training phases by constructing training maps from model parameter statistics <Paper corpusId=\"276937763\" paperTitle=\"(Wal et al., 2025)\" isShortName></Paper>.\n\n* **Influence Analysis**: Step-wise decomposition analysis of how influence accumulates among different potential responses helps understand the learning dynamics during different types of fine-tuning <Paper corpusId=\"271213641\" paperTitle=\"(Ren et al., 2024)\" isShortName></Paper>.\n\n* **Activation Convergence Analysis**: By examining how quickly Attention and MLP activations converge to their final state across different model sizes, researchers have found that larger models' layers stabilize much earlier in training (within the first 20%) compared to smaller models <Paper corpusId=\"273351173\" paperTitle=\"(Martinez et al., 2024)\" isShortName></Paper>.\n\n* **Feature Evolution Tracking**: Techniques like SAE-Track use sparse autoencoders to obtain a continual series of features throughout training, enabling analysis of semantic evolution, feature formation processes, and directional drift of feature vectors <Paper corpusId=\"274981612\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>.\n\n* **Multi-Metric Correlation**: Analyzing how validation loss correlates with performance on downstream benchmarks (e.g., Hellaswag, ARC) across thousands of training steps provides insights into improvements in text fluency, coherence, and factual accuracy <Paper corpusId=\"274823085\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>.\n\n* **Phase Transition Detection**: Multiple metrics track when capabilities emerge rapidly during training, identifying inflection points that characterize phase transitions where discrete, additive capabilities emerge independently <Paper corpusId=\"264935245\" paperTitle=\"(Quirke et al., 2023)\" isShortName></Paper>.\n\n* **Circuit Analysis**: Techniques to identify and ablate neural circuits causally linked to specific capabilities help understand how these components develop during training and contribute to emergent abilities <Paper corpusId=\"264935245\" paperTitle=\"(Quirke et al., 2023)\" isShortName></Paper>.\n\n* **Developmental Timeline Mapping**: Research has identified consistent learning phases across model sizes, with linguistic information beginning to be encoded between 10\u00b3-10\u2074 steps (approximately 2-20 billion tokens), followed by a \"critical\" learning phase between 10\u2074-10\u2075 steps where most improvements occur <Paper corpusId=\"276937763\" paperTitle=\"(Wal et al., 2025)\" isShortName></Paper>.\n\n* **Cross-Model Consistency Analysis**: Comparing developmental patterns across different model architectures, initializations, and data shuffling reveals that many learning patterns are consistent regardless of these variations <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"222140842\" paperTitle=\"(Chiang et al., 2020)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Biderman et al., 2023)",
                        "snippets": [
                            "How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics."
                        ],
                        "paper": {
                            "corpus_id": 257921893,
                            "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
                            "authors": [
                                {
                                    "authorId": "103476203",
                                    "name": "Stella Biderman"
                                },
                                {
                                    "authorId": "2184031883",
                                    "name": "Hailey Schoelkopf"
                                },
                                {
                                    "authorId": "1404060481",
                                    "name": "Quentin G. Anthony"
                                },
                                {
                                    "authorId": "2070768742",
                                    "name": "Herbie Bradley"
                                },
                                {
                                    "authorId": "2212970046",
                                    "name": "Kyle O'Brien"
                                },
                                {
                                    "authorId": "2162462983",
                                    "name": "Eric Hallahan"
                                },
                                {
                                    "authorId": "2168771748",
                                    "name": "Mohammad Aflah Khan"
                                },
                                {
                                    "authorId": "2162467233",
                                    "name": "Shivanshu Purohit"
                                },
                                {
                                    "authorId": "2162462141",
                                    "name": "USVSN Sai Prashanth"
                                },
                                {
                                    "authorId": "34885007",
                                    "name": "Edward Raff"
                                },
                                {
                                    "authorId": "2213349418",
                                    "name": "Aviya Skowron"
                                },
                                {
                                    "authorId": "35566806",
                                    "name": "Lintang Sutawika"
                                },
                                {
                                    "authorId": "1986356851",
                                    "name": "Oskar van der Wal"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1306
                        },
                        "score": 0.99267578125
                    },
                    {
                        "id": "(Lin et al., 2024)",
                        "snippets": [
                            "To explore how language models learn at the token level, we initially examined training dynamics, particularly how the token-level loss evolves during usual pretraining. In \u00a72.1, we evaluated the model's token perplexity at different checkpoints and categorized tokens into different types. Our findings reveal that significant loss reduction is limited to a select group of tokens. Many tokens are \"easy tokens\" that are already learned, and some are \"hard tokens\" that exhibit variable losses and resist convergence. These tokens can lead to numerous ineffective gradient updates",
                            ".Investigating the training dynamics of language models is essential for understanding their behavior throughout the training process. This research includes studying internal representations [Saphra and Lopez, 2018], the acquisition of linguistic knowledge [Choshen et al., 2021, Liu et al., 2021], and the phenomenon of grokking [Power et al., 2022]. The analysis by Xia et al. [2022] is the most related to ours, which examines token-level training trajectories in models of varying sizes. Our findings, however, diverge from those of Xia et al. [2022], who posit that tokens with little change in perplexity are \"already learned\". We identify a spectrum of token patterns, including \"easy tokens\" and \"hard tokens\" that resist convergence."
                        ],
                        "paper": {
                            "corpus_id": 269042762,
                            "title": "Rho-1: Not All Tokens Are What You Need",
                            "authors": [
                                {
                                    "authorId": "31113759",
                                    "name": "Zheng-Wen Lin"
                                },
                                {
                                    "authorId": "1797090",
                                    "name": "Zhibin Gou"
                                },
                                {
                                    "authorId": "2254121650",
                                    "name": "Yeyun Gong"
                                },
                                {
                                    "authorId": "49544272",
                                    "name": "Xiao Liu"
                                },
                                {
                                    "authorId": "2237948786",
                                    "name": "Yelong Shen"
                                },
                                {
                                    "authorId": "2266367743",
                                    "name": "Ruochen Xu"
                                },
                                {
                                    "authorId": "2269773814",
                                    "name": "Chen Lin"
                                },
                                {
                                    "authorId": "2284727148",
                                    "name": "Yujiu Yang"
                                },
                                {
                                    "authorId": "2143968416",
                                    "name": "Jian Jiao"
                                },
                                {
                                    "authorId": "2269471632",
                                    "name": "Nan Duan"
                                },
                                {
                                    "authorId": "2249538838",
                                    "name": "Weizhu Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 75
                        },
                        "score": 0.99169921875
                    },
                    {
                        "id": "(Xiong et al., 2024)",
                        "snippets": [
                            "Recently, Large Language Models (LLMs) have been widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed Scaling Laws, have discovered that the final test loss of LLMs scales as power-laws with model size, computational budget, and dataset size. However, the temporal change of the test loss of an LLM throughout its pre-training process remains unexplored, though it is valuable in many aspects, such as selecting better hyperparameters \\textit{directly} on the target LLM. In this paper, we propose the novel concept of Temporal Scaling Law, studying how the test loss of an LLM evolves as the training steps scale up. In contrast to modeling the test loss as a whole in a coarse-grained manner, we break it down and dive into the fine-grained test loss of each token position, and further develop a dynamic hyperbolic-law. Afterwards, we derive the much more precise temporal scaling law by studying the temporal patterns of the parameters in the dynamic hyperbolic-law. Results on both in-distribution (ID) and out-of-distribution (OOD) validation datasets demonstrate that our temporal scaling law accurately predicts the test loss of LLMs across training steps."
                        ],
                        "paper": {
                            "corpus_id": 269449894,
                            "title": "Temporal Scaling Law for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2249971338",
                                    "name": "Yizhe Xiong"
                                },
                                {
                                    "authorId": "2298904872",
                                    "name": "Xiansheng Chen"
                                },
                                {
                                    "authorId": "2299108794",
                                    "name": "Xin Ye"
                                },
                                {
                                    "authorId": "2298921971",
                                    "name": "Hui Chen"
                                },
                                {
                                    "authorId": "1818920",
                                    "name": "Zijia Lin"
                                },
                                {
                                    "authorId": "2298903058",
                                    "name": "Haoran Lian"
                                },
                                {
                                    "authorId": "2293626051",
                                    "name": "Jianwei Niu"
                                },
                                {
                                    "authorId": "2242661989",
                                    "name": "Guiguang Ding"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.99365234375
                    },
                    {
                        "id": "(Quirke et al., 2023)",
                        "snippets": [
                            "Training dynamics Several works (Olsson et al., 2022;Nanda et al., 2023) discovered circuits in language models that seem causally relevant to emergent capabilities and can be ablated to reduce the capabilities. Michaud et al. (2023) postulated that many capabilities in language models emerge rapidly in phase transitions and proposed a model in which any capabilities which appear to instead form gradually can be decomposed into discrete, additive capabilities which emerge in independent phase transitions. Finally, several works (Varma et al., 2023;Nanda et al., 2023) observed the phenomenon of grokking, where a model that has already achieved near-perfect performance on the training data transitions to a mechanism that generalizes. \n\nWhile only a minority of circuits-style analysis has focused on model development, a wealth of results in the field of developmental interpretability have characterized the training dynamics of language models using other units of analysis. Behavioral analysis has shown that many model capabilities, such as predicting parts of speech and individual tokens, are learned during independent phase transitions characterized by varying slopes and points of maximum curvature (Chiang et al., 2020)Wei et al., 2022). Raghu et al. (2017) proposed singular vector canonical correlation analysis, a tool which allows different model layers and networks to be compared, and showed that early network layers converge earlier in training than late layers. Saphra and Lopez (2020b,a) found that LSTM models learn hierarchical sequential features in natural language bottom up -shorter sequences are learned first and form the basis for the representation of longer sequences that contain the shorter ones. Finally, Saxe et al. (2019) found that the features are learned in an order determined by the magnitude of the corresponding singular value in the model."
                        ],
                        "paper": {
                            "corpus_id": 264935245,
                            "title": "Training Dynamics of Contextual N-Grams in Language Models",
                            "authors": [
                                {
                                    "authorId": "2243240285",
                                    "name": "Lucia Quirke"
                                },
                                {
                                    "authorId": "2047549746",
                                    "name": "Lovis Heindrich"
                                },
                                {
                                    "authorId": "2056771333",
                                    "name": "Wes Gurnee"
                                },
                                {
                                    "authorId": "2051128902",
                                    "name": "Neel Nanda"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Gu et al., 2024)",
                        "snippets": [
                            "This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an\"LM-training-as-lossless-compression\"view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task."
                        ],
                        "paper": {
                            "corpus_id": 268041376,
                            "title": "Towards Optimal Learning of Language Models",
                            "authors": [
                                {
                                    "authorId": "2116405624",
                                    "name": "Yuxian Gu"
                                },
                                {
                                    "authorId": "2286153844",
                                    "name": "Li Dong"
                                },
                                {
                                    "authorId": "34128716",
                                    "name": "Y. Hao"
                                },
                                {
                                    "authorId": "2287927238",
                                    "name": "Qingxiu Dong"
                                },
                                {
                                    "authorId": "2285704485",
                                    "name": "Minlie Huang"
                                },
                                {
                                    "authorId": "2253471545",
                                    "name": "Furu Wei"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.986328125
                    },
                    {
                        "id": "(Wal et al., 2025)",
                        "snippets": [
                            "In this work, we define stability as the change in a metric of interest (e.g., validation loss) caused by changes in randomness factors and quantify it using the standard deviation of that metric (see (Du et al., 2023) for other approaches to quantify stability)",
                            "using training maps constructed from statistics of the model parameters, we identify the characteristics of stable training runs and the early signals of instability.\n\nLanguage modelling is largely stable. Generally, we observe LM pre-training dynamics to follow consistent trajectories. Across seeds and model sizes, downstream performance and representational efficiency consistently increase during pre-training, and training maps are linear (except for the outlier seeds).\n\nLinguistic information is encoded in the initial learning phase (10^3-10^4 steps). Across all experiments, metrics start moving away from the initial random baseline around step 10^3 (2B tokens circa) and reach their convergence level around step 10^4 (20B tokens circa). In this initial phase, representational shift peaks and linguistic information begins to be encoded into the models' latent representations.\n\nMost improvements happen in the \"critical\" learning phase (10^4-10^5 steps). In the range of 10^3 to 10^4 steps, most learning occurs, as measured by all of our metrics."
                        ],
                        "paper": {
                            "corpus_id": 276937763,
                            "title": "PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs",
                            "authors": [
                                {
                                    "authorId": "1986356851",
                                    "name": "Oskar van der Wal"
                                },
                                {
                                    "authorId": "2325954375",
                                    "name": "Pietro Lesci"
                                },
                                {
                                    "authorId": "1416353805",
                                    "name": "Max M\u00fcller-Eberstein"
                                },
                                {
                                    "authorId": "2308101135",
                                    "name": "Naomi Saphra"
                                },
                                {
                                    "authorId": "2184031883",
                                    "name": "Hailey Schoelkopf"
                                },
                                {
                                    "authorId": "2254288138",
                                    "name": "Willem Zuidema"
                                },
                                {
                                    "authorId": "2273535086",
                                    "name": "Stella Biderman"
                                }
                            ],
                            "year": 2025,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 2
                        },
                        "score": 0.99609375
                    },
                    {
                        "id": "(Ren et al., 2024)",
                        "snippets": [
                            "Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, gives us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during different types of finetuning, by analyzing the step-wise decomposition of how influence accumulates among different potential responses."
                        ],
                        "paper": {
                            "corpus_id": 271213641,
                            "title": "Learning Dynamics of LLM Finetuning",
                            "authors": [
                                {
                                    "authorId": "2115242507",
                                    "name": "Yi Ren"
                                },
                                {
                                    "authorId": "2262445067",
                                    "name": "Danica J. Sutherland"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 21
                        },
                        "score": 0.98681640625
                    },
                    {
                        "id": "(Martinez et al., 2024)",
                        "snippets": [
                            "We use the Pythia model suite to analyse the training dynamics that underlie this phenomenon. Across different model sizes, we investigate the convergence of the Attention and MLP activations to their final state and examine how the effective rank of their parameters influences this process. We find that nearly all layers in larger models stabilise early in training - within the first 20% - whereas layers in smaller models exhibit slower and less stable convergence, especially when their parameters have lower effective rank. By linking the convergence of layers' activations to their parameters' effective rank, our analyses can guide future work to address inefficiencies in the learning dynamics of small models."
                        ],
                        "paper": {
                            "corpus_id": 273351173,
                            "title": "Tending Towards Stability: Convergence Challenges in Small Language Models",
                            "authors": [
                                {
                                    "authorId": "2266941716",
                                    "name": "Richard Diehl Martinez"
                                },
                                {
                                    "authorId": "2325954375",
                                    "name": "Pietro Lesci"
                                },
                                {
                                    "authorId": "33490976",
                                    "name": "P. Buttery"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 4
                        },
                        "score": 0.9931640625
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "Understanding training dynamics and feature evolution is crucial for the mechanistic interpretability of large language models (LLMs). Although sparse autoencoders (SAEs) have been used to identify features within LLMs, a clear picture of how these features evolve during training remains elusive. In this study, we (1) introduce SAE-Track, a novel method for efficiently obtaining a continual series of SAEs, providing the foundation for a mechanistic study that covers (2) the semantic evolution of features, (3) the underlying processes of feature formation, and (4) the directional drift of feature vectors. Our work provides new insights into the dynamics of features in LLMs, enhancing our understanding of training mechanisms and feature evolution."
                        ],
                        "paper": {
                            "corpus_id": 274981612,
                            "title": "Tracking the Feature Dynamics in LLM Training: A Mechanistic Study",
                            "authors": [
                                {
                                    "authorId": "2154895781",
                                    "name": "Yangfan Xu"
                                },
                                {
                                    "authorId": "2226917099",
                                    "name": "Yi Wang"
                                },
                                {
                                    "authorId": "2328190828",
                                    "name": "Hao Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.99462890625
                    },
                    {
                        "id": "(Li et al._1, 2024)",
                        "snippets": [
                            "Training Dynamics: We analyze how validation loss and downstream benchmarks (e.g., Hellaswag, ARC) evolve over 40,000+ training steps and correlate these metrics with improvements in text fluency, coherence, and factual accuracy."
                        ],
                        "paper": {
                            "corpus_id": 274823085,
                            "title": "Training Dynamics of a 1.7B LLaMa Model: A Data-Efficient Approach",
                            "authors": [
                                {
                                    "authorId": "2139317485",
                                    "name": "Miles Q. Li"
                                },
                                {
                                    "authorId": "2333234336",
                                    "name": "Benjamin C. M. Fung"
                                },
                                {
                                    "authorId": "2333311177",
                                    "name": "Shih-Chia Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.9951171875
                    },
                    {
                        "id": "(Chang et al., 2023)",
                        "snippets": [
                            "Previous work has studied the pre-training dynamics of language models (Saphra, 0). Choshen et al. (2022) and (Evanson et al., 2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021). Olsson et al. (2022) find that copy mechanisms (\"induction heads\" for in-context learning) appear at an inflection point during pre-training. These results establish when a variety of abilities emerge in language models. Our work studies more fine-grained learning trajectories by evaluating individual tokens in context",
                            "Previous work has demonstrated that language models exhibit fine-grained learning patterns that are not captured by the corpus-level loss curve (related work in \u00a72). In particular, sudden increases and decreases in example loss ( \u00a75 and (Xia et al., 2022)) may be somewhat surprising given that the pre-training text is i.i.d. for all pre-training steps. By demonstrating that many of these sudden changes are consistent regardless of random initialization and data shuffling ( \u00a75.2), our work indicates that some instances of sudden learning and \"forgetting\" are not due to random chance or the specific examples observed in a given step."
                        ],
                        "paper": {
                            "corpus_id": 261277016,
                            "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
                            "authors": [
                                {
                                    "authorId": "2087001989",
                                    "name": "Tyler A. Chang"
                                },
                                {
                                    "authorId": "144035504",
                                    "name": "Z. Tu"
                                },
                                {
                                    "authorId": "24316216",
                                    "name": "B. Bergen"
                                }
                            ],
                            "year": 2023,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 13
                        },
                        "score": 0.994140625
                    },
                    {
                        "id": "(Chiang et al., 2020)",
                        "snippets": [
                            "While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL."
                        ],
                        "paper": {
                            "corpus_id": 222140842,
                            "title": "Pretrained Language Model Embryology: The Birth of ALBERT",
                            "authors": [
                                {
                                    "authorId": "1992777064",
                                    "name": "Cheng-Han Chiang"
                                },
                                {
                                    "authorId": "2210669195",
                                    "name": "Sung-Feng Huang"
                                },
                                {
                                    "authorId": "1706104",
                                    "name": "Hung-yi Lee"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 42
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Emergent Capabilities and Phenomena",
                "tldr": "Language models exhibit distinctive phase transitions during training where capabilities emerge abruptly rather than gradually, with different abilities appearing at specific thresholds. These emergent phenomena include memorization patterns that favor certain parts of speech, sudden improvements in factual recall abilities, and transitions from memorization to generalization that follow predictable patterns governed by data size and model complexity. (11 sources)",
                "text": "\nLanguage model training is characterized by non-uniform development of capabilities, with certain abilities emerging suddenly at specific points in training. These discontinuous dynamics are often described as \"breakthroughs,\" \"emergence,\" or \"phase transitions\" in the literature <Paper corpusId=\"261822542\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper> <Paper corpusId=\"253117181\" paperTitle=\"(Caballero et al., 2022)\" isShortName></Paper>. Rather than smooth, continuous improvement, many capabilities exhibit abrupt shifts that mark fundamental changes in model behavior.\n\nResearchers have discovered that these phase transitions often follow predictable patterns. Specific model circuits causally linked to emergent capabilities develop at consistent points during training and can be ablated to reduce these capabilities <Paper corpusId=\"264935245\" paperTitle=\"(Quirke et al., 2023)\" isShortName></Paper>. Some researchers have proposed that even capabilities appearing to develop gradually can be decomposed into discrete, additive capabilities emerging through independent phase transitions <Paper corpusId=\"264935245\" paperTitle=\"(Quirke et al., 2023)\" isShortName></Paper>.\n\nBehavioral analysis supports this view, showing that various model capabilities\u2014such as predicting parts of speech and individual tokens\u2014are learned during independent phase transitions characterized by varying slopes and points of maximum curvature <Paper corpusId=\"264935245\" paperTitle=\"(Quirke et al., 2023)\" isShortName></Paper> <Paper corpusId=\"222140842\" paperTitle=\"(Chiang et al., 2020)\" isShortName></Paper>. For example, in a synthetic factual recall task, models exhibit three distinct learning phases with a performance plateau that coincides with the formation of attention-based circuits supporting recall capabilities <Paper corpusId=\"277349236\" paperTitle=\"(Zucchet et al., 2025)\" isShortName></Paper>.\n\nToken-level analysis reveals that during training, significant loss reduction is limited to specific groups of tokens. Many tokens fall into categories of either \"easy tokens\" (already learned) or \"hard tokens\" that exhibit variable losses and resist convergence <Paper corpusId=\"269042762\" paperTitle=\"(Lin et al., 2024)\" isShortName></Paper>. These different token behaviors contribute to the complex, non-linear training dynamics observed in language models.\n\nMemorization patterns also follow consistent developmental trends. Larger language models memorize training data faster across all settings and can memorize a larger portion of data before over-fitting compared to smaller models <Paper corpusId=\"248986465\" paperTitle=\"(Tirumala et al., 2022)\" isShortName></Paper> <Paper corpusId=\"271860164\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>. Interestingly, models memorize different parts of speech at different rates, with nouns and numbers being memorized first\u2014likely because they serve as unique identifiers for individual training examples <Paper corpusId=\"248986465\" paperTitle=\"(Tirumala et al., 2022)\" isShortName></Paper>.\n\nThe transition from memorization to generalization represents another critical phase change in training dynamics. Research has formalized the concept of a \"critical data size\" that marks a fundamental shift from quick memorization to slow generalization <Paper corpusId=\"267061159\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>. This threshold creates distinct regimes in training: data insufficiency, data sufficiency, and data surplus, each with characteristic learning patterns. As model size increases, this critical point also becomes larger, indicating that larger models require proportionally more data to reach their generalization potential <Paper corpusId=\"267061159\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>.\n\nThe order in which models learn to extrapolate rules appears to be governed by their relative simplicity. Analysis of Transformer models learning formal languages shows that models first learn simpler rules before progressing to more complex ones, with simplicity quantified through Kolmogorov complexity <Paper corpusId=\"272826680\" paperTitle=\"(M'esz'aros et al., 2024)\" isShortName></Paper>. This \"simplicity bias\" means that models learn to generate sequences satisfying simpler constraints before mastering sequences that satisfy multiple, more complex constraints simultaneously.\n\nTraining dynamics also reveal interesting patterns in how attention mechanisms develop. Researchers analyzing shallow transformers have identified a \"scan and snap\" dynamic where self-attention initially scans for relevant tokens before snapping to a stable configuration <Paper corpusId=\"271860164\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258947127\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper>. This process involves a gradual shift from uniform attention to attending more to distinctive key tokens for specific next-token predictions while paying less attention to common tokens appearing across different contexts.\n\nImportantly, not all emergent capabilities represent improvements. The acquisition of factual knowledge coincides with the emergence of hallucinations, and integrating new knowledge through fine-tuning often corrupts existing parametric memories <Paper corpusId=\"277349236\" paperTitle=\"(Zucchet et al., 2025)\" isShortName></Paper>. This suggests complex interactions between different capabilities during training that can lead to trade-offs rather than uniform improvements.",
                "citations": [
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2022), or phase transitions (Olsson et al., 2022). The interpretability literature rarely illuminates how these capabilities emerge, in part because most analyses only examine the final trained model. Instead, we consider developmental analysis as a complementary explanatory lens."
                        ],
                        "paper": {
                            "corpus_id": 261822542,
                            "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
                            "authors": [
                                {
                                    "authorId": "13336152",
                                    "name": "Angelica Chen"
                                },
                                {
                                    "authorId": "2240524527",
                                    "name": "Ravid Schwartz-Ziv"
                                },
                                {
                                    "authorId": "1979489",
                                    "name": "Kyunghyun Cho"
                                },
                                {
                                    "authorId": "2240527814",
                                    "name": "Matthew L. Leavitt"
                                },
                                {
                                    "authorId": "2362960",
                                    "name": "Naomi Saphra"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 74
                        },
                        "score": 0.99169921875
                    },
                    {
                        "id": "(Caballero et al., 2022)",
                        "snippets": [
                            "We present a smoothly broken power law functional form (that we refer to as a Broken Neural Scaling Law (BNSL)) that accurately models&extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as amount of compute used for training (or inference), number of model parameters, training dataset size, model input size, number of training steps, or upstream performance varies) for various architectures&for each of various tasks within a large&diverse set of upstream&downstream tasks, in zero-shot, prompted,&finetuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, AI capabilities, robotics, out-of-distribution (OOD) generalization, continual learning, transfer learning, uncertainty estimation / calibration, OOD detection, adversarial robustness, distillation, sparsity, retrieval, quantization, pruning, fairness, molecules, computer programming/coding, math word problems,\"emergent phase transitions\", arithmetic, supervised learning, unsupervised/self-supervised learning,&reinforcement learning (single agent&multi-agent). When compared to other functional forms for neural scaling, this functional form yields extrapolations of scaling behavior that are considerably more accurate on this set. Moreover, this functional form accurately models&extrapolates scaling behavior that other functional forms are incapable of expressing such as the nonmonotonic transitions present in the scaling behavior of phenomena such as double descent&the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Lastly, we use this functional form to glean insights about the limit of the predictability of scaling behavior. Code is available at https://github.com/ethancaballero/broken_neural_scaling_laws"
                        ],
                        "paper": {
                            "corpus_id": 253117181,
                            "title": "Broken Neural Scaling Laws",
                            "authors": [
                                {
                                    "authorId": "24130593",
                                    "name": "Ethan Caballero"
                                },
                                {
                                    "authorId": "2066789106",
                                    "name": "Kshitij Gupta"
                                },
                                {
                                    "authorId": "2109771",
                                    "name": "I. Rish"
                                },
                                {
                                    "authorId": "145055042",
                                    "name": "David Krueger"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 76
                        },
                        "score": 0
                    },
                    {
                        "id": "(Quirke et al., 2023)",
                        "snippets": [
                            "Training dynamics Several works (Olsson et al., 2022;Nanda et al., 2023) discovered circuits in language models that seem causally relevant to emergent capabilities and can be ablated to reduce the capabilities. Michaud et al. (2023) postulated that many capabilities in language models emerge rapidly in phase transitions and proposed a model in which any capabilities which appear to instead form gradually can be decomposed into discrete, additive capabilities which emerge in independent phase transitions. Finally, several works (Varma et al., 2023;Nanda et al., 2023) observed the phenomenon of grokking, where a model that has already achieved near-perfect performance on the training data transitions to a mechanism that generalizes. \n\nWhile only a minority of circuits-style analysis has focused on model development, a wealth of results in the field of developmental interpretability have characterized the training dynamics of language models using other units of analysis. Behavioral analysis has shown that many model capabilities, such as predicting parts of speech and individual tokens, are learned during independent phase transitions characterized by varying slopes and points of maximum curvature (Chiang et al., 2020)Wei et al., 2022). Raghu et al. (2017) proposed singular vector canonical correlation analysis, a tool which allows different model layers and networks to be compared, and showed that early network layers converge earlier in training than late layers. Saphra and Lopez (2020b,a) found that LSTM models learn hierarchical sequential features in natural language bottom up -shorter sequences are learned first and form the basis for the representation of longer sequences that contain the shorter ones. Finally, Saxe et al. (2019) found that the features are learned in an order determined by the magnitude of the corresponding singular value in the model."
                        ],
                        "paper": {
                            "corpus_id": 264935245,
                            "title": "Training Dynamics of Contextual N-Grams in Language Models",
                            "authors": [
                                {
                                    "authorId": "2243240285",
                                    "name": "Lucia Quirke"
                                },
                                {
                                    "authorId": "2047549746",
                                    "name": "Lovis Heindrich"
                                },
                                {
                                    "authorId": "2056771333",
                                    "name": "Wes Gurnee"
                                },
                                {
                                    "authorId": "2051128902",
                                    "name": "Neel Nanda"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Chiang et al., 2020)",
                        "snippets": [
                            "While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL."
                        ],
                        "paper": {
                            "corpus_id": 222140842,
                            "title": "Pretrained Language Model Embryology: The Birth of ALBERT",
                            "authors": [
                                {
                                    "authorId": "1992777064",
                                    "name": "Cheng-Han Chiang"
                                },
                                {
                                    "authorId": "2210669195",
                                    "name": "Sung-Feng Huang"
                                },
                                {
                                    "authorId": "1706104",
                                    "name": "Hung-yi Lee"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 42
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zucchet et al., 2025)",
                        "snippets": [
                            "This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distributions lead to shorter plateaus. Finally, hallucinations emerge simultaneously with knowledge, and integrating new knowledge into the model through fine-tuning is challenging, as it quickly corrupts its existing parametric memories."
                        ],
                        "paper": {
                            "corpus_id": 277349236,
                            "title": "How do language models learn facts? Dynamics, curricula and hallucinations",
                            "authors": [
                                {
                                    "authorId": "1729494470",
                                    "name": "Nicolas Zucchet"
                                },
                                {
                                    "authorId": "2320771936",
                                    "name": "J\u00f6rg Bornschein"
                                },
                                {
                                    "authorId": "2316336431",
                                    "name": "Stephanie Chan"
                                },
                                {
                                    "authorId": "2270676283",
                                    "name": "Andrew K. Lampinen"
                                },
                                {
                                    "authorId": "1996134",
                                    "name": "Razvan Pascanu"
                                },
                                {
                                    "authorId": "2289159449",
                                    "name": "Soham De"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Lin et al., 2024)",
                        "snippets": [
                            "To explore how language models learn at the token level, we initially examined training dynamics, particularly how the token-level loss evolves during usual pretraining. In \u00a72.1, we evaluated the model's token perplexity at different checkpoints and categorized tokens into different types. Our findings reveal that significant loss reduction is limited to a select group of tokens. Many tokens are \"easy tokens\" that are already learned, and some are \"hard tokens\" that exhibit variable losses and resist convergence. These tokens can lead to numerous ineffective gradient updates",
                            ".Investigating the training dynamics of language models is essential for understanding their behavior throughout the training process. This research includes studying internal representations [Saphra and Lopez, 2018], the acquisition of linguistic knowledge [Choshen et al., 2021, Liu et al., 2021], and the phenomenon of grokking [Power et al., 2022]. The analysis by Xia et al. [2022] is the most related to ours, which examines token-level training trajectories in models of varying sizes. Our findings, however, diverge from those of Xia et al. [2022], who posit that tokens with little change in perplexity are \"already learned\". We identify a spectrum of token patterns, including \"easy tokens\" and \"hard tokens\" that resist convergence."
                        ],
                        "paper": {
                            "corpus_id": 269042762,
                            "title": "Rho-1: Not All Tokens Are What You Need",
                            "authors": [
                                {
                                    "authorId": "31113759",
                                    "name": "Zheng-Wen Lin"
                                },
                                {
                                    "authorId": "1797090",
                                    "name": "Zhibin Gou"
                                },
                                {
                                    "authorId": "2254121650",
                                    "name": "Yeyun Gong"
                                },
                                {
                                    "authorId": "49544272",
                                    "name": "Xiao Liu"
                                },
                                {
                                    "authorId": "2237948786",
                                    "name": "Yelong Shen"
                                },
                                {
                                    "authorId": "2266367743",
                                    "name": "Ruochen Xu"
                                },
                                {
                                    "authorId": "2269773814",
                                    "name": "Chen Lin"
                                },
                                {
                                    "authorId": "2284727148",
                                    "name": "Yujiu Yang"
                                },
                                {
                                    "authorId": "2143968416",
                                    "name": "Jian Jiao"
                                },
                                {
                                    "authorId": "2269471632",
                                    "name": "Nan Duan"
                                },
                                {
                                    "authorId": "2249538838",
                                    "name": "Weizhu Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 75
                        },
                        "score": 0.99169921875
                    },
                    {
                        "id": "(Tirumala et al., 2022)",
                        "snippets": [
                            "We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process. We measure the effects of dataset size, learning rate, and model size on memorization, finding that larger language models memorize training data faster across all settings. Surprisingly, we show that larger models can memorize a larger portion of the data before over-fitting and tend to forget less throughout the training process. We also analyze the memorization dynamics of different parts of speech and find that models memorize nouns and numbers first; we hypothesize and provide empirical evidence that nouns and numbers act as a unique identifier for memorizing individual training examples."
                        ],
                        "paper": {
                            "corpus_id": 248986465,
                            "title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2551387",
                                    "name": "Kushal Tirumala"
                                },
                                {
                                    "authorId": "153608000",
                                    "name": "Aram H. Markosyan"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "2201435",
                                    "name": "Armen Aghajanyan"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 197
                        },
                        "score": 0.9990234375
                    },
                    {
                        "id": "(Sun et al., 2024)",
                        "snippets": [
                            "Recent studies identify phase transition of model training (Olsson et al., 2022;Wei et al., 2022), where new capabilities or behaviors suddenly emerge when certain thresholds of model complexity are reached. The aspects of complexity often include model size, amount of training by FLOPs or tokens, and model architecture. Several prior works studied the training dynamics of language models by analyzing the internals of train-fromscratch models (Tirumala et al., 2022)Chen et al., 2023;(Tian et al., 2023)Chen et al., 2024;(Chang et al., 2023). The results of these works suggest that the behaviors that are often overlooked after training could be valuable signals for model analysis. In addition to train-from-scratch models, Ren and Sutherland (2024) studied the fine-tuning dynamics of language models."
                        ],
                        "paper": {
                            "corpus_id": 271860164,
                            "title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2087314342",
                                    "name": "Kaiser Sun"
                                },
                                {
                                    "authorId": "1478928280",
                                    "name": "Mark Dredze"
                                }
                            ],
                            "year": 2024,
                            "venue": "Workshop on Representation Learning for NLP",
                            "n_citations": 2
                        },
                        "score": 0.99609375
                    },
                    {
                        "id": "(Zhu et al., 2024)",
                        "snippets": [
                            "We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data."
                        ],
                        "paper": {
                            "corpus_id": 267061159,
                            "title": "Critical Data Size of Language Models from a Grokking Perspective",
                            "authors": [
                                {
                                    "authorId": "2145238612",
                                    "name": "Xuekai Zhu"
                                },
                                {
                                    "authorId": "2280103402",
                                    "name": "Yao Fu"
                                },
                                {
                                    "authorId": "2218723159",
                                    "name": "Bowen Zhou"
                                },
                                {
                                    "authorId": "2280367391",
                                    "name": "Zhouhan Lin"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 18
                        },
                        "score": 0.99560546875
                    },
                    {
                        "id": "(M'esz'aros et al., 2024)",
                        "snippets": [
                            "We analize the dynamics of learning rule extrapolation. We report results on the Transformer (training dynamics of Mamba and the LSTM are in Appx. A), trained on the a n b n language-where high rule extrapolation ability is achieved. Fig. 3 shows that first, the model learns the sequences obeying (R2), then it learns the language (R1) \u2229 (R2) as its subset. We argue that the order in which rules are learnt is governed by the relative simplicity of the rules, quantified by Kolmogorov complexity. Given a formal language with rules (R1) and (R2), let p 1 , p 2 and p 1,2 be distributions defined by LMs that generate sequences that satisfy (R1), (R2) and (R1) \u2229 (R2), respectively. If, e.g., K(p 2 ) \u226a K(p 1,2 ), our normative algorithm will first learn (R2), and then learn the (R1) \u2229 (R2) as its subset. In the a n b n language, (R2) (a's before b's), is, on average, simpler to generate than (R1) (#a=#b) and (R1) \u2229 (R2). Therefore, we expect our normative algorithm to first learn (R2), and then learn (R1) \u2229 (R2) as its subset. Remarkably, our Transformer employs the same strategy ( Fig. 3), verifying the presence of simplicity bias. This result is matches past observations that Transformers are biased towards low Kolmogorov complexity [Goldblum et al., 2023]."
                        ],
                        "paper": {
                            "corpus_id": 272826680,
                            "title": "Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts",
                            "authors": [
                                {
                                    "authorId": "2214094238",
                                    "name": "Anna M'esz'aros"
                                },
                                {
                                    "authorId": "2299943014",
                                    "name": "Szilvia Ujv'ary"
                                },
                                {
                                    "authorId": "40634590",
                                    "name": "Wieland Brendel"
                                },
                                {
                                    "authorId": "1382657853",
                                    "name": "Patrik Reizinger"
                                },
                                {
                                    "authorId": "2322443584",
                                    "name": "Ferenc Husz'ar"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.99365234375
                    },
                    {
                        "id": "(Tian et al., 2023)",
                        "snippets": [
                            "Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \\emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \\emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends more to distinct key tokens for a specific next token to be predicted, and pays less attention to common key tokens that occur across different next tokens. Among distinct tokens, it progressively drops attention weights, following the order of low to high co-occurrence between the key and the query token in the training set. Interestingly, this procedure does not lead to winner-takes-all, but decelerates due to a \\emph{phase transition} that is controllable by the learning rates of the two layers, leaving (almost) fixed token combination. We verify this \\textbf{\\emph{scan and snap}} dynamics on synthetic and real-world data (WikiText)."
                        ],
                        "paper": {
                            "corpus_id": 258947127,
                            "title": "Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer",
                            "authors": [
                                {
                                    "authorId": "1932187449",
                                    "name": "Yuandong Tian"
                                },
                                {
                                    "authorId": "2167496459",
                                    "name": "Yiping Wang"
                                },
                                {
                                    "authorId": "4319427",
                                    "name": "Beidi Chen"
                                },
                                {
                                    "authorId": "145697585",
                                    "name": "S. Du"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 79
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Applications and Implications",
                "tldr": "Understanding training dynamics enables practical applications that optimize language model development, including efficient training termination points, improved learning rate schedules, and more effective fine-tuning strategies. These insights have broad implications for reducing computational costs, enhancing model performance across domains, and developing theoretical frameworks that predict and explain language model behavior. (7 sources)",
                "text": "\nThe analysis of language model training dynamics has significant practical applications for optimizing training processes. One direct application is the identification of efficient termination points for training. Research has demonstrated that bifurcations in weight dynamics mark transitions to stationary states where further training yields diminishing returns. These transitions are observable across models of varying sizes trained on different datasets, suggesting they represent fundamental properties of the training process rather than artifacts of specific implementations <Paper corpusId=\"268379408\" paperTitle=\"(Nicolini et al., 2024)\" isShortName></Paper>. By detecting these transition points, practitioners can avoid unnecessary computation while maintaining model quality.\n\nLearning rate optimization represents another valuable application of training dynamics research. The discovery that cross-entropy loss curves follow a predictable scaling law with learning rate annealing has enabled more precise predictions of how loss evolves throughout training <Paper corpusId=\"271909320\" paperTitle=\"(Tissue et al., 2024)\" isShortName></Paper>. Building on this understanding, techniques like \"SkipLR,\" which involves switching learning rates at predetermined times during training, have been shown to cause loss curves to contract toward each other, providing new ways to accelerate convergence <Paper corpusId=\"274060474\" paperTitle=\"(Subramanian et al., 2024)\" isShortName></Paper>. These insights can inform the design of more efficient learning rate schedules that reduce training time without sacrificing performance.\n\nContinual pre-training (CPT) represents a third application area where training dynamics insights prove valuable. Recent work has shown that CPT loss curves can be modeled by decoupling the effects of distribution shift and learning rate annealing, enabling prediction of loss at any training step across different learning rate schedules <Paper corpusId=\"278534923\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>. This predictive capability allows practitioners to better plan and manage the continual learning process as models are adapted to new domains or updated with fresh data.\n\nIn the fine-tuning context, understanding training dynamics has led to improved techniques. Analysis of Linear Probing followed by Fine-Tuning (LP-FT) for classification tasks has revealed that the linear head norm alongside initial prediction accuracy significantly influences fine-tuning outcomes <Paper corpusId=\"270062468\" paperTitle=\"(Tomihari et al., 2024)\" isShortName></Paper>. This insight can guide more effective fine-tuning strategies that balance feature preservation with adaptation to downstream tasks.\n\nThe broader implications of training dynamics research extend to theoretical frameworks for language model learning. Work on optimizing language model learning through maximizing data compression ratios has led to the development of the \"Learning Law,\" which characterizes properties of optimal learning processes <Paper corpusId=\"268041376\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper>. Such theoretical advances provide foundational understanding that can guide future improvements in language model training.\n\nResearch on cross-domain knowledge transfer has revealed that enhancement of abilities across various domains tends to progress from basic to advanced levels, similar to curriculum learning in humans <Paper corpusId=\"268820276\" paperTitle=\"(Yang et al._1, 2024)\" isShortName></Paper>. This finding suggests that insights gained in one domain can potentially accelerate learning in others, pointing toward more efficient multi-task training strategies. Additionally, the observation that task dynamics within a domain can predict dynamics of unseen tasks indicates the potential for forecasting model performance on novel tasks based on existing benchmarks <Paper corpusId=\"268820276\" paperTitle=\"(Yang et al._1, 2024)\" isShortName></Paper>.\n\nThe analysis of training strategies across various 7B-scale models has highlighted that factors including dataset quality, learning rate adjustments, batch size, and regularization techniques significantly impact learning efficiency, particularly in early training stages <Paper corpusId=\"268820276\" paperTitle=\"(Yang et al._1, 2024)\" isShortName></Paper>. These insights can guide practitioners in making critical decisions about training configuration that maximize performance while minimizing computational resources.\n\nAs our understanding of training dynamics continues to deepen, it opens new possibilities for developing more efficient, effective, and predictable language models. By leveraging these insights, researchers and practitioners can better navigate the complex landscape of model development, ultimately producing more capable AI systems with fewer resources.",
                "citations": [
                    {
                        "id": "(Nicolini et al., 2024)",
                        "snippets": [
                            "In this study we have analyzed both the temporal and spatial dimensions of training a large language model.As discussed above, our work is the first one dealing with distribution of network weights as a whole, by means of computational methods borrowed from statistical mechanics.More specifically, this work shows that a bifurcation occurs in the dynamics of the weights during the training process.Such transitions are observed across various models of different sizes trained with distinct datasets.We have conducted a thorough and meticulous analysis of this aspect and concluded that this bifurcation marks a transition to a stationary state, indicating that further training is unlikely to significantly alter the weight values.Thus, training can be efficiently terminated upon reaching such a stationary state.Moreover, our study has offered a possible interpretation of the bifurcation phenomenon in terms of model perplexity."
                        ],
                        "paper": {
                            "corpus_id": 268379408,
                            "title": "The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2256990455",
                                    "name": "Carlo Nicolini"
                                },
                                {
                                    "authorId": "2256994086",
                                    "name": "Jacopo Staiano"
                                },
                                {
                                    "authorId": "2291065942",
                                    "name": "Bruno Lepri"
                                },
                                {
                                    "authorId": "2291066066",
                                    "name": "Raffaele Marino"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9951171875
                    },
                    {
                        "id": "(Tissue et al., 2024)",
                        "snippets": [
                            "We find that the cross-entropy loss curves of neural language models empirically adhere to a scaling law with learning rate (LR) annealing over training steps: $$L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C\\cdot S_2,$$ where $L(s)$ is the validation loss at step $s$, $S_1$ is the area under the LR curve, $S_2$ is the LR annealing area, and $L_0$, $A$, $C$, $\\alpha$ are constant parameters. This formulation takes into account two factors: (1) power-law scaling over data size, and (2) the additional loss reduction during LR annealing. Therefore, this formulation can describe the full loss curve at each step, rather than the single loss point at the end of training."
                        ],
                        "paper": {
                            "corpus_id": 271909320,
                            "title": "Scaling Law with Learning Rate Annealing",
                            "authors": [
                                {
                                    "authorId": "2316486693",
                                    "name": "Howe Tissue"
                                },
                                {
                                    "authorId": "2316485233",
                                    "name": "Venus Wang"
                                },
                                {
                                    "authorId": "2316501479",
                                    "name": "Lu Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.98828125
                    },
                    {
                        "id": "(Subramanian et al., 2024)",
                        "snippets": [
                            "We consider the effect of switching the learning rate at a predetermined time during training, which we refer to as \"SkipLR\". We model SGD as a stochastic gradient flow and show that when starting from the same initial parameters, switching the learning rate causes the loss curves to contract towards each other. We demonstrate this theoretically for some simple cases, and empirically on large language models. Our analysis provides insight into how learning rate schedules affect the training dynamics, and could inform the design of new schedules to accelerate convergence."
                        ],
                        "paper": {
                            "corpus_id": 274060474,
                            "title": "Hop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2237161748",
                                    "name": "Shreyas Vathul Subramanian"
                                },
                                {
                                    "authorId": "7194698",
                                    "name": "Vignesh Ganapathiraman"
                                },
                                {
                                    "authorId": "2258574389",
                                    "name": "Corey Barrett"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 0
                        },
                        "score": 0.99658203125
                    },
                    {
                        "id": "(Wang et al., 2025)",
                        "snippets": [
                            "In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT."
                        ],
                        "paper": {
                            "corpus_id": 278534923,
                            "title": "Learning Dynamics in Continual Pre-Training for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2268303033",
                                    "name": "Xingjin Wang"
                                },
                                {
                                    "authorId": "2316486693",
                                    "name": "Howe Tissue"
                                },
                                {
                                    "authorId": "2316501479",
                                    "name": "Lu Wang"
                                },
                                {
                                    "authorId": "2107923812",
                                    "name": "Linjing Li"
                                },
                                {
                                    "authorId": "2263800857",
                                    "name": "D. Zeng"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.99560546875
                    },
                    {
                        "id": "(Tomihari et al., 2024)",
                        "snippets": [
                            "In this paper, we analyze the training dynamics of LP-FT for classification tasks on the basis of the neural tangent kernel (NTK) theory. Our analysis decomposes the NTK matrix into two components. This decomposition highlights the importance of the linear head norm alongside the prediction accuracy at the start of the FT stage. We also observe a significant increase in the linear head norm during LP, which stems from training with the cross-entropy (CE) loss. This increase in the linear head norm effectively reduces changes in learned features."
                        ],
                        "paper": {
                            "corpus_id": 270062468,
                            "title": "Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective",
                            "authors": [
                                {
                                    "authorId": "2303396796",
                                    "name": "Akiyoshi Tomihari"
                                },
                                {
                                    "authorId": "2303397982",
                                    "name": "Issei Sato"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 4
                        },
                        "score": 0.99560546875
                    },
                    {
                        "id": "(Gu et al., 2024)",
                        "snippets": [
                            "This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an\"LM-training-as-lossless-compression\"view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task."
                        ],
                        "paper": {
                            "corpus_id": 268041376,
                            "title": "Towards Optimal Learning of Language Models",
                            "authors": [
                                {
                                    "authorId": "2116405624",
                                    "name": "Yuxian Gu"
                                },
                                {
                                    "authorId": "2286153844",
                                    "name": "Li Dong"
                                },
                                {
                                    "authorId": "34128716",
                                    "name": "Y. Hao"
                                },
                                {
                                    "authorId": "2287927238",
                                    "name": "Qingxiu Dong"
                                },
                                {
                                    "authorId": "2285704485",
                                    "name": "Minlie Huang"
                                },
                                {
                                    "authorId": "2253471545",
                                    "name": "Furu Wei"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.986328125
                    },
                    {
                        "id": "(Yang et al._1, 2024)",
                        "snippets": [
                            "In this work, we first investigate the dynamics of several open-sourced large language models (i.e., Baichuan-7B (Yang et al., 2023), DeepSeek-7B (DeepSeek-AI et al., 2024), Amber-7B (Liu et al., 2023), OpenLLaMA-7B (Geng & Liu, 2023;Touvron et al., 2023;Computer, 2023), Yi-34B (AI et al., 2024) and DeepSeek-67B), and analyze their performance results across diverse tasks using the corresponding intermediate checkpoints based on the number of pre-trained tokens. Then, based on the theoretical framework of the Scaling Law (Kaplan et al., 2020), we analyze the performance patterns of different models across various downstream tasks, and provide findings to facilitate further research on the training dynamics of the LLMs, where the findings are shown as follows: \n\n\u2022 Findings on task dynamic prediction: Within the training process of LLMs, we observe that the dynamics of existing downstream tasks within a domain can predict the dynamics of unseen tasks. This suggests that a model's performance on known tasks can inform us about how it might perform on similar, yet unseen tasks within the same domain. (Section 4.1.1) \n\n\u2022 Findings on cross-domain promotion: Similar to the human cognitive process, the enhancement of different abilities across various domains progresses from basic to advanced levels following curriculum learning. The curriculum between crossdomain tasks can guide the training direction of models and the insights gained from one domain can potentially promote the learning process of other domains. (Section 4.1.2) \n\n\u2022 Findings on the effect of training strategies, model architecture, etc. : Based on the results of several 7b-scale models (Baichuan2-7b, DeepSeek-7b, OpenLLaMA-7b, and Amber-7b), we comprehensively analyze the effect of training strategies, dataset quality, and model architecture in training LLMs. For example, we observe that the training dataset, learning rate adjustments, batch size, and regularization techniques play a significant role in the learning efficiency in the early training stage (Section 4.2.1)."
                        ],
                        "paper": {
                            "corpus_id": 268820276,
                            "title": "The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis",
                            "authors": [
                                {
                                    "authorId": "2345264293",
                                    "name": "Chenghao Yang"
                                },
                                {
                                    "authorId": "2294388803",
                                    "name": "Junzhuo Li"
                                },
                                {
                                    "authorId": "2290184043",
                                    "name": "Xinyao Niu"
                                },
                                {
                                    "authorId": "2279346001",
                                    "name": "Xinrun Du"
                                },
                                {
                                    "authorId": "2294382707",
                                    "name": "Songyang Gao"
                                },
                                {
                                    "authorId": "2281020035",
                                    "name": "Haoran Zhang"
                                },
                                {
                                    "authorId": "2294810112",
                                    "name": "Zhaoliang Chen"
                                },
                                {
                                    "authorId": "2239104064",
                                    "name": "Xingwei Qu"
                                },
                                {
                                    "authorId": "2032236274",
                                    "name": "Ruibin Yuan"
                                },
                                {
                                    "authorId": "2129449392",
                                    "name": "Yizhi Li"
                                },
                                {
                                    "authorId": "2294523552",
                                    "name": "Jiaheng Liu"
                                },
                                {
                                    "authorId": "2283188391",
                                    "name": "Stephen W. Huang"
                                },
                                {
                                    "authorId": "2282979169",
                                    "name": "Shawn Yue"
                                },
                                {
                                    "authorId": "2249847177",
                                    "name": "Wenhu Chen"
                                },
                                {
                                    "authorId": "2265967208",
                                    "name": "Jie Fu"
                                },
                                {
                                    "authorId": "2143853895",
                                    "name": "Ge Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.99169921875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.251262
    }
}