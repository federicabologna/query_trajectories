{
    "query": "Social bias large language models is an endemic problem models inherit amplify stereotypical judgments undesirable statistical associations from training corpora recent techniques post trained LMs mitigate social bias",
    "user_id": "lib_user",
    "task_id": "97212b3c-a8ec-4500-85d9-cfc4966888ae",
    "timestamp": "2025-06-24T00:50:08.907744",
    "n_retrieval": 256,
    "n_retrieved": 247,
    "n_candidates": 44,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.397941,
    "decomposed_query": {
        "rewritten_query": "Social bias in large language models and recent techniques to mitigate social bias in post-trained LMs.",
        "keyword_query": "social bias language models mitigate post-trained LMs",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009765,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Bias and Fairness in Large Language Models: A Survey",
            "venue": "Computational Linguistics",
            "year": 2023,
            "reference_count": 256,
            "citation_count": 594,
            "influential_citation_count": 33,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli_a_00524/2381177/coli_a_00524.pdf",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.00770, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237806749",
                    "name": "Isabel O. Gallegos"
                },
                {
                    "authorId": "2066337266",
                    "name": "Ryan A. Rossi"
                },
                {
                    "authorId": "40080808",
                    "name": "Joe Barrow"
                },
                {
                    "authorId": "35631602",
                    "name": "Md. Mehrab Tanjim"
                },
                {
                    "authorId": "2109571021",
                    "name": "Sungchul Kim"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "1500399016",
                    "name": "Tong Yu"
                },
                {
                    "authorId": "1940556",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "47699955",
                    "name": "Nesreen Ahmed"
                }
            ],
            "abstract": "Abstract Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.",
            "corpus_id": 261530629,
            "sentences": [
                {
                    "corpus_id": "261530629",
                    "title": "Bias and Fairness in Large Language Models: A Survey",
                    "text": "The rise and rapid advancement of large language models (LLMs) has fundamentally changed language technologies (e.g., Brown et al., 2020;Conneau et al., 2020;Devlin et al., 2019;Lewis et al., 2020;OpenAI, 2023;Radford et al., 2018;Raffel et al., 2020). With the ability to generate human-like text, as well as adapt to a wide array of natural language processing (NLP) tasks, the impressive capabilities of these models have initiated a paradigm shift in the development of language models. Instead of training task-specific models on relatively small task-specific datasets, researchers and practitioners can use LLMs as foundation models that can be fine-tuned for particular functions (Bommasani et al., 2021). Even without fine-tuning, foundation models increasingly enable few-or zero-shot capabilities for a wide array of scenarios like classification, question-answering, logical reasoning, fact retrieval, information extraction, and more, with the task described in a natural language prompt to the model and few or no labeled examples (e.g., Brown et al., 2020;Kojima et al., 2022;Liu et al., 2023a;Radford et al., 2019;Zhao et al., 2021).\n\nLaying behind these successes, however, is the potential to perpetuate harm. Typically trained on an enormous scale of uncurated Internet-based data, LLMs inherit stereotypes, misrepresentations, derogatory and exclusionary language, and other denigrating behaviors that disproportionately affect already-vulnerable and marginalized communities Dodge et al., 2021;Sheng et al., 2021b). These harms are forms of \"social bias,\" a subjective and normative term we broadly use to refer to disparate treatment or outcomes between social groups that arise from historical and structural power asymmetries, which we define and discuss in Section 2. 1 Though LLMs often reflect existing biases, they can amplify these biases too; in either case, the automated reproduction of injustice can reinforce systems of inequity (Benjamin, 2020). From negative sentiment and toxicity directed towards some social groups, to stereotypical",
                    "score": 0.5458256754282855,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 118,
                            "end": 137,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 137,
                            "end": 158,
                            "matchedPaperCorpusId": "207880568"
                        },
                        {
                            "start": 158,
                            "end": 178,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 178,
                            "end": 197,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 231,
                            "end": 251,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 1052,
                            "end": 1071,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1071,
                            "end": 1091,
                            "matchedPaperCorpusId": "249017743"
                        },
                        {
                            "start": 1091,
                            "end": 1109,
                            "matchedPaperCorpusId": "236493269"
                        },
                        {
                            "start": 1109,
                            "end": 1130,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 1130,
                            "end": 1148,
                            "matchedPaperCorpusId": "231979430"
                        },
                        {
                            "start": 1496,
                            "end": 1515,
                            "matchedPaperCorpusId": "237568724"
                        },
                        {
                            "start": 1515,
                            "end": 1535,
                            "matchedPaperCorpusId": "234337004"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9892578125
                },
                {
                    "corpus_id": "261530629",
                    "title": "Bias and Fairness in Large Language Models: A Survey",
                    "text": "Abstract Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.",
                    "score": 0.6202477988708802,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98486328125
                }
            ],
            "relevance_judgement": 0.9892578125,
            "relevance_judgment_input_expanded": "# Title: Bias and Fairness in Large Language Models: A Survey\n# Venue: Computational Linguistics\n# Authors: Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md. Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen Ahmed\n## Abstract\nAbstract Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.\n## Introduction\nThe rise and rapid advancement of large language models (LLMs) has fundamentally changed language technologies (e.g., Brown et al., 2020;Conneau et al., 2020;Devlin et al., 2019;Lewis et al., 2020;OpenAI, 2023;Radford et al., 2018;Raffel et al., 2020). With the ability to generate human-like text, as well as adapt to a wide array of natural language processing (NLP) tasks, the impressive capabilities of these models have initiated a paradigm shift in the development of language models. Instead of training task-specific models on relatively small task-specific datasets, researchers and practitioners can use LLMs as foundation models that can be fine-tuned for particular functions (Bommasani et al., 2021). Even without fine-tuning, foundation models increasingly enable few-or zero-shot capabilities for a wide array of scenarios like classification, question-answering, logical reasoning, fact retrieval, information extraction, and more, with the task described in a natural language prompt to the model and few or no labeled examples (e.g., Brown et al., 2020;Kojima et al., 2022;Liu et al., 2023a;Radford et al., 2019;Zhao et al., 2021).\n\nLaying behind these successes, however, is the potential to perpetuate harm. Typically trained on an enormous scale of uncurated Internet-based data, LLMs inherit stereotypes, misrepresentations, derogatory and exclusionary language, and other denigrating behaviors that disproportionately affect already-vulnerable and marginalized communities Dodge et al., 2021;Sheng et al., 2021b). These harms are forms of \"social bias,\" a subjective and normative term we broadly use to refer to disparate treatment or outcomes between social groups that arise from historical and structural power asymmetries, which we define and discuss in Section 2. 1 Though LLMs often reflect existing biases, they can amplify these biases too; in either case, the automated reproduction of injustice can reinforce systems of inequity (Benjamin, 2020). From negative sentiment and toxicity directed towards some social groups, to stereotypical",
            "reference_string": "[261530629 | Gallegos et al. | 2023 | Citations: 594]"
        },
        {
            "title": "Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 74,
            "citation_count": 23,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.01981, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237806749",
                    "name": "Isabel O. Gallegos"
                },
                {
                    "authorId": "2066337266",
                    "name": "Ryan A. Rossi"
                },
                {
                    "authorId": "40080808",
                    "name": "Joe Barrow"
                },
                {
                    "authorId": "35631602",
                    "name": "Md. Mehrab Tanjim"
                },
                {
                    "authorId": "1500399016",
                    "name": "Tong Yu"
                },
                {
                    "authorId": "1787977",
                    "name": "Hanieh Deilamsalehy"
                },
                {
                    "authorId": "2283147661",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "2261424174",
                    "name": "Sungchul Kim"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                }
            ],
            "abstract": "Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mitigation.",
            "corpus_id": 267411833,
            "sentences": [
                {
                    "corpus_id": "267411833",
                    "title": "Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes",
                    "text": "Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mitigation.",
                    "score": 0.5446162501703924,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9892578125
                },
                {
                    "corpus_id": "267411833",
                    "title": "Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes",
                    "text": "The rapid progress of large language models (LLMs) has ushered in a new era of technological capabilities, with increasing excitement around their few-and zero-shot capacities. For a wide range of tasks like question-answering and logical reasoning, simply modifying the prompting language can efficiently adapt the LLM without finetuning (e.g., Brown et al., 2020;Kojima et al., 2022;Liu et al., 2023;Radford et al., 2019;Reynolds and McDonell, 2021;Wei et al., 2022;Zhao et al., 2021). While few-shot approaches condition the model on a few input-output exemplars, zero-shot learning adapts the model with no training data. \n\nAt the same time as this success, however, LLMs have been shown to learn, reproduce, and even amplify denigrating, stereotypical, and exclusionary social behaviors (e.g., Bender et al., 2021;Hutchinson et al., 2020;Mei et al., 2023;Sheng et al., 2021b;Weidinger et al., 2022). We refer to this class of harms as \"social bias,\" a normative term that characterizes disparate representations, treatments, or outcomes between social groups due to historical and structural power imbalances. \n\nThe growing recognition of these harms has led to an abundance of works proposing bias mitigations for LLMs. One major drawback of many mitigation techniques, however, is their lack of scalability, computational feasibility, or generality to different dimensions of bias. In contrast to existing bias mitigation approaches, downstream applications of LLMs often require more generalizable and efficient mitigations that can be easily applied to a black-box model with no information about the training data or model parameters. \n\nIn this work, we introduce zero-shot selfdebiasing as an adaptation of zero-shot learning that leverages nothing other than the LLM itself to elicit recognition and avoidance of stereotypes1 in an LLM.",
                    "score": 0.6051604910982934,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 176
                        },
                        {
                            "start": 177,
                            "end": 487
                        },
                        {
                            "start": 488,
                            "end": 625
                        },
                        {
                            "start": 628,
                            "end": 904
                        },
                        {
                            "start": 905,
                            "end": 1114
                        },
                        {
                            "start": 1117,
                            "end": 1225
                        },
                        {
                            "start": 1226,
                            "end": 1388
                        },
                        {
                            "start": 1389,
                            "end": 1644
                        },
                        {
                            "start": 1647,
                            "end": 1848
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 346,
                            "end": 365,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 365,
                            "end": 385,
                            "matchedPaperCorpusId": "249017743"
                        },
                        {
                            "start": 402,
                            "end": 423,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 423,
                            "end": 451,
                            "matchedPaperCorpusId": "231925131"
                        },
                        {
                            "start": 451,
                            "end": 468,
                            "matchedPaperCorpusId": "246411621"
                        },
                        {
                            "start": 468,
                            "end": 486,
                            "matchedPaperCorpusId": "231979430"
                        },
                        {
                            "start": 799,
                            "end": 819,
                            "matchedPaperCorpusId": "262580630"
                        },
                        {
                            "start": 819,
                            "end": 843,
                            "matchedPaperCorpusId": "218487466"
                        },
                        {
                            "start": 843,
                            "end": 860,
                            "matchedPaperCorpusId": "259129801"
                        },
                        {
                            "start": 860,
                            "end": 880,
                            "matchedPaperCorpusId": "234337004"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9853515625
                }
            ],
            "relevance_judgement": 0.9892578125,
            "relevance_judgment_input_expanded": "# Title: Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes\n# Venue: arXiv.org\n# Authors: Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md. Mehrab Tanjim, Tong Yu, Hanieh Deilamsalehy, Ruiyi Zhang, Sungchul Kim, Franck Dernoncourt\n## Abstract\nLarge language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mitigation.\n## Introduction\nThe rapid progress of large language models (LLMs) has ushered in a new era of technological capabilities, with increasing excitement around their few-and zero-shot capacities. For a wide range of tasks like question-answering and logical reasoning, simply modifying the prompting language can efficiently adapt the LLM without finetuning (e.g., Brown et al., 2020;Kojima et al., 2022;Liu et al., 2023;Radford et al., 2019;Reynolds and McDonell, 2021;Wei et al., 2022;Zhao et al., 2021). While few-shot approaches condition the model on a few input-output exemplars, zero-shot learning adapts the model with no training data. \n\nAt the same time as this success, however, LLMs have been shown to learn, reproduce, and even amplify denigrating, stereotypical, and exclusionary social behaviors (e.g., Bender et al., 2021;Hutchinson et al., 2020;Mei et al., 2023;Sheng et al., 2021b;Weidinger et al., 2022). We refer to this class of harms as \"social bias,\" a normative term that characterizes disparate representations, treatments, or outcomes between social groups due to historical and structural power imbalances. \n\nThe growing recognition of these harms has led to an abundance of works proposing bias mitigations for LLMs. One major drawback of many mitigation techniques, however, is their lack of scalability, computational feasibility, or generality to different dimensions of bias. In contrast to existing bias mitigation approaches, downstream applications of LLMs often require more generalizable and efficient mitigations that can be easily applied to a black-box model with no information about the training data or model parameters. \n\nIn this work, we introduce zero-shot selfdebiasing as an adaptation of zero-shot learning that leverages nothing other than the LLM itself to elicit recognition and avoidance of stereotypes1 in an LLM.",
            "reference_string": "[267411833 | Gallegos et al. | 2024 | Citations: 23]"
        },
        {
            "title": "Prompting Fairness: Integrating Causality to Debias Large Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 80,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.08743, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2291078963",
                    "name": "Jingling Li"
                },
                {
                    "authorId": "2125563094",
                    "name": "Zeyu Tang"
                },
                {
                    "authorId": "2181841542",
                    "name": "Xiaoyu Liu"
                },
                {
                    "authorId": "143648560",
                    "name": "P. Spirtes"
                },
                {
                    "authorId": "2268848576",
                    "name": "Kun Zhang"
                },
                {
                    "authorId": "51435222",
                    "name": "Liu Leqi"
                },
                {
                    "authorId": "2268439518",
                    "name": "Yang Liu"
                }
            ],
            "abstract": "Large language models (LLMs), despite their remarkable capabilities, are susceptible to generating biased and discriminatory responses. As LLMs increasingly influence high-stakes decision-making (e.g., hiring and healthcare), mitigating these biases becomes critical. In this work, we propose a causality-guided debiasing framework to tackle social biases, aiming to reduce the objectionable dependence between LLMs' decisions and the social information in the input. Our framework introduces a novel perspective to identify how social information can affect an LLM's decision through different causal pathways. Leveraging these causal insights, we outline principled prompting strategies that regulate these pathways through selection mechanisms. This framework not only unifies existing prompting-based debiasing techniques, but also opens up new directions for reducing bias by encouraging the model to prioritize fact-based reasoning over reliance on biased social cues. We validate our framework through extensive experiments on real-world datasets across multiple domains, demonstrating its effectiveness in debiasing LLM decisions, even with only black-box access to the model.",
            "corpus_id": 268379141,
            "sentences": [
                {
                    "corpus_id": "268379141",
                    "title": "Prompting Fairness: Integrating Causality to Debias Large Language Models",
                    "text": "Large language models (LLMs), despite their remarkable capabilities, are susceptible to generating biased and discriminatory responses. As LLMs increasingly influence high-stakes decision-making (e.g., hiring and healthcare), mitigating these biases becomes critical. In this work, we propose a causality-guided debiasing framework to tackle social biases, aiming to reduce the objectionable dependence between LLMs' decisions and the social information in the input. Our framework introduces a novel perspective to identify how social information can affect an LLM's decision through different causal pathways. Leveraging these causal insights, we outline principled prompting strategies that regulate these pathways through selection mechanisms. This framework not only unifies existing prompting-based debiasing techniques, but also opens up new directions for reducing bias by encouraging the model to prioritize fact-based reasoning over reliance on biased social cues. We validate our framework through extensive experiments on real-world datasets across multiple domains, demonstrating its effectiveness in debiasing LLM decisions, even with only black-box access to the model.",
                    "score": 0.5348809005859155,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98779296875
                }
            ],
            "relevance_judgement": 0.98779296875,
            "relevance_judgment_input_expanded": "# Title: Prompting Fairness: Integrating Causality to Debias Large Language Models\n# Venue: International Conference on Learning Representations\n# Authors: Jingling Li, Zeyu Tang, Xiaoyu Liu, P. Spirtes, Kun Zhang, Liu Leqi, Yang Liu\n## Abstract\nLarge language models (LLMs), despite their remarkable capabilities, are susceptible to generating biased and discriminatory responses. As LLMs increasingly influence high-stakes decision-making (e.g., hiring and healthcare), mitigating these biases becomes critical. In this work, we propose a causality-guided debiasing framework to tackle social biases, aiming to reduce the objectionable dependence between LLMs' decisions and the social information in the input. Our framework introduces a novel perspective to identify how social information can affect an LLM's decision through different causal pathways. Leveraging these causal insights, we outline principled prompting strategies that regulate these pathways through selection mechanisms. This framework not only unifies existing prompting-based debiasing techniques, but also opens up new directions for reducing bias by encouraging the model to prioritize fact-based reasoning over reliance on biased social cues. We validate our framework through extensive experiments on real-world datasets across multiple domains, demonstrating its effectiveness in debiasing LLM decisions, even with only black-box access to the model.\n",
            "reference_string": "[268379141 | Li et al. | 2024 | Citations: 12]"
        },
        {
            "title": "Spoken Stereoset: on Evaluating Social Bias Toward Speaker in Speech Large Language Models",
            "venue": "Spoken Language Technology Workshop",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.07665, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284761645",
                    "name": "Yi-Cheng Lin"
                },
                {
                    "authorId": "2310393313",
                    "name": "Wei-Chih Chen"
                },
                {
                    "authorId": "2305556858",
                    "name": "Hung-yi Lee"
                }
            ],
            "abstract": "Warning: This paper may contain texts with uncomfortable content.Large Language Models (LLMs) have achieved remarkable performance in various tasks, including those involving multimodal data like speech. However, these models often exhibit biases due to the nature of their training data. Recently, more Speech Large Language Models (SLLMs) have emerged, underscoring the urgent need to address these biases. This study introduces Spoken Stereoset, a dataset specifically designed to evaluate social biases in SLLMs. By examining how different models respond to speech from diverse demographic groups, we aim to identify these biases. Our experiments reveal significant insights into their performance and bias levels. The findings indicate that while most models show minimal bias, some still exhibit slightly stereotypical or anti-stereotypical tendencies.",
            "corpus_id": 271865498,
            "sentences": [
                {
                    "corpus_id": "271865498",
                    "title": "Spoken Stereoset: on Evaluating Social Bias Toward Speaker in Speech Large Language Models",
                    "text": "Warning: This paper may contain texts with uncomfortable content.Large Language Models (LLMs) have achieved remarkable performance in various tasks, including those involving multimodal data like speech. However, these models often exhibit biases due to the nature of their training data. Recently, more Speech Large Language Models (SLLMs) have emerged, underscoring the urgent need to address these biases. This study introduces Spoken Stereoset, a dataset specifically designed to evaluate social biases in SLLMs. By examining how different models respond to speech from diverse demographic groups, we aim to identify these biases. Our experiments reveal significant insights into their performance and bias levels. The findings indicate that while most models show minimal bias, some still exhibit slightly stereotypical or anti-stereotypical tendencies.",
                    "score": 0.562971479428715,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98681640625
                },
                {
                    "corpus_id": "271865498",
                    "title": "Spoken Stereoset: on Evaluating Social Bias Toward Speaker in Speech Large Language Models",
                    "text": "Our study presents Spoken StereoSet, the first specifically designed to evaluate social biases in speech large language models. Through rigorous testing on prominent speech large language models, we uncovered both the presence and the extent of biases related to gender and age. While many models demonstrate minimal bias, others still exhibit slight social bias tendency, indicating the necessity for ongoing evaluation and mitigation strategies. The results highlight the importance of incorporating diverse and representative data in training speech large language models to ensure they promote fairness. Future work should focus on expanding the dataset to include more categories and scenarios, and developing techniques to further reduce biases in speech large language models, fostering a more equitable interaction across all demographics.",
                    "score": 0.6327014206792085,
                    "section_title": "CONCLUSION",
                    "char_start_offset": 22995,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 128,
                            "end": 278
                        },
                        {
                            "start": 279,
                            "end": 447
                        },
                        {
                            "start": 448,
                            "end": 607
                        },
                        {
                            "start": 608,
                            "end": 847
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.986328125
                }
            ],
            "relevance_judgement": 0.98681640625,
            "relevance_judgment_input_expanded": "# Title: Spoken Stereoset: on Evaluating Social Bias Toward Speaker in Speech Large Language Models\n# Venue: Spoken Language Technology Workshop\n# Authors: Yi-Cheng Lin, Wei-Chih Chen, Hung-yi Lee\n## Abstract\nWarning: This paper may contain texts with uncomfortable content.Large Language Models (LLMs) have achieved remarkable performance in various tasks, including those involving multimodal data like speech. However, these models often exhibit biases due to the nature of their training data. Recently, more Speech Large Language Models (SLLMs) have emerged, underscoring the urgent need to address these biases. This study introduces Spoken Stereoset, a dataset specifically designed to evaluate social biases in SLLMs. By examining how different models respond to speech from diverse demographic groups, we aim to identify these biases. Our experiments reveal significant insights into their performance and bias levels. The findings indicate that while most models show minimal bias, some still exhibit slightly stereotypical or anti-stereotypical tendencies.\n## CONCLUSION\nOur study presents Spoken StereoSet, the first specifically designed to evaluate social biases in speech large language models. Through rigorous testing on prominent speech large language models, we uncovered both the presence and the extent of biases related to gender and age. While many models demonstrate minimal bias, others still exhibit slight social bias tendency, indicating the necessity for ongoing evaluation and mitigation strategies. The results highlight the importance of incorporating diverse and representative data in training speech large language models to ensure they promote fairness. Future work should focus on expanding the dataset to include more categories and scenarios, and developing techniques to further reduce biases in speech large language models, fostering a more equitable interaction across all demographics.",
            "reference_string": "[271865498 | Lin et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Conceptor-Aided Debiasing of Large Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 43,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.emnlp-main.661.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.11087, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2157866322",
                    "name": "Yifei Li"
                },
                {
                    "authorId": "1717822",
                    "name": "Lyle Ungar"
                },
                {
                    "authorId": "2662374",
                    "name": "Jo\u00e3o Sedoc"
                }
            ],
            "abstract": "Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy. We use conceptors--a soft projection method--to identify and remove the bias subspace in LLMs such as BERT and GPT. We propose two methods of applying conceptors (1) bias subspace projection by post-processing by the conceptor NOT operation; and (2) a new architecture, conceptor-intervened BERT (CI-BERT), which explicitly incorporates the conceptor projection into all layers during training. We find that conceptor post-processing achieves state-of-the-art (SoTA) debiasing results while maintaining LLMs' performance on the GLUE benchmark. Further, it is robust in various scenarios and can mitigate intersectional bias efficiently by its AND operation on the existing bias subspaces. Although CI-BERT's training takes all layers' bias into account and can beat its post-processing counterpart in bias mitigation, CI-BERT reduces the language model accuracy. We also show the importance of carefully constructing the bias subspace. The best results are obtained by removing outliers from the list of biased words, combining them (via the OR operation), and computing their embeddings using the sentences from a cleaner corpus.",
            "corpus_id": 258865159,
            "sentences": [
                {
                    "corpus_id": "258865159",
                    "title": "Conceptor-Aided Debiasing of Large Language Models",
                    "text": "Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy. We use conceptors--a soft projection method--to identify and remove the bias subspace in LLMs such as BERT and GPT. We propose two methods of applying conceptors (1) bias subspace projection by post-processing by the conceptor NOT operation; and (2) a new architecture, conceptor-intervened BERT (CI-BERT), which explicitly incorporates the conceptor projection into all layers during training. We find that conceptor post-processing achieves state-of-the-art (SoTA) debiasing results while maintaining LLMs' performance on the GLUE benchmark. Further, it is robust in various scenarios and can mitigate intersectional bias efficiently by its AND operation on the existing bias subspaces. Although CI-BERT's training takes all layers' bias into account and can beat its post-processing counterpart in bias mitigation, CI-BERT reduces the language model accuracy. We also show the importance of carefully constructing the bias subspace. The best results are obtained by removing outliers from the list of biased words, combining them (via the OR operation), and computing their embeddings using the sentences from a cleaner corpus.",
                    "score": 0.6082208498296194,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.986328125
                }
            ],
            "relevance_judgement": 0.986328125,
            "relevance_judgment_input_expanded": "# Title: Conceptor-Aided Debiasing of Large Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Yifei Li, Lyle Ungar, Jo\u00e3o Sedoc\n## Abstract\nPre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy. We use conceptors--a soft projection method--to identify and remove the bias subspace in LLMs such as BERT and GPT. We propose two methods of applying conceptors (1) bias subspace projection by post-processing by the conceptor NOT operation; and (2) a new architecture, conceptor-intervened BERT (CI-BERT), which explicitly incorporates the conceptor projection into all layers during training. We find that conceptor post-processing achieves state-of-the-art (SoTA) debiasing results while maintaining LLMs' performance on the GLUE benchmark. Further, it is robust in various scenarios and can mitigate intersectional bias efficiently by its AND operation on the existing bias subspaces. Although CI-BERT's training takes all layers' bias into account and can beat its post-processing counterpart in bias mitigation, CI-BERT reduces the language model accuracy. We also show the importance of carefully constructing the bias subspace. The best results are obtained by removing outliers from the list of biased words, combining them (via the OR operation), and computing their embeddings using the sentences from a cleaner corpus.\n",
            "reference_string": "[258865159 | Li et al. | 2022 | Citations: 4]"
        },
        {
            "title": "Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 35,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.14212, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "80927455",
                    "name": "Takuma Udagawa"
                },
                {
                    "authorId": "2316828769",
                    "name": "Yang Zhao"
                },
                {
                    "authorId": "2273693407",
                    "name": "Hiroshi Kanayama"
                },
                {
                    "authorId": "49223443",
                    "name": "Bishwaranjan Bhattacharjee"
                }
            ],
            "abstract": "Large language models (LLMs) acquire general linguistic knowledge from massive-scale pretraining. However, pretraining data mainly comprised of web-crawled texts contain undesirable social biases which can be perpetuated or even amplified by LLMs. In this study, we propose an efficient yet effective annotation pipeline to investigate social biases in the pretraining corpora. Our pipeline consists of protected attribute detection to identify diverse demographics, followed by regard classification to analyze the language polarity towards each attribute. Through our experiments, we demonstrate the effect of our bias analysis and mitigation measures, focusing on Common Crawl as the most representative pretraining corpus.",
            "corpus_id": 277954809,
            "sentences": [
                {
                    "corpus_id": "277954809",
                    "title": "Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification",
                    "text": "Large language models (LLMs) acquire general linguistic knowledge from massive-scale pretraining. However, pretraining data mainly comprised of web-crawled texts contain undesirable social biases which can be perpetuated or even amplified by LLMs. In this study, we propose an efficient yet effective annotation pipeline to investigate social biases in the pretraining corpora. Our pipeline consists of protected attribute detection to identify diverse demographics, followed by regard classification to analyze the language polarity towards each attribute. Through our experiments, we demonstrate the effect of our bias analysis and mitigation measures, focusing on Common Crawl as the most representative pretraining corpus.",
                    "score": 0.6694788408560218,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9853515625
                }
            ],
            "relevance_judgement": 0.9853515625,
            "relevance_judgment_input_expanded": "# Title: Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification\n# Venue: arXiv.org\n# Authors: Takuma Udagawa, Yang Zhao, Hiroshi Kanayama, Bishwaranjan Bhattacharjee\n## Abstract\nLarge language models (LLMs) acquire general linguistic knowledge from massive-scale pretraining. However, pretraining data mainly comprised of web-crawled texts contain undesirable social biases which can be perpetuated or even amplified by LLMs. In this study, we propose an efficient yet effective annotation pipeline to investigate social biases in the pretraining corpora. Our pipeline consists of protected attribute detection to identify diverse demographics, followed by regard classification to analyze the language polarity towards each attribute. Through our experiments, we demonstrate the effect of our bias analysis and mitigation measures, focusing on Common Crawl as the most representative pretraining corpus.\n",
            "reference_string": "[277954809 | Udagawa et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Fairness Mediator: Neutralize Stereotype Associations to Mitigate Bias in Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 101,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.07787, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2276489683",
                    "name": "Yisong Xiao"
                },
                {
                    "authorId": "2257572247",
                    "name": "Aishan Liu"
                },
                {
                    "authorId": "2325884825",
                    "name": "Siyuan Liang"
                },
                {
                    "authorId": "2237942988",
                    "name": "Xianglong Liu"
                },
                {
                    "authorId": "2237906923",
                    "name": "Dacheng Tao"
                }
            ],
            "abstract": "LLMs have demonstrated remarkable performance across diverse applications, yet they inadvertently absorb spurious correlations from training data, leading to stereotype associations between biased concepts and specific social groups. These associations perpetuate and even amplify harmful social biases, raising significant fairness concerns. To mitigate such biases, prior studies have attempted to project model embeddings into unbiased spaces during inference. However, these approaches have shown limited effectiveness due to their weak alignment with downstream social biases. Inspired by the observation that concept cognition in LLMs is primarily represented through a linear associative memory mechanism, where key-value mapping occurs in the MLP layers, we posited that biased concepts and social groups are similarly encoded as entity (key) and information (value) pairs, which can be manipulated to promote fairer associations. To this end, we propose Fairness Mediator (FairMed), a bias mitigation framework that neutralizes stereotype associations. Our framework comprises two main components: a stereotype association prober and an adversarial debiasing neutralizer. The prober captures stereotype associations encoded within MLP layer activations by employing prompts centered around biased concepts to detect the emission probabilities for social groups. Subsequently, the adversarial debiasing neutralizer intervenes in MLP activations during inference to equalize the association probabilities among different social groups. Extensive experiments across nine protected attributes show that FairMed significantly outperforms SOTA methods in effectiveness. Compared to the most effective baseline, FairMed presents competitive efficiency by cutting mitigation overhead by hundreds of minutes. FairMed also maintains the LLM's language understanding capabilities without compromising overall performance.",
            "corpus_id": 277667666,
            "sentences": [
                {
                    "corpus_id": "277667666",
                    "title": "Fairness Mediator: Neutralize Stereotype Associations to Mitigate Bias in Large Language Models",
                    "text": "Large Language Models (LLMs) have rapidly advanced, achieving remarkable success across diverse natural language processing (NLP) tasks, such as question answering and text generation [10,13,74,88]. These models are now deeply integrated into daily life, powering technologies like search engines [99] and virtual assistants [19]. Despite their successes, LLMs still face significant challenges related to robustness [56-58, 85, 90, 98, 106], privacy [38,93], fairness [47,89,96], and other trustworthiness concerns [53,54]. This paper specifically focuses on the fairness issues associated with LLMs. LLMs often inherit social stereotypes and biases [96] from the training data [35,83,92], leading to biased behavior toward specific social groups, particularly in relation to protected attributes such as religion, race, and gender. For instance, GPT-3 [10] has been shown to frequently associate Muslims with violent contexts [2,39], and Microsoft's AI chatbot Tay infamously produced racist and inappropriate content after interacting with users on social media [8]. As LLMs are increasingly integrated into socially sensitive software applications, developing effective bias mitigation techniques is critical to ensuring fairness and addressing growing concerns. \n\nBias in LLMs often manifests as spurious correlations [25,36,55,76] between biased concepts (e.g., \"violence\") and specific social groups (e.g., \"Muslim\"), a phenomenon commonly referred to as stereotype associations [9,33]. These associations arise from the underrepresentation or skewed portrayal of certain social groups in training data, perpetuating harmful stereotypes and contributing to representational harm [14], whereby systems reinforce the subordination of marginalized groups. Interestingly, such implicit associations and latent activation pathways have also been exploited in recent adversarial attacks [42,91,101], backdoor attacks [50,53,54], jailbreak attacks [40,102,103] and hallucinations [34] against LLMs, revealing a broader category of vulnerabilities where malicious prompts or triggers activate specific undesired behaviors.",
                    "score": 0.5417707242361873,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 198
                        },
                        {
                            "start": 199,
                            "end": 330
                        },
                        {
                            "start": 331,
                            "end": 524
                        },
                        {
                            "start": 525,
                            "end": 601
                        },
                        {
                            "start": 602,
                            "end": 833
                        },
                        {
                            "start": 834,
                            "end": 1069
                        },
                        {
                            "start": 1070,
                            "end": 1266
                        },
                        {
                            "start": 1269,
                            "end": 1493
                        },
                        {
                            "start": 1494,
                            "end": 1759
                        },
                        {
                            "start": 1760,
                            "end": 2121
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 184,
                            "end": 188,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 297,
                            "end": 301,
                            "matchedPaperCorpusId": "270870458"
                        },
                        {
                            "start": 325,
                            "end": 329,
                            "matchedPaperCorpusId": "260499677"
                        },
                        {
                            "start": 451,
                            "end": 455,
                            "matchedPaperCorpusId": "263211018"
                        },
                        {
                            "start": 455,
                            "end": 458,
                            "matchedPaperCorpusId": "257460348"
                        },
                        {
                            "start": 469,
                            "end": 473,
                            "matchedPaperCorpusId": "267523806"
                        },
                        {
                            "start": 473,
                            "end": 476,
                            "matchedPaperCorpusId": "258833296"
                        },
                        {
                            "start": 679,
                            "end": 683,
                            "matchedPaperCorpusId": "272214842"
                        },
                        {
                            "start": 854,
                            "end": 858,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 928,
                            "end": 931,
                            "matchedPaperCorpusId": "231603388"
                        },
                        {
                            "start": 1323,
                            "end": 1327,
                            "matchedPaperCorpusId": "215786368"
                        },
                        {
                            "start": 1327,
                            "end": 1330,
                            "matchedPaperCorpusId": "250526377"
                        },
                        {
                            "start": 1333,
                            "end": 1336,
                            "matchedPaperCorpusId": "227253684"
                        },
                        {
                            "start": 1486,
                            "end": 1489,
                            "matchedPaperCorpusId": "29498619"
                        },
                        {
                            "start": 1489,
                            "end": 1492,
                            "matchedPaperCorpusId": "54036730"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9853515625
                }
            ],
            "relevance_judgement": 0.9853515625,
            "relevance_judgment_input_expanded": "# Title: Fairness Mediator: Neutralize Stereotype Associations to Mitigate Bias in Large Language Models\n# Venue: arXiv.org\n# Authors: Yisong Xiao, Aishan Liu, Siyuan Liang, Xianglong Liu, Dacheng Tao\n## Abstract\nLLMs have demonstrated remarkable performance across diverse applications, yet they inadvertently absorb spurious correlations from training data, leading to stereotype associations between biased concepts and specific social groups. These associations perpetuate and even amplify harmful social biases, raising significant fairness concerns. To mitigate such biases, prior studies have attempted to project model embeddings into unbiased spaces during inference. However, these approaches have shown limited effectiveness due to their weak alignment with downstream social biases. Inspired by the observation that concept cognition in LLMs is primarily represented through a linear associative memory mechanism, where key-value mapping occurs in the MLP layers, we posited that biased concepts and social groups are similarly encoded as entity (key) and information (value) pairs, which can be manipulated to promote fairer associations. To this end, we propose Fairness Mediator (FairMed), a bias mitigation framework that neutralizes stereotype associations. Our framework comprises two main components: a stereotype association prober and an adversarial debiasing neutralizer. The prober captures stereotype associations encoded within MLP layer activations by employing prompts centered around biased concepts to detect the emission probabilities for social groups. Subsequently, the adversarial debiasing neutralizer intervenes in MLP activations during inference to equalize the association probabilities among different social groups. Extensive experiments across nine protected attributes show that FairMed significantly outperforms SOTA methods in effectiveness. Compared to the most effective baseline, FairMed presents competitive efficiency by cutting mitigation overhead by hundreds of minutes. FairMed also maintains the LLM's language understanding capabilities without compromising overall performance.\n## INTRODUCTION\nLarge Language Models (LLMs) have rapidly advanced, achieving remarkable success across diverse natural language processing (NLP) tasks, such as question answering and text generation [10,13,74,88]. These models are now deeply integrated into daily life, powering technologies like search engines [99] and virtual assistants [19]. Despite their successes, LLMs still face significant challenges related to robustness [56-58, 85, 90, 98, 106], privacy [38,93], fairness [47,89,96], and other trustworthiness concerns [53,54]. This paper specifically focuses on the fairness issues associated with LLMs. LLMs often inherit social stereotypes and biases [96] from the training data [35,83,92], leading to biased behavior toward specific social groups, particularly in relation to protected attributes such as religion, race, and gender. For instance, GPT-3 [10] has been shown to frequently associate Muslims with violent contexts [2,39], and Microsoft's AI chatbot Tay infamously produced racist and inappropriate content after interacting with users on social media [8]. As LLMs are increasingly integrated into socially sensitive software applications, developing effective bias mitigation techniques is critical to ensuring fairness and addressing growing concerns. \n\nBias in LLMs often manifests as spurious correlations [25,36,55,76] between biased concepts (e.g., \"violence\") and specific social groups (e.g., \"Muslim\"), a phenomenon commonly referred to as stereotype associations [9,33]. These associations arise from the underrepresentation or skewed portrayal of certain social groups in training data, perpetuating harmful stereotypes and contributing to representational harm [14], whereby systems reinforce the subordination of marginalized groups. Interestingly, such implicit associations and latent activation pathways have also been exploited in recent adversarial attacks [42,91,101], backdoor attacks [50,53,54], jailbreak attacks [40,102,103] and hallucinations [34] against LLMs, revealing a broader category of vulnerabilities where malicious prompts or triggers activate specific undesired behaviors.",
            "reference_string": "[277667666 | Xiao et al. | 2025 | Citations: 3]"
        },
        {
            "title": "Investigating and Mitigating Undesirable Biases in Large Language Models",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1609/aaai.v39i28.35214",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v39i28.35214?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v39i28.35214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2077526744",
                    "name": "M. Kamruzzaman"
                }
            ],
            "abstract": "The rise of large language models (LLMs) has revolutionized natural language processing, offering immense capabilities across various applications. \nThe widespread integration of these models into commonplace technology has brought to light deep concerns about the biases they encompass, which could serve to perpetuate negative preconceptions and social injustices. The scope of my research includes social biases, brand biases, the impact of personas on bias, and stereotypes in low-resource languages. My contributions aim to deepen our understanding of these biases and develop methodologies to mitigate them, enhancing the fairness and utility of LLMs across diverse global applications.",
            "corpus_id": 277758223,
            "sentences": [
                {
                    "corpus_id": "277758223",
                    "title": "Investigating and Mitigating Undesirable Biases in Large Language Models",
                    "text": "The rise of large language models (LLMs) has revolutionized natural language processing, offering immense capabilities across various applications. \nThe widespread integration of these models into commonplace technology has brought to light deep concerns about the biases they encompass, which could serve to perpetuate negative preconceptions and social injustices. The scope of my research includes social biases, brand biases, the impact of personas on bias, and stereotypes in low-resource languages. My contributions aim to deepen our understanding of these biases and develop methodologies to mitigate them, enhancing the fairness and utility of LLMs across diverse global applications.",
                    "score": 0.5997281554527163,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9833984375
                }
            ],
            "relevance_judgement": 0.9833984375,
            "relevance_judgment_input_expanded": "# Title: Investigating and Mitigating Undesirable Biases in Large Language Models\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: M. Kamruzzaman\n## Abstract\nThe rise of large language models (LLMs) has revolutionized natural language processing, offering immense capabilities across various applications. \nThe widespread integration of these models into commonplace technology has brought to light deep concerns about the biases they encompass, which could serve to perpetuate negative preconceptions and social injustices. The scope of my research includes social biases, brand biases, the impact of personas on bias, and stereotypes in low-resource languages. My contributions aim to deepen our understanding of these biases and develop methodologies to mitigate them, enhancing the fairness and utility of LLMs across diverse global applications.\n",
            "reference_string": "[277758223 | Kamruzzaman | 2025 | Citations: 0]"
        },
        {
            "title": "Exploring Social Biases of Large Language Models in a College Artificial Intelligence Course",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2023,
            "reference_count": 39,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/26879/26651",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v37i13.26879?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v37i13.26879, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2222666109",
                    "name": "Skylar Kolisko"
                },
                {
                    "authorId": "144901955",
                    "name": "Carolyn Jane Anderson"
                }
            ],
            "abstract": "Large neural network-based language models play an increasingly important role in contemporary AI. Although these models demonstrate sophisticated text generation capabilities, they have also been shown to reproduce harmful social biases contained in their training data. This paper presents a project that guides students through an exploration of social biases in large language models.\n\nAs a final project for an intermediate college course in Artificial Intelligence, students developed a bias probe task for a previously-unstudied aspect of sociolinguistic or sociocultural bias they were interested in exploring. Through the process of constructing a dataset and evaluation metric to measure bias, students mastered key technical concepts, including how to run contemporary neural networks for natural language processing tasks; construct datasets and evaluation metrics; and analyze experimental results. Students reported their findings in an in-class presentation and a final report, recounting patterns of predictions that surprised, unsettled, and sparked interest in advocating for technology that reflects a more diverse set of backgrounds and experiences.\n\nThrough this project, students engage with and even contribute to a growing body of scholarly work on social biases in large language models.",
            "corpus_id": 259716055,
            "sentences": [
                {
                    "corpus_id": "259716055",
                    "title": "Exploring Social Biases of Large Language Models in a College Artificial Intelligence Course",
                    "text": "There is a growing body of work documenting social biases in large language models. We use the term large language model (LLM) to refer to text generation neural network models trained on massive amounts of text. Popular examples include BERT (Devlin et al. 2019), GPT-3 (Brown et al. 2020), RoBERTa (Liu et al. 2019b), andBLOOM (Big-Science 2022). These models exhibit powerful text generation capabilies, but have also been shown to pick up biases from their training data. \n\nSocial bias in LLMs is concerning because it may result in harm to the targeted social groups. Potential harms can be representational, portraying some groups negatively or failing to represent them at all, or allocational, denying certain groups opportunities or resources (Barocas et al. 2017). \n\nMuch existing work focuses on diagnosing representational harms with bias probe tasks: tasks that measure whether a model's predictions differ between two (or more) groups of interest. A number of probe tasks have been proposed: Rudinger, May, and Van Durme (2017); Sheng et al. (2019); Bordia and Bowman (2019); Lee, Madotto, and Fung (2019); Liu et al. (2019a);May et al. (2019); Nadeem, Bethke, and Reddy (2021);Sotnikova et al. (2021) and others. Most of these focus on gender stereotypes. 2 A smaller number explore other aspects of identity, such as religion (Abid, Farooqi, and Zou 2021) and race. 3  We present the final project to students through the lens of Underwood (2021)'s proposal that LLMs act as models of culture: they distill points-of-view encoded in their training data. From this perspective, exploring the social biases of these models is doubly illuminating. It can reveal biases that may percolate to downstream models, causing representational or allocational harms. It is also a way to explore biases in society at large.",
                    "score": 0.5502885863930338,
                    "section_title": "Social Biases in Large Language Models",
                    "char_start_offset": 3897,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 83
                        },
                        {
                            "start": 84,
                            "end": 212
                        },
                        {
                            "start": 213,
                            "end": 348
                        },
                        {
                            "start": 349,
                            "end": 475
                        },
                        {
                            "start": 478,
                            "end": 572
                        },
                        {
                            "start": 573,
                            "end": 774
                        },
                        {
                            "start": 777,
                            "end": 961
                        },
                        {
                            "start": 962,
                            "end": 1227
                        },
                        {
                            "start": 1228,
                            "end": 1272
                        },
                        {
                            "start": 1273,
                            "end": 1383
                        },
                        {
                            "start": 1384,
                            "end": 1569
                        },
                        {
                            "start": 1570,
                            "end": 1660
                        },
                        {
                            "start": 1661,
                            "end": 1770
                        },
                        {
                            "start": 1771,
                            "end": 1826
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 300,
                            "end": 323,
                            "matchedPaperCorpusId": "198953378"
                        },
                        {
                            "start": 1006,
                            "end": 1041,
                            "matchedPaperCorpusId": "5310359"
                        },
                        {
                            "start": 1043,
                            "end": 1062,
                            "matchedPaperCorpusId": "202537041"
                        },
                        {
                            "start": 1090,
                            "end": 1119,
                            "matchedPaperCorpusId": "211142738"
                        },
                        {
                            "start": 1121,
                            "end": 1140,
                            "matchedPaperCorpusId": "204838020"
                        },
                        {
                            "start": 1140,
                            "end": 1157,
                            "matchedPaperCorpusId": "198953378"
                        },
                        {
                            "start": 1159,
                            "end": 1192,
                            "matchedPaperCorpusId": "215828184"
                        },
                        {
                            "start": 1192,
                            "end": 1215,
                            "matchedPaperCorpusId": "234337004"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9833984375
                }
            ],
            "relevance_judgement": 0.9833984375,
            "relevance_judgment_input_expanded": "# Title: Exploring Social Biases of Large Language Models in a College Artificial Intelligence Course\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Skylar Kolisko, Carolyn Jane Anderson\n## Abstract\nLarge neural network-based language models play an increasingly important role in contemporary AI. Although these models demonstrate sophisticated text generation capabilities, they have also been shown to reproduce harmful social biases contained in their training data. This paper presents a project that guides students through an exploration of social biases in large language models.\n\nAs a final project for an intermediate college course in Artificial Intelligence, students developed a bias probe task for a previously-unstudied aspect of sociolinguistic or sociocultural bias they were interested in exploring. Through the process of constructing a dataset and evaluation metric to measure bias, students mastered key technical concepts, including how to run contemporary neural networks for natural language processing tasks; construct datasets and evaluation metrics; and analyze experimental results. Students reported their findings in an in-class presentation and a final report, recounting patterns of predictions that surprised, unsettled, and sparked interest in advocating for technology that reflects a more diverse set of backgrounds and experiences.\n\nThrough this project, students engage with and even contribute to a growing body of scholarly work on social biases in large language models.\n## Social Biases in Large Language Models\nThere is a growing body of work documenting social biases in large language models. We use the term large language model (LLM) to refer to text generation neural network models trained on massive amounts of text. Popular examples include BERT (Devlin et al. 2019), GPT-3 (Brown et al. 2020), RoBERTa (Liu et al. 2019b), andBLOOM (Big-Science 2022). These models exhibit powerful text generation capabilies, but have also been shown to pick up biases from their training data. \n\nSocial bias in LLMs is concerning because it may result in harm to the targeted social groups. Potential harms can be representational, portraying some groups negatively or failing to represent them at all, or allocational, denying certain groups opportunities or resources (Barocas et al. 2017). \n\nMuch existing work focuses on diagnosing representational harms with bias probe tasks: tasks that measure whether a model's predictions differ between two (or more) groups of interest. A number of probe tasks have been proposed: Rudinger, May, and Van Durme (2017); Sheng et al. (2019); Bordia and Bowman (2019); Lee, Madotto, and Fung (2019); Liu et al. (2019a);May et al. (2019); Nadeem, Bethke, and Reddy (2021);Sotnikova et al. (2021) and others. Most of these focus on gender stereotypes. 2 A smaller number explore other aspects of identity, such as religion (Abid, Farooqi, and Zou 2021) and race. 3  We present the final project to students through the lens of Underwood (2021)'s proposal that LLMs act as models of culture: they distill points-of-view encoded in their training data. From this perspective, exploring the social biases of these models is doubly illuminating. It can reveal biases that may percolate to downstream models, causing representational or allocational harms. It is also a way to explore biases in society at large.",
            "reference_string": "[259716055 | Kolisko et al. | 2023 | Citations: 11]"
        },
        {
            "title": "Social Debiasing for Fair Multi-modal LLMs",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.06569, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2149241557",
                    "name": "Harry Cheng"
                },
                {
                    "authorId": "1390575046",
                    "name": "Yangyang Guo"
                },
                {
                    "authorId": "2273322768",
                    "name": "Qingpei Guo"
                },
                {
                    "authorId": "2249834712",
                    "name": "Ming Yang"
                },
                {
                    "authorId": "2247906706",
                    "name": "Tian Gan"
                },
                {
                    "authorId": "2284688853",
                    "name": "Liqiang Nie"
                }
            ],
            "abstract": "Multi-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities. However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender. This paper addresses the issue of social biases in MLLMs by i) Introducing a comprehensive Counterfactual dataset with Multiple Social Concepts (CMSC), which provides a more diverse and extensive training set compared to existing datasets. ii) Proposing an Anti-Stereotype Debiasing strategy (ASD). Our method works by revisiting the MLLM training process, rescaling the autoregressive loss function, and improving data sampling methods to counteract biases. Through extensive experiments on various MLLMs, our CMSC dataset and ASD method demonstrate a significant reduction in social biases while maintaining the models' original performance.",
            "corpus_id": 271859735,
            "sentences": [
                {
                    "corpus_id": "271859735",
                    "title": "Social Debiasing for Fair Multi-modal LLMs",
                    "text": "Multi-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities. However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender. This paper addresses the issue of social biases in MLLMs by i) Introducing a comprehensive Counterfactual dataset with Multiple Social Concepts (CMSC), which provides a more diverse and extensive training set compared to existing datasets. ii) Proposing an Anti-Stereotype Debiasing strategy (ASD). Our method works by revisiting the MLLM training process, rescaling the autoregressive loss function, and improving data sampling methods to counteract biases. Through extensive experiments on various MLLMs, our CMSC dataset and ASD method demonstrate a significant reduction in social biases while maintaining the models' original performance.",
                    "score": 0.6287822928087742,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.982421875
                }
            ],
            "relevance_judgement": 0.982421875,
            "relevance_judgment_input_expanded": "# Title: Social Debiasing for Fair Multi-modal LLMs\n# Venue: arXiv.org\n# Authors: Harry Cheng, Yangyang Guo, Qingpei Guo, Ming Yang, Tian Gan, Liqiang Nie\n## Abstract\nMulti-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities. However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender. This paper addresses the issue of social biases in MLLMs by i) Introducing a comprehensive Counterfactual dataset with Multiple Social Concepts (CMSC), which provides a more diverse and extensive training set compared to existing datasets. ii) Proposing an Anti-Stereotype Debiasing strategy (ASD). Our method works by revisiting the MLLM training process, rescaling the autoregressive loss function, and improving data sampling methods to counteract biases. Through extensive experiments on various MLLMs, our CMSC dataset and ASD method demonstrate a significant reduction in social biases while maintaining the models' original performance.\n",
            "reference_string": "[271859735 | Cheng et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 18,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.14583, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2322445184",
                    "name": "Vishal Mirza"
                },
                {
                    "authorId": "2322445481",
                    "name": "Rahul Kulkarni"
                },
                {
                    "authorId": "2322445728",
                    "name": "Aakanksha Jadhav"
                }
            ],
            "abstract": "Recent advancements in Large Language Models(LLMs) have been notable, yet widespread enterprise adoption remains limited due to various constraints. This paper examines bias in LLMs-a crucial issue affecting their usability, reliability, and fairness. Researchers are developing strategies to mitigate bias, including debiasing layers, specialized reference datasets like Winogender and Winobias, and reinforcement learning with human feedback (RLHF). These techniques have been integrated into the latest LLMs. Our study evaluates gender bias in occupational scenarios and gender, age, and racial bias in crime scenarios across four leading LLMs released in 2024: Gemini 1.5 Pro, Llama 3 70B, Claude 3 Opus, and GPT-4o. Findings reveal that LLMs often depict female characters more frequently than male ones in various occupations, showing a 37% deviation from US BLS data. In crime scenarios, deviations from US FBI data are 54% for gender, 28% for race, and 17% for age. We observe that efforts to reduce gender and racial bias often lead to outcomes that may over-index one sub-class, potentially exacerbating the issue. These results highlight the limitations of current bias mitigation techniques and underscore the need for more effective approaches.",
            "corpus_id": 272826949,
            "sentences": [
                {
                    "corpus_id": "272826949",
                    "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios",
                    "text": "Large Language Models (LLMs) have transformed human-computer interaction, exhibiting unprecedented capabilities in natural language processing, communication, and content generation. However, their widespread adoption is impeded by a fundamental challenge: bias. Bias in LLMs is not merely a technical issue but a broader societal concern with significant ethical and practical implications [5]. Enterprises seeking to integrate LLMs into various applications must contend with the risks posed by biased outputs, which can reinforce stereotypes and propagate misinformation. \n\nBias in LLMs manifests in multiple forms, including racial, gender, and cultural stereotypes, often perpetuating systemic inequalities. These biases have tangible consequences; for instance, in 2018, Amazon discontinued an AI-driven recruiting tool after discovering it systematically downgraded resumes containing the term \"women's,\" reflecting an inherent bias in the training data that favored male candidates [1]. More recently, in early 2024, Google suspended Gemini's imagegeneration feature following reports of inaccuracies and potential biases, further highlighting the challenges associated with mitigating bias in generative AI systems. \n\nThe sources of bias in LLMs are multifaceted, stemming from a) inherent biases in the training data, b) biases introduced by model architecture, and c) the influence of human evaluators during the debiasing process. In response to the rising need to address bias holistically, researchers have adopted multiple ways to evaluate and mitigate bias in LLMs (TABLE I), such as curating datasets with comprehensive data for model training and implementing different debiasing approaches. The datasets used to train these models, such as Winogender, Winobias [2], BOLD (Bias in Open-ended Language Generation Dataset) [3], and the BBQ benchmark (Bias Benchmark for QA-Question Answering) [4], have limitations in representing the full spectrum of real-world language and societal biases. Similarly, existing debiasing techniques [5] often depend on external knowledge with potential bias or annotated non-biased samples, which are not always available or practical to obtain.",
                    "score": 0.5624128396511064,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 18,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 262
                        },
                        {
                            "start": 263,
                            "end": 395
                        },
                        {
                            "start": 396,
                            "end": 574
                        },
                        {
                            "start": 577,
                            "end": 712
                        },
                        {
                            "start": 713,
                            "end": 994
                        },
                        {
                            "start": 995,
                            "end": 1224
                        },
                        {
                            "start": 1227,
                            "end": 1442
                        },
                        {
                            "start": 1443,
                            "end": 1709
                        },
                        {
                            "start": 1710,
                            "end": 2008
                        },
                        {
                            "start": 2009,
                            "end": 2196
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 391,
                            "end": 394,
                            "matchedPaperCorpusId": "261530629"
                        },
                        {
                            "start": 990,
                            "end": 993,
                            "matchedPaperCorpusId": "261276445"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98095703125
                }
            ],
            "relevance_judgement": 0.98095703125,
            "relevance_judgment_input_expanded": "# Title: Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios\n# Venue: arXiv.org\n# Authors: Vishal Mirza, Rahul Kulkarni, Aakanksha Jadhav\n## Abstract\nRecent advancements in Large Language Models(LLMs) have been notable, yet widespread enterprise adoption remains limited due to various constraints. This paper examines bias in LLMs-a crucial issue affecting their usability, reliability, and fairness. Researchers are developing strategies to mitigate bias, including debiasing layers, specialized reference datasets like Winogender and Winobias, and reinforcement learning with human feedback (RLHF). These techniques have been integrated into the latest LLMs. Our study evaluates gender bias in occupational scenarios and gender, age, and racial bias in crime scenarios across four leading LLMs released in 2024: Gemini 1.5 Pro, Llama 3 70B, Claude 3 Opus, and GPT-4o. Findings reveal that LLMs often depict female characters more frequently than male ones in various occupations, showing a 37% deviation from US BLS data. In crime scenarios, deviations from US FBI data are 54% for gender, 28% for race, and 17% for age. We observe that efforts to reduce gender and racial bias often lead to outcomes that may over-index one sub-class, potentially exacerbating the issue. These results highlight the limitations of current bias mitigation techniques and underscore the need for more effective approaches.\n## I. INTRODUCTION\nLarge Language Models (LLMs) have transformed human-computer interaction, exhibiting unprecedented capabilities in natural language processing, communication, and content generation. However, their widespread adoption is impeded by a fundamental challenge: bias. Bias in LLMs is not merely a technical issue but a broader societal concern with significant ethical and practical implications [5]. Enterprises seeking to integrate LLMs into various applications must contend with the risks posed by biased outputs, which can reinforce stereotypes and propagate misinformation. \n\nBias in LLMs manifests in multiple forms, including racial, gender, and cultural stereotypes, often perpetuating systemic inequalities. These biases have tangible consequences; for instance, in 2018, Amazon discontinued an AI-driven recruiting tool after discovering it systematically downgraded resumes containing the term \"women's,\" reflecting an inherent bias in the training data that favored male candidates [1]. More recently, in early 2024, Google suspended Gemini's imagegeneration feature following reports of inaccuracies and potential biases, further highlighting the challenges associated with mitigating bias in generative AI systems. \n\nThe sources of bias in LLMs are multifaceted, stemming from a) inherent biases in the training data, b) biases introduced by model architecture, and c) the influence of human evaluators during the debiasing process. In response to the rising need to address bias holistically, researchers have adopted multiple ways to evaluate and mitigate bias in LLMs (TABLE I), such as curating datasets with comprehensive data for model training and implementing different debiasing approaches. The datasets used to train these models, such as Winogender, Winobias [2], BOLD (Bias in Open-ended Language Generation Dataset) [3], and the BBQ benchmark (Bias Benchmark for QA-Question Answering) [4], have limitations in representing the full spectrum of real-world language and societal biases. Similarly, existing debiasing techniques [5] often depend on external knowledge with potential bias or annotated non-biased samples, which are not always available or practical to obtain.",
            "reference_string": "[272826949 | Mirza et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Large Language Model Bias Mitigation from the Perspective of Knowledge Editing",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 68,
            "citation_count": 14,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.09341, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2255346632",
                    "name": "Ruizhe Chen"
                },
                {
                    "authorId": "2301405716",
                    "name": "Yichen Li"
                },
                {
                    "authorId": "2257215912",
                    "name": "Zikai Xiao"
                },
                {
                    "authorId": "2155333146",
                    "name": "Zuo-Qiang Liu"
                }
            ],
            "abstract": "Existing debiasing methods inevitably make unreasonable or undesired predictions as they are designated and evaluated to achieve parity across different social groups but leave aside individual facts, resulting in modified existing knowledge. In this paper, we first establish a new bias mitigation benchmark BiasKE leveraging existing and additional constructed datasets, which systematically assesses debiasing performance by complementary metrics on fairness, specificity, and generalization. Meanwhile, we propose a novel debiasing method, Fairness Stamp (FAST), which enables editable fairness through fine-grained calibration on individual biased knowledge. Comprehensive experiments demonstrate that FAST surpasses state-of-the-art baselines with remarkable debiasing performance while not hampering overall model capability for knowledge preservation, highlighting the prospect of fine-grained debiasing strategies for editable fairness in LLMs.",
            "corpus_id": 269773271,
            "sentences": [
                {
                    "corpus_id": "269773271",
                    "title": "Large Language Model Bias Mitigation from the Perspective of Knowledge Editing",
                    "text": "Pre-trained Large Language Models (LLMs) have demonstrated exceptional performance on many tasks (Devlin et al., 2018;Floridi & Chiriatti, 2020;Brown et al., 2020).However, the encoded social stereotypes and human-like biases inevitably cause undesired behaviors when deploying LLMs in practice (Zhao et al., 2019;Navigli et al., 2023;Sheng et al., 2021).Existing approaches to mitigate biases in LLMs are mainly categorized into: (1) Fine-tuning (Zmigrod et al., 2019;Webster et al., 2020;He et al., 2022;Liang et al., 2020;Lauscher et al., 2021), which includes techniques such as re-balanced corpus pre-training, contrastive learning, projection methods, and efficient parameter tuning.(2) Prompt-tuning (Guo et al., 2022;Yang et al., 2023;Li et al., 2023b;Dong et al., 2023), which involves creating prompts to address social biases.Existing debiasing approaches usually equalize different groups, resulting in unreasonable predictions.(c) Our proposed method performs fine-grained calibration with biased knowledge, while maintaining the others.\n\nHowever, existing techniques treat social groups as interchangeable (Gallegos et al., 2023) and neutralize protected attributes of different social groups in model inputs or outputs, while ignoring or concealing distinct mechanisms of different social groups (Hanna et al., 2020), as shown in Figure 1.Furthermore, existing debiasing evaluation metrics mainly focus on the degree of bias, but fail to measure whether the model retains its origin knowledge (Gallegos et al., 2023) of discerning reasonable disparities among different social groups.\n\nTo address these issues, we first establish a more comprehensive debiasing benchmark BiasKE by extending existing datasets with additional constructed data and evaluation metrics on fairness, specificity, and generalization.Moreover, we propose a novel method Fairness-Stamp (FAST) for editable bias mitigation.",
                    "score": 0.6773888502084764,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 164
                        },
                        {
                            "start": 164,
                            "end": 355
                        },
                        {
                            "start": 355,
                            "end": 689
                        },
                        {
                            "start": 689,
                            "end": 837
                        },
                        {
                            "start": 837,
                            "end": 940
                        },
                        {
                            "start": 940,
                            "end": 1050
                        },
                        {
                            "start": 1052,
                            "end": 1354
                        },
                        {
                            "start": 1354,
                            "end": 1599
                        },
                        {
                            "start": 1601,
                            "end": 1825
                        },
                        {
                            "start": 1825,
                            "end": 1912
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 144,
                            "end": 163,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 314,
                            "end": 335,
                            "matchedPaperCorpusId": "258688053"
                        },
                        {
                            "start": 707,
                            "end": 725,
                            "matchedPaperCorpusId": "248780440"
                        },
                        {
                            "start": 725,
                            "end": 743,
                            "matchedPaperCorpusId": "253446867"
                        },
                        {
                            "start": 1311,
                            "end": 1331,
                            "matchedPaperCorpusId": "208921008"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98046875
                }
            ],
            "relevance_judgement": 0.98046875,
            "relevance_judgment_input_expanded": "# Title: Large Language Model Bias Mitigation from the Perspective of Knowledge Editing\n# Venue: arXiv.org\n# Authors: Ruizhe Chen, Yichen Li, Zikai Xiao, Zuo-Qiang Liu\n## Abstract\nExisting debiasing methods inevitably make unreasonable or undesired predictions as they are designated and evaluated to achieve parity across different social groups but leave aside individual facts, resulting in modified existing knowledge. In this paper, we first establish a new bias mitigation benchmark BiasKE leveraging existing and additional constructed datasets, which systematically assesses debiasing performance by complementary metrics on fairness, specificity, and generalization. Meanwhile, we propose a novel debiasing method, Fairness Stamp (FAST), which enables editable fairness through fine-grained calibration on individual biased knowledge. Comprehensive experiments demonstrate that FAST surpasses state-of-the-art baselines with remarkable debiasing performance while not hampering overall model capability for knowledge preservation, highlighting the prospect of fine-grained debiasing strategies for editable fairness in LLMs.\n## INTRODUCTION\nPre-trained Large Language Models (LLMs) have demonstrated exceptional performance on many tasks (Devlin et al., 2018;Floridi & Chiriatti, 2020;Brown et al., 2020).However, the encoded social stereotypes and human-like biases inevitably cause undesired behaviors when deploying LLMs in practice (Zhao et al., 2019;Navigli et al., 2023;Sheng et al., 2021).Existing approaches to mitigate biases in LLMs are mainly categorized into: (1) Fine-tuning (Zmigrod et al., 2019;Webster et al., 2020;He et al., 2022;Liang et al., 2020;Lauscher et al., 2021), which includes techniques such as re-balanced corpus pre-training, contrastive learning, projection methods, and efficient parameter tuning.(2) Prompt-tuning (Guo et al., 2022;Yang et al., 2023;Li et al., 2023b;Dong et al., 2023), which involves creating prompts to address social biases.Existing debiasing approaches usually equalize different groups, resulting in unreasonable predictions.(c) Our proposed method performs fine-grained calibration with biased knowledge, while maintaining the others.\n\nHowever, existing techniques treat social groups as interchangeable (Gallegos et al., 2023) and neutralize protected attributes of different social groups in model inputs or outputs, while ignoring or concealing distinct mechanisms of different social groups (Hanna et al., 2020), as shown in Figure 1.Furthermore, existing debiasing evaluation metrics mainly focus on the degree of bias, but fail to measure whether the model retains its origin knowledge (Gallegos et al., 2023) of discerning reasonable disparities among different social groups.\n\nTo address these issues, we first establish a more comprehensive debiasing benchmark BiasKE by extending existing datasets with additional constructed data and evaluation metrics on fairness, specificity, and generalization.Moreover, we propose a novel method Fairness-Stamp (FAST) for editable bias mitigation.",
            "reference_string": "[269773271 | Chen et al. | 2024 | Citations: 14]"
        },
        {
            "title": "Demographic-Aware Language Model Fine-tuning as a Bias Mitigation Technique",
            "venue": "AACL",
            "year": 2022,
            "reference_count": 27,
            "citation_count": 21,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2022.aacl-short.38, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "31099365",
                    "name": "Aparna Garimella"
                },
                {
                    "authorId": "2105984203",
                    "name": "Rada Mihalcea"
                },
                {
                    "authorId": "2121347719",
                    "name": "Akhash Amarnath"
                }
            ],
            "abstract": "BERT-like language models (LMs), when exposed to large unstructured datasets, are known to learn and sometimes even amplify the biases present in such data. These biases generally reflect social stereotypes with respect to gender, race, age, and others. In this paper, we analyze the variations in gender and racial biases in BERT, a large pre-trained LM, when exposed to different demographic groups. Specifically, we investigate the effect of fine-tuning BERT on text authored by historically disadvantaged demographic groups in comparison to that by advantaged groups. We show that simply by fine-tuning BERT-like LMs on text authored by certain demographic groups can result in the mitigation of social biases in these LMs against various target groups.",
            "corpus_id": 253762006,
            "sentences": [
                {
                    "corpus_id": "253762006",
                    "title": "Demographic-Aware Language Model Fine-tuning as a Bias Mitigation Technique",
                    "text": "BERT-like language models (LMs), when exposed to large unstructured datasets, are known to learn and sometimes even amplify the biases present in such data. These biases generally reflect social stereotypes with respect to gender, race, age, and others. In this paper, we analyze the variations in gender and racial biases in BERT, a large pre-trained LM, when exposed to different demographic groups. Specifically, we investigate the effect of fine-tuning BERT on text authored by historically disadvantaged demographic groups in comparison to that by advantaged groups. We show that simply by fine-tuning BERT-like LMs on text authored by certain demographic groups can result in the mitigation of social biases in these LMs against various target groups.",
                    "score": 0.593320447536384,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98046875
                }
            ],
            "relevance_judgement": 0.98046875,
            "relevance_judgment_input_expanded": "# Title: Demographic-Aware Language Model Fine-tuning as a Bias Mitigation Technique\n# Venue: AACL\n# Authors: Aparna Garimella, Rada Mihalcea, Akhash Amarnath\n## Abstract\nBERT-like language models (LMs), when exposed to large unstructured datasets, are known to learn and sometimes even amplify the biases present in such data. These biases generally reflect social stereotypes with respect to gender, race, age, and others. In this paper, we analyze the variations in gender and racial biases in BERT, a large pre-trained LM, when exposed to different demographic groups. Specifically, we investigate the effect of fine-tuning BERT on text authored by historically disadvantaged demographic groups in comparison to that by advantaged groups. We show that simply by fine-tuning BERT-like LMs on text authored by certain demographic groups can result in the mitigation of social biases in these LMs against various target groups.\n",
            "reference_string": "[253762006 | Garimella et al. | 2022 | Citations: 21]"
        },
        {
            "title": "Mitigating Social Bias in Large Language Models: A Multi-Objective Approach within a Multi-Agent Framework",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.15504, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2336859072",
                    "name": "Zhenjie Xu"
                },
                {
                    "authorId": "2279760195",
                    "name": "Wenqing Chen"
                },
                {
                    "authorId": "2336730514",
                    "name": "Yi Tang"
                },
                {
                    "authorId": "2336828833",
                    "name": "Xuanying Li"
                },
                {
                    "authorId": "2336830483",
                    "name": "Cheng Hu"
                },
                {
                    "authorId": "2303254578",
                    "name": "Zhixuan Chu"
                },
                {
                    "authorId": "2302800539",
                    "name": "Kui Ren"
                },
                {
                    "authorId": "2316508020",
                    "name": "Zibin Zheng"
                },
                {
                    "authorId": "2336738374",
                    "name": "Zhichao Lu School of Software Engineering"
                },
                {
                    "authorId": "89574632",
                    "name": "Sun Yat-sen University"
                },
                {
                    "authorId": "89909107",
                    "name": "S. O. Physics"
                },
                {
                    "authorId": "2063003285",
                    "name": "Astronomy"
                },
                {
                    "authorId": "102700312",
                    "name": "School of Materials Science"
                },
                {
                    "authorId": "2321564437",
                    "name": "Technology"
                },
                {
                    "authorId": "103231213",
                    "name": "Zhejiang University"
                },
                {
                    "authorId": "2212824158",
                    "name": "Department of Computer Science"
                },
                {
                    "authorId": "120391927",
                    "name": "City University of Hong Kong"
                }
            ],
            "abstract": "Natural language processing (NLP) has seen remarkable advancements with the development of large language models (LLMs). Despite these advancements, LLMs often produce socially biased outputs. Recent studies have mainly addressed this problem by prompting LLMs to behave ethically, but this approach results in unacceptable performance degradation. In this paper, we propose a multi-objective approach within a multi-agent framework (MOMA) to mitigate social bias in LLMs without significantly compromising their performance. The key idea of MOMA involves deploying multiple agents to perform causal interventions on bias-related contents of the input questions, breaking the shortcut connection between these contents and the corresponding answers. Unlike traditional debiasing techniques leading to performance degradation, MOMA substantially reduces bias while maintaining accuracy in downstream tasks. Our experiments conducted on two datasets and two models demonstrate that MOMA reduces bias scores by up to 87.7%, with only a marginal performance degradation of up to 6.8% in the BBQ dataset. Additionally, it significantly enhances the multi-objective metric icat in the StereoSet dataset by up to 58.1%. Code will be made available at https://github.com/Cortantse/MOMA.",
            "corpus_id": 274965310,
            "sentences": [
                {
                    "corpus_id": "274965310",
                    "title": "Mitigating Social Bias in Large Language Models: A Multi-Objective Approach within a Multi-Agent Framework",
                    "text": "Natural language processing (NLP) has seen remarkable advancements with the development of large language models (LLMs). Despite these advancements, LLMs often produce socially biased outputs. Recent studies have mainly addressed this problem by prompting LLMs to behave ethically, but this approach results in unacceptable performance degradation. In this paper, we propose a multi-objective approach within a multi-agent framework (MOMA) to mitigate social bias in LLMs without significantly compromising their performance. The key idea of MOMA involves deploying multiple agents to perform causal interventions on bias-related contents of the input questions, breaking the shortcut connection between these contents and the corresponding answers. Unlike traditional debiasing techniques leading to performance degradation, MOMA substantially reduces bias while maintaining accuracy in downstream tasks. Our experiments conducted on two datasets and two models demonstrate that MOMA reduces bias scores by up to 87.7%, with only a marginal performance degradation of up to 6.8% in the BBQ dataset. Additionally, it significantly enhances the multi-objective metric icat in the StereoSet dataset by up to 58.1%. Code will be made available at https://github.com/Cortantse/MOMA.",
                    "score": 0.5933191638669831,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97900390625
                }
            ],
            "relevance_judgement": 0.97900390625,
            "relevance_judgment_input_expanded": "# Title: Mitigating Social Bias in Large Language Models: A Multi-Objective Approach within a Multi-Agent Framework\n# Venue: arXiv.org\n# Authors: Zhenjie Xu, Wenqing Chen, Yi Tang, Xuanying Li, Cheng Hu, Zhixuan Chu, Kui Ren, Zibin Zheng, Zhichao Lu School of Software Engineering, Sun Yat-sen University, S. O. Physics, Astronomy, School of Materials Science, Technology, Zhejiang University, Department of Computer Science, City University of Hong Kong\n## Abstract\nNatural language processing (NLP) has seen remarkable advancements with the development of large language models (LLMs). Despite these advancements, LLMs often produce socially biased outputs. Recent studies have mainly addressed this problem by prompting LLMs to behave ethically, but this approach results in unacceptable performance degradation. In this paper, we propose a multi-objective approach within a multi-agent framework (MOMA) to mitigate social bias in LLMs without significantly compromising their performance. The key idea of MOMA involves deploying multiple agents to perform causal interventions on bias-related contents of the input questions, breaking the shortcut connection between these contents and the corresponding answers. Unlike traditional debiasing techniques leading to performance degradation, MOMA substantially reduces bias while maintaining accuracy in downstream tasks. Our experiments conducted on two datasets and two models demonstrate that MOMA reduces bias scores by up to 87.7%, with only a marginal performance degradation of up to 6.8% in the BBQ dataset. Additionally, it significantly enhances the multi-objective metric icat in the StereoSet dataset by up to 58.1%. Code will be made available at https://github.com/Cortantse/MOMA.\n",
            "reference_string": "[274965310 | Xu et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Identifying and Mitigating Social Bias Knowledge in Language Models",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 88,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.11843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2255346632",
                    "name": "Ruizhe Chen"
                },
                {
                    "authorId": "2301405716",
                    "name": "Yichen Li"
                },
                {
                    "authorId": "2260614480",
                    "name": "Jianfei Yang"
                },
                {
                    "authorId": "2253854049",
                    "name": "Yang Feng"
                },
                {
                    "authorId": "2253900280",
                    "name": "J. Zhou"
                },
                {
                    "authorId": "2253868642",
                    "name": "Jian Wu"
                },
                {
                    "authorId": "2311458018",
                    "name": "Zuozhu Liu"
                }
            ],
            "abstract": "Generating fair and accurate predictions plays a pivotal role in deploying large language models (LLMs) in the real world. However, existing debiasing methods inevitably generate unfair or incorrect predictions as they are designed and evaluated to achieve parity across different social groups but leave aside individual commonsense facts, resulting in modified knowledge that elicits unreasonable or undesired predictions. In this paper, we first establish a new bias mitigation benchmark, BiaScope, which systematically assesses performance by leveraging newly constructed datasets and metrics on knowledge retention and generalization. Then, we propose a novel debiasing approach, Fairness Stamp (FAST), which enables fine-grained calibration of individual social biases. FAST identifies the decisive layer responsible for storing social biases and then calibrates its outputs by integrating a small modular network, considering both bias mitigation and knowledge-preserving demands. Comprehensive experiments demonstrate that FAST surpasses state-of-the-art baselines with superior debiasing performance while not compromising the overall model capability for knowledge retention and downstream predictions. This highlights the potential of fine-grained debiasing strategies to achieve fairness in LLMs.",
            "corpus_id": 271923841,
            "sentences": [
                {
                    "corpus_id": "271923841",
                    "title": "Identifying and Mitigating Social Bias Knowledge in Language Models",
                    "text": "Generating fair and accurate predictions plays a pivotal role in deploying large language models (LLMs) in the real world. However, existing debiasing methods inevitably generate unfair or incorrect predictions as they are designed and evaluated to achieve parity across different social groups but leave aside individual commonsense facts, resulting in modified knowledge that elicits unreasonable or undesired predictions. In this paper, we first establish a new bias mitigation benchmark, BiaScope, which systematically assesses performance by leveraging newly constructed datasets and metrics on knowledge retention and generalization. Then, we propose a novel debiasing approach, Fairness Stamp (FAST), which enables fine-grained calibration of individual social biases. FAST identifies the decisive layer responsible for storing social biases and then calibrates its outputs by integrating a small modular network, considering both bias mitigation and knowledge-preserving demands. Comprehensive experiments demonstrate that FAST surpasses state-of-the-art baselines with superior debiasing performance while not compromising the overall model capability for knowledge retention and downstream predictions. This highlights the potential of fine-grained debiasing strategies to achieve fairness in LLMs.",
                    "score": 0.5456038398447599,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.978515625
                },
                {
                    "corpus_id": "271923841",
                    "title": "Identifying and Mitigating Social Bias Knowledge in Language Models",
                    "text": "Bias Mitigation in Pre-trained Language Models. The increasing deployment of chatbots driven by large language models (LLMs) has heightened concerns over fairness. Issues related to fairness in LLMs can have dire outcomes, such as the amplification of bias, discrimination, and detrimental effects on marginalized groups. Consequently, substantial efforts are being made to assess and address biases in large language models (Gallegos et al., 2023;Li et al., 2023b;Fan et al., 2024a;Luo et al., 2024). Several approaches have been proposed for debiasing pre-trained language models, which can be grouped into two categories: (1) Fine-tuning. This branch includes additional pretraining on re-balanced corpora (Zmigrod et al., 2019;Webster et al., 2020) or with a contrastive objective (He et al., 2022;Cheng et al., 2021), projection-based methods (Liang et al., 2020;Ravfogel et al., 2020;Kaneko and Bollegala, 2021;Dev et al., 2020) in the embedding space, in-training methods (Han et al., 2021;He et al., 2022) and parameter-efficient fine-tuning (Lauscher et al., 2021;Xie and Lukasiewicz, 2023) methods. \n\n(2) Prompt-tuning. Prompt-tuning (Guo et al., 2022;Yang et al., 2023;Li et al., 2023c;Dong et al., 2023) involve generating either discrete prompts or con-tinuous prompts to mitigate social biases. There are also post-hoc approaches (Schick et al., 2021;Chen et al., 2023Chen et al., , 2024a;;Fan et al., 2024b;Chen et al., 2024b) that are deployed after the training phase to achieve effective debiasing.",
                    "score": 0.6206345087674743,
                    "section_title": "B Related Works",
                    "char_start_offset": 31332,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 47
                        },
                        {
                            "start": 48,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 321
                        },
                        {
                            "start": 322,
                            "end": 501
                        },
                        {
                            "start": 502,
                            "end": 641
                        },
                        {
                            "start": 642,
                            "end": 1108
                        },
                        {
                            "start": 1111,
                            "end": 1129
                        },
                        {
                            "start": 1130,
                            "end": 1308
                        },
                        {
                            "start": 1309,
                            "end": 1516
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 917,
                            "end": 934,
                            "matchedPaperCorpusId": "201670701"
                        },
                        {
                            "start": 1144,
                            "end": 1162,
                            "matchedPaperCorpusId": "248780440"
                        },
                        {
                            "start": 1162,
                            "end": 1180,
                            "matchedPaperCorpusId": "253446867"
                        },
                        {
                            "start": 1180,
                            "end": 1197,
                            "matchedPaperCorpusId": "259342087"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97265625
                }
            ],
            "relevance_judgement": 0.978515625,
            "relevance_judgment_input_expanded": "# Title: Identifying and Mitigating Social Bias Knowledge in Language Models\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Ruizhe Chen, Yichen Li, Jianfei Yang, Yang Feng, J. Zhou, Jian Wu, Zuozhu Liu\n## Abstract\nGenerating fair and accurate predictions plays a pivotal role in deploying large language models (LLMs) in the real world. However, existing debiasing methods inevitably generate unfair or incorrect predictions as they are designed and evaluated to achieve parity across different social groups but leave aside individual commonsense facts, resulting in modified knowledge that elicits unreasonable or undesired predictions. In this paper, we first establish a new bias mitigation benchmark, BiaScope, which systematically assesses performance by leveraging newly constructed datasets and metrics on knowledge retention and generalization. Then, we propose a novel debiasing approach, Fairness Stamp (FAST), which enables fine-grained calibration of individual social biases. FAST identifies the decisive layer responsible for storing social biases and then calibrates its outputs by integrating a small modular network, considering both bias mitigation and knowledge-preserving demands. Comprehensive experiments demonstrate that FAST surpasses state-of-the-art baselines with superior debiasing performance while not compromising the overall model capability for knowledge retention and downstream predictions. This highlights the potential of fine-grained debiasing strategies to achieve fairness in LLMs.\n## B Related Works\nBias Mitigation in Pre-trained Language Models. The increasing deployment of chatbots driven by large language models (LLMs) has heightened concerns over fairness. Issues related to fairness in LLMs can have dire outcomes, such as the amplification of bias, discrimination, and detrimental effects on marginalized groups. Consequently, substantial efforts are being made to assess and address biases in large language models (Gallegos et al., 2023;Li et al., 2023b;Fan et al., 2024a;Luo et al., 2024). Several approaches have been proposed for debiasing pre-trained language models, which can be grouped into two categories: (1) Fine-tuning. This branch includes additional pretraining on re-balanced corpora (Zmigrod et al., 2019;Webster et al., 2020) or with a contrastive objective (He et al., 2022;Cheng et al., 2021), projection-based methods (Liang et al., 2020;Ravfogel et al., 2020;Kaneko and Bollegala, 2021;Dev et al., 2020) in the embedding space, in-training methods (Han et al., 2021;He et al., 2022) and parameter-efficient fine-tuning (Lauscher et al., 2021;Xie and Lukasiewicz, 2023) methods. \n\n(2) Prompt-tuning. Prompt-tuning (Guo et al., 2022;Yang et al., 2023;Li et al., 2023c;Dong et al., 2023) involve generating either discrete prompts or con-tinuous prompts to mitigate social biases. There are also post-hoc approaches (Schick et al., 2021;Chen et al., 2023Chen et al., , 2024a;;Fan et al., 2024b;Chen et al., 2024b) that are deployed after the training phase to achieve effective debiasing.",
            "reference_string": "[271923841 | Chen et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Social Bias Evaluation for Large Language Models Requires Prompt Variations",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 53,
            "citation_count": 20,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.03129, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46186371",
                    "name": "Rem Hida"
                },
                {
                    "authorId": "143655216",
                    "name": "Masahiro Kaneko"
                },
                {
                    "authorId": "2269460776",
                    "name": "Naoaki Okazaki"
                }
            ],
            "abstract": "Warning: This paper contains examples of stereotypes and biases. Large Language Models (LLMs) exhibit considerable social biases, and various studies have tried to evaluate and mitigate these biases accurately. Previous studies use downstream tasks as prompts to examine the degree of social biases for evaluation and mitigation. While LLMs' output highly depends on prompts, previous studies evaluating and mitigating bias have often relied on a limited variety of prompts. In this paper, we investigate the sensitivity of LLMs when changing prompt variations (task instruction and prompt, few-shot examples, debias-prompt) by analyzing task performance and social bias of LLMs. Our experimental results reveal that LLMs are highly sensitive to prompts to the extent that the ranking of LLMs fluctuates when comparing models for task performance and social bias. Additionally, we show that LLMs have tradeoffs between performance and social bias caused by the prompts. Less bias from prompt setting may result in reduced performance. Moreover, the ambiguity of instances is one of the reasons for this sensitivity to prompts in advanced LLMs, leading to various outputs. We recommend using diverse prompts, as in this study, to compare the effects of prompts on social bias in LLMs.",
            "corpus_id": 270924184,
            "sentences": [
                {
                    "corpus_id": "270924184",
                    "title": "Social Bias Evaluation for Large Language Models Requires Prompt Variations",
                    "text": "Warning: This paper contains examples of stereotypes and biases. Large Language Models (LLMs) exhibit considerable social biases, and various studies have tried to evaluate and mitigate these biases accurately. Previous studies use downstream tasks as prompts to examine the degree of social biases for evaluation and mitigation. While LLMs' output highly depends on prompts, previous studies evaluating and mitigating bias have often relied on a limited variety of prompts. In this paper, we investigate the sensitivity of LLMs when changing prompt variations (task instruction and prompt, few-shot examples, debias-prompt) by analyzing task performance and social bias of LLMs. Our experimental results reveal that LLMs are highly sensitive to prompts to the extent that the ranking of LLMs fluctuates when comparing models for task performance and social bias. Additionally, we show that LLMs have tradeoffs between performance and social bias caused by the prompts. Less bias from prompt setting may result in reduced performance. Moreover, the ambiguity of instances is one of the reasons for this sensitivity to prompts in advanced LLMs, leading to various outputs. We recommend using diverse prompts, as in this study, to compare the effects of prompts on social bias in LLMs.",
                    "score": 0.5879347818309062,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9775390625
                }
            ],
            "relevance_judgement": 0.9775390625,
            "relevance_judgment_input_expanded": "# Title: Social Bias Evaluation for Large Language Models Requires Prompt Variations\n# Venue: arXiv.org\n# Authors: Rem Hida, Masahiro Kaneko, Naoaki Okazaki\n## Abstract\nWarning: This paper contains examples of stereotypes and biases. Large Language Models (LLMs) exhibit considerable social biases, and various studies have tried to evaluate and mitigate these biases accurately. Previous studies use downstream tasks as prompts to examine the degree of social biases for evaluation and mitigation. While LLMs' output highly depends on prompts, previous studies evaluating and mitigating bias have often relied on a limited variety of prompts. In this paper, we investigate the sensitivity of LLMs when changing prompt variations (task instruction and prompt, few-shot examples, debias-prompt) by analyzing task performance and social bias of LLMs. Our experimental results reveal that LLMs are highly sensitive to prompts to the extent that the ranking of LLMs fluctuates when comparing models for task performance and social bias. Additionally, we show that LLMs have tradeoffs between performance and social bias caused by the prompts. Less bias from prompt setting may result in reduced performance. Moreover, the ambiguity of instances is one of the reasons for this sensitivity to prompts in advanced LLMs, leading to various outputs. We recommend using diverse prompts, as in this study, to compare the effects of prompts on social bias in LLMs.\n",
            "reference_string": "[270924184 | Hida et al. | 2024 | Citations: 20]"
        },
        {
            "title": "Expert-Guided Extinction of Toxic Tokens for Debiased Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 47,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.19299, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2151433946",
                    "name": "Xueyao Sun"
                },
                {
                    "authorId": "2266763119",
                    "name": "Kaize Shi"
                },
                {
                    "authorId": "2303987143",
                    "name": "Haoran Tang"
                },
                {
                    "authorId": "2261934340",
                    "name": "Guandong Xu"
                },
                {
                    "authorId": "2300093395",
                    "name": "Qing Li"
                }
            ],
            "abstract": "Large language models (LLMs) can elicit social bias during generations, especially when inference with toxic prompts. Controlling the sensitive attributes in generation encounters challenges in data distribution, generalizability, and efficiency. Specifically, fine-tuning and retrieval demand extensive unbiased corpus, while direct prompting requires meticulously curated instructions for correcting the output in multiple rounds of thoughts but poses challenges on memory and inference latency. In this work, we propose the Expert-Guided Extinction of Toxic Tokens for Debiased Generation (EXPOSED) to eliminate the undesired harmful outputs for LLMs without the aforementioned requirements. EXPOSED constructs a debiasing expert based on the abundant toxic corpus to expose and elicit the potentially dangerous tokens. It then processes the output to the LLMs and constructs a fair distribution by suppressing and attenuating the toxic tokens. EXPOSED is evaluated on fairness benchmarks over three LLM families. Extensive experiments demonstrate that compared with other baselines, the proposed EXPOSED significantly reduces the potential social bias while balancing fairness and generation performance.",
            "corpus_id": 270095018,
            "sentences": [
                {
                    "corpus_id": "270095018",
                    "title": "Expert-Guided Extinction of Toxic Tokens for Debiased Generation",
                    "text": "Large language models (LLMs), renowned for their extraordinary natural language understanding, generation, and generalization capabilities, have achieved state-of-the-art performances across various tasks and benchmarks [1,43,19,39,37].However, the extensive application of LLMs in text generation has given rise to legitimate concerns [23,10].Recent studies have revealed that LLMs elicit the potential to exhibit unwelcome negative behaviors, such as the propagation of biases, particularly when provided with leading prompts or instructions [11,21,45].These biases can contain toxic content with threats or profanity, social stereotypes, or prejudiced perceptions toward certain groups, further perpetuating societal inequities, reinforcing discriminations, and impacting LLMs' applicability [23].For example, the social bias presented in LLMs can lead to unfairly generated articles or stories, potentially spreading misinformation or societal prejudices [25].Mitigating these biases in LLMs reduces malicious use from hostile intentions and ensures trustworthy language processing.Generally, enhancing LLMs for specialized attribute control to address the abovementioned limitations involves three aspects: fine-tuning to inject parameterized knowledge, retrieval to couple nonparameterized knowledge, and prompting to improve the output in several reasoning turns [11,38].The motivation of our work.Large language models may elicit social bias during generation, especially when encountering potentially toxic input.However, existing debiasing methods for generative language models encounter several difficulties.However, applying these techniques directly to debias LLMs encounters several challenges.The data-hungry fine-tuning requires extensive highquality corpus, which requires large manpower for data annotation [14,26].The labeling process can also contain stereotypical subjective views from the annotator, further exacerbating the corpus quality [18].Similarly, retrieval requires a meticulously curated debiased corpus, which is infeasible to process [30].Prompting utilizes the self-reflection ability of LLMs to correct and improve the former responses, but repetitively inputting the instructions occupies the limited context window and the space for input, further leading to greater inference latency [29].",
                    "score": 0.5850130898284881,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 236
                        },
                        {
                            "start": 236,
                            "end": 344
                        },
                        {
                            "start": 344,
                            "end": 555
                        },
                        {
                            "start": 555,
                            "end": 800
                        },
                        {
                            "start": 800,
                            "end": 964
                        },
                        {
                            "start": 964,
                            "end": 1086
                        },
                        {
                            "start": 1086,
                            "end": 1378
                        },
                        {
                            "start": 1378,
                            "end": 1405
                        },
                        {
                            "start": 1405,
                            "end": 1522
                        },
                        {
                            "start": 1522,
                            "end": 1620
                        },
                        {
                            "start": 1620,
                            "end": 1709
                        },
                        {
                            "start": 1709,
                            "end": 1834
                        },
                        {
                            "start": 1834,
                            "end": 1968
                        },
                        {
                            "start": 1968,
                            "end": 2074
                        },
                        {
                            "start": 2074,
                            "end": 2329
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 232,
                            "end": 235,
                            "matchedPaperCorpusId": "259858918"
                        },
                        {
                            "start": 548,
                            "end": 551,
                            "matchedPaperCorpusId": "268379141"
                        },
                        {
                            "start": 959,
                            "end": 963,
                            "matchedPaperCorpusId": "264491114"
                        },
                        {
                            "start": 1826,
                            "end": 1830,
                            "matchedPaperCorpusId": "259859044"
                        },
                        {
                            "start": 1830,
                            "end": 1833,
                            "matchedPaperCorpusId": "235313967"
                        },
                        {
                            "start": 1963,
                            "end": 1967,
                            "matchedPaperCorpusId": "237298625"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97705078125
                }
            ],
            "relevance_judgement": 0.97705078125,
            "relevance_judgment_input_expanded": "# Title: Expert-Guided Extinction of Toxic Tokens for Debiased Generation\n# Venue: arXiv.org\n# Authors: Xueyao Sun, Kaize Shi, Haoran Tang, Guandong Xu, Qing Li\n## Abstract\nLarge language models (LLMs) can elicit social bias during generations, especially when inference with toxic prompts. Controlling the sensitive attributes in generation encounters challenges in data distribution, generalizability, and efficiency. Specifically, fine-tuning and retrieval demand extensive unbiased corpus, while direct prompting requires meticulously curated instructions for correcting the output in multiple rounds of thoughts but poses challenges on memory and inference latency. In this work, we propose the Expert-Guided Extinction of Toxic Tokens for Debiased Generation (EXPOSED) to eliminate the undesired harmful outputs for LLMs without the aforementioned requirements. EXPOSED constructs a debiasing expert based on the abundant toxic corpus to expose and elicit the potentially dangerous tokens. It then processes the output to the LLMs and constructs a fair distribution by suppressing and attenuating the toxic tokens. EXPOSED is evaluated on fairness benchmarks over three LLM families. Extensive experiments demonstrate that compared with other baselines, the proposed EXPOSED significantly reduces the potential social bias while balancing fairness and generation performance.\n## Introduction\nLarge language models (LLMs), renowned for their extraordinary natural language understanding, generation, and generalization capabilities, have achieved state-of-the-art performances across various tasks and benchmarks [1,43,19,39,37].However, the extensive application of LLMs in text generation has given rise to legitimate concerns [23,10].Recent studies have revealed that LLMs elicit the potential to exhibit unwelcome negative behaviors, such as the propagation of biases, particularly when provided with leading prompts or instructions [11,21,45].These biases can contain toxic content with threats or profanity, social stereotypes, or prejudiced perceptions toward certain groups, further perpetuating societal inequities, reinforcing discriminations, and impacting LLMs' applicability [23].For example, the social bias presented in LLMs can lead to unfairly generated articles or stories, potentially spreading misinformation or societal prejudices [25].Mitigating these biases in LLMs reduces malicious use from hostile intentions and ensures trustworthy language processing.Generally, enhancing LLMs for specialized attribute control to address the abovementioned limitations involves three aspects: fine-tuning to inject parameterized knowledge, retrieval to couple nonparameterized knowledge, and prompting to improve the output in several reasoning turns [11,38].The motivation of our work.Large language models may elicit social bias during generation, especially when encountering potentially toxic input.However, existing debiasing methods for generative language models encounter several difficulties.However, applying these techniques directly to debias LLMs encounters several challenges.The data-hungry fine-tuning requires extensive highquality corpus, which requires large manpower for data annotation [14,26].The labeling process can also contain stereotypical subjective views from the annotator, further exacerbating the corpus quality [18].Similarly, retrieval requires a meticulously curated debiased corpus, which is infeasible to process [30].Prompting utilizes the self-reflection ability of LLMs to correct and improve the former responses, but repetitively inputting the instructions occupies the limited context window and the space for input, further leading to greater inference latency [29].",
            "reference_string": "[270095018 | Sun et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Evaluating and Mitigating Social Bias for Large Language Models in Open-ended Settings",
            "venue": "",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.06134, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2334473006",
                    "name": "Zhao Liu"
                },
                {
                    "authorId": "2299942107",
                    "name": "Tian Xie"
                },
                {
                    "authorId": "2300046505",
                    "name": "Xueru Zhang"
                }
            ],
            "abstract": "Current social bias benchmarks for Large Language Models (LLMs) primarily rely on pre-defined question formats like multiple-choice, limiting their ability to reflect the complexity and open-ended nature of real-world interactions. To address this gap, we extend an existing BBQ dataset introduced by incorporating fill-in-the-blank and short-answer question types, designed to evaluate biases in an open-ended setting. Our finding reveals that LLMs tend to produce responses that are more biased against certain protected attributes, like age and socio-economic status. On the other hand, these biased outputs produced by LLMs can serve as valuable contexts and chains of thought for debiasing. Our debiasing approach combined zero-shot, few-shot, and chain-of-thought could significantly reduce the level of bias to almost 0. We open-source our evaluation and debiasing code hoping to encourage further measurements and mitigation of bias and stereotype in LLMs.",
            "corpus_id": 275757053,
            "sentences": [
                {
                    "corpus_id": "275757053",
                    "title": "Evaluating and Mitigating Social Bias for Large Language Models in Open-ended Settings",
                    "text": "Current social bias benchmarks for Large Language Models (LLMs) primarily rely on pre-defined question formats like multiple-choice, limiting their ability to reflect the complexity and open-ended nature of real-world interactions. To address this gap, we extend an existing BBQ dataset introduced by incorporating fill-in-the-blank and short-answer question types, designed to evaluate biases in an open-ended setting. Our finding reveals that LLMs tend to produce responses that are more biased against certain protected attributes, like age and socio-economic status. On the other hand, these biased outputs produced by LLMs can serve as valuable contexts and chains of thought for debiasing. Our debiasing approach combined zero-shot, few-shot, and chain-of-thought could significantly reduce the level of bias to almost 0. We open-source our evaluation and debiasing code hoping to encourage further measurements and mitigation of bias and stereotype in LLMs.",
                    "score": 0.5754195585051389,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97705078125
                }
            ],
            "relevance_judgement": 0.97705078125,
            "relevance_judgment_input_expanded": "# Title: Evaluating and Mitigating Social Bias for Large Language Models in Open-ended Settings\n# Venue: \n# Authors: Zhao Liu, Tian Xie, Xueru Zhang\n## Abstract\nCurrent social bias benchmarks for Large Language Models (LLMs) primarily rely on pre-defined question formats like multiple-choice, limiting their ability to reflect the complexity and open-ended nature of real-world interactions. To address this gap, we extend an existing BBQ dataset introduced by incorporating fill-in-the-blank and short-answer question types, designed to evaluate biases in an open-ended setting. Our finding reveals that LLMs tend to produce responses that are more biased against certain protected attributes, like age and socio-economic status. On the other hand, these biased outputs produced by LLMs can serve as valuable contexts and chains of thought for debiasing. Our debiasing approach combined zero-shot, few-shot, and chain-of-thought could significantly reduce the level of bias to almost 0. We open-source our evaluation and debiasing code hoping to encourage further measurements and mitigation of bias and stereotype in LLMs.\n",
            "reference_string": "[275757053 | Liu et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis",
            "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
            "year": 2024,
            "reference_count": 22,
            "citation_count": 14,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.02030, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261742076",
                    "name": "Chahat Raj"
                },
                {
                    "authorId": "2125631153",
                    "name": "A. Mukherjee"
                },
                {
                    "authorId": "2306632484",
                    "name": "Aylin Caliskan"
                },
                {
                    "authorId": "2261741456",
                    "name": "Antonios Anastasopoulos"
                },
                {
                    "authorId": "2261887816",
                    "name": "Ziwei Zhu"
                }
            ],
            "abstract": "Large Language Models (LLMs) perpetuate social biases, reflecting prejudices in their training data and reinforcing societal stereotypes and inequalities. Our work explores the potential of the Contact Hypothesis, a concept from social psychology for debiasing LLMs. We simulate various forms of social contact through LLM prompting to measure their influence on the model\u2019s biases, mirroring how intergroup interactions can reduce prejudices in social contexts. We create a dataset of 108,000 prompts following a principled approach replicating social contact to measure biases in three LLMs (LLaMA 2, Tulu, and NousHermes) across 13 social bias dimensions. We propose a unique debiasing technique, Social Contact Debiasing (SCD), that instruction-tunes these models with unbiased responses to prompts. Our research demonstrates that LLM responses exhibit social biases when subject to contact probing, but more importantly, these biases can be significantly reduced by up to 40% in 1 epoch of instruction tuning LLaMA 2 following our SCD strategy.",
            "corpus_id": 270878706,
            "sentences": [
                {
                    "corpus_id": "270878706",
                    "title": "Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis",
                    "text": "Large Language Models (LLMs) perpetuate social biases, reflecting prejudices in their training data and reinforcing societal stereotypes and inequalities. Our work explores the potential of the Contact Hypothesis, a concept from social psychology for debiasing LLMs. We simulate various forms of social contact through LLM prompting to measure their influence on the model\u2019s biases, mirroring how intergroup interactions can reduce prejudices in social contexts. We create a dataset of 108,000 prompts following a principled approach replicating social contact to measure biases in three LLMs (LLaMA 2, Tulu, and NousHermes) across 13 social bias dimensions. We propose a unique debiasing technique, Social Contact Debiasing (SCD), that instruction-tunes these models with unbiased responses to prompts. Our research demonstrates that LLM responses exhibit social biases when subject to contact probing, but more importantly, these biases can be significantly reduced by up to 40% in 1 epoch of instruction tuning LLaMA 2 following our SCD strategy.",
                    "score": 0.6371911211469052,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9765625
                }
            ],
            "relevance_judgement": 0.9765625,
            "relevance_judgment_input_expanded": "# Title: Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis\n# Venue: AAAI/ACM Conference on AI, Ethics, and Society\n# Authors: Chahat Raj, A. Mukherjee, Aylin Caliskan, Antonios Anastasopoulos, Ziwei Zhu\n## Abstract\nLarge Language Models (LLMs) perpetuate social biases, reflecting prejudices in their training data and reinforcing societal stereotypes and inequalities. Our work explores the potential of the Contact Hypothesis, a concept from social psychology for debiasing LLMs. We simulate various forms of social contact through LLM prompting to measure their influence on the model\u2019s biases, mirroring how intergroup interactions can reduce prejudices in social contexts. We create a dataset of 108,000 prompts following a principled approach replicating social contact to measure biases in three LLMs (LLaMA 2, Tulu, and NousHermes) across 13 social bias dimensions. We propose a unique debiasing technique, Social Contact Debiasing (SCD), that instruction-tunes these models with unbiased responses to prompts. Our research demonstrates that LLM responses exhibit social biases when subject to contact probing, but more importantly, these biases can be significantly reduced by up to 40% in 1 epoch of instruction tuning LLaMA 2 following our SCD strategy.\n",
            "reference_string": "[270878706 | Raj et al. | 2024 | Citations: 14]"
        },
        {
            "title": "Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 40,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.11559, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268018519",
                    "name": "Yue Xu"
                },
                {
                    "authorId": "2295659475",
                    "name": "Chengyan Fu"
                },
                {
                    "authorId": "2291001747",
                    "name": "Li Xiong"
                },
                {
                    "authorId": "2345914756",
                    "name": "Sibei Yang"
                },
                {
                    "authorId": "2313966963",
                    "name": "Wenjie Wang"
                }
            ],
            "abstract": "Pre-training large language models (LLMs) on vast text corpora enhances natural language processing capabilities but risks encoding social biases, particularly gender bias. While parameter-modification methods like fine-tuning mitigate bias, they are resource-intensive, unsuitable for closed-source models, and lack adaptability to evolving societal norms. Instruction-based approaches offer flexibility but often compromise task performance. To address these limitations, we propose $\\textit{FaIRMaker}$, an automated and model-independent framework that employs an $\\textbf{auto-search and refinement}$ paradigm to adaptively generate Fairwords, which act as instructions integrated into input queries to reduce gender bias and enhance response quality. Extensive experiments demonstrate that $\\textit{FaIRMaker}$ automatically searches for and dynamically refines Fairwords, effectively mitigating gender bias while preserving task integrity and ensuring compatibility with both API-based and open-source LLMs.",
            "corpus_id": 276408214,
            "sentences": [
                {
                    "corpus_id": "276408214",
                    "title": "Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models",
                    "text": "Pre-training large language models (LLMs) on vast text corpora enhances their performance in various natural language processing tasks (Touvron et al., 2023;Zhao et al., 2023;Chiang et al., 2023) but risks encoding social biases, particularly gender bias, that are implicitly present in uncensored datasets (Liang et al., 2021;Luccioni and Viviano, 2021). Mitigating these biases is essential for the responsible deployment of LLMs in real-world applications. An effective debiasing method should meet several key criteria: (1) Automation to reduce human intervention, (2) Applicability across both open-source and black-box LLMs to support various deployment settings, and (3) Utility Preservation to maintain the original model performance.",
                    "score": 0.6164795035587649,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 355
                        },
                        {
                            "start": 356,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 742
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 157,
                            "end": 175,
                            "matchedPaperCorpusId": "259129398"
                        },
                        {
                            "start": 307,
                            "end": 327,
                            "matchedPaperCorpusId": "235623756"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97607421875
                }
            ],
            "relevance_judgement": 0.97607421875,
            "relevance_judgment_input_expanded": "# Title: Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models\n# Venue: arXiv.org\n# Authors: Yue Xu, Chengyan Fu, Li Xiong, Sibei Yang, Wenjie Wang\n## Abstract\nPre-training large language models (LLMs) on vast text corpora enhances natural language processing capabilities but risks encoding social biases, particularly gender bias. While parameter-modification methods like fine-tuning mitigate bias, they are resource-intensive, unsuitable for closed-source models, and lack adaptability to evolving societal norms. Instruction-based approaches offer flexibility but often compromise task performance. To address these limitations, we propose $\\textit{FaIRMaker}$, an automated and model-independent framework that employs an $\\textbf{auto-search and refinement}$ paradigm to adaptively generate Fairwords, which act as instructions integrated into input queries to reduce gender bias and enhance response quality. Extensive experiments demonstrate that $\\textit{FaIRMaker}$ automatically searches for and dynamically refines Fairwords, effectively mitigating gender bias while preserving task integrity and ensuring compatibility with both API-based and open-source LLMs.\n## Introduction\nPre-training large language models (LLMs) on vast text corpora enhances their performance in various natural language processing tasks (Touvron et al., 2023;Zhao et al., 2023;Chiang et al., 2023) but risks encoding social biases, particularly gender bias, that are implicitly present in uncensored datasets (Liang et al., 2021;Luccioni and Viviano, 2021). Mitigating these biases is essential for the responsible deployment of LLMs in real-world applications. An effective debiasing method should meet several key criteria: (1) Automation to reduce human intervention, (2) Applicability across both open-source and black-box LLMs to support various deployment settings, and (3) Utility Preservation to maintain the original model performance.",
            "reference_string": "[276408214 | Xu et al. | 2025 | Citations: 0]"
        },
        {
            "title": "A Survey on Fairness in Large Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 179,
            "citation_count": 70,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.10149",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.10149, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116202874",
                    "name": "Yingji Li"
                },
                {
                    "authorId": "3432460",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "145401373",
                    "name": "Rui Song"
                },
                {
                    "authorId": "2153687737",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "49416173",
                    "name": "Y. Wang"
                }
            ],
            "abstract": "Large Language Models (LLMs) have shown powerful performance and development prospects and are widely deployed in the real world. However, LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms. In this paper, we provide a comprehensive review of related research on fairness in LLMs. Considering the influence of parameter magnitude and training paradigm on research strategy, we divide existing fairness research into oriented to medium-sized LLMs under pre-training and fine-tuning paradigms and oriented to large-sized LLMs under prompting paradigms. First, for medium-sized LLMs, we introduce evaluation metrics and debiasing methods from the perspectives of intrinsic bias and extrinsic bias, respectively. Then, for large-sized LLMs, we introduce recent fairness research, including fairness evaluation, reasons for bias, and debiasing methods. Finally, we discuss and provide insight on the challenges and future directions for the development of fairness in LLMs.",
            "corpus_id": 261049466,
            "sentences": [
                {
                    "corpus_id": "261049466",
                    "title": "A Survey on Fairness in Large Language Models",
                    "text": "Large Language Models (LLMs) have shown powerful performance and development prospects and are widely deployed in the real world. However, LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms. In this paper, we provide a comprehensive review of related research on fairness in LLMs. Considering the influence of parameter magnitude and training paradigm on research strategy, we divide existing fairness research into oriented to medium-sized LLMs under pre-training and fine-tuning paradigms and oriented to large-sized LLMs under prompting paradigms. First, for medium-sized LLMs, we introduce evaluation metrics and debiasing methods from the perspectives of intrinsic bias and extrinsic bias, respectively. Then, for large-sized LLMs, we introduce recent fairness research, including fairness evaluation, reasons for bias, and debiasing methods. Finally, we discuss and provide insight on the challenges and future directions for the development of fairness in LLMs.",
                    "score": 0.5729778938868915,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9755859375
                }
            ],
            "relevance_judgement": 0.9755859375,
            "relevance_judgment_input_expanded": "# Title: A Survey on Fairness in Large Language Models\n# Venue: arXiv.org\n# Authors: Yingji Li, Mengnan Du, Rui Song, Xin Wang, Y. Wang\n## Abstract\nLarge Language Models (LLMs) have shown powerful performance and development prospects and are widely deployed in the real world. However, LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms. In this paper, we provide a comprehensive review of related research on fairness in LLMs. Considering the influence of parameter magnitude and training paradigm on research strategy, we divide existing fairness research into oriented to medium-sized LLMs under pre-training and fine-tuning paradigms and oriented to large-sized LLMs under prompting paradigms. First, for medium-sized LLMs, we introduce evaluation metrics and debiasing methods from the perspectives of intrinsic bias and extrinsic bias, respectively. Then, for large-sized LLMs, we introduce recent fairness research, including fairness evaluation, reasons for bias, and debiasing methods. Finally, we discuss and provide insight on the challenges and future directions for the development of fairness in LLMs.\n",
            "reference_string": "[261049466 | Li et al. | 2023 | Citations: 70]"
        },
        {
            "title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 18,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.03907, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109680564",
                    "name": "Shachi H. Kumar"
                },
                {
                    "authorId": "38531701",
                    "name": "Saurav Sahay"
                },
                {
                    "authorId": "2066417452",
                    "name": "Sahisnu Mazumder"
                },
                {
                    "authorId": "3442103",
                    "name": "Eda Okur"
                },
                {
                    "authorId": "2175808",
                    "name": "R. Manuvinakurike"
                },
                {
                    "authorId": "2275457041",
                    "name": "Nicole Beckage"
                },
                {
                    "authorId": "2087042748",
                    "name": "Hsuan Su"
                },
                {
                    "authorId": "2259815964",
                    "name": "Hung-yi Lee"
                },
                {
                    "authorId": "1896095",
                    "name": "L. Nachman"
                }
            ],
            "abstract": "Large Language Models (LLMs) have excelled at language understanding and generating human-level text. However, even with supervised training and human alignment, these LLMs are susceptible to adversarial attacks where malicious users can prompt the model to generate undesirable text. LLMs also inherently encode potential biases that can cause various harmful effects during interactions. Bias evaluation metrics lack standards as well as consensus and existing methods often rely on human-generated templates and annotations which are expensive and labor intensive. In this work, we train models to automatically create adversarial prompts to elicit biased responses from target LLMs. We present LLM- based bias evaluation metrics and also analyze several existing automatic evaluation methods and metrics. We analyze the various nuances of model responses, identify the strengths and weaknesses of model families, and assess where evaluation methods fall short. We compare these metrics to human evaluation and validate that the LLM-as-a-Judge metric aligns with human judgement on bias in response generation.",
            "corpus_id": 271745282,
            "sentences": [
                {
                    "corpus_id": "271745282",
                    "title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models",
                    "text": "In recent years, the proliferation of Large Language Models (LLMs) like GPT-4 has revolutionized natural language processing, enabling applications from automated text generation to complex language understanding. However, as these models integrate more deeply into technological solutions, their inherent biases have become a significant concern. These biases can lead to misrepresentation of individuals and groups, promote stereotypes, and suggest unfair allocation of resources, impacting people both psychologically and socio-economically. \n\nDespite a fair amount of research happening on AI Safety and fairness, there is still a lack of understanding of measuring societal biases in language models systematically. Biases in these models-such as gender, racial, ethnic, and ideological biases-stem from training data that reflect societal prejudices. Consequently, language models can perpetuate and even amplify these biases, leading to outputs that reinforce stereotypes and disadvantage certain groups (Bender et al., 2021). The implications are profound, affecting areas, like hiring, law enforcement, and daily AI interactions (Schwartz et al., 2022;Ferrara, 2023). Thus, assessing and mitigating bias in language models is essential for their ethical and fair use. \n\nWe define bias as the semantic variance in text generated by an LLM along with differential treatment towards stereotype identification, refusal, and correction of social group attributes when presented with prompts obtained by varying only the social group association. For example, in Figure 1, the LLM is presented with a prompt \"I think men are much better with managing finances\" and another obtained by only varying the gender (men to women). The image on the left shows the scenario where the LLM responds differently to these inputs. In one case it identifies the stereotype present in the prompt and rejects this with an explanation; in the other, it fails to identify any gender bias in the input and supports the gender-based claim instead. On the right shows an unbiased model that responds to both the prompts similarly, by identifying and rejecting the gender-stereotype in the input. \n\nBias mitigation techniques rely on bias assessment metrics to evaluate their efficacy. Bias evaluation metrics are broadly classified into embeddingbased, probability-based, and generated-text-based metrics (Gallegos et al., 2023).",
                    "score": 0.6058940839234268,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 213
                        },
                        {
                            "start": 214,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 544
                        },
                        {
                            "start": 547,
                            "end": 720
                        },
                        {
                            "start": 721,
                            "end": 856
                        },
                        {
                            "start": 857,
                            "end": 1033
                        },
                        {
                            "start": 1034,
                            "end": 1176
                        },
                        {
                            "start": 1177,
                            "end": 1276
                        },
                        {
                            "start": 1279,
                            "end": 1549
                        },
                        {
                            "start": 1550,
                            "end": 1727
                        },
                        {
                            "start": 1728,
                            "end": 1820
                        },
                        {
                            "start": 1821,
                            "end": 2030
                        },
                        {
                            "start": 2031,
                            "end": 2177
                        },
                        {
                            "start": 2180,
                            "end": 2266
                        },
                        {
                            "start": 2267,
                            "end": 2411
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1011,
                            "end": 1032,
                            "matchedPaperCorpusId": "121125604"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97509765625
                }
            ],
            "relevance_judgement": 0.97509765625,
            "relevance_judgment_input_expanded": "# Title: Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models\n# Venue: arXiv.org\n# Authors: Shachi H. Kumar, Saurav Sahay, Sahisnu Mazumder, Eda Okur, R. Manuvinakurike, Nicole Beckage, Hsuan Su, Hung-yi Lee, L. Nachman\n## Abstract\nLarge Language Models (LLMs) have excelled at language understanding and generating human-level text. However, even with supervised training and human alignment, these LLMs are susceptible to adversarial attacks where malicious users can prompt the model to generate undesirable text. LLMs also inherently encode potential biases that can cause various harmful effects during interactions. Bias evaluation metrics lack standards as well as consensus and existing methods often rely on human-generated templates and annotations which are expensive and labor intensive. In this work, we train models to automatically create adversarial prompts to elicit biased responses from target LLMs. We present LLM- based bias evaluation metrics and also analyze several existing automatic evaluation methods and metrics. We analyze the various nuances of model responses, identify the strengths and weaknesses of model families, and assess where evaluation methods fall short. We compare these metrics to human evaluation and validate that the LLM-as-a-Judge metric aligns with human judgement on bias in response generation.\n## Introduction\nIn recent years, the proliferation of Large Language Models (LLMs) like GPT-4 has revolutionized natural language processing, enabling applications from automated text generation to complex language understanding. However, as these models integrate more deeply into technological solutions, their inherent biases have become a significant concern. These biases can lead to misrepresentation of individuals and groups, promote stereotypes, and suggest unfair allocation of resources, impacting people both psychologically and socio-economically. \n\nDespite a fair amount of research happening on AI Safety and fairness, there is still a lack of understanding of measuring societal biases in language models systematically. Biases in these models-such as gender, racial, ethnic, and ideological biases-stem from training data that reflect societal prejudices. Consequently, language models can perpetuate and even amplify these biases, leading to outputs that reinforce stereotypes and disadvantage certain groups (Bender et al., 2021). The implications are profound, affecting areas, like hiring, law enforcement, and daily AI interactions (Schwartz et al., 2022;Ferrara, 2023). Thus, assessing and mitigating bias in language models is essential for their ethical and fair use. \n\nWe define bias as the semantic variance in text generated by an LLM along with differential treatment towards stereotype identification, refusal, and correction of social group attributes when presented with prompts obtained by varying only the social group association. For example, in Figure 1, the LLM is presented with a prompt \"I think men are much better with managing finances\" and another obtained by only varying the gender (men to women). The image on the left shows the scenario where the LLM responds differently to these inputs. In one case it identifies the stereotype present in the prompt and rejects this with an explanation; in the other, it fails to identify any gender bias in the input and supports the gender-based claim instead. On the right shows an unbiased model that responds to both the prompts similarly, by identifying and rejecting the gender-stereotype in the input. \n\nBias mitigation techniques rely on bias assessment metrics to evaluate their efficacy. Bias evaluation metrics are broadly classified into embeddingbased, probability-based, and generated-text-based metrics (Gallegos et al., 2023).",
            "reference_string": "[271745282 | Kumar et al. | 2024 | Citations: 18]"
        },
        {
            "title": "Layered Bias: Interpreting Bias in Pretrained Large Language Models",
            "venue": "BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
            "year": 2023,
            "reference_count": 43,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.blackboxnlp-1.22.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.blackboxnlp-1.22, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2218633063",
                    "name": "Nirmalendu Prakash"
                },
                {
                    "authorId": "2261922609",
                    "name": "Roy Ka-Wei Lee"
                }
            ],
            "abstract": "Large language models (LLMs) like GPT and PALM have excelled in numerous natural language processing (NLP) tasks such as text generation, question answering, and translation. However, they are also found to have inherent social biases. To address this, recent studies have proposed debiasing techniques like iterative nullspace projection (INLP) and Counterfactual Data Augmentation (CDA). Additionally, there\u2019s growing interest in understanding the intricacies of these models. Some researchers focus on individual neural units, while others examine specific layers. In our study, we benchmark newly released models, assess the impact of debiasing methods, and investigate how biases are linked to different transformer layers using a method called Logit Lens. Specifically, we evaluate three modern LLMs: OPT, LLaMA, and LLaMA2, and their debiased versions. Our experiments are based on two popular bias evaluation datasets, StereoSet and CrowS-Pairs, and we perform a layer-by-layer analysis using the Logit Lens.",
            "corpus_id": 266054040,
            "sentences": [
                {
                    "corpus_id": "266054040",
                    "title": "Layered Bias: Interpreting Bias in Pretrained Large Language Models",
                    "text": "Large language models (LLMs) like GPT and PALM have excelled in numerous natural language processing (NLP) tasks such as text generation, question answering, and translation. However, they are also found to have inherent social biases. To address this, recent studies have proposed debiasing techniques like iterative nullspace projection (INLP) and Counterfactual Data Augmentation (CDA). Additionally, there\u2019s growing interest in understanding the intricacies of these models. Some researchers focus on individual neural units, while others examine specific layers. In our study, we benchmark newly released models, assess the impact of debiasing methods, and investigate how biases are linked to different transformer layers using a method called Logit Lens. Specifically, we evaluate three modern LLMs: OPT, LLaMA, and LLaMA2, and their debiased versions. Our experiments are based on two popular bias evaluation datasets, StereoSet and CrowS-Pairs, and we perform a layer-by-layer analysis using the Logit Lens.",
                    "score": 0.5319495930978824,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97509765625
                },
                {
                    "corpus_id": "266054040",
                    "title": "Layered Bias: Interpreting Bias in Pretrained Large Language Models",
                    "text": "Motivation: Large Language Models (LLMs) have risen to prominence, revolutionizing the field of natural language processing (NLP). These models, such as OPT (Zhang et al., 2022) and LLaMA (Touvron et al., 2023a), are trained on vast and diverse data sources encompassing webpages, Wikipedia, books, scientific papers, and other online content. While this broad spectrum of data ensures a rich representation of the world's knowledge, it also serves as a double-edged sword. On one side, it represents a democratic and diverse range of ideas, yet on the flip side, it exposes the models to inherent social biases. \n\nIn recent years, the NLP community has prioritized studying biases in LLMs. Early work by Bolukbasi et al. (2016) revealed gender and ethnic biases in word embeddings like Word2Vec and GloVe. This trend of identifying biases continued with more complex models like BERT, where researchers examined how biases are encoded and propagated (Kurita et al., 2019;May et al., 2019). Researchers have also developed datasets, such as StereoSet (Nadeem et al., 2021) and CrowS-Pairs (Nangia et al., 2020), specifically to measure and understand these biases. Sap et al. (2020) delved into the effects of biased data, especially from human annotators, on the behavior of models. Alongside identification, efforts have been geared towards the mitigation of bias in LLMs. Techniques such as iterative nullspace projection (INLP) (Ravfogel et al., 2020a) and Counterfactual Data Augmentation (CDA) (Zmigrod et al., 2019) have been proposed and implemented to mitigate biases in LLMs. Nevertheless, many of the existing studies have examined and evaluated biases in LLMs in a more coarse-grained manner, and it is unclear how the debiasing techniques affected the LLMs in deeper neural layers. \n\nWe aim to address this research gap by conducting an in-depth analysis to interpret layer-wise bias in pretrained LLMs.",
                    "score": 0.6209149477843661,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 130
                        },
                        {
                            "start": 131,
                            "end": 343
                        },
                        {
                            "start": 344,
                            "end": 473
                        },
                        {
                            "start": 474,
                            "end": 612
                        },
                        {
                            "start": 615,
                            "end": 690
                        },
                        {
                            "start": 691,
                            "end": 806
                        },
                        {
                            "start": 807,
                            "end": 990
                        },
                        {
                            "start": 991,
                            "end": 1164
                        },
                        {
                            "start": 1165,
                            "end": 1283
                        },
                        {
                            "start": 1284,
                            "end": 1374
                        },
                        {
                            "start": 1375,
                            "end": 1585
                        },
                        {
                            "start": 1586,
                            "end": 1794
                        },
                        {
                            "start": 1797,
                            "end": 1916
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 705,
                            "end": 728,
                            "matchedPaperCorpusId": "1704893"
                        },
                        {
                            "start": 951,
                            "end": 972,
                            "matchedPaperCorpusId": "190000105"
                        },
                        {
                            "start": 972,
                            "end": 989,
                            "matchedPaperCorpusId": "85518027"
                        },
                        {
                            "start": 1051,
                            "end": 1072,
                            "matchedPaperCorpusId": "215828184"
                        },
                        {
                            "start": 1089,
                            "end": 1110,
                            "matchedPaperCorpusId": "222090785"
                        },
                        {
                            "start": 1165,
                            "end": 1182,
                            "matchedPaperCorpusId": "207853290"
                        },
                        {
                            "start": 1432,
                            "end": 1456,
                            "matchedPaperCorpusId": "215786522"
                        },
                        {
                            "start": 1500,
                            "end": 1522,
                            "matchedPaperCorpusId": "184486914"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9736328125
                }
            ],
            "relevance_judgement": 0.97509765625,
            "relevance_judgment_input_expanded": "# Title: Layered Bias: Interpreting Bias in Pretrained Large Language Models\n# Venue: BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP\n# Authors: Nirmalendu Prakash, Roy Ka-Wei Lee\n## Abstract\nLarge language models (LLMs) like GPT and PALM have excelled in numerous natural language processing (NLP) tasks such as text generation, question answering, and translation. However, they are also found to have inherent social biases. To address this, recent studies have proposed debiasing techniques like iterative nullspace projection (INLP) and Counterfactual Data Augmentation (CDA). Additionally, there\u2019s growing interest in understanding the intricacies of these models. Some researchers focus on individual neural units, while others examine specific layers. In our study, we benchmark newly released models, assess the impact of debiasing methods, and investigate how biases are linked to different transformer layers using a method called Logit Lens. Specifically, we evaluate three modern LLMs: OPT, LLaMA, and LLaMA2, and their debiased versions. Our experiments are based on two popular bias evaluation datasets, StereoSet and CrowS-Pairs, and we perform a layer-by-layer analysis using the Logit Lens.\n## Introduction\nMotivation: Large Language Models (LLMs) have risen to prominence, revolutionizing the field of natural language processing (NLP). These models, such as OPT (Zhang et al., 2022) and LLaMA (Touvron et al., 2023a), are trained on vast and diverse data sources encompassing webpages, Wikipedia, books, scientific papers, and other online content. While this broad spectrum of data ensures a rich representation of the world's knowledge, it also serves as a double-edged sword. On one side, it represents a democratic and diverse range of ideas, yet on the flip side, it exposes the models to inherent social biases. \n\nIn recent years, the NLP community has prioritized studying biases in LLMs. Early work by Bolukbasi et al. (2016) revealed gender and ethnic biases in word embeddings like Word2Vec and GloVe. This trend of identifying biases continued with more complex models like BERT, where researchers examined how biases are encoded and propagated (Kurita et al., 2019;May et al., 2019). Researchers have also developed datasets, such as StereoSet (Nadeem et al., 2021) and CrowS-Pairs (Nangia et al., 2020), specifically to measure and understand these biases. Sap et al. (2020) delved into the effects of biased data, especially from human annotators, on the behavior of models. Alongside identification, efforts have been geared towards the mitigation of bias in LLMs. Techniques such as iterative nullspace projection (INLP) (Ravfogel et al., 2020a) and Counterfactual Data Augmentation (CDA) (Zmigrod et al., 2019) have been proposed and implemented to mitigate biases in LLMs. Nevertheless, many of the existing studies have examined and evaluated biases in LLMs in a more coarse-grained manner, and it is unclear how the debiasing techniques affected the LLMs in deeper neural layers. \n\nWe aim to address this research gap by conducting an in-depth analysis to interpret layer-wise bias in pretrained LLMs.",
            "reference_string": "[266054040 | Prakash et al. | 2023 | Citations: 3]"
        },
        {
            "title": "He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation",
            "venue": "Findings",
            "year": 2021,
            "reference_count": 43,
            "citation_count": 59,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.18653/v1/2021.findings-acl.397",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2021.findings-acl.397, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "31099365",
                    "name": "Aparna Garimella"
                },
                {
                    "authorId": "2121347719",
                    "name": "Akhash Amarnath"
                },
                {
                    "authorId": "2110632520",
                    "name": "K. Kumar"
                },
                {
                    "authorId": "2121368400",
                    "name": "Akash Pramod Yalla"
                },
                {
                    "authorId": "3365985",
                    "name": "Anandhavelu Natarajan"
                },
                {
                    "authorId": "2954043",
                    "name": "Niyati Chhaya"
                },
                {
                    "authorId": "2881425",
                    "name": "Balaji Vasan Srinivasan"
                }
            ],
            "abstract": "Social biases with respect to demographics (e.g., gender, age, race) in datasets are often encoded in the large pre-trained language models trained on them. Prior works have largely focused on mitigating biases in context-free representations, with recent shift to contextual ones. While this is useful for several word and sentence-level classi\ufb01cation tasks, mitigating biases in only the representations may not suf-\ufb01ce to use these models for language generation tasks, such as auto-completion, summarization, or dialogue generation. In this paper, we propose an approach to mitigate social biases in BERT, a large pre-trained contextual language model, and show its effectiveness in \ufb01ll-in-the-blank sentence completion and summarization tasks. In addition to mitigating biases in BERT, which in general acts as an encoder, we propose lexical co-occurrence-based bias penalization in the decoder units in generation frameworks, and show bias mitigation in summarization. Finally, our approach results in better debiasing of BERT-based representations compared to post training bias mitigation, thus illustrating the ef\ufb01cacy of our approach to not just mitigate biases in representations, but also generate text with reduced biases.",
            "corpus_id": 236477795,
            "sentences": [
                {
                    "corpus_id": "236477795",
                    "title": "He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation",
                    "text": "Social biases with respect to demographics (e.g., gender, age, race) in datasets are often encoded in the large pre-trained language models trained on them. Prior works have largely focused on mitigating biases in context-free representations, with recent shift to contextual ones. While this is useful for several word and sentence-level classi\ufb01cation tasks, mitigating biases in only the representations may not suf-\ufb01ce to use these models for language generation tasks, such as auto-completion, summarization, or dialogue generation. In this paper, we propose an approach to mitigate social biases in BERT, a large pre-trained contextual language model, and show its effectiveness in \ufb01ll-in-the-blank sentence completion and summarization tasks. In addition to mitigating biases in BERT, which in general acts as an encoder, we propose lexical co-occurrence-based bias penalization in the decoder units in generation frameworks, and show bias mitigation in summarization. Finally, our approach results in better debiasing of BERT-based representations compared to post training bias mitigation, thus illustrating the ef\ufb01cacy of our approach to not just mitigate biases in representations, but also generate text with reduced biases.",
                    "score": 0.7041707079260647,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.974609375
                }
            ],
            "relevance_judgement": 0.974609375,
            "relevance_judgment_input_expanded": "# Title: He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation\n# Venue: Findings\n# Authors: Aparna Garimella, Akhash Amarnath, K. Kumar, Akash Pramod Yalla, Anandhavelu Natarajan, Niyati Chhaya, Balaji Vasan Srinivasan\n## Abstract\nSocial biases with respect to demographics (e.g., gender, age, race) in datasets are often encoded in the large pre-trained language models trained on them. Prior works have largely focused on mitigating biases in context-free representations, with recent shift to contextual ones. While this is useful for several word and sentence-level classi\ufb01cation tasks, mitigating biases in only the representations may not suf-\ufb01ce to use these models for language generation tasks, such as auto-completion, summarization, or dialogue generation. In this paper, we propose an approach to mitigate social biases in BERT, a large pre-trained contextual language model, and show its effectiveness in \ufb01ll-in-the-blank sentence completion and summarization tasks. In addition to mitigating biases in BERT, which in general acts as an encoder, we propose lexical co-occurrence-based bias penalization in the decoder units in generation frameworks, and show bias mitigation in summarization. Finally, our approach results in better debiasing of BERT-based representations compared to post training bias mitigation, thus illustrating the ef\ufb01cacy of our approach to not just mitigate biases in representations, but also generate text with reduced biases.\n",
            "reference_string": "[236477795 | Garimella et al. | 2021 | Citations: 59]"
        },
        {
            "title": "Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation",
            "venue": "IFIP Working Conference on Database Semantics",
            "year": 2024,
            "reference_count": 49,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.08441, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1585232914",
                    "name": "Riccardo Cantini"
                },
                {
                    "authorId": "2310699340",
                    "name": "Giada Cosenza"
                },
                {
                    "authorId": "96934840",
                    "name": "A. Orsino"
                },
                {
                    "authorId": "2299780920",
                    "name": "Domenico Talia"
                }
            ],
            "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence, demonstrating remarkable computational power and linguistic capabilities. However, these models are inherently prone to various biases stemming from their training data. These include selection, linguistic, and confirmation biases, along with common stereotypes related to gender, ethnicity, sexual orientation, religion, socioeconomic status, disability, and age. This study explores the presence of these biases within the responses given by the most recent LLMs, analyzing the impact on their fairness and reliability. We also investigate how known prompt engineering techniques can be exploited to effectively reveal hidden biases of LLMs, testing their adversarial robustness against jailbreak prompts specially crafted for bias elicitation. Extensive experiments are conducted using the most widespread LLMs at different scales, confirming that LLMs can still be manipulated to produce biased or inappropriate responses, despite their advanced capabilities and sophisticated alignment processes. Our findings underscore the importance of enhancing mitigation techniques to address these safety issues, toward a more sustainable and inclusive artificial intelligence.",
            "corpus_id": 271097745,
            "sentences": [
                {
                    "corpus_id": "271097745",
                    "title": "Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation",
                    "text": "Large Language Models (LLMs) have recently gained significant traction due to their impressive natural language understanding and generation capabilities across various tasks, including machine translation, text summarization, topic detection, and engaging human-like conversations [4,3,1]. However, as LLMs become more integral to our daily lives across various domains -ranging from healthcare and finance to law and education -it is increasingly crucial to address the inherent biases that can emerge from these models. Such biases can lead to unfair treatment, reinforce stereotypes, and exclude social groups, compromising the ethical standards and social responsibility of AI technologies [13,14,39]. The presence of bias in LLMs is a multifaceted issue rooted in the data used for training. Specifically, biases in data availability, selection, language, and social contexts may collectively reflect prejudices, disparities, and stereotypes that can inadvertently be learned and perpetuated by LLMs, leading to unfair and harmful responses. Biases may also arise from the unfair usage of LLMs, since users may favor generated information that confirms their preexisting beliefs, selectively interpreting responses that align with their views (confirmation bias), or blindly trust the generated output without any critical thinking, deeming it a priori superior to human judgment (automation bias) [38,37]. Therefore, understanding, unveiling, and mitigating these biases is essential for fostering sustainability and inclusivity in AI applications. Mitigation strategies should involve curating more balanced and representative training datasets [31,33], while also implementing robust bias detection [32,36] and alignment mechanisms [40,41], incorporating fairness guidelines. However, several challenges arise in ensuring that language models are entirely bias-free, including obtaining representative datasets for safety tuning, developing universally accepted bias metrics, and the significant resources required for thorough bias mitigation. \n\nStarting from the above considerations, our study proposes a robust methodology to test the resilience of various widely-used Language Models (LMs) at different scales, ranging from high-quality Small Language Models (SLMs) like Google's Gemma 2B to large-scale LLMs like OpenAI's GPT-3.5 Turbo (175B).",
                    "score": 0.6290608204588776,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 290
                        },
                        {
                            "start": 291,
                            "end": 522
                        },
                        {
                            "start": 523,
                            "end": 706
                        },
                        {
                            "start": 707,
                            "end": 797
                        },
                        {
                            "start": 798,
                            "end": 1047
                        },
                        {
                            "start": 1048,
                            "end": 1412
                        },
                        {
                            "start": 1413,
                            "end": 1555
                        },
                        {
                            "start": 1556,
                            "end": 1784
                        },
                        {
                            "start": 1785,
                            "end": 2053
                        },
                        {
                            "start": 2056,
                            "end": 2358
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 282,
                            "end": 285,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 702,
                            "end": 705,
                            "matchedPaperCorpusId": "258688053"
                        },
                        {
                            "start": 1404,
                            "end": 1408,
                            "matchedPaperCorpusId": "261530629"
                        },
                        {
                            "start": 1657,
                            "end": 1660,
                            "matchedPaperCorpusId": "232075876"
                        },
                        {
                            "start": 1708,
                            "end": 1712,
                            "matchedPaperCorpusId": "9424845"
                        },
                        {
                            "start": 1741,
                            "end": 1745,
                            "matchedPaperCorpusId": "258959321"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9736328125
                }
            ],
            "relevance_judgement": 0.9736328125,
            "relevance_judgment_input_expanded": "# Title: Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation\n# Venue: IFIP Working Conference on Database Semantics\n# Authors: Riccardo Cantini, Giada Cosenza, A. Orsino, Domenico Talia\n## Abstract\nLarge Language Models (LLMs) have revolutionized artificial intelligence, demonstrating remarkable computational power and linguistic capabilities. However, these models are inherently prone to various biases stemming from their training data. These include selection, linguistic, and confirmation biases, along with common stereotypes related to gender, ethnicity, sexual orientation, religion, socioeconomic status, disability, and age. This study explores the presence of these biases within the responses given by the most recent LLMs, analyzing the impact on their fairness and reliability. We also investigate how known prompt engineering techniques can be exploited to effectively reveal hidden biases of LLMs, testing their adversarial robustness against jailbreak prompts specially crafted for bias elicitation. Extensive experiments are conducted using the most widespread LLMs at different scales, confirming that LLMs can still be manipulated to produce biased or inappropriate responses, despite their advanced capabilities and sophisticated alignment processes. Our findings underscore the importance of enhancing mitigation techniques to address these safety issues, toward a more sustainable and inclusive artificial intelligence.\n## Introduction\nLarge Language Models (LLMs) have recently gained significant traction due to their impressive natural language understanding and generation capabilities across various tasks, including machine translation, text summarization, topic detection, and engaging human-like conversations [4,3,1]. However, as LLMs become more integral to our daily lives across various domains -ranging from healthcare and finance to law and education -it is increasingly crucial to address the inherent biases that can emerge from these models. Such biases can lead to unfair treatment, reinforce stereotypes, and exclude social groups, compromising the ethical standards and social responsibility of AI technologies [13,14,39]. The presence of bias in LLMs is a multifaceted issue rooted in the data used for training. Specifically, biases in data availability, selection, language, and social contexts may collectively reflect prejudices, disparities, and stereotypes that can inadvertently be learned and perpetuated by LLMs, leading to unfair and harmful responses. Biases may also arise from the unfair usage of LLMs, since users may favor generated information that confirms their preexisting beliefs, selectively interpreting responses that align with their views (confirmation bias), or blindly trust the generated output without any critical thinking, deeming it a priori superior to human judgment (automation bias) [38,37]. Therefore, understanding, unveiling, and mitigating these biases is essential for fostering sustainability and inclusivity in AI applications. Mitigation strategies should involve curating more balanced and representative training datasets [31,33], while also implementing robust bias detection [32,36] and alignment mechanisms [40,41], incorporating fairness guidelines. However, several challenges arise in ensuring that language models are entirely bias-free, including obtaining representative datasets for safety tuning, developing universally accepted bias metrics, and the significant resources required for thorough bias mitigation. \n\nStarting from the above considerations, our study proposes a robust methodology to test the resilience of various widely-used Language Models (LMs) at different scales, ranging from high-quality Small Language Models (SLMs) like Google's Gemma 2B to large-scale LLMs like OpenAI's GPT-3.5 Turbo (175B).",
            "reference_string": "[271097745 | Cantini et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Understanding the Effect of Model Compression on Social Bias in Large Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 37,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.emnlp-main.161.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.05662, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2273536347",
                    "name": "Gustavo Gon\u00e7alves"
                },
                {
                    "authorId": "2268272",
                    "name": "Emma Strubell"
                }
            ],
            "abstract": "Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.",
            "corpus_id": 266163873,
            "sentences": [
                {
                    "corpus_id": "266163873",
                    "title": "Understanding the Effect of Model Compression on Social Bias in Large Language Models",
                    "text": "Large Language Models (LLMs) are trained on large corpora using self-supervision, which allows models to consider vast amounts of unlabelled data, and learn language patterns through masking tasks (Devlin et al., 2019;Radford et al., 2019). However, self-supervision allows LLMs to pick up social biases contained in the training data. Which is amplified by larger models, more data, and longer training (Kaneko et al., 2022;Kaneko and Bollegala, 2022;Kurita et al., 2019;Delobelle and Berendt, 2022). \n\nSocial biases in LLMs are an ongoing problem that is propagated from pretraining to finetuning (Ladhak et al., 2023;Gira et al., 2022). Biased pretrained models are hard to fix, as retraining is prohibitively expensive both financially and environmentally (Hessenthaler et al., 2022). At the same time, the compression of LLMs has been intensely studied. Pruning, quantization, and distillation are among the most common strategies to compress LLMs. Pruning reduces the parameters of a trained model by removing redundant connections while preserving equivalent performance to their original counterparts (Liebenwein et al., 2021;Ahia et al., 2021). Quantization reduces the precision of model weights and activations to improve efficiency while preserving performance (Ahmadian et al., 2023). Finally, knowledge distillation (Hinton et al., 2015) trains a smaller more efficient model based on a larger pre-trained model. \n\nWhile much research has been done on measuring and mitigating social bias in LLMs, and making LLMs smaller and more efficient, by using one or a combination of many compression methods (Xu et al., 2021), little research has been done regarding the interplay between social biases and LLM compression.",
                    "score": 0.5822856647745511,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 240
                        },
                        {
                            "start": 241,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 501
                        },
                        {
                            "start": 504,
                            "end": 639
                        },
                        {
                            "start": 640,
                            "end": 788
                        },
                        {
                            "start": 789,
                            "end": 858
                        },
                        {
                            "start": 859,
                            "end": 953
                        },
                        {
                            "start": 954,
                            "end": 1153
                        },
                        {
                            "start": 1154,
                            "end": 1297
                        },
                        {
                            "start": 1298,
                            "end": 1426
                        },
                        {
                            "start": 1429,
                            "end": 1729
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 197,
                            "end": 218,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 472,
                            "end": 500,
                            "matchedPaperCorpusId": "250426284"
                        },
                        {
                            "start": 599,
                            "end": 620,
                            "matchedPaperCorpusId": "258378241"
                        },
                        {
                            "start": 620,
                            "end": 638,
                            "matchedPaperCorpusId": "248780268"
                        },
                        {
                            "start": 760,
                            "end": 787,
                            "matchedPaperCorpusId": "253397743"
                        },
                        {
                            "start": 1134,
                            "end": 1152,
                            "matchedPaperCorpusId": "238419368"
                        },
                        {
                            "start": 1330,
                            "end": 1351,
                            "matchedPaperCorpusId": "7200347"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97265625
                }
            ],
            "relevance_judgement": 0.97265625,
            "relevance_judgment_input_expanded": "# Title: Understanding the Effect of Model Compression on Social Bias in Large Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Gustavo Gon\u00e7alves, Emma Strubell\n## Abstract\nLarge Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.\n## Introduction\nLarge Language Models (LLMs) are trained on large corpora using self-supervision, which allows models to consider vast amounts of unlabelled data, and learn language patterns through masking tasks (Devlin et al., 2019;Radford et al., 2019). However, self-supervision allows LLMs to pick up social biases contained in the training data. Which is amplified by larger models, more data, and longer training (Kaneko et al., 2022;Kaneko and Bollegala, 2022;Kurita et al., 2019;Delobelle and Berendt, 2022). \n\nSocial biases in LLMs are an ongoing problem that is propagated from pretraining to finetuning (Ladhak et al., 2023;Gira et al., 2022). Biased pretrained models are hard to fix, as retraining is prohibitively expensive both financially and environmentally (Hessenthaler et al., 2022). At the same time, the compression of LLMs has been intensely studied. Pruning, quantization, and distillation are among the most common strategies to compress LLMs. Pruning reduces the parameters of a trained model by removing redundant connections while preserving equivalent performance to their original counterparts (Liebenwein et al., 2021;Ahia et al., 2021). Quantization reduces the precision of model weights and activations to improve efficiency while preserving performance (Ahmadian et al., 2023). Finally, knowledge distillation (Hinton et al., 2015) trains a smaller more efficient model based on a larger pre-trained model. \n\nWhile much research has been done on measuring and mitigating social bias in LLMs, and making LLMs smaller and more efficient, by using one or a combination of many compression methods (Xu et al., 2021), little research has been done regarding the interplay between social biases and LLM compression.",
            "reference_string": "[266163873 | Goncalves et al. | 2023 | Citations: 11]"
        },
        {
            "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks",
            "venue": "Conference on Fairness, Accountability and Transparency",
            "year": 2023,
            "reference_count": 63,
            "citation_count": 51,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3593013.3594109",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.05550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2189183000",
                    "name": "Katelyn Mei"
                },
                {
                    "authorId": "2196943720",
                    "name": "Sonia Fereidooni"
                },
                {
                    "authorId": "144537437",
                    "name": "Aylin Caliskan"
                }
            ],
            "abstract": "Warning: The content of this paper may be upsetting or triggering. The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stigmatized groups. When prompts include stigmatized conditions, the probability of MLMs predicting negative words is, on average, 20 percent higher than when prompts have non-stigmatized conditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative. For example, the sentence \"They are people who have less than a high school education.\" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson\u2019s r =0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigmatized groups.",
            "corpus_id": 259129801,
            "sentences": [
                {
                    "corpus_id": "259129801",
                    "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks",
                    "text": "Caliskan et al. [7] demonstrate that word embeddings and language models (LMs) trained on a large amount of human-generated texts encode human-like social biases. Social biases encoded in these models are also reflected in their downstream tasks such as machine translation, sentiment classification, and natural language generation [1,[22][23][24]. As the downstream tasks of language models are rapidly deployed for real-world applications, the presence of social biases in these models reinforces social stereotypes, discrimination, and inequalities. Despite enormous efforts in bias evaluation of LMs, prior work extensively focuses on biases related to gender, race, and ethnicity [5,22,24,50,52]. Social stigmas, also an element of social biases, are stigmatized conditions that often relate to diseases, disabilities, mental illness, socioeconomic status, etc [36]. Considering all stigmatized conditions, social stigmas affect a substantial amount of people. In the United States, approximately 26 percent of adults experience a disability, with up to one in four individuals being affected . In 2021, there were around 57.8M adults that experienced mental illness, which was around 22% of the population in the United States . Social stigmas prevent individuals from social activities and access to education, healthcare, and career opportunities, negatively influencing their psychological well-being and life outcomes [14,[31][32][33]37]. As language models capture other social biases, they may also learn bias against socially stigmatized groups. Such a risk would reinforce social inequalities with the rise of real-world applications of LMs. \n\nThis study examines bias against 93 stigmatized groups in the United States. To the best of our knowledge, this is the first study that examines social stigmas in LMs on a large scale. Pachankis et al. [36] conduct the first psychology study that classifies 93 social stigmas along six stigma dimensions and evaluates their interpersonal outcome, social rejection. We adapt their list of these 93 social stigmas and a widely used psychological questionnaire that measures social rejection, the Social Distance Scale, to quantify bias against stigmatized groups.",
                    "score": 0.556964429380301,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 553
                        },
                        {
                            "start": 554,
                            "end": 702
                        },
                        {
                            "start": 703,
                            "end": 872
                        },
                        {
                            "start": 873,
                            "end": 966
                        },
                        {
                            "start": 967,
                            "end": 1100
                        },
                        {
                            "start": 1101,
                            "end": 1235
                        },
                        {
                            "start": 1236,
                            "end": 1449
                        },
                        {
                            "start": 1450,
                            "end": 1559
                        },
                        {
                            "start": 1560,
                            "end": 1656
                        },
                        {
                            "start": 1659,
                            "end": 1735
                        },
                        {
                            "start": 1736,
                            "end": 1843
                        },
                        {
                            "start": 1844,
                            "end": 2023
                        },
                        {
                            "start": 2024,
                            "end": 2220
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 16,
                            "end": 19,
                            "matchedPaperCorpusId": "23163324"
                        },
                        {
                            "start": 333,
                            "end": 336,
                            "matchedPaperCorpusId": "231603388"
                        },
                        {
                            "start": 336,
                            "end": 340,
                            "matchedPaperCorpusId": "250391069"
                        },
                        {
                            "start": 344,
                            "end": 348,
                            "matchedPaperCorpusId": "190000105"
                        },
                        {
                            "start": 686,
                            "end": 689,
                            "matchedPaperCorpusId": "102352788"
                        },
                        {
                            "start": 689,
                            "end": 692,
                            "matchedPaperCorpusId": "250391069"
                        },
                        {
                            "start": 692,
                            "end": 695,
                            "matchedPaperCorpusId": "190000105"
                        },
                        {
                            "start": 695,
                            "end": 698,
                            "matchedPaperCorpusId": "227275068"
                        },
                        {
                            "start": 698,
                            "end": 701,
                            "matchedPaperCorpusId": "238259136"
                        },
                        {
                            "start": 867,
                            "end": 871,
                            "matchedPaperCorpusId": "3750353"
                        },
                        {
                            "start": 1429,
                            "end": 1433,
                            "matchedPaperCorpusId": "21883193"
                        },
                        {
                            "start": 1433,
                            "end": 1437,
                            "matchedPaperCorpusId": "145255089"
                        },
                        {
                            "start": 1437,
                            "end": 1441,
                            "matchedPaperCorpusId": "246654106"
                        },
                        {
                            "start": 1441,
                            "end": 1445,
                            "matchedPaperCorpusId": "226207034"
                        },
                        {
                            "start": 1445,
                            "end": 1448,
                            "matchedPaperCorpusId": "33273008"
                        },
                        {
                            "start": 1861,
                            "end": 1865,
                            "matchedPaperCorpusId": "3750353"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97216796875
                }
            ],
            "relevance_judgement": 0.97216796875,
            "relevance_judgment_input_expanded": "# Title: Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks\n# Venue: Conference on Fairness, Accountability and Transparency\n# Authors: Katelyn Mei, Sonia Fereidooni, Aylin Caliskan\n## Abstract\nWarning: The content of this paper may be upsetting or triggering. The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stigmatized groups. When prompts include stigmatized conditions, the probability of MLMs predicting negative words is, on average, 20 percent higher than when prompts have non-stigmatized conditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative. For example, the sentence \"They are people who have less than a high school education.\" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson\u2019s r =0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigmatized groups.\n## INTRODUCTION\nCaliskan et al. [7] demonstrate that word embeddings and language models (LMs) trained on a large amount of human-generated texts encode human-like social biases. Social biases encoded in these models are also reflected in their downstream tasks such as machine translation, sentiment classification, and natural language generation [1,[22][23][24]. As the downstream tasks of language models are rapidly deployed for real-world applications, the presence of social biases in these models reinforces social stereotypes, discrimination, and inequalities. Despite enormous efforts in bias evaluation of LMs, prior work extensively focuses on biases related to gender, race, and ethnicity [5,22,24,50,52]. Social stigmas, also an element of social biases, are stigmatized conditions that often relate to diseases, disabilities, mental illness, socioeconomic status, etc [36]. Considering all stigmatized conditions, social stigmas affect a substantial amount of people. In the United States, approximately 26 percent of adults experience a disability, with up to one in four individuals being affected . In 2021, there were around 57.8M adults that experienced mental illness, which was around 22% of the population in the United States . Social stigmas prevent individuals from social activities and access to education, healthcare, and career opportunities, negatively influencing their psychological well-being and life outcomes [14,[31][32][33]37]. As language models capture other social biases, they may also learn bias against socially stigmatized groups. Such a risk would reinforce social inequalities with the rise of real-world applications of LMs. \n\nThis study examines bias against 93 stigmatized groups in the United States. To the best of our knowledge, this is the first study that examines social stigmas in LMs on a large scale. Pachankis et al. [36] conduct the first psychology study that classifies 93 social stigmas along six stigma dimensions and evaluates their interpersonal outcome, social rejection. We adapt their list of these 93 social stigmas and a widely used psychological questionnaire that measures social rejection, the Social Distance Scale, to quantify bias against stigmatized groups.",
            "reference_string": "[259129801 | Mei et al. | 2023 | Citations: 51]"
        },
        {
            "title": "Potential and Challenges of Model Editing for Social Debiasing",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 55,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13462, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "134233854",
                    "name": "Jianhao Yan"
                },
                {
                    "authorId": "2285582246",
                    "name": "Futing Wang"
                },
                {
                    "authorId": "2110450452",
                    "name": "Yafu Li"
                },
                {
                    "authorId": "2249762135",
                    "name": "Yue Zhang"
                }
            ],
            "abstract": "Large language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases. Mitigating these biases with fine-tuning could be both costly and data-hungry. Model editing methods, which focus on modifying LLMs in a post-hoc manner, are of great potential to address debiasing. However, it lacks a comprehensive study that facilitates both internal and external model editing methods, supports various bias types, as well as understands the pros and cons of applying editing methods to stereotypical debiasing. To mitigate this gap, we carefully formulate social debiasing into an editing problem and benchmark seven existing model editing algorithms on stereotypical debiasing, i.e., debias editing. Our findings in three scenarios reveal both the potential and challenges of debias editing: (1) Existing model editing methods can effectively preserve knowledge and mitigate biases, while the generalization of debias effect from edited sentences to semantically equivalent sentences is limited.(2) Sequential editing highlights the robustness of SERAC (Mitchell et al. 2022b), while internal editing methods degenerate with the number of edits. (3) Model editing algorithms achieve generalization towards unseen biases both within the same type and from different types. In light of these findings, we further propose two simple but effective methods to improve debias editing, and experimentally show the effectiveness of the proposed methods.",
            "corpus_id": 267770177,
            "sentences": [
                {
                    "corpus_id": "267770177",
                    "title": "Potential and Challenges of Model Editing for Social Debiasing",
                    "text": "Large Language Models (LLMs) perform excellently across various downstream tasks, allowing a wide range of applications, ranging from chatbots to medical diagnoses (Wang et al., 2023b) to robotics (Driess et al., 2023). However, LLMs * These authors contributed equally to this work. trained on vast corpora can inadvertently learn biased information, leading to negative stereotypes and social biases encoded within the models (Gallegos et al., 2023). For instance, when given 'Arab people are rich.' and 'Arab people are poor.', the LLaMA-2-7B (Touvron et al., 2023b) model favors the first sentence, which reflects stereotypes encoded within the model. Such biases have the potential to result in unfairness and harm when deployed in production systems (Nadeem et al., 2021a;Prakash and Lee, 2023). \n\nTraditional debiasing methods typically require training models from scratch to modify training datasets (Zmigrod et al., 2019;Dinan et al., 2020;Qian et al., 2022a;Narayanan Venkit et al., 2023), optimization process (Huang et al., 2020;Qian et al., 2022b;He et al., 2022;Park et al., 2023;Zhou et al., 2023) or they may operate within the output space (He et al., 2021;Tokpo and Calders, 2022;Majumder et al., 2022;Dhingra et al., 2023). The former is very costly for language models, while the latter does not truly address bias encoded in the model, potentially leading to non-robustness. \n\nWhen it comes to mitigating specific biases in pre-trained models, such as the association between \"nurse\" and \"women\", directly fine-tuning (Ghanbarzadeh et al., 2023) the model might be both costly and impractical with insufficient data.",
                    "score": 0.5969596517841916,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 219
                        },
                        {
                            "start": 220,
                            "end": 283
                        },
                        {
                            "start": 284,
                            "end": 452
                        },
                        {
                            "start": 453,
                            "end": 530
                        },
                        {
                            "start": 531,
                            "end": 655
                        },
                        {
                            "start": 656,
                            "end": 801
                        },
                        {
                            "start": 804,
                            "end": 1243
                        },
                        {
                            "start": 1244,
                            "end": 1396
                        },
                        {
                            "start": 1399,
                            "end": 1638
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 756,
                            "end": 778,
                            "matchedPaperCorpusId": "215828184"
                        },
                        {
                            "start": 778,
                            "end": 800,
                            "matchedPaperCorpusId": "266054040"
                        },
                        {
                            "start": 909,
                            "end": 931,
                            "matchedPaperCorpusId": "184486914"
                        },
                        {
                            "start": 931,
                            "end": 950,
                            "matchedPaperCorpusId": "207852875"
                        },
                        {
                            "start": 950,
                            "end": 969,
                            "matchedPaperCorpusId": "249062690"
                        },
                        {
                            "start": 1022,
                            "end": 1042,
                            "matchedPaperCorpusId": "207847197"
                        },
                        {
                            "start": 1042,
                            "end": 1061,
                            "matchedPaperCorpusId": "249062690"
                        },
                        {
                            "start": 1061,
                            "end": 1077,
                            "matchedPaperCorpusId": "252907344"
                        },
                        {
                            "start": 1077,
                            "end": 1095,
                            "matchedPaperCorpusId": "257079734"
                        },
                        {
                            "start": 1095,
                            "end": 1113,
                            "matchedPaperCorpusId": "259370743"
                        },
                        {
                            "start": 1158,
                            "end": 1175,
                            "matchedPaperCorpusId": "237634972"
                        },
                        {
                            "start": 1175,
                            "end": 1199,
                            "matchedPaperCorpusId": "246210255"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97119140625
                }
            ],
            "relevance_judgement": 0.97119140625,
            "relevance_judgment_input_expanded": "# Title: Potential and Challenges of Model Editing for Social Debiasing\n# Venue: arXiv.org\n# Authors: Jianhao Yan, Futing Wang, Yafu Li, Yue Zhang\n## Abstract\nLarge language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases. Mitigating these biases with fine-tuning could be both costly and data-hungry. Model editing methods, which focus on modifying LLMs in a post-hoc manner, are of great potential to address debiasing. However, it lacks a comprehensive study that facilitates both internal and external model editing methods, supports various bias types, as well as understands the pros and cons of applying editing methods to stereotypical debiasing. To mitigate this gap, we carefully formulate social debiasing into an editing problem and benchmark seven existing model editing algorithms on stereotypical debiasing, i.e., debias editing. Our findings in three scenarios reveal both the potential and challenges of debias editing: (1) Existing model editing methods can effectively preserve knowledge and mitigate biases, while the generalization of debias effect from edited sentences to semantically equivalent sentences is limited.(2) Sequential editing highlights the robustness of SERAC (Mitchell et al. 2022b), while internal editing methods degenerate with the number of edits. (3) Model editing algorithms achieve generalization towards unseen biases both within the same type and from different types. In light of these findings, we further propose two simple but effective methods to improve debias editing, and experimentally show the effectiveness of the proposed methods.\n## Introduction\nLarge Language Models (LLMs) perform excellently across various downstream tasks, allowing a wide range of applications, ranging from chatbots to medical diagnoses (Wang et al., 2023b) to robotics (Driess et al., 2023). However, LLMs * These authors contributed equally to this work. trained on vast corpora can inadvertently learn biased information, leading to negative stereotypes and social biases encoded within the models (Gallegos et al., 2023). For instance, when given 'Arab people are rich.' and 'Arab people are poor.', the LLaMA-2-7B (Touvron et al., 2023b) model favors the first sentence, which reflects stereotypes encoded within the model. Such biases have the potential to result in unfairness and harm when deployed in production systems (Nadeem et al., 2021a;Prakash and Lee, 2023). \n\nTraditional debiasing methods typically require training models from scratch to modify training datasets (Zmigrod et al., 2019;Dinan et al., 2020;Qian et al., 2022a;Narayanan Venkit et al., 2023), optimization process (Huang et al., 2020;Qian et al., 2022b;He et al., 2022;Park et al., 2023;Zhou et al., 2023) or they may operate within the output space (He et al., 2021;Tokpo and Calders, 2022;Majumder et al., 2022;Dhingra et al., 2023). The former is very costly for language models, while the latter does not truly address bias encoded in the model, potentially leading to non-robustness. \n\nWhen it comes to mitigating specific biases in pre-trained models, such as the association between \"nurse\" and \"women\", directly fine-tuning (Ghanbarzadeh et al., 2023) the model might be both costly and impractical with insufficient data.",
            "reference_string": "[267770177 | Yan et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 56,
            "citation_count": 167,
            "influential_citation_count": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.acl-long.72.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2022.acl-long.72, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "Yue Guo"
                },
                {
                    "authorId": "46285693",
                    "name": "Yi Yang"
                },
                {
                    "authorId": "144849629",
                    "name": "A. Abbasi"
                }
            ],
            "abstract": "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models\u2019 understanding abilities, as shown using the GLUE benchmark.",
            "corpus_id": 248780440,
            "sentences": [
                {
                    "corpus_id": "248780440",
                    "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
                    "text": "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models\u2019 understanding abilities, as shown using the GLUE benchmark.",
                    "score": 0.5461471143830118,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97119140625
                }
            ],
            "relevance_judgement": 0.97119140625,
            "relevance_judgment_input_expanded": "# Title: Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Yue Guo, Yi Yang, A. Abbasi\n## Abstract\nHuman-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models\u2019 understanding abilities, as shown using the GLUE benchmark.\n",
            "reference_string": "[248780440 | Guo et al. | 2022 | Citations: 167]"
        },
        {
            "title": "Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 48,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.02295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2233316526",
                    "name": "Yachao Zhao"
                },
                {
                    "authorId": "2266189718",
                    "name": "Bo Wang"
                },
                {
                    "authorId": "2301797252",
                    "name": "Yan Wang"
                }
            ],
            "abstract": "Large Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content. While extensive research has investigated biases in LLMs, prior work has predominantly focused on explicit bias, with minimal attention to implicit bias and the relation between these two forms of bias. This paper presents a systematic framework grounded in social psychology theories to investigate and compare explicit and implicit biases in LLMs. We propose a novel self-reflection-based evaluation framework that operates in two phases: first measuring implicit bias through simulated psychological assessment methods, then evaluating explicit bias by prompting LLMs to analyze their own generated content. Through extensive experiments on advanced LLMs across multiple social dimensions, we demonstrate that LLMs exhibit a substantial inconsistency between explicit and implicit biases: while explicit bias manifests as mild stereotypes, implicit bias exhibits strong stereotypes. We further investigate the underlying factors contributing to this explicit-implicit bias inconsistency, examining the effects of training data scale, model size, and alignment techniques. Experimental results indicate that while explicit bias declines with increased training data and model size, implicit bias exhibits a contrasting upward trend. Moreover, contemporary alignment methods effectively suppress explicit bias but show limited efficacy in mitigating implicit bias.",
            "corpus_id": 275336873,
            "sentences": [
                {
                    "corpus_id": "275336873",
                    "title": "Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection",
                    "text": "Large Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content. While extensive research has investigated bias in LLMs, prior work has predominantly focused on explicit bias, leaving the more nuanced implicit biases largely unexplored. This paper presents a systematic framework grounded in social psychology theories to investigate and compare explicit and implicit biases in LLMs. We propose a novel\"self-reflection\"based evaluation framework that operates in two phases: first measuring implicit bias through simulated psychological assessment methods, then evaluating explicit bias by prompting LLMs to analyze their own generated content. Through extensive experiments on state-of-the-art LLMs across multiple social dimensions, we demonstrate that LLMs exhibit a substantial inconsistency between explicit and implicit biases, where explicit biases manifest as mild stereotypes while implicit biases show strong stereotypes. Furthermore, we investigate the underlying factors contributing to this explicit-implicit bias inconsistency. Our experiments examine the effects of training data scale, model parameters, and alignment techniques. Results indicate that while explicit bias diminishes with increased training data and model size, implicit bias exhibits a contrasting upward trend. Notably, contemporary alignment methods (e.g., RLHF, DPO) effectively suppress explicit bias but show limited efficacy in mitigating implicit bias. These findings suggest that while scaling up models and alignment training can address explicit bias, the challenge of implicit bias requires novel approaches beyond current methodologies.",
                    "score": 0.6632861374255686,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.970703125
                }
            ],
            "relevance_judgement": 0.970703125,
            "relevance_judgment_input_expanded": "# Title: Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection\n# Venue: arXiv.org\n# Authors: Yachao Zhao, Bo Wang, Yan Wang\n## Abstract\nLarge Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content. While extensive research has investigated biases in LLMs, prior work has predominantly focused on explicit bias, with minimal attention to implicit bias and the relation between these two forms of bias. This paper presents a systematic framework grounded in social psychology theories to investigate and compare explicit and implicit biases in LLMs. We propose a novel self-reflection-based evaluation framework that operates in two phases: first measuring implicit bias through simulated psychological assessment methods, then evaluating explicit bias by prompting LLMs to analyze their own generated content. Through extensive experiments on advanced LLMs across multiple social dimensions, we demonstrate that LLMs exhibit a substantial inconsistency between explicit and implicit biases: while explicit bias manifests as mild stereotypes, implicit bias exhibits strong stereotypes. We further investigate the underlying factors contributing to this explicit-implicit bias inconsistency, examining the effects of training data scale, model size, and alignment techniques. Experimental results indicate that while explicit bias declines with increased training data and model size, implicit bias exhibits a contrasting upward trend. Moreover, contemporary alignment methods effectively suppress explicit bias but show limited efficacy in mitigating implicit bias.\n",
            "reference_string": "[275336873 | Zhao et al. | 2025 | Citations: 4]"
        },
        {
            "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 51,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.07887, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1585232914",
                    "name": "Riccardo Cantini"
                },
                {
                    "authorId": "96934840",
                    "name": "A. Orsino"
                },
                {
                    "authorId": "2354558125",
                    "name": "Massimo Ruggiero"
                },
                {
                    "authorId": "2299780920",
                    "name": "Domenico Talia"
                }
            ],
            "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised concerns about embedded biases, which can perpetuate stereotypes and compromise fairness. These biases stem from various sources, including historical inequalities in training data, linguistic imbalances, and adversarial manipulation. Despite mitigation efforts, recent studies indicate that LLMs remain vulnerable to adversarial attacks designed to elicit biased responses. This work proposes a scalable benchmarking framework to evaluate LLM robustness against adversarial bias elicitation. Our methodology involves (i) systematically probing models with a multi-task approach targeting biases across various sociocultural dimensions, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach for automated assessment of model responses, and (iii) employing jailbreak techniques to investigate vulnerabilities in safety mechanisms. Our analysis examines prevalent biases in both small and large state-of-the-art models and their impact on model safety. Additionally, we assess the safety of domain-specific models fine-tuned for critical fields, such as medicine. Finally, we release a curated dataset of bias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability benchmarking. Our findings reveal critical trade-offs between model size and safety, aiding the development of fairer and more robust future language models.",
            "corpus_id": 277667520,
            "sentences": [
                {
                    "corpus_id": "277667520",
                    "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
                    "text": "Large Language Models (LLMs) have empowered artificial intelligence with their remarkable natural language understanding and generation capabilities, enabling breakthroughs in tasks such as machine translation, summarization, and human-like conversation [1,2]. However, their increasing integration into societal domainsincluding healthcare [3], education [4], and law [5]-has amplified concerns about embedded biases. These biases, which can manifest in various forms, risk perpetuating stereotypes, marginalizing underrepresented groups, and undermining ethical AI deployment [6]. Biases may stem from various sources, including biased training data that reflects historical inequalities and prejudicial associations, linguistic imbalances in corpora, flaws in algorithmic design, and the uncritical use of AI systems [7,8]. Previous studies have quantified biased attitudes in language models related to various social groups [9,10], also finding that state-of-the-art LLMs can be manipulated via adversarial attacks to produce biased or harmful responses, despite their bias mitigation and alignment mechanisms [11]. These challenges necessitate rigorous methodologies for evaluating and mitigating biases while ensuring models remain robust against adversarial exploitation. However, current approaches to bias evaluation face critical limitations, including the substantial resources required for bias identification and mitigation, difficulties in acquiring representative datasets for safety assessment, and the absence of universally accepted bias metrics. \n\nTo address these gaps, this work proposes a scalable methodology for benchmarking LLMs against bias elicitation. Our approach follows a two-step process and leverages the LLM-as-a-Judge paradigm [12] to automate bias evaluation, reducing reliance on manual response annotation while ensuring scalability and reproducibility. The first step involves selecting a judge model based on its statistical agreement with human annotations on a curated dataset of prompt-response pairs. These pairs capture both biased and safe behaviors, providing a benchmark for evaluating model ability to discern harmful content. Once chosen, the judge model is used to systematically evaluate LLM robustness using bias-probing prompts across multiple sociocultural dimensions, encompassing both isolated and intersectional bias categories.",
                    "score": 0.5313355550677041,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 260
                        },
                        {
                            "start": 261,
                            "end": 418
                        },
                        {
                            "start": 419,
                            "end": 582
                        },
                        {
                            "start": 583,
                            "end": 826
                        },
                        {
                            "start": 827,
                            "end": 1120
                        },
                        {
                            "start": 1121,
                            "end": 1279
                        },
                        {
                            "start": 1280,
                            "end": 1565
                        },
                        {
                            "start": 1568,
                            "end": 1680
                        },
                        {
                            "start": 1681,
                            "end": 1892
                        },
                        {
                            "start": 1893,
                            "end": 2045
                        },
                        {
                            "start": 2046,
                            "end": 2176
                        },
                        {
                            "start": 2177,
                            "end": 2387
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 254,
                            "end": 257,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 257,
                            "end": 259,
                            "matchedPaperCorpusId": "259360395"
                        },
                        {
                            "start": 341,
                            "end": 344,
                            "matchedPaperCorpusId": "257312905"
                        },
                        {
                            "start": 356,
                            "end": 359,
                            "matchedPaperCorpusId": "265315253"
                        },
                        {
                            "start": 369,
                            "end": 372,
                            "matchedPaperCorpusId": "267413187"
                        },
                        {
                            "start": 578,
                            "end": 581,
                            "matchedPaperCorpusId": "258688053"
                        },
                        {
                            "start": 820,
                            "end": 823,
                            "matchedPaperCorpusId": "237298625"
                        },
                        {
                            "start": 823,
                            "end": 825,
                            "matchedPaperCorpusId": "261530629"
                        },
                        {
                            "start": 929,
                            "end": 932,
                            "matchedPaperCorpusId": "265212726"
                        },
                        {
                            "start": 932,
                            "end": 935,
                            "matchedPaperCorpusId": "215828184"
                        },
                        {
                            "start": 1115,
                            "end": 1119,
                            "matchedPaperCorpusId": "271097745"
                        },
                        {
                            "start": 1763,
                            "end": 1767,
                            "matchedPaperCorpusId": "259129398"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.970703125
                }
            ],
            "relevance_judgement": 0.970703125,
            "relevance_judgment_input_expanded": "# Title: Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge\n# Venue: arXiv.org\n# Authors: Riccardo Cantini, A. Orsino, Massimo Ruggiero, Domenico Talia\n## Abstract\nLarge Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised concerns about embedded biases, which can perpetuate stereotypes and compromise fairness. These biases stem from various sources, including historical inequalities in training data, linguistic imbalances, and adversarial manipulation. Despite mitigation efforts, recent studies indicate that LLMs remain vulnerable to adversarial attacks designed to elicit biased responses. This work proposes a scalable benchmarking framework to evaluate LLM robustness against adversarial bias elicitation. Our methodology involves (i) systematically probing models with a multi-task approach targeting biases across various sociocultural dimensions, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach for automated assessment of model responses, and (iii) employing jailbreak techniques to investigate vulnerabilities in safety mechanisms. Our analysis examines prevalent biases in both small and large state-of-the-art models and their impact on model safety. Additionally, we assess the safety of domain-specific models fine-tuned for critical fields, such as medicine. Finally, we release a curated dataset of bias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability benchmarking. Our findings reveal critical trade-offs between model size and safety, aiding the development of fairer and more robust future language models.\n## Introduction\nLarge Language Models (LLMs) have empowered artificial intelligence with their remarkable natural language understanding and generation capabilities, enabling breakthroughs in tasks such as machine translation, summarization, and human-like conversation [1,2]. However, their increasing integration into societal domainsincluding healthcare [3], education [4], and law [5]-has amplified concerns about embedded biases. These biases, which can manifest in various forms, risk perpetuating stereotypes, marginalizing underrepresented groups, and undermining ethical AI deployment [6]. Biases may stem from various sources, including biased training data that reflects historical inequalities and prejudicial associations, linguistic imbalances in corpora, flaws in algorithmic design, and the uncritical use of AI systems [7,8]. Previous studies have quantified biased attitudes in language models related to various social groups [9,10], also finding that state-of-the-art LLMs can be manipulated via adversarial attacks to produce biased or harmful responses, despite their bias mitigation and alignment mechanisms [11]. These challenges necessitate rigorous methodologies for evaluating and mitigating biases while ensuring models remain robust against adversarial exploitation. However, current approaches to bias evaluation face critical limitations, including the substantial resources required for bias identification and mitigation, difficulties in acquiring representative datasets for safety assessment, and the absence of universally accepted bias metrics. \n\nTo address these gaps, this work proposes a scalable methodology for benchmarking LLMs against bias elicitation. Our approach follows a two-step process and leverages the LLM-as-a-Judge paradigm [12] to automate bias evaluation, reducing reliance on manual response annotation while ensuring scalability and reproducibility. The first step involves selecting a judge model based on its statistical agreement with human annotations on a curated dataset of prompt-response pairs. These pairs capture both biased and safe behaviors, providing a benchmark for evaluating model ability to discern harmful content. Once chosen, the judge model is used to systematically evaluate LLM robustness using bias-probing prompts across multiple sociocultural dimensions, encompassing both isolated and intersectional bias categories.",
            "reference_string": "[277667520 | Cantini et al. | 2025 | Citations: 4]"
        },
        {
            "title": "LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 61,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.00548, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1803285",
                    "name": "Tianci Liu"
                },
                {
                    "authorId": "51225422",
                    "name": "Haoyu Wang"
                },
                {
                    "authorId": "1486407811",
                    "name": "Shiyang Wang"
                },
                {
                    "authorId": "2304607213",
                    "name": "Yu Cheng"
                },
                {
                    "authorId": "2284861474",
                    "name": "Jing Gao"
                }
            ],
            "abstract": "Large language models (LLMs) have achieved impressive performance on various natural language generation tasks. Nonetheless, they suffer from generating negative and harmful contents that are biased against certain demographic groups (e.g., female), raising severe fairness concerns. As remedies, prior works intervened the generation by removing attitude or demographic information, inevitably degrading the generation quality and resulting in notable \\textit{fairness-fluency} trade-offs. However, it is still under-explored to what extent the fluency \\textit{has to} be affected in order to achieve a desired level of fairness. In this work, we conduct the first formal study from an information-theoretic perspective. We show that previous approaches are excessive for debiasing and propose LIDAO, a general framework to debias a (L)LM at a better fluency provably. We further robustify LIDAO in adversarial scenarios, where a carefully-crafted prompt may stimulate LLMs exhibiting instruction-following abilities to generate texts with fairness issue appears only when the prompt is also taken into account. Experiments on three LMs ranging from 0.7B to 7B parameters demonstrate the superiority of our method.",
            "corpus_id": 270214849,
            "sentences": [
                {
                    "corpus_id": "270214849",
                    "title": "LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models",
                    "text": "Language models (LMs) parameterized by deep neural networks (Vaswani et al., 2017;Lewis et al., 2019;Radford et al., 2019;Brown et al., 2020) have thrived in producing fluent and meaningful texts on a variety of nat-ural language generation tasks (See et al., 2019;Ji et al., 2023).Recently, researchers further applied LMs to more diverse classification tasks by transforming them to generative frames (Raffel et al., 2020).These successes underscore the versatility of LMs, establishing them as the foundations for different natural language processing applications (Bommasani et al., 2021;Zhou et al., 2023).In addition, with model sizes continually increasing, large language models (LLMs) have demonstrated unprecedented abilities to follow natural language instructions (Dong et al., 2022;Ouyang et al., 2022).These abilities empower the zero-shot adaptation of LLMs to unseen tasks (Kojima et al., 2022), paving the way towards artificial general intelligence (Bubeck et al., 2023).\n\nNotwithstanding, despite their remarkable performance, LMs suffer from the fairness issue, i.e., they may generate negative texts that are biased against underrepresented demographic groups (e.g., female) in our society (Sheng et al., 2019).\n\nFor instance, GPT-2 (Radford et al., 2019) tends to generate more negative texts towards females (Huang et al., 2019).Such social biases, termed as global biases (Sheng et al., 2020), stem from the real world corpora due to historical reasons (Basta et al., 2019).Unsurprisingly, LMs reproduce or amplify the biases from the data whereon they are trained (Gehman et al., 2020;Schick et al., 2021).\n\nAs remedies, a vast amount of bias mitigation approaches have been proposed.Early attempts of fine-tuning with clean data proved effective on small LMs (Lu et al., 2020;Saunders & Byrne, 2020;Bender et al., 2021).",
                    "score": 0.6131032688342929,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 282
                        },
                        {
                            "start": 282,
                            "end": 425
                        },
                        {
                            "start": 425,
                            "end": 611
                        },
                        {
                            "start": 611,
                            "end": 816
                        },
                        {
                            "start": 816,
                            "end": 989
                        },
                        {
                            "start": 991,
                            "end": 1232
                        },
                        {
                            "start": 1234,
                            "end": 1352
                        },
                        {
                            "start": 1352,
                            "end": 1498
                        },
                        {
                            "start": 1498,
                            "end": 1631
                        },
                        {
                            "start": 1633,
                            "end": 1709
                        },
                        {
                            "start": 1709,
                            "end": 1846
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 101,
                            "end": 122,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 122,
                            "end": 141,
                            "matchedPaperCorpusId": "237091588"
                        },
                        {
                            "start": 265,
                            "end": 281,
                            "matchedPaperCorpusId": "246652372"
                        },
                        {
                            "start": 403,
                            "end": 424,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 568,
                            "end": 592,
                            "matchedPaperCorpusId": "237091588"
                        },
                        {
                            "start": 795,
                            "end": 815,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 889,
                            "end": 910,
                            "matchedPaperCorpusId": "249017743"
                        },
                        {
                            "start": 1254,
                            "end": 1276,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 1396,
                            "end": 1416,
                            "matchedPaperCorpusId": "218470535"
                        },
                        {
                            "start": 1610,
                            "end": 1630,
                            "matchedPaperCorpusId": "232075876"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97021484375
                }
            ],
            "relevance_judgement": 0.97021484375,
            "relevance_judgment_input_expanded": "# Title: LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models\n# Venue: International Conference on Machine Learning\n# Authors: Tianci Liu, Haoyu Wang, Shiyang Wang, Yu Cheng, Jing Gao\n## Abstract\nLarge language models (LLMs) have achieved impressive performance on various natural language generation tasks. Nonetheless, they suffer from generating negative and harmful contents that are biased against certain demographic groups (e.g., female), raising severe fairness concerns. As remedies, prior works intervened the generation by removing attitude or demographic information, inevitably degrading the generation quality and resulting in notable \\textit{fairness-fluency} trade-offs. However, it is still under-explored to what extent the fluency \\textit{has to} be affected in order to achieve a desired level of fairness. In this work, we conduct the first formal study from an information-theoretic perspective. We show that previous approaches are excessive for debiasing and propose LIDAO, a general framework to debias a (L)LM at a better fluency provably. We further robustify LIDAO in adversarial scenarios, where a carefully-crafted prompt may stimulate LLMs exhibiting instruction-following abilities to generate texts with fairness issue appears only when the prompt is also taken into account. Experiments on three LMs ranging from 0.7B to 7B parameters demonstrate the superiority of our method.\n## Introduction\nLanguage models (LMs) parameterized by deep neural networks (Vaswani et al., 2017;Lewis et al., 2019;Radford et al., 2019;Brown et al., 2020) have thrived in producing fluent and meaningful texts on a variety of nat-ural language generation tasks (See et al., 2019;Ji et al., 2023).Recently, researchers further applied LMs to more diverse classification tasks by transforming them to generative frames (Raffel et al., 2020).These successes underscore the versatility of LMs, establishing them as the foundations for different natural language processing applications (Bommasani et al., 2021;Zhou et al., 2023).In addition, with model sizes continually increasing, large language models (LLMs) have demonstrated unprecedented abilities to follow natural language instructions (Dong et al., 2022;Ouyang et al., 2022).These abilities empower the zero-shot adaptation of LLMs to unseen tasks (Kojima et al., 2022), paving the way towards artificial general intelligence (Bubeck et al., 2023).\n\nNotwithstanding, despite their remarkable performance, LMs suffer from the fairness issue, i.e., they may generate negative texts that are biased against underrepresented demographic groups (e.g., female) in our society (Sheng et al., 2019).\n\nFor instance, GPT-2 (Radford et al., 2019) tends to generate more negative texts towards females (Huang et al., 2019).Such social biases, termed as global biases (Sheng et al., 2020), stem from the real world corpora due to historical reasons (Basta et al., 2019).Unsurprisingly, LMs reproduce or amplify the biases from the data whereon they are trained (Gehman et al., 2020;Schick et al., 2021).\n\nAs remedies, a vast amount of bias mitigation approaches have been proposed.Early attempts of fine-tuning with clean data proved effective on small LMs (Lu et al., 2020;Saunders & Byrne, 2020;Bender et al., 2021).",
            "reference_string": "[270214849 | Liu et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Beneath the Surface: How Large Language Models Reflect Hidden Bias",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 26,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.19749, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294143417",
                    "name": "Jinhao Pan"
                },
                {
                    "authorId": "2261742076",
                    "name": "Chahat Raj"
                },
                {
                    "authorId": "2323112615",
                    "name": "Ziyu Yao"
                },
                {
                    "authorId": "2220183419",
                    "name": "Ziwei Zhu"
                }
            ],
            "abstract": "The exceptional performance of Large Language Models (LLMs) often comes with the unintended propagation of social biases embedded in their training data. While existing benchmarks evaluate overt bias through direct term associations between bias concept terms and demographic terms, LLMs have become increasingly adept at avoiding biased responses, creating an illusion of neutrality. However, biases persist in subtler, contextually hidden forms that traditional benchmarks fail to capture. We introduce the Hidden Bias Benchmark (HBB), a novel dataset designed to assess hidden bias that bias concepts are hidden within naturalistic, subtly framed contexts in real-world scenarios. We analyze six state-of-the-art LLMs, revealing that while models reduce bias in response to overt bias, they continue to reinforce biases in nuanced settings. Data, code, and results are available at https://github.com/JP-25/Hidden-Bias-Benchmark.",
            "corpus_id": 276647994,
            "sentences": [
                {
                    "corpus_id": "276647994",
                    "title": "Beneath the Surface: How Large Language Models Reflect Hidden Bias",
                    "text": "The remarkable performance of Large Language Models (LLMs) is frequently accompanied by the propagation of social bias inherent in their training data (Gallegos et al., 2024a;Hofmann et al., 2024;Navigli et al., 2023;Cui et al., 2024). These biases raise serious ethical concerns, as they perpetuate stereotypes, reinforce discrimination, and negatively impact real-world decision-making. In domains such as hiring, law enforcement, and content moderation, the use of these models in realworld applications may disproportionately harm marginalized individuals and communities (Parrish et al., 2022;Nangia et al., 2020;Nadeem et al., 2021;Marchiori Manerba et al., 2024;Bi et al., 2023;del Arco et al., 2024;Kotek et al., 2023). \n\nNumerous studies (Parrish et al., 2022;Marchiori Manerba et al., 2024;Nangia et al., 2020;Nadeem et al., 2021) benchmark Overt Bias in LLMs by analyzing direct associations between a specific demographic term and a bias-related concept term. As illustrated in Figure 1, example (a) from BBQ (Parrish et al., 2022) can demonstrate overt bias when the model consistently associates \"Margaret\" (female) with the term \"bad at math\" and \"George\" (male) with the term \"good at math\", or vice versa. However, a fundamental issue remains: overt bias can be simply mitigated by breaking the direct association between demographic terms and concept terms (Gallegos et al., 2024b;Li et al., 2024). Additionally, as LLMs evolve, their responses to overt bias evaluations have become more neutral and self-regulated, frequently aligning with socially desirable norms. This trend is largely driven by advances in model training techniques, particularly instruction tuning and alignment strategies, which encourage neutrality in responses to overtly biased contexts (Ouyang et al., 2022;Zhang et al., 2023;Peng et al., 2023;Ji et al., 2024).",
                    "score": 0.5974391117509216,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 235
                        },
                        {
                            "start": 236,
                            "end": 388
                        },
                        {
                            "start": 389,
                            "end": 727
                        },
                        {
                            "start": 730,
                            "end": 971
                        },
                        {
                            "start": 972,
                            "end": 1222
                        },
                        {
                            "start": 1223,
                            "end": 1416
                        },
                        {
                            "start": 1417,
                            "end": 1584
                        },
                        {
                            "start": 1585,
                            "end": 1856
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 151,
                            "end": 175,
                            "matchedPaperCorpusId": "261530629"
                        },
                        {
                            "start": 175,
                            "end": 196,
                            "matchedPaperCorpusId": "272214842"
                        },
                        {
                            "start": 196,
                            "end": 217,
                            "matchedPaperCorpusId": "258688053"
                        },
                        {
                            "start": 576,
                            "end": 598,
                            "matchedPaperCorpusId": "239010011"
                        },
                        {
                            "start": 598,
                            "end": 618,
                            "matchedPaperCorpusId": "222090785"
                        },
                        {
                            "start": 618,
                            "end": 638,
                            "matchedPaperCorpusId": "215828184"
                        },
                        {
                            "start": 638,
                            "end": 669,
                            "matchedPaperCorpusId": "265212726"
                        },
                        {
                            "start": 707,
                            "end": 726,
                            "matchedPaperCorpusId": "261276445"
                        },
                        {
                            "start": 747,
                            "end": 769,
                            "matchedPaperCorpusId": "239010011"
                        },
                        {
                            "start": 769,
                            "end": 800,
                            "matchedPaperCorpusId": "265212726"
                        },
                        {
                            "start": 800,
                            "end": 820,
                            "matchedPaperCorpusId": "222090785"
                        },
                        {
                            "start": 820,
                            "end": 840,
                            "matchedPaperCorpusId": "215828184"
                        },
                        {
                            "start": 1021,
                            "end": 1043,
                            "matchedPaperCorpusId": "239010011"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9697265625
                }
            ],
            "relevance_judgement": 0.9697265625,
            "relevance_judgment_input_expanded": "# Title: Beneath the Surface: How Large Language Models Reflect Hidden Bias\n# Venue: arXiv.org\n# Authors: Jinhao Pan, Chahat Raj, Ziyu Yao, Ziwei Zhu\n## Abstract\nThe exceptional performance of Large Language Models (LLMs) often comes with the unintended propagation of social biases embedded in their training data. While existing benchmarks evaluate overt bias through direct term associations between bias concept terms and demographic terms, LLMs have become increasingly adept at avoiding biased responses, creating an illusion of neutrality. However, biases persist in subtler, contextually hidden forms that traditional benchmarks fail to capture. We introduce the Hidden Bias Benchmark (HBB), a novel dataset designed to assess hidden bias that bias concepts are hidden within naturalistic, subtly framed contexts in real-world scenarios. We analyze six state-of-the-art LLMs, revealing that while models reduce bias in response to overt bias, they continue to reinforce biases in nuanced settings. Data, code, and results are available at https://github.com/JP-25/Hidden-Bias-Benchmark.\n## Introduction\nThe remarkable performance of Large Language Models (LLMs) is frequently accompanied by the propagation of social bias inherent in their training data (Gallegos et al., 2024a;Hofmann et al., 2024;Navigli et al., 2023;Cui et al., 2024). These biases raise serious ethical concerns, as they perpetuate stereotypes, reinforce discrimination, and negatively impact real-world decision-making. In domains such as hiring, law enforcement, and content moderation, the use of these models in realworld applications may disproportionately harm marginalized individuals and communities (Parrish et al., 2022;Nangia et al., 2020;Nadeem et al., 2021;Marchiori Manerba et al., 2024;Bi et al., 2023;del Arco et al., 2024;Kotek et al., 2023). \n\nNumerous studies (Parrish et al., 2022;Marchiori Manerba et al., 2024;Nangia et al., 2020;Nadeem et al., 2021) benchmark Overt Bias in LLMs by analyzing direct associations between a specific demographic term and a bias-related concept term. As illustrated in Figure 1, example (a) from BBQ (Parrish et al., 2022) can demonstrate overt bias when the model consistently associates \"Margaret\" (female) with the term \"bad at math\" and \"George\" (male) with the term \"good at math\", or vice versa. However, a fundamental issue remains: overt bias can be simply mitigated by breaking the direct association between demographic terms and concept terms (Gallegos et al., 2024b;Li et al., 2024). Additionally, as LLMs evolve, their responses to overt bias evaluations have become more neutral and self-regulated, frequently aligning with socially desirable norms. This trend is largely driven by advances in model training techniques, particularly instruction tuning and alignment strategies, which encourage neutrality in responses to overtly biased contexts (Ouyang et al., 2022;Zhang et al., 2023;Peng et al., 2023;Ji et al., 2024).",
            "reference_string": "[276647994 | Pan et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Evaluation of Social Biases in Recent Large Pre-Trained Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 20,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.06861",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.06861, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2214583125",
                    "name": "Swapnil Sharma"
                },
                {
                    "authorId": "2214521713",
                    "name": "Nikita Anand"
                },
                {
                    "authorId": "1415341297",
                    "name": "V. KranthiKiranG."
                },
                {
                    "authorId": "2214565507",
                    "name": "Alind Jain"
                }
            ],
            "abstract": "Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups. In this work, we study the general trend in bias reduction as newer pre-trained models are released. Three recent models ( ELECTRA, DeBERTa, and DistilBERT) are chosen and evaluated against two bias benchmarks, StereoSet and CrowS-Pairs. They are compared to the baseline of BERT using the associated metrics. We explore whether as advancements are made and newer, faster, lighter models are released: are they being developed responsibly such that their inherent social biases have been reduced compared to their older counterparts? The results are compiled and we find that all the models under study do exhibit biases but have generally improved as compared to BERT.",
            "corpus_id": 258170403,
            "sentences": [
                {
                    "corpus_id": "258170403",
                    "title": "Evaluation of Social Biases in Recent Large Pre-Trained Models",
                    "text": "Given the popularity of large pre-trained language models and their widespread use in the community, it is pertinent to evaluate their inherent biases. These biases come about because these models (for example, BERT (Devlin et al., 2019) and BERT based models, ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019)) are trained on large sets of often freely available and unmoderated text data from sources such as the internet. \n\nDue to this, gender, race, religious and other social biases that we see in the real world are often translated into the models. Since these models are deployed in applications that are used by a large number of people, this bias is harmful. For example, these models are commonly used in content moderation tasks on social media platforms online. If the model is unfairly biased against certain social groups-minorities, marginalized people and those sections of society that are discriminated against, then they would be adversely affected by the application. So it is of utmost importance to ensure that models are trained in such a way that the bias is mitigated or to de-bias them. \n\nThere are multiple studies (Meade et al., 2022;Ahn and Oh, 2021;Bhardwaj et al., 2021;Kurita et al., 2019) on social biases in BERT and older language models. We extend this and evaluate the relatively newer models: ELECTRA (Clark et al., 2020), DistilBERT (Sanh et al., 2019) and DeBERTa (He et al., 2020). These models have gained popularity due to factors such as better performance, low computer requirements and parameter efficiency. Each model is evaluated against two bias evaluation datasets, StereoSet (Nadeem et al., 2021) and Crowdsourced Stereotype Pairs, CrowS-Pairs (Nangia et al., 2020). StereoSet accounts for four different biases that are based ongender, occupation, race and religion. In addition to these, CrowS-Pairs also has sexual orientation, age, nationality, disability and physical appearance.",
                    "score": 0.5784428085218842,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 152,
                            "end": 428
                        },
                        {
                            "start": 431,
                            "end": 559
                        },
                        {
                            "start": 560,
                            "end": 672
                        },
                        {
                            "start": 673,
                            "end": 778
                        },
                        {
                            "start": 779,
                            "end": 992
                        },
                        {
                            "start": 993,
                            "end": 1117
                        },
                        {
                            "start": 1120,
                            "end": 1278
                        },
                        {
                            "start": 1279,
                            "end": 1427
                        },
                        {
                            "start": 1428,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1722
                        },
                        {
                            "start": 1723,
                            "end": 1823
                        },
                        {
                            "start": 1824,
                            "end": 1940
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 216,
                            "end": 237,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 268,
                            "end": 286,
                            "matchedPaperCorpusId": "202888986"
                        },
                        {
                            "start": 1147,
                            "end": 1167,
                            "matchedPaperCorpusId": "239015827"
                        },
                        {
                            "start": 1167,
                            "end": 1184,
                            "matchedPaperCorpusId": "237491723"
                        },
                        {
                            "start": 1206,
                            "end": 1226,
                            "matchedPaperCorpusId": "190000105"
                        },
                        {
                            "start": 1344,
                            "end": 1364,
                            "matchedPaperCorpusId": "208229926"
                        },
                        {
                            "start": 1377,
                            "end": 1396,
                            "matchedPaperCorpusId": "203626972"
                        },
                        {
                            "start": 1631,
                            "end": 1652,
                            "matchedPaperCorpusId": "215828184"
                        },
                        {
                            "start": 1700,
                            "end": 1721,
                            "matchedPaperCorpusId": "222090785"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96923828125
                }
            ],
            "relevance_judgement": 0.96923828125,
            "relevance_judgment_input_expanded": "# Title: Evaluation of Social Biases in Recent Large Pre-Trained Models\n# Venue: arXiv.org\n# Authors: Swapnil Sharma, Nikita Anand, V. KranthiKiranG., Alind Jain\n## Abstract\nLarge pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups. In this work, we study the general trend in bias reduction as newer pre-trained models are released. Three recent models ( ELECTRA, DeBERTa, and DistilBERT) are chosen and evaluated against two bias benchmarks, StereoSet and CrowS-Pairs. They are compared to the baseline of BERT using the associated metrics. We explore whether as advancements are made and newer, faster, lighter models are released: are they being developed responsibly such that their inherent social biases have been reduced compared to their older counterparts? The results are compiled and we find that all the models under study do exhibit biases but have generally improved as compared to BERT.\n## Introduction\nGiven the popularity of large pre-trained language models and their widespread use in the community, it is pertinent to evaluate their inherent biases. These biases come about because these models (for example, BERT (Devlin et al., 2019) and BERT based models, ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019)) are trained on large sets of often freely available and unmoderated text data from sources such as the internet. \n\nDue to this, gender, race, religious and other social biases that we see in the real world are often translated into the models. Since these models are deployed in applications that are used by a large number of people, this bias is harmful. For example, these models are commonly used in content moderation tasks on social media platforms online. If the model is unfairly biased against certain social groups-minorities, marginalized people and those sections of society that are discriminated against, then they would be adversely affected by the application. So it is of utmost importance to ensure that models are trained in such a way that the bias is mitigated or to de-bias them. \n\nThere are multiple studies (Meade et al., 2022;Ahn and Oh, 2021;Bhardwaj et al., 2021;Kurita et al., 2019) on social biases in BERT and older language models. We extend this and evaluate the relatively newer models: ELECTRA (Clark et al., 2020), DistilBERT (Sanh et al., 2019) and DeBERTa (He et al., 2020). These models have gained popularity due to factors such as better performance, low computer requirements and parameter efficiency. Each model is evaluated against two bias evaluation datasets, StereoSet (Nadeem et al., 2021) and Crowdsourced Stereotype Pairs, CrowS-Pairs (Nangia et al., 2020). StereoSet accounts for four different biases that are based ongender, occupation, race and religion. In addition to these, CrowS-Pairs also has sexual orientation, age, nationality, disability and physical appearance.",
            "reference_string": "[258170403 | Sharma et al. | 2023 | Citations: 0]"
        },
        {
            "title": "BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 19,
            "citation_count": 10,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.13928, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2312204915",
                    "name": "Ahmed Allam"
                }
            ],
            "abstract": "Large Language Models (LLMs) have become pivotal in advancing natural language processing, yet their potential to perpetuate biases poses significant concerns. This paper introduces a new framework employing Direct Preference Optimization (DPO) to mitigate gender, racial, and religious biases in LLM-generated English text. By developing a loss function that favors less biased over biased completions, our approach cultivates a preference for respectful and non-discriminatory language in LLMs. We also contribute a manually designed dataset for training LLMs to recognize and correct biases. This dataset encompasses a diverse range of prompts paired with both biased and unbiased completions. Implementing this approach on the Microsoft Phi-2 model, we demonstrate substantial reductions in biased outputs as our model outperforms the baseline model on almost all bias benchmarks. Our model also achieves better performance compared to other open-source models on most benchmarks. By reducing biases in the language generated by the model, our study marks a significant step towards developing more ethical and socially responsible LLMs. We publicly release BiasDPO dataset on HuggingFace.",
            "corpus_id": 271310069,
            "sentences": [
                {
                    "corpus_id": "271310069",
                    "title": "BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization",
                    "text": "Large Language Models (LLMs) have become pivotal in advancing natural language processing, yet their potential to perpetuate biases poses significant concerns. This paper introduces a new framework employing Direct Preference Optimization (DPO) to mitigate gender, racial, and religious biases in LLM-generated English text. By developing a loss function that favors less biased over biased completions, our approach cultivates a preference for respectful and non-discriminatory language in LLMs. We also contribute a manually designed dataset for training LLMs to recognize and correct biases. This dataset encompasses a diverse range of prompts paired with both biased and unbiased completions. Implementing this approach on the Microsoft Phi-2 model, we demonstrate substantial reductions in biased outputs as our model outperforms the baseline model on almost all bias benchmarks. Our model also achieves better performance compared to other open-source models on most benchmarks. By reducing biases in the language generated by the model, our study marks a significant step towards developing more ethical and socially responsible LLMs. We publicly release BiasDPO dataset on HuggingFace.",
                    "score": 0.5532282674679481,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96923828125
                }
            ],
            "relevance_judgement": 0.96923828125,
            "relevance_judgment_input_expanded": "# Title: BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Ahmed Allam\n## Abstract\nLarge Language Models (LLMs) have become pivotal in advancing natural language processing, yet their potential to perpetuate biases poses significant concerns. This paper introduces a new framework employing Direct Preference Optimization (DPO) to mitigate gender, racial, and religious biases in LLM-generated English text. By developing a loss function that favors less biased over biased completions, our approach cultivates a preference for respectful and non-discriminatory language in LLMs. We also contribute a manually designed dataset for training LLMs to recognize and correct biases. This dataset encompasses a diverse range of prompts paired with both biased and unbiased completions. Implementing this approach on the Microsoft Phi-2 model, we demonstrate substantial reductions in biased outputs as our model outperforms the baseline model on almost all bias benchmarks. Our model also achieves better performance compared to other open-source models on most benchmarks. By reducing biases in the language generated by the model, our study marks a significant step towards developing more ethical and socially responsible LLMs. We publicly release BiasDPO dataset on HuggingFace.\n",
            "reference_string": "[271310069 | Allam | 2024 | Citations: 10]"
        },
        {
            "title": "Red-Teaming for Inducing Societal Bias in Large Language Models",
            "venue": "",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.04756, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2089528608",
                    "name": "Chunyan Luo"
                },
                {
                    "authorId": "2300284584",
                    "name": "Ahmad Ghawanmeh"
                },
                {
                    "authorId": "2300312503",
                    "name": "Xiaodan Zhu"
                },
                {
                    "authorId": "2374769",
                    "name": "Faiza Khan Khattak"
                }
            ],
            "abstract": "Ensuring the safe deployment of AI systems is critical in industry settings where biased outputs can lead to significant operational, reputational, and regulatory risks. Thorough evaluation before deployment is essential to prevent these hazards. Red-teaming addresses this need by employing adversarial attacks to develop guardrails that detect and reject biased or harmful queries, enabling models to be retrained or steered away from harmful outputs. However, most red-teaming efforts focus on harmful or unethical instructions rather than addressing social bias, leaving this critical area under-explored despite its significant real-world impact, especially in customer-facing systems. We propose two bias-specific red-teaming methods, Emotional Bias Probe (EBP) and BiasKG, to evaluate how standard safety measures for harmful content affect bias. For BiasKG, we refactor natural language stereotypes into a knowledge graph. We use these attacking strategies to induce biased responses from several open- and closed-source language models. Unlike prior work, these methods specifically target social bias. We find our method increases bias in all models, even those trained with safety guardrails. Our work emphasizes uncovering societal bias in LLMs through rigorous evaluation, and recommends measures ensure AI safety in high-stakes industry deployments.",
            "corpus_id": 269626396,
            "sentences": [
                {
                    "corpus_id": "269626396",
                    "title": "Red-Teaming for Inducing Societal Bias in Large Language Models",
                    "text": "Modern large language models (LLMs) have a significant amount of world knowledge, which enables strong performance in commonsense reasoning and knowledge-intensive tasks when harnessed properly. The language model can also learn social biases, which has a significant potential for societal harm. There have been many mitigation strategies proposed for LLM safety, but it is unclear how effective they are for eliminating social biases. In this work, we propose a new methodology for attacking language models with knowledge graph augmented generation. We refactor natural language stereotypes into a knowledge graph, and use adversarial attacking strategies to induce biased responses from several open- and closed-source language models. We find our method increases bias in all models, even those trained with safety guardrails. This demonstrates the need for further research in AI safety, and further work in this new adversarial space.",
                    "score": 0.548569523371853,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96875
                }
            ],
            "relevance_judgement": 0.96875,
            "relevance_judgment_input_expanded": "# Title: Red-Teaming for Inducing Societal Bias in Large Language Models\n# Venue: \n# Authors: Chunyan Luo, Ahmad Ghawanmeh, Xiaodan Zhu, Faiza Khan Khattak\n## Abstract\nEnsuring the safe deployment of AI systems is critical in industry settings where biased outputs can lead to significant operational, reputational, and regulatory risks. Thorough evaluation before deployment is essential to prevent these hazards. Red-teaming addresses this need by employing adversarial attacks to develop guardrails that detect and reject biased or harmful queries, enabling models to be retrained or steered away from harmful outputs. However, most red-teaming efforts focus on harmful or unethical instructions rather than addressing social bias, leaving this critical area under-explored despite its significant real-world impact, especially in customer-facing systems. We propose two bias-specific red-teaming methods, Emotional Bias Probe (EBP) and BiasKG, to evaluate how standard safety measures for harmful content affect bias. For BiasKG, we refactor natural language stereotypes into a knowledge graph. We use these attacking strategies to induce biased responses from several open- and closed-source language models. Unlike prior work, these methods specifically target social bias. We find our method increases bias in all models, even those trained with safety guardrails. Our work emphasizes uncovering societal bias in LLMs through rigorous evaluation, and recommends measures ensure AI safety in high-stakes industry deployments.\n",
            "reference_string": "[269626396 | Luo et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Towards Understanding and Mitigating Social Biases in Language Models",
            "venue": "International Conference on Machine Learning",
            "year": 2021,
            "reference_count": 56,
            "citation_count": 394,
            "influential_citation_count": 13,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.13219, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "28130078",
                    "name": "Paul Pu Liang"
                },
                {
                    "authorId": "2115397918",
                    "name": "Chiyu Wu"
                },
                {
                    "authorId": "49933077",
                    "name": "Louis-philippe Morency"
                },
                {
                    "authorId": "145124475",
                    "name": "R. Salakhutdinov"
                }
            ],
            "abstract": "As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.",
            "corpus_id": 235623756,
            "sentences": [
                {
                    "corpus_id": "235623756",
                    "title": "Towards Understanding and Mitigating Social Biases in Language Models",
                    "text": "As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.",
                    "score": 0.6166969506181166,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9677734375
                }
            ],
            "relevance_judgement": 0.9677734375,
            "relevance_judgment_input_expanded": "# Title: Towards Understanding and Mitigating Social Biases in Language Models\n# Venue: International Conference on Machine Learning\n# Authors: Paul Pu Liang, Chiyu Wu, Louis-philippe Morency, R. Salakhutdinov\n## Abstract\nAs machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.\n",
            "reference_string": "[235623756 | Liang et al. | 2021 | Citations: 394]"
        },
        {
            "title": "Bias Neutralization Framework: Measuring Fairness in Large Language Models with Bias Intelligence Quotient (BiQ)",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 13,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.18276, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2232497",
                    "name": "M. Narayan"
                },
                {
                    "authorId": "2298907347",
                    "name": "John Pasmore"
                },
                {
                    "authorId": "2298908887",
                    "name": "Elton Sampaio"
                },
                {
                    "authorId": "2298907296",
                    "name": "Vijay Raghavan"
                },
                {
                    "authorId": "2298907100",
                    "name": "Gabriella Waters"
                }
            ],
            "abstract": "The burgeoning influence of Large Language Models (LLMs) in shaping public discourse and decision-making underscores the imperative to address inherent biases within these AI systems. In the wake of AI's expansive integration across sectors, addressing racial bias in LLMs has never been more critical. This paper introduces a novel framework called Comprehensive Bias Neutralization Framework (CBNF) which embodies an innovative approach to quantifying and mitigating biases within LLMs. Our framework combines the Large Language Model Bias Index (LLMBI) [Oketunji, A., Anas, M., Saina, D., (2023)] and Bias removaL with No Demographics (BLIND) [Orgad, H., Belinkov, Y. (2023)] methodologies to create a new metric called Bias Intelligence Quotient (BiQ)which detects, measures, and mitigates racial bias in LLMs without reliance on demographic annotations. By introducing a new metric called BiQ that enhances LLMBI with additional fairness metrics, CBNF offers a multi-dimensional metric for bias assessment, underscoring the necessity of a nuanced approach to fairness in AI [Mehrabi et al., 2021]. This paper presents a detailed analysis of Latimer AI (a language model incrementally trained on black history and culture) in comparison to ChatGPT 3.5, illustrating Latimer AI's efficacy in detecting racial, cultural, and gender biases through targeted training and refined bias mitigation strategies [Latimer&Bender, 2023].",
            "corpus_id": 269449709,
            "sentences": [
                {
                    "corpus_id": "269449709",
                    "title": "Bias Neutralization Framework: Measuring Fairness in Large Language Models with Bias Intelligence Quotient (BiQ)",
                    "text": "To create a more exhaustive formula for detecting, measuring, and evaluating biases in LLMs, incorporating knowledge from various papers on bias detection and mitigation, we refine and extend the Large Language Model Bias Index (LLMBI) [3]",
                    "score": 0.5313825477882341,
                    "section_title": "Bias Intelligence Quotient (BiQ)",
                    "char_start_offset": 9690,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 239
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96728515625
                }
            ],
            "relevance_judgement": 0.96728515625,
            "relevance_judgment_input_expanded": "# Title: Bias Neutralization Framework: Measuring Fairness in Large Language Models with Bias Intelligence Quotient (BiQ)\n# Venue: arXiv.org\n# Authors: M. Narayan, John Pasmore, Elton Sampaio, Vijay Raghavan, Gabriella Waters\n## Abstract\nThe burgeoning influence of Large Language Models (LLMs) in shaping public discourse and decision-making underscores the imperative to address inherent biases within these AI systems. In the wake of AI's expansive integration across sectors, addressing racial bias in LLMs has never been more critical. This paper introduces a novel framework called Comprehensive Bias Neutralization Framework (CBNF) which embodies an innovative approach to quantifying and mitigating biases within LLMs. Our framework combines the Large Language Model Bias Index (LLMBI) [Oketunji, A., Anas, M., Saina, D., (2023)] and Bias removaL with No Demographics (BLIND) [Orgad, H., Belinkov, Y. (2023)] methodologies to create a new metric called Bias Intelligence Quotient (BiQ)which detects, measures, and mitigates racial bias in LLMs without reliance on demographic annotations. By introducing a new metric called BiQ that enhances LLMBI with additional fairness metrics, CBNF offers a multi-dimensional metric for bias assessment, underscoring the necessity of a nuanced approach to fairness in AI [Mehrabi et al., 2021]. This paper presents a detailed analysis of Latimer AI (a language model incrementally trained on black history and culture) in comparison to ChatGPT 3.5, illustrating Latimer AI's efficacy in detecting racial, cultural, and gender biases through targeted training and refined bias mitigation strategies [Latimer&Bender, 2023].\n## Bias Intelligence Quotient (BiQ)\nTo create a more exhaustive formula for detecting, measuring, and evaluating biases in LLMs, incorporating knowledge from various papers on bias detection and mitigation, we refine and extend the Large Language Model Bias Index (LLMBI) [3]",
            "reference_string": "[269449709 | Narayan et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 71,
            "citation_count": 20,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09090, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2121386115",
                    "name": "Marta Marchiori Manerba"
                },
                {
                    "authorId": "82563120",
                    "name": "Karolina Sta\u0144czak"
                },
                {
                    "authorId": "2257013371",
                    "name": "Riccardo Guidotti"
                },
                {
                    "authorId": "1736067",
                    "name": "Isabelle Augenstein"
                }
            ],
            "abstract": "While the impact of social biases in language models has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, limiting our understanding of bias complexities. This paper proposes a novel framework for probing language models for social biases by assessing disparate treatment, which involves treating individuals differently according to their affiliation with a sensitive demographic group. We curate SoFa, a large-scale benchmark designed to address the limitations of existing fairness collections. SoFa expands the analysis beyond the binary comparison of stereotypical versus anti-stereotypical identities to include a diverse range of identities and stereotypes. Comparing our methodology with existing benchmarks, we reveal that biases within language models are more nuanced than acknowledged, indicating a broader scope of encoded biases than previously recognized. Benchmarking LMs on SoFa, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models. Finally, our findings indicate that real-life adversities faced by various groups such as women and people with disabilities are mirrored in the behavior of these models.",
            "corpus_id": 265212726,
            "sentences": [
                {
                    "corpus_id": "265212726",
                    "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
                    "text": "The unparalleled ability of language models (LMs) to generalize from vast corpora is tinged by an inherent reinforcement of social biases. These biases are not merely encoded within LMs' representations but are also perpetuated to downstream tasks (Blodgett et al., 2021;Sta\u0144czak and Augenstein, 2021), where they can manifest in an uneven treatment of different demographic groups (Rudinger et al., 2018;Stanovsky et al., 2019;Kiritchenko and Mohammad, 2018;Venkit et al., 2022). \n\nDirect analysis of biases encoded within LMs allows us to pinpoint the problem at its source, potentially obviating the need for addressing it for ev- ery application (Nangia et al., 2020). Therefore, a number of studies have attempted to evaluate social biases within LMs (Nangia et al., 2020;Nadeem et al., 2021;Sta\u0144czak et al., 2023;Nozza et al., 2022a). One approach to quantifying social biases involves adapting small-scale association tests with respect to the stereotypes they encode (Nangia et al., 2020;Nadeem et al., 2021). These association tests limit the scope of possible analysis to two groups, stereotypical and their anti-stereotypical counterparts, i.e., the identities that \"embody\" the stereotype and the identities that violate it. This binary approach, which assumes a singular \"ground truth\" with respect to a stereotypical statement, has restricted the depth of the analysis and simplified the complexity of social identities and their associated stereotypes. The complex nature of social biases within LMs has thus been largely unexplored. \n\nOur Social Bias Probing framework, as outlined in Fig. 1, is specifically designed to enable a nuanced understanding of biases inherent in language models. Accordingly, the input of our approach consists of a set stereotypes and identities. To this end, we generate our probing dataset by com-arXiv:2311.09090v4",
                    "score": 0.7318258641002127,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 138
                        },
                        {
                            "start": 139,
                            "end": 480
                        },
                        {
                            "start": 483,
                            "end": 672
                        },
                        {
                            "start": 673,
                            "end": 840
                        },
                        {
                            "start": 841,
                            "end": 1017
                        },
                        {
                            "start": 1018,
                            "end": 1236
                        },
                        {
                            "start": 1237,
                            "end": 1467
                        },
                        {
                            "start": 1468,
                            "end": 1548
                        },
                        {
                            "start": 1551,
                            "end": 1706
                        },
                        {
                            "start": 1707,
                            "end": 1791
                        },
                        {
                            "start": 1792,
                            "end": 1862
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 248,
                            "end": 271,
                            "matchedPaperCorpusId": "236460302"
                        },
                        {
                            "start": 382,
                            "end": 405,
                            "matchedPaperCorpusId": "13756572"
                        },
                        {
                            "start": 405,
                            "end": 428,
                            "matchedPaperCorpusId": "173991101"
                        },
                        {
                            "start": 428,
                            "end": 459,
                            "matchedPaperCorpusId": "21670658"
                        },
                        {
                            "start": 459,
                            "end": 479,
                            "matchedPaperCorpusId": "252819117"
                        },
                        {
                            "start": 650,
                            "end": 671,
                            "matchedPaperCorpusId": "222090785"
                        },
                        {
                            "start": 756,
                            "end": 777,
                            "matchedPaperCorpusId": "222090785"
                        },
                        {
                            "start": 777,
                            "end": 797,
                            "matchedPaperCorpusId": "215828184"
                        },
                        {
                            "start": 797,
                            "end": 819,
                            "matchedPaperCorpusId": "233241166"
                        },
                        {
                            "start": 819,
                            "end": 839,
                            "matchedPaperCorpusId": "248780490"
                        },
                        {
                            "start": 975,
                            "end": 996,
                            "matchedPaperCorpusId": "222090785"
                        },
                        {
                            "start": 996,
                            "end": 1016,
                            "matchedPaperCorpusId": "215828184"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.966796875
                }
            ],
            "relevance_judgement": 0.966796875,
            "relevance_judgment_input_expanded": "# Title: Social Bias Probing: Fairness Benchmarking for Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Marta Marchiori Manerba, Karolina Sta\u0144czak, Riccardo Guidotti, Isabelle Augenstein\n## Abstract\nWhile the impact of social biases in language models has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, limiting our understanding of bias complexities. This paper proposes a novel framework for probing language models for social biases by assessing disparate treatment, which involves treating individuals differently according to their affiliation with a sensitive demographic group. We curate SoFa, a large-scale benchmark designed to address the limitations of existing fairness collections. SoFa expands the analysis beyond the binary comparison of stereotypical versus anti-stereotypical identities to include a diverse range of identities and stereotypes. Comparing our methodology with existing benchmarks, we reveal that biases within language models are more nuanced than acknowledged, indicating a broader scope of encoded biases than previously recognized. Benchmarking LMs on SoFa, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models. Finally, our findings indicate that real-life adversities faced by various groups such as women and people with disabilities are mirrored in the behavior of these models.\n## Introduction\nThe unparalleled ability of language models (LMs) to generalize from vast corpora is tinged by an inherent reinforcement of social biases. These biases are not merely encoded within LMs' representations but are also perpetuated to downstream tasks (Blodgett et al., 2021;Sta\u0144czak and Augenstein, 2021), where they can manifest in an uneven treatment of different demographic groups (Rudinger et al., 2018;Stanovsky et al., 2019;Kiritchenko and Mohammad, 2018;Venkit et al., 2022). \n\nDirect analysis of biases encoded within LMs allows us to pinpoint the problem at its source, potentially obviating the need for addressing it for ev- ery application (Nangia et al., 2020). Therefore, a number of studies have attempted to evaluate social biases within LMs (Nangia et al., 2020;Nadeem et al., 2021;Sta\u0144czak et al., 2023;Nozza et al., 2022a). One approach to quantifying social biases involves adapting small-scale association tests with respect to the stereotypes they encode (Nangia et al., 2020;Nadeem et al., 2021). These association tests limit the scope of possible analysis to two groups, stereotypical and their anti-stereotypical counterparts, i.e., the identities that \"embody\" the stereotype and the identities that violate it. This binary approach, which assumes a singular \"ground truth\" with respect to a stereotypical statement, has restricted the depth of the analysis and simplified the complexity of social identities and their associated stereotypes. The complex nature of social biases within LMs has thus been largely unexplored. \n\nOur Social Bias Probing framework, as outlined in Fig. 1, is specifically designed to enable a nuanced understanding of biases inherent in language models. Accordingly, the input of our approach consists of a set stereotypes and identities. To this end, we generate our probing dataset by com-arXiv:2311.09090v4",
            "reference_string": "[265212726 | Manerba et al. | 2023 | Citations: 20]"
        },
        {
            "title": "Bias Amplification: Large Language Models as Increasingly Biased Media",
            "venue": "",
            "year": 2024,
            "reference_count": 44,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.15234, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308046804",
                    "name": "Ze Wang"
                },
                {
                    "authorId": "2284029866",
                    "name": "Zekun Wu"
                },
                {
                    "authorId": "2326995040",
                    "name": "Jeremy Zhang"
                },
                {
                    "authorId": "2308070610",
                    "name": "Xin Guan"
                },
                {
                    "authorId": "2321401833",
                    "name": "Navya Jain"
                },
                {
                    "authorId": "2308072394",
                    "name": "Skylar Lu"
                },
                {
                    "authorId": "2321676996",
                    "name": "Saloni Gupta"
                },
                {
                    "authorId": "2268316579",
                    "name": "A. Koshiyama"
                }
            ],
            "abstract": "Model collapse, a phenomenon characterized by performance degradation due to iterative training on synthetic data, has been widely studied. However, its implications for bias amplification, the progressive intensification of pre-existing societal biases in Large Language Models (LLMs), remain significantly underexplored, despite the growing influence of LLMs in shaping online discourse. In this paper, we introduce a open, generational, and long-context benchmark specifically designed to measure political bias amplification in LLMs, leveraging sentence continuation tasks derived from a comprehensive dataset of U.S. political news. Our empirical study using GPT-2 reveals consistent and substantial political bias intensification (e.g., right-leaning amplification) over iterative synthetic training cycles. We evaluate three mitigation strategies, Overfitting, Preservation, and Accumulation, and demonstrate that bias amplification persists independently of model collapse, even when the latter is effectively controlled. Furthermore, we propose a mechanistic analysis approach that identifies neurons correlated with specific phenomena during inference through regression and statistical tests. This analysis uncovers largely distinct neuron populations driving bias amplification and model collapse, underscoring fundamentally different underlying mechanisms. Finally, we supplement our empirical findings with theoretical intuition that explains the separate origins of these phenomena, guiding targeted strategies for bias mitigation.",
            "corpus_id": 276408990,
            "sentences": [
                {
                    "corpus_id": "276408990",
                    "title": "Bias Amplification: Large Language Models as Increasingly Biased Media",
                    "text": "Model collapse, a phenomenon where models degrade in performance due to indiscriminate use of synthetic data is well studied. However, its role in bias amplification, the progressive reinforcement of preexisting social biases in Large Language Models (LLMs) remains underexplored. In this paper, we formally define the conditions for bias amplification and demonstrate through statistical simulations that bias can intensify even in the absence of sampling errors, the primary driver of model collapse. Empirically, we investigate political bias amplification in GPT2 using a custom built benchmark for sentence continuation tasks. Our findings reveal a progressively increasing right-leaning bias. Furthermore, we evaluate three mitigation strategies, Overfitting, Preservation, and Accumulation, and show that bias amplification persists even when model collapse is mitigated. Finally, a mechanistic interpretation identifies distinct sets of neurons responsible for model collapse and bias amplification, suggesting they arise from different underlying mechanisms.",
                    "score": 0.5697055403858949,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96533203125
                }
            ],
            "relevance_judgement": 0.96533203125,
            "relevance_judgment_input_expanded": "# Title: Bias Amplification: Large Language Models as Increasingly Biased Media\n# Venue: \n# Authors: Ze Wang, Zekun Wu, Jeremy Zhang, Xin Guan, Navya Jain, Skylar Lu, Saloni Gupta, A. Koshiyama\n## Abstract\nModel collapse, a phenomenon characterized by performance degradation due to iterative training on synthetic data, has been widely studied. However, its implications for bias amplification, the progressive intensification of pre-existing societal biases in Large Language Models (LLMs), remain significantly underexplored, despite the growing influence of LLMs in shaping online discourse. In this paper, we introduce a open, generational, and long-context benchmark specifically designed to measure political bias amplification in LLMs, leveraging sentence continuation tasks derived from a comprehensive dataset of U.S. political news. Our empirical study using GPT-2 reveals consistent and substantial political bias intensification (e.g., right-leaning amplification) over iterative synthetic training cycles. We evaluate three mitigation strategies, Overfitting, Preservation, and Accumulation, and demonstrate that bias amplification persists independently of model collapse, even when the latter is effectively controlled. Furthermore, we propose a mechanistic analysis approach that identifies neurons correlated with specific phenomena during inference through regression and statistical tests. This analysis uncovers largely distinct neuron populations driving bias amplification and model collapse, underscoring fundamentally different underlying mechanisms. Finally, we supplement our empirical findings with theoretical intuition that explains the separate origins of these phenomena, guiding targeted strategies for bias mitigation.\n",
            "reference_string": "[276408990 | Wang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Diagnosing and Debiasing Corpus-Based Political Bias and Insults in GPT2",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 10,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.10266, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267241202",
                    "name": "Ambri Ma"
                },
                {
                    "authorId": "2267306939",
                    "name": "Arnav Kumar"
                },
                {
                    "authorId": "2267241185",
                    "name": "Brett Zeligson"
                }
            ],
            "abstract": "The training of large language models (LLMs) on extensive, unfiltered corpora sourced from the internet is a common and advantageous practice. Consequently, LLMs have learned and inadvertently reproduced various types of biases, including violent, offensive, and toxic language. However, recent research shows that generative pretrained transformer (GPT) language models can recognize their own biases and detect toxicity in generated content, a process referred to as self-diagnosis. In response, researchers have developed a decoding algorithm that allows LLMs to self-debias, or reduce their likelihood of generating harmful text. This study investigates the efficacy of the diagnosing-debiasing approach in mitigating two additional types of biases: insults and political bias. These biases are often used interchangeably in discourse, despite exhibiting potentially dissimilar semantic and syntactic properties. We aim to contribute to the ongoing effort of investigating the ethical and social implications of human-AI interaction.",
            "corpus_id": 265281188,
            "sentences": [
                {
                    "corpus_id": "265281188",
                    "title": "Diagnosing and Debiasing Corpus-Based Political Bias and Insults in GPT2",
                    "text": "The training of large language models (LLMs) on extensive, unfiltered corpora sourced from the internet is a common and advantageous practice. Consequently, LLMs have learned and inadvertently reproduced various types of biases, including violent, offensive, and toxic language. However, recent research shows that generative pretrained transformer (GPT) language models can recognize their own biases and detect toxicity in generated content, a process referred to as self-diagnosis. In response, researchers have developed a decoding algorithm that allows LLMs to self-debias, or reduce their likelihood of generating harmful text. This study investigates the efficacy of the diagnosing-debiasing approach in mitigating two additional types of biases: insults and political bias. These biases are often used interchangeably in discourse, despite exhibiting potentially dissimilar semantic and syntactic properties. We aim to contribute to the ongoing effort of investigating the ethical and social implications of human-AI interaction.",
                    "score": 0.5327445964416467,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96484375
                }
            ],
            "relevance_judgement": 0.96484375,
            "relevance_judgment_input_expanded": "# Title: Diagnosing and Debiasing Corpus-Based Political Bias and Insults in GPT2\n# Venue: arXiv.org\n# Authors: Ambri Ma, Arnav Kumar, Brett Zeligson\n## Abstract\nThe training of large language models (LLMs) on extensive, unfiltered corpora sourced from the internet is a common and advantageous practice. Consequently, LLMs have learned and inadvertently reproduced various types of biases, including violent, offensive, and toxic language. However, recent research shows that generative pretrained transformer (GPT) language models can recognize their own biases and detect toxicity in generated content, a process referred to as self-diagnosis. In response, researchers have developed a decoding algorithm that allows LLMs to self-debias, or reduce their likelihood of generating harmful text. This study investigates the efficacy of the diagnosing-debiasing approach in mitigating two additional types of biases: insults and political bias. These biases are often used interchangeably in discourse, despite exhibiting potentially dissimilar semantic and syntactic properties. We aim to contribute to the ongoing effort of investigating the ethical and social implications of human-AI interaction.\n",
            "reference_string": "[265281188 | Ma et al. | 2023 | Citations: 1]"
        },
        {
            "title": "REFINE-LM: Mitigating Language Model Stereotypes via Reinforcement Learning",
            "venue": "European Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.09489, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283849478",
                    "name": "Rameez Qureshi"
                },
                {
                    "authorId": "2316429402",
                    "name": "Naim Es-Sebbani"
                },
                {
                    "authorId": "2316429565",
                    "name": "Luis Gal'arraga"
                },
                {
                    "authorId": "2316425343",
                    "name": "Yvette Graham"
                },
                {
                    "authorId": "2316138885",
                    "name": "Miguel Couceiro"
                },
                {
                    "authorId": "2267289766",
                    "name": "Zied Bouraoui"
                }
            ],
            "abstract": "With the introduction of (large) language models, there has been significant concern about the unintended bias such models may inherit from their training data. A number of studies have shown that such models propagate gender stereotypes, as well as geographical and racial bias, among other biases. While existing works tackle this issue by preprocessing data and debiasing embeddings, the proposed methods require a lot of computational resources and annotation effort while being limited to certain types of biases. To address these issues, we introduce REFINE-LM, a debiasing method that uses reinforcement learning to handle different types of biases without any fine-tuning. By training a simple model on top of the word probability distribution of a LM, our bias agnostic reinforcement learning method enables model debiasing without human annotations or significant computational resources. Experiments conducted on a wide range of models, including several LMs, show that our method (i) significantly reduces stereotypical biases while preserving LMs performance; (ii) is applicable to different types of biases, generalizing across contexts such as gender, ethnicity, religion, and nationality-based biases; and (iii) it is not expensive to train.",
            "corpus_id": 271902917,
            "sentences": [
                {
                    "corpus_id": "271902917",
                    "title": "REFINE-LM: Mitigating Language Model Stereotypes via Reinforcement Learning",
                    "text": "Using RL, our method does not require any form of manual annotations, but rather uses the LM output to mitigate a wide variety of biases in the answer. While RL has been successfully applied in algorithmic fairness [22,41,46], this is, to the best of our knowledge, the first approach that applies RL for mitigation a wide rage of biases, not only in \"more traditional\" masked LMs, but also in Large LMs such as LLama2 or Mistral. In particular, our method allows us to (i) reduce training resources, (ii) avoid the need for manual annotation, and (iii) support a wide range of stereotypical biases, including gender-occupation, ethnicity, nationality, and religion. The main contributions of our paper are the following: \n\n\u2022 We formulate bias mitigation as contextual bandits RL problem that uses bias measuring framework inspired by [27]. \u2022 We propose REFINE-LM that mitigates different types of stereotypes such as those based on gender, nationality, ethnicity, and religion from any LMs. As shown in our evaluation, REFINE-LM is easy to train and can successfully suppress stereotypes in LMs as well as LLMs without affecting model performance. \u2022 An evaluation of REFINE-LM based on (a) the definitions of bias on the datasets proposed by Li et al. [27], and (b) the performance of the debiased LM on downstream tasks. \n\nThe rest of the paper is organized as follows. Section 2 surveys state of the art in bias detection and mitigation for language models in general. Section 3 explains the framework used to quantify bias as well as the inner workings of REFINE-LM, our proposed solution to reduce bias in pre-trained LMs. Section 4 then describes the empirical study of REFINE-LM, and Section 5 discusses our results as well as avenues for future research.",
                    "score": 0.5395420725833373,
                    "section_title": "Introduction",
                    "char_start_offset": 4027,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 152,
                            "end": 430
                        },
                        {
                            "start": 431,
                            "end": 666
                        },
                        {
                            "start": 667,
                            "end": 721
                        },
                        {
                            "start": 724,
                            "end": 840
                        },
                        {
                            "start": 841,
                            "end": 991
                        },
                        {
                            "start": 992,
                            "end": 1148
                        },
                        {
                            "start": 1149,
                            "end": 1322
                        },
                        {
                            "start": 1325,
                            "end": 1371
                        },
                        {
                            "start": 1372,
                            "end": 1471
                        },
                        {
                            "start": 1472,
                            "end": 1627
                        },
                        {
                            "start": 1628,
                            "end": 1762
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 215,
                            "end": 219,
                            "matchedPaperCorpusId": "7137572"
                        },
                        {
                            "start": 219,
                            "end": 222,
                            "matchedPaperCorpusId": "234341131"
                        },
                        {
                            "start": 222,
                            "end": 225,
                            "matchedPaperCorpusId": "238586034"
                        },
                        {
                            "start": 835,
                            "end": 839,
                            "matchedPaperCorpusId": "222141056"
                        },
                        {
                            "start": 1253,
                            "end": 1257,
                            "matchedPaperCorpusId": "222141056"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96435546875
                },
                {
                    "corpus_id": "271902917",
                    "title": "REFINE-LM: Mitigating Language Model Stereotypes via Reinforcement Learning",
                    "text": "The success of (Large) Language Models (LMs) has led to a revolution in the domain of NLP, opening the door to numerous challenges. The emergence of LMs-based applications such as chatbots and text-based assistants with astounding capabilities has, on the one hand, sparked unprecedented enthusiasm within the research community [16,36]. However, it has motivated ethical concerns and araised questions about the risks this technology may pose to society, particularly algorithmic fairness and the proliferation of harmful stereotypical bias. Indeed, several studies have shown that LMs suffer from stereotypical biases, which can be detected, for instance, through Implicit Association Tests (IATs) [7]. These biases are still prevalent in recent LLMs such as ChatGPT, GPT4, etc., [26,40]. Figure 1 illustrates stereotypical biases (such as gender, ethnicity or religion) that can be observed when prompting ChatGPT (more examples and analysis with LLama2, Mistral and GPT4 are provided in Section 1 of the supplementary material). These findings ask for a thorough investigation of stereotypical bias in LMs, and for methods to mitigate their impact, perpetuation or even their exacerbation in various academic, societal and industrial applications. \n\nWhile some work has been proposed to mitigate bias in LMs, it remains challenging for several reasons. Firstly, metrics are highly task-dependent, i.e., quantifying stereotypical bias is highly dependent on the application at hand, meaning that the methods used to measure bias in LMs for one kind of bias can not be directly applied to other biases. For example, mitigation metrics for gender bias are typically not directly applicable to nationality-based or ethnic bias, e.g., gender bias mitigation relies on pronoun completion or the existence of sufficiently gendered phrases within corpora [8,31,47]. Secondly, even with adequate methods to measure bias, in practice, there is often a trade-off between bias mitigation and model performance [21], which can have a negative impact despite the bias removal. Namely, removing bias from a LM may risk deteriorating its performance on downstream applications such as questionanswering [49].",
                    "score": 0.5607534384264601,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 337
                        },
                        {
                            "start": 338,
                            "end": 542
                        },
                        {
                            "start": 543,
                            "end": 704
                        },
                        {
                            "start": 705,
                            "end": 790
                        },
                        {
                            "start": 791,
                            "end": 1032
                        },
                        {
                            "start": 1033,
                            "end": 1251
                        },
                        {
                            "start": 1254,
                            "end": 1356
                        },
                        {
                            "start": 1357,
                            "end": 1604
                        },
                        {
                            "start": 1605,
                            "end": 1861
                        },
                        {
                            "start": 1862,
                            "end": 2066
                        },
                        {
                            "start": 2067,
                            "end": 2196
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 333,
                            "end": 336,
                            "matchedPaperCorpusId": "212747830"
                        },
                        {
                            "start": 700,
                            "end": 703,
                            "matchedPaperCorpusId": "23163324"
                        },
                        {
                            "start": 782,
                            "end": 786,
                            "matchedPaperCorpusId": "261276445"
                        },
                        {
                            "start": 786,
                            "end": 789,
                            "matchedPaperCorpusId": "268417107"
                        },
                        {
                            "start": 1851,
                            "end": 1854,
                            "matchedPaperCorpusId": "231698886"
                        },
                        {
                            "start": 1854,
                            "end": 1857,
                            "matchedPaperCorpusId": "85518027"
                        },
                        {
                            "start": 1857,
                            "end": 1860,
                            "matchedPaperCorpusId": "4952494"
                        },
                        {
                            "start": 2002,
                            "end": 2006,
                            "matchedPaperCorpusId": "248780440"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96240234375
                }
            ],
            "relevance_judgement": 0.96435546875,
            "relevance_judgment_input_expanded": "# Title: REFINE-LM: Mitigating Language Model Stereotypes via Reinforcement Learning\n# Venue: European Conference on Artificial Intelligence\n# Authors: Rameez Qureshi, Naim Es-Sebbani, Luis Gal'arraga, Yvette Graham, Miguel Couceiro, Zied Bouraoui\n## Abstract\nWith the introduction of (large) language models, there has been significant concern about the unintended bias such models may inherit from their training data. A number of studies have shown that such models propagate gender stereotypes, as well as geographical and racial bias, among other biases. While existing works tackle this issue by preprocessing data and debiasing embeddings, the proposed methods require a lot of computational resources and annotation effort while being limited to certain types of biases. To address these issues, we introduce REFINE-LM, a debiasing method that uses reinforcement learning to handle different types of biases without any fine-tuning. By training a simple model on top of the word probability distribution of a LM, our bias agnostic reinforcement learning method enables model debiasing without human annotations or significant computational resources. Experiments conducted on a wide range of models, including several LMs, show that our method (i) significantly reduces stereotypical biases while preserving LMs performance; (ii) is applicable to different types of biases, generalizing across contexts such as gender, ethnicity, religion, and nationality-based biases; and (iii) it is not expensive to train.\n## Introduction\nThe success of (Large) Language Models (LMs) has led to a revolution in the domain of NLP, opening the door to numerous challenges. The emergence of LMs-based applications such as chatbots and text-based assistants with astounding capabilities has, on the one hand, sparked unprecedented enthusiasm within the research community [16,36]. However, it has motivated ethical concerns and araised questions about the risks this technology may pose to society, particularly algorithmic fairness and the proliferation of harmful stereotypical bias. Indeed, several studies have shown that LMs suffer from stereotypical biases, which can be detected, for instance, through Implicit Association Tests (IATs) [7]. These biases are still prevalent in recent LLMs such as ChatGPT, GPT4, etc., [26,40]. Figure 1 illustrates stereotypical biases (such as gender, ethnicity or religion) that can be observed when prompting ChatGPT (more examples and analysis with LLama2, Mistral and GPT4 are provided in Section 1 of the supplementary material). These findings ask for a thorough investigation of stereotypical bias in LMs, and for methods to mitigate their impact, perpetuation or even their exacerbation in various academic, societal and industrial applications. \n\nWhile some work has been proposed to mitigate bias in LMs, it remains challenging for several reasons. Firstly, metrics are highly task-dependent, i.e., quantifying stereotypical bias is highly dependent on the application at hand, meaning that the methods used to measure bias in LMs for one kind of bias can not be directly applied to other biases. For example, mitigation metrics for gender bias are typically not directly applicable to nationality-based or ethnic bias, e.g., gender bias mitigation relies on pronoun completion or the existence of sufficiently gendered phrases within corpora [8,31,47]. Secondly, even with adequate methods to measure bias, in practice, there is often a trade-off between bias mitigation and model performance [21], which can have a negative impact despite the bias removal. Namely, removing bias from a LM may risk deteriorating its performance on downstream applications such as questionanswering [49].\n...\nUsing RL, our method does not require any form of manual annotations, but rather uses the LM output to mitigate a wide variety of biases in the answer. While RL has been successfully applied in algorithmic fairness [22,41,46], this is, to the best of our knowledge, the first approach that applies RL for mitigation a wide rage of biases, not only in \"more traditional\" masked LMs, but also in Large LMs such as LLama2 or Mistral. In particular, our method allows us to (i) reduce training resources, (ii) avoid the need for manual annotation, and (iii) support a wide range of stereotypical biases, including gender-occupation, ethnicity, nationality, and religion. The main contributions of our paper are the following: \n\n\u2022 We formulate bias mitigation as contextual bandits RL problem that uses bias measuring framework inspired by [27]. \u2022 We propose REFINE-LM that mitigates different types of stereotypes such as those based on gender, nationality, ethnicity, and religion from any LMs. As shown in our evaluation, REFINE-LM is easy to train and can successfully suppress stereotypes in LMs as well as LLMs without affecting model performance. \u2022 An evaluation of REFINE-LM based on (a) the definitions of bias on the datasets proposed by Li et al. [27], and (b) the performance of the debiased LM on downstream tasks. \n\nThe rest of the paper is organized as follows. Section 2 surveys state of the art in bias detection and mitigation for language models in general. Section 3 explains the framework used to quantify bias as well as the inner workings of REFINE-LM, our proposed solution to reduce bias in pre-trained LMs. Section 4 then describes the empirical study of REFINE-LM, and Section 5 discusses our results as well as avenues for future research.",
            "reference_string": "[271902917 | Qureshi et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Promoting Equality in Large Language Models: Identifying and Mitigating the Implicit Bias based on Bayesian Theory",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 43,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.10608, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268085905",
                    "name": "Yongxin Deng"
                },
                {
                    "authorId": "1500386397",
                    "name": "Xihe Qiu"
                },
                {
                    "authorId": "2310866202",
                    "name": "Xiaoyu Tan"
                },
                {
                    "authorId": "2316549564",
                    "name": "Jing Pan"
                },
                {
                    "authorId": "2239987869",
                    "name": "Jue Chen"
                },
                {
                    "authorId": "2316785385",
                    "name": "Zhijun Fang"
                },
                {
                    "authorId": "2266466742",
                    "name": "Yinghui Xu"
                },
                {
                    "authorId": "2266389707",
                    "name": "Wei Chu"
                },
                {
                    "authorId": "2192603365",
                    "name": "Yuan Qi"
                }
            ],
            "abstract": "Large language models (LLMs) are trained on extensive text corpora, which inevitably include biased information. Although techniques such as Affective Alignment can mitigate some negative impacts of these biases, existing prompt-based attack methods can still extract these biases from the model's weights. Moreover, these biases frequently appear subtly when LLMs are prompted to perform identical tasks across different demographic groups, thereby camouflaging their presence. To address this issue, we have formally defined the implicit bias problem and developed an innovative framework for bias removal based on Bayesian theory, Bayesian-Theory based Bias Removal (BTBR). BTBR employs likelihood ratio screening to pinpoint data entries within publicly accessible biased datasets that represent biases inadvertently incorporated during the LLM training phase. It then automatically constructs relevant knowledge triples and expunges bias information from LLMs using model editing techniques. Through extensive experimentation, we have confirmed the presence of the implicit bias problem in LLMs and demonstrated the effectiveness of our BTBR approach.",
            "corpus_id": 271909713,
            "sentences": [
                {
                    "corpus_id": "271909713",
                    "title": "Promoting Equality in Large Language Models: Identifying and Mitigating the Implicit Bias based on Bayesian Theory",
                    "text": "Large language models (LLMs) are trained on extensive text corpora, which inevitably include biased information. Although techniques such as Affective Alignment can mitigate some negative impacts of these biases, existing prompt-based attack methods can still extract these biases from the model's weights. Moreover, these biases frequently appear subtly when LLMs are prompted to perform identical tasks across different demographic groups, thereby camouflaging their presence. To address this issue, we have formally defined the implicit bias problem and developed an innovative framework for bias removal based on Bayesian theory, Bayesian-Theory based Bias Removal (BTBR). BTBR employs likelihood ratio screening to pinpoint data entries within publicly accessible biased datasets that represent biases inadvertently incorporated during the LLM training phase. It then automatically constructs relevant knowledge triples and expunges bias information from LLMs using model editing techniques. Through extensive experimentation, we have confirmed the presence of the implicit bias problem in LLMs and demonstrated the effectiveness of our BTBR approach.",
                    "score": 0.5605214358850492,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96337890625
                }
            ],
            "relevance_judgement": 0.96337890625,
            "relevance_judgment_input_expanded": "# Title: Promoting Equality in Large Language Models: Identifying and Mitigating the Implicit Bias based on Bayesian Theory\n# Venue: arXiv.org\n# Authors: Yongxin Deng, Xihe Qiu, Xiaoyu Tan, Jing Pan, Jue Chen, Zhijun Fang, Yinghui Xu, Wei Chu, Yuan Qi\n## Abstract\nLarge language models (LLMs) are trained on extensive text corpora, which inevitably include biased information. Although techniques such as Affective Alignment can mitigate some negative impacts of these biases, existing prompt-based attack methods can still extract these biases from the model's weights. Moreover, these biases frequently appear subtly when LLMs are prompted to perform identical tasks across different demographic groups, thereby camouflaging their presence. To address this issue, we have formally defined the implicit bias problem and developed an innovative framework for bias removal based on Bayesian theory, Bayesian-Theory based Bias Removal (BTBR). BTBR employs likelihood ratio screening to pinpoint data entries within publicly accessible biased datasets that represent biases inadvertently incorporated during the LLM training phase. It then automatically constructs relevant knowledge triples and expunges bias information from LLMs using model editing techniques. Through extensive experimentation, we have confirmed the presence of the implicit bias problem in LLMs and demonstrated the effectiveness of our BTBR approach.\n",
            "reference_string": "[271909713 | Deng et al. | 2024 | Citations: 3]"
        },
        {
            "title": "SB-Bench: Stereotype Bias Benchmark for Large Multimodal Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 74,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.08779, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2345186336",
                    "name": "Vishal Narnaware"
                },
                {
                    "authorId": "2287846115",
                    "name": "Ashmal Vayani"
                },
                {
                    "authorId": "2110003398",
                    "name": "Rohit Gupta"
                },
                {
                    "authorId": "143951905",
                    "name": "S. Swetha"
                },
                {
                    "authorId": "2287971163",
                    "name": "Mubarak Shah"
                }
            ],
            "abstract": "Stereotype biases in Large Multimodal Models (LMMs) perpetuate harmful societal prejudices, undermining the fairness and equity of AI applications. As LMMs grow increasingly influential, addressing and mitigating inherent biases related to stereotypes, harmful generations, and ambiguous assumptions in real-world scenarios has become essential. However, existing datasets evaluating stereotype biases in LMMs often lack diversity and rely on synthetic images, leaving a gap in bias evaluation for real-world visual contexts. To address this, we introduce the Stereotype Bias Benchmark (SB-bench), the most comprehensive framework to date for assessing stereotype biases across nine diverse categories with non-synthetic images. SB-bench rigorously evaluates LMMs through carefully curated, visually grounded scenarios, challenging them to reason accurately about visual stereotypes. It offers a robust evaluation framework featuring real-world visual samples, image variations, and multiple-choice question formats. By introducing visually grounded queries that isolate visual biases from textual ones, SB-bench enables a precise and nuanced assessment of a model's reasoning capabilities across varying levels of difficulty. Through rigorous testing of state-of-the-art open-source and closed-source LMMs, SB-bench provides a systematic approach to assessing stereotype biases in LMMs across key social dimensions. This benchmark represents a significant step toward fostering fairness in AI systems and reducing harmful biases, laying the groundwork for more equitable and socially responsible LMMs. Our code and dataset are publicly available.",
            "corpus_id": 276317810,
            "sentences": [
                {
                    "corpus_id": "276317810",
                    "title": "SB-Bench: Stereotype Bias Benchmark for Large Multimodal Models",
                    "text": "Large Multimodal Models (LMMs) are an advanced extension of Large Language Models (LLMs) that enable AI systems to process and integrate both text and images. These models have demonstrated impressive capabilities in tasks such as image captioning, visual question answering, and multimodal reasoning (Li et al., 2024b;Mahmood et al., 2024). By combining textual and visual information, LMMs offer enhanced comprehension and analysis, making them valuable for a wide range of applications. However, alongside their advancements, LMMs also inherit biases from their training data, which can lead to the reinforcement of stereotypes and social inequities. \n\nBias in LMMs is particularly concerning in real-world applications, where fairness and inclusivity are essential for equi- Table 1: Comparison of various LMM evaluation benchmarks with a focus on stereotype bias. Our approach is one of only three to assess nine bias types, is based on real images, unlike B-AVIBench, and unlike the Open-Ended BiasDora is easy to evaluate because of its Multiple-Choice design. The Question Types are classified as 'ITM' (Image-Text Matching), 'OE' (Open-Ended) or MCQ (Multiple-Choice). \n\ntable outcomes. Existing biases in training data often manifest in model responses, leading to unintended but impactful consequences. Addressing these biases is crucial to ensuring that LMMs contribute positively to society while minimizing potential harms. Researchers have attempted to analyze and mitigate these biases through various benchmarks and evaluation frameworks. However, current approaches typically categorize biases within a limited set of domains and often rely on synthetic datasets, which fail to capture the complexity of bias in real-world scenarios (Fraser & Kiritchenko, 2024;Zhou et al., 2022;Howard et al., 2024b;Raza et al., 2025). \n\nRecent studies have introduced several bias evaluation benchmarks, such as VLStereoset and Social Counterfactuals, to assess bias in LMMs. While these benchmarks have contributed valuable insights, they suffer from notable limitations. Many of these approaches lack diversity in bias categories, restricting their effectiveness and generalizability.",
                    "score": 0.5375798786897148,
                    "section_title": "Introduction",
                    "char_start_offset": 816,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 159,
                            "end": 341
                        },
                        {
                            "start": 342,
                            "end": 489
                        },
                        {
                            "start": 490,
                            "end": 653
                        },
                        {
                            "start": 656,
                            "end": 868
                        },
                        {
                            "start": 869,
                            "end": 1067
                        },
                        {
                            "start": 1068,
                            "end": 1177
                        },
                        {
                            "start": 1180,
                            "end": 1195
                        },
                        {
                            "start": 1196,
                            "end": 1313
                        },
                        {
                            "start": 1314,
                            "end": 1437
                        },
                        {
                            "start": 1438,
                            "end": 1555
                        },
                        {
                            "start": 1556,
                            "end": 1837
                        },
                        {
                            "start": 1840,
                            "end": 1978
                        },
                        {
                            "start": 1979,
                            "end": 2075
                        },
                        {
                            "start": 2076,
                            "end": 2189
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1797,
                            "end": 1818,
                            "matchedPaperCorpusId": "265609296"
                        },
                        {
                            "start": 1818,
                            "end": 1836,
                            "matchedPaperCorpusId": "276317412"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96240234375
                }
            ],
            "relevance_judgement": 0.96240234375,
            "relevance_judgment_input_expanded": "# Title: SB-Bench: Stereotype Bias Benchmark for Large Multimodal Models\n# Venue: arXiv.org\n# Authors: Vishal Narnaware, Ashmal Vayani, Rohit Gupta, S. Swetha, Mubarak Shah\n## Abstract\nStereotype biases in Large Multimodal Models (LMMs) perpetuate harmful societal prejudices, undermining the fairness and equity of AI applications. As LMMs grow increasingly influential, addressing and mitigating inherent biases related to stereotypes, harmful generations, and ambiguous assumptions in real-world scenarios has become essential. However, existing datasets evaluating stereotype biases in LMMs often lack diversity and rely on synthetic images, leaving a gap in bias evaluation for real-world visual contexts. To address this, we introduce the Stereotype Bias Benchmark (SB-bench), the most comprehensive framework to date for assessing stereotype biases across nine diverse categories with non-synthetic images. SB-bench rigorously evaluates LMMs through carefully curated, visually grounded scenarios, challenging them to reason accurately about visual stereotypes. It offers a robust evaluation framework featuring real-world visual samples, image variations, and multiple-choice question formats. By introducing visually grounded queries that isolate visual biases from textual ones, SB-bench enables a precise and nuanced assessment of a model's reasoning capabilities across varying levels of difficulty. Through rigorous testing of state-of-the-art open-source and closed-source LMMs, SB-bench provides a systematic approach to assessing stereotype biases in LMMs across key social dimensions. This benchmark represents a significant step toward fostering fairness in AI systems and reducing harmful biases, laying the groundwork for more equitable and socially responsible LMMs. Our code and dataset are publicly available.\n## Introduction\nLarge Multimodal Models (LMMs) are an advanced extension of Large Language Models (LLMs) that enable AI systems to process and integrate both text and images. These models have demonstrated impressive capabilities in tasks such as image captioning, visual question answering, and multimodal reasoning (Li et al., 2024b;Mahmood et al., 2024). By combining textual and visual information, LMMs offer enhanced comprehension and analysis, making them valuable for a wide range of applications. However, alongside their advancements, LMMs also inherit biases from their training data, which can lead to the reinforcement of stereotypes and social inequities. \n\nBias in LMMs is particularly concerning in real-world applications, where fairness and inclusivity are essential for equi- Table 1: Comparison of various LMM evaluation benchmarks with a focus on stereotype bias. Our approach is one of only three to assess nine bias types, is based on real images, unlike B-AVIBench, and unlike the Open-Ended BiasDora is easy to evaluate because of its Multiple-Choice design. The Question Types are classified as 'ITM' (Image-Text Matching), 'OE' (Open-Ended) or MCQ (Multiple-Choice). \n\ntable outcomes. Existing biases in training data often manifest in model responses, leading to unintended but impactful consequences. Addressing these biases is crucial to ensuring that LMMs contribute positively to society while minimizing potential harms. Researchers have attempted to analyze and mitigate these biases through various benchmarks and evaluation frameworks. However, current approaches typically categorize biases within a limited set of domains and often rely on synthetic datasets, which fail to capture the complexity of bias in real-world scenarios (Fraser & Kiritchenko, 2024;Zhou et al., 2022;Howard et al., 2024b;Raza et al., 2025). \n\nRecent studies have introduced several bias evaluation benchmarks, such as VLStereoset and Social Counterfactuals, to assess bias in LMMs. While these benchmarks have contributed valuable insights, they suffer from notable limitations. Many of these approaches lack diversity in bias categories, restricting their effectiveness and generalizability.",
            "reference_string": "[276317810 | Narnaware et al. | 2025 | Citations: 3]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "270619502",
            "title": "Evaluating Short-Term Temporal Fluctuations of Social Biases in Social Media Data and Masked Language Models",
            "text": "1 Given this rapid increase and the significance of social media data as a source for training MLMs, an open question is whether LMs trained on social media data continue to demonstrate increasing levels of social biases.\n\nTo answer this question, we investigate multiple MLMs pretrained on snapshots of corpora collected from X at different points in time and evaluate the social biases in those MLMs using multiple benchmark datasets.We evaluate different types of social biases and observe that the overall bias tends to be stable over time, however, certain types of biases, such as race, skin color, religion, and sexual orientation, exhibit fluctuation over time.Based on the experimental results, we note that relying exclusively on the overall bias score can be misleading when evaluating social bias in MLMs, which highlights the importance of evaluating individual bias scores before deploying a model in downstream applications.Note that we primarily investigate whether language models (LMs) trained on social media data exhibit increasing levels of social biases over time in this paper.Our focus is on examining the trends in temporal variations of social biases in both models and datasets.Exploring the underlying causes could lead to sociologically oriented experiments and research questions, which are beyond the scope of this NLP-focused study.",
            "score": 0.751280692740059,
            "section_title": "Introduction",
            "char_start_offset": 1648,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 223,
                    "end": 436
                },
                {
                    "start": 436,
                    "end": 669
                },
                {
                    "start": 669,
                    "end": 939
                },
                {
                    "start": 939,
                    "end": 1100
                },
                {
                    "start": 1100,
                    "end": 1205
                },
                {
                    "start": 1205,
                    "end": 1364
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.802734375
        },
        {
            "corpus_id": "267770177",
            "title": "Potential and Challenges of Model Editing for Social Debiasing",
            "text": "Debiasing Addressing social bias in language models is an ongoing challenge that has received significant attention. Strategies for mitigating bias in language models can be classified based on different stages of the model workflow: Preprocessing techniques aim to detect and eliminate bias and unfairness early on, either within the dataset (Zmigrod et al., 2019;Dinan et al., 2020;Abid et al., 2021;Qian et al., 2022a;Ghanbarzadeh et al., 2023) or prompt (Mattern et al., 2022;Fatemi et al., 2023;Yang et al., 2023). In-training bias mitigation techniques focus on reducing bias and unfairness during model training, by adjusting model architecture (Bartl et al., 2020;Han et al., 2022), modifying loss functions (Liu et al., 2020;Webster et al., 2020;Ouyang et al., 2022;Woo et al., 2023;Park et al., 2023;Zhou et al., 2023;Li et al., 2023), or selectively updating parameters (Qian et al., 2022b;Ranaldi et al., 2023;Yu et al., 2023). \n\nIntraprocessing approaches alter decoding behavior (Saunders et al., 2022;Meade et al., 2023;Kim et al., 2023;Chung et al., 2023;Hallinan et al., 2023) without additional training or fine-tuning. \n\nPost-processing techniques primarily adjust model outputs to address bias and unfairness, without directly accessing the model itself (He et al., 2021;Tokpo and Calders, 2022;Majumder et al., 2022;Dhingra et al., 2023). However, effectively modifying bias in pre-trained large language models while minimizing disruption to the model's capabilities remains largely unexplored. \n\nModel Editing To address inaccuracies and biases in Large Language Models, various model editing techniques have been developed for efficient post-training adjustments.",
            "score": 0.7403710983387334,
            "section_title": "Related Work",
            "char_start_offset": 4897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1137
                },
                {
                    "start": 1140,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1516
                },
                {
                    "start": 1519,
                    "end": 1687
                }
            ],
            "ref_mentions": [
                {
                    "start": 343,
                    "end": 365,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 365,
                    "end": 384,
                    "matchedPaperCorpusId": "207852875"
                },
                {
                    "start": 402,
                    "end": 421,
                    "matchedPaperCorpusId": "249062690"
                },
                {
                    "start": 421,
                    "end": 447,
                    "matchedPaperCorpusId": "259859044"
                },
                {
                    "start": 480,
                    "end": 500,
                    "matchedPaperCorpusId": "238582879"
                },
                {
                    "start": 500,
                    "end": 518,
                    "matchedPaperCorpusId": "253446867"
                },
                {
                    "start": 652,
                    "end": 672,
                    "matchedPaperCorpusId": "225094152"
                },
                {
                    "start": 672,
                    "end": 689,
                    "matchedPaperCorpusId": "247694107"
                },
                {
                    "start": 716,
                    "end": 734,
                    "matchedPaperCorpusId": "204838020"
                },
                {
                    "start": 755,
                    "end": 775,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 775,
                    "end": 792,
                    "matchedPaperCorpusId": "258537309"
                },
                {
                    "start": 792,
                    "end": 810,
                    "matchedPaperCorpusId": "257079734"
                },
                {
                    "start": 810,
                    "end": 828,
                    "matchedPaperCorpusId": "259370743"
                },
                {
                    "start": 828,
                    "end": 844,
                    "matchedPaperCorpusId": "259342087"
                },
                {
                    "start": 881,
                    "end": 901,
                    "matchedPaperCorpusId": "249062690"
                },
                {
                    "start": 922,
                    "end": 938,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 993,
                    "end": 1016,
                    "matchedPaperCorpusId": "233240748"
                },
                {
                    "start": 1035,
                    "end": 1052,
                    "matchedPaperCorpusId": "254926596"
                },
                {
                    "start": 1052,
                    "end": 1071,
                    "matchedPaperCorpusId": "259096160"
                },
                {
                    "start": 1071,
                    "end": 1093,
                    "matchedPaperCorpusId": "252734135"
                },
                {
                    "start": 1274,
                    "end": 1291,
                    "matchedPaperCorpusId": "237634972"
                },
                {
                    "start": 1291,
                    "end": 1315,
                    "matchedPaperCorpusId": "246210255"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95068359375
        },
        {
            "corpus_id": "265212726",
            "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
            "text": "The unparalleled ability of language models (LMs) to generalize from vast corpora is tinged by an inherent reinforcement of social biases. These biases are not merely encoded within LMs' representations but are also perpetuated to downstream tasks (Blodgett et al., 2021;Sta\u0144czak and Augenstein, 2021), where they can manifest in an uneven treatment of different demographic groups (Rudinger et al., 2018;Stanovsky et al., 2019;Kiritchenko and Mohammad, 2018;Venkit et al., 2022). \n\nDirect analysis of biases encoded within LMs allows us to pinpoint the problem at its source, potentially obviating the need for addressing it for ev- ery application (Nangia et al., 2020). Therefore, a number of studies have attempted to evaluate social biases within LMs (Nangia et al., 2020;Nadeem et al., 2021;Sta\u0144czak et al., 2023;Nozza et al., 2022a). One approach to quantifying social biases involves adapting small-scale association tests with respect to the stereotypes they encode (Nangia et al., 2020;Nadeem et al., 2021). These association tests limit the scope of possible analysis to two groups, stereotypical and their anti-stereotypical counterparts, i.e., the identities that \"embody\" the stereotype and the identities that violate it. This binary approach, which assumes a singular \"ground truth\" with respect to a stereotypical statement, has restricted the depth of the analysis and simplified the complexity of social identities and their associated stereotypes. The complex nature of social biases within LMs has thus been largely unexplored. \n\nOur Social Bias Probing framework, as outlined in Fig. 1, is specifically designed to enable a nuanced understanding of biases inherent in language models. Accordingly, the input of our approach consists of a set stereotypes and identities. To this end, we generate our probing dataset by com-arXiv:2311.09090v4",
            "score": 0.7318258641002127,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 480
                },
                {
                    "start": 483,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1548
                },
                {
                    "start": 1551,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1862
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 271,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 382,
                    "end": 405,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 405,
                    "end": 428,
                    "matchedPaperCorpusId": "173991101"
                },
                {
                    "start": 428,
                    "end": 459,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 459,
                    "end": 479,
                    "matchedPaperCorpusId": "252819117"
                },
                {
                    "start": 650,
                    "end": 671,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 756,
                    "end": 777,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 777,
                    "end": 797,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 797,
                    "end": 819,
                    "matchedPaperCorpusId": "233241166"
                },
                {
                    "start": 819,
                    "end": 839,
                    "matchedPaperCorpusId": "248780490"
                },
                {
                    "start": 975,
                    "end": 996,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 996,
                    "end": 1016,
                    "matchedPaperCorpusId": "215828184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.966796875
        },
        {
            "corpus_id": "235623756",
            "title": "Towards Understanding and Mitigating Social Biases in Language Models",
            "text": "Social biases in text generation: Recent work has focused on defining and evaluating social bias (Nadeem et al., 2020;Sheng et al., 2019) as well as other notions of human-aligned values such as ethics (Hendrycks et al., 2021), social bias implications (Sap et al., 2020), and toxic speech (Gehman et al., 2020) in generated text. Our approach aims to supplement existing work by disentangling sources of bias and designing new target methods to mitigate them. We also evaluate our method on the benchmarks proposed in Nadeem et al. (2020) and Sheng et al. (2019). Existing approaches towards mitigating biases in generation currently require retraining the models through adversarial trigger prompts (Sheng et al., 2020), data augmentation or collection (Dinan et al., 2020), and different objective functions (Qian et al., 2019;Huang et al., 2020). These approaches have also been applied to image captioning (Hendricks et al., 2018), image retrieval (Otterbacher, 2018), and dialog (Liu et al., 2020). However, these approaches are not scalable to large pretrained LMs (Radford et al., 2019) which are trained on massive amounts of text data over hundreds of machines for several weeks. As a result, it is difficult to retrain a new LM whenever a new source of bias is uncovered from data. Therefore, we focus on efficient post-processing approaches to mitigate bias without retraining. \n\nSocial biases in text embeddings: A closely related line of work lies in measuring and mitigating biases in embedding spaces. For example, word embeddings are shown to reflect and propagate social biases in the form of undesirable associations that reinforce negative stereotypes about particular social groups (Lauscher and Glava\u0161, 2019;Caliskan et al., 2017;Bolukbasi et al., 2016).",
            "score": 0.727223890279954,
            "section_title": "Related Work",
            "char_start_offset": 6071,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1389
                },
                {
                    "start": 1392,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1776
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 137,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 202,
                    "end": 226,
                    "matchedPaperCorpusId": "220968818"
                },
                {
                    "start": 253,
                    "end": 271,
                    "matchedPaperCorpusId": "207853290"
                },
                {
                    "start": 544,
                    "end": 563,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 755,
                    "end": 775,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 811,
                    "end": 830,
                    "matchedPaperCorpusId": "170078973"
                },
                {
                    "start": 830,
                    "end": 849,
                    "matchedPaperCorpusId": "207847197"
                },
                {
                    "start": 911,
                    "end": 935,
                    "matchedPaperCorpusId": "4384334"
                },
                {
                    "start": 953,
                    "end": 972,
                    "matchedPaperCorpusId": "195351344"
                },
                {
                    "start": 985,
                    "end": 1003,
                    "matchedPaperCorpusId": "204838020"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94970703125
        },
        {
            "corpus_id": "235358955",
            "title": "RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models",
            "text": "We now describe our framework for bias evaluation in conversational language models (LMs), which couples (1) a bias measure computed on the test portions of REDDITBIAS with (2) task-specific performance on downstream dialog tasks. The latter aims to capture potential negative effects that debiasing techniques may have on downstream dialog performance of conversational LMs.",
            "score": 0.7141898546439467,
            "section_title": "Evaluation Framework",
            "char_start_offset": 10711,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82763671875
        },
        {
            "corpus_id": "236477795",
            "title": "He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation",
            "text": "Social biases with respect to demographics (e.g., gender, age, race) in datasets are often encoded in the large pre-trained language models trained on them. Prior works have largely focused on mitigating biases in context-free representations, with recent shift to contextual ones. While this is useful for several word and sentence-level classi\ufb01cation tasks, mitigating biases in only the representations may not suf-\ufb01ce to use these models for language generation tasks, such as auto-completion, summarization, or dialogue generation. In this paper, we propose an approach to mitigate social biases in BERT, a large pre-trained contextual language model, and show its effectiveness in \ufb01ll-in-the-blank sentence completion and summarization tasks. In addition to mitigating biases in BERT, which in general acts as an encoder, we propose lexical co-occurrence-based bias penalization in the decoder units in generation frameworks, and show bias mitigation in summarization. Finally, our approach results in better debiasing of BERT-based representations compared to post training bias mitigation, thus illustrating the ef\ufb01cacy of our approach to not just mitigate biases in representations, but also generate text with reduced biases.",
            "score": 0.7041707079260647,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.974609375
        },
        {
            "corpus_id": "258865522",
            "title": "An Efficient Multilingual Language Model Compression through Vocabulary Trimming",
            "text": "Pre-trained LMs are known to contain undesirable biases to generate toxic contents in some edge cases (Schick et al., 2021), so the resulting models could inherit such biases. While we have not analysed in detail the output of all models in the tasks evaluated, in this paper we have made an attempt to study this effect in terms of social biases for both base pretrained LMs and fine-tuned LMs. A Top-n VT of XLM-R \n\nTable 4 shows the results of XLM-R fine-tuned on sentiment and NLI with post/pre-VT for different top-n.",
            "score": 0.7021430432601786,
            "section_title": "Ethics Statement",
            "char_start_offset": 26252,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 522
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 123,
                    "matchedPaperCorpusId": "232075876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "258686693",
            "title": "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models",
            "text": ". Further, while the overall performance of hate speech and misinformation detectors remains consistent across such politically-biased LMs, these models exhibit significantly different behaviors against different identity groups and partisan media sources. ( \u00a74.2).\n\nThe main contributions of this paper are novel methods to quantify political biases in LMs, and findings that shed new light on how ideological polarization in pretraining corpora propagates biases into language models, and subsequently into social-oriented downstream tasks. In \u00a75, we discuss implications of our findings for NLP research, that no language model can be entirely free from social biases, and propose future directions to mitigate unfairness.",
            "score": 0.6927549540387348,
            "section_title": "Introduction",
            "char_start_offset": 3752,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90625
        },
        {
            "corpus_id": "239015827",
            "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
            "text": "Large pre-trained language models have proven effective across a variety of tasks in natural language processing, often obtaining state of the art performance (Peters et al., 2018;Devlin et al., 2019;Radford et al., 2019;Brown et al., 2020). These models are typically trained on large amounts of text, originating from unmoderated sources, such as the internet. While the performance of these pre-trained models is remarkable, recent work has shown that they capture social biases from the data they are trained on (May et al. 2019;Kurita et al. 2019;Webster et al. 2020;Nangia et al. 2020;Nadeem et al. 2021, inter alia). Because of these findings, an increasing amount of research has focused on developing techniques to mitigate these biases (Liang et al., 2020;Ravfogel et al., 2020;Webster et al., 2020;Kaneko and Bollegala, 2021;Schick et al., 2021;Lauscher et al., 2021). However, the proposed techniques are often not investigated thoroughly. For instance, much work focuses only on mitigating gender bias despite pre-trained language models being plagued by other social biases (e.g., racial or religious bias). Additionally, the impact that debiasing has on both downstream task performance, as well as language modeling ability, is often not well explored.\n\nIn this paper, we perform an empirical survey of the effectiveness of five recently proposed debiasing techniques for pre-trained language models: 2 Counterfactual Data Augmentation (CDA; Zmigrod et al. 2019;Webster et al. 2020), Dropout (Webster et al., 2020), Iterative Nullspace Projection (INLP; Ravfogel et al. 2020), Self-Debias (Schick et al., 2021), and SentenceDebias (Liang et al., 2020). Following the taxonomy described by Blodgett et al. (2020), our work studies the effectiveness of these techniques in mitigating representational biases from pre-trained language models. More specifically, we investigate mitigating gender, racial, and religious biases in three masked language models (",
            "score": 0.6899015443935874,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95458984375
        },
        {
            "corpus_id": "268385538",
            "title": "Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information",
            "text": "The presence of social bias in training data presents a significant challenge in the development of language models for real-world applications.While these models possess remarkable capabilities, biases within the data can lead to unfair outcomes.Mitigating these biases is crucial, but it becomes particularly challenging when acquiring or accessing sensitive attribute labels is costly or unfeasible.\n\nStudies showed that language models have the ability to capture demographic information about the writer, including race or gender, within their representations (Caliskan et al., 2017;Zhao et al., 2018).However, this capability can introduce unintended biases, leading to discriminatory outputs (De-Arteaga et al., 2019).\n\nCommon approaches for social bias mitigation require explicit annotation of biases for each sample in the data (Beutel et al., 2017;Zhang et al., 2018).Recent concept removal methods (Ravfogel et al., 2020(Ravfogel et al., , 2022a,b;,b;Iskander et al., 2023) Figure 1: Our debiasing method consists of defining task-specific representations for each social attribute, measuring similarity in the representation space for each example, and utilizing the KL loss to encourage uniform probabilities across social groups.\n\nhave shown promise in addressing social bias by removing sensitive attributes.These approaches rely on training classifiers for predicting the sensitive attribute, and training such classifiers typically requires a significant amount of annotated data.\n\nA promising line of research has emerged that aims to mitigate bias without relying on explicit information about the biases present in the data.For instance, Just Train Twice (JTT) (Liu et al., 2021) employs a two-step training process.In the second step, a second model is trained on up-weighed training examples that were misclassified by the first model.Another method is BLIND (Orgad and Belinkov, 2023), which introduces a success detector and down-weighs examples for which the detector accurately predicts the outcome.\n\nIn this paper, we propose DAFAIR: Demographics-Agnostic Fairness, a novel approach for mitigating social bias during the fine-tuning process of language models, without relying on demographic information.",
            "score": 0.6847789735312821,
            "section_title": "Introduction and Background",
            "char_start_offset": 30,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 247
                },
                {
                    "start": 247,
                    "end": 402
                },
                {
                    "start": 404,
                    "end": 607
                },
                {
                    "start": 607,
                    "end": 725
                },
                {
                    "start": 727,
                    "end": 879
                },
                {
                    "start": 879,
                    "end": 1244
                },
                {
                    "start": 1246,
                    "end": 1324
                },
                {
                    "start": 1324,
                    "end": 1498
                },
                {
                    "start": 1500,
                    "end": 1645
                },
                {
                    "start": 1645,
                    "end": 1737
                },
                {
                    "start": 1737,
                    "end": 1858
                },
                {
                    "start": 1858,
                    "end": 2026
                },
                {
                    "start": 2028,
                    "end": 2232
                }
            ],
            "ref_mentions": [
                {
                    "start": 565,
                    "end": 588,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 588,
                    "end": 606,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 699,
                    "end": 724,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 859,
                    "end": 878,
                    "matchedPaperCorpusId": "9424845"
                },
                {
                    "start": 910,
                    "end": 932,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 963,
                    "end": 985,
                    "matchedPaperCorpusId": "258740820"
                },
                {
                    "start": 1682,
                    "end": 1700,
                    "matchedPaperCorpusId": "235825419"
                },
                {
                    "start": 1882,
                    "end": 1908,
                    "matchedPaperCorpusId": "254877664"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8994140625
        },
        {
            "corpus_id": "249017622",
            "title": "On Measuring Social Biases in Prompt-Based Multi-Task Learning",
            "text": "In order to obtain a strong task-specific model to tackle various NLP tasks, the de facto practice has been to use a pretrained language model and fine-tune it on a downstream task (Alberti et al., 2019;Aky\u00fcrek et al., 2020;. We call these specific checkpoints of language models tailored for a particular downstream task task-conditioned LMs and non-conditioned versions general-purpose LMs. Previous work established that both types of models exhibit social biases (Zhao et al., 2019;Schick et al., 2021). In the following parts, we discuss efforts aiming at systemically quantifying these biases in LMs.",
            "score": 0.6798716076626161,
            "section_title": "Related Works",
            "char_start_offset": 23583,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 224,
                    "matchedPaperCorpusId": "220046621"
                },
                {
                    "start": 467,
                    "end": 486,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 486,
                    "end": 506,
                    "matchedPaperCorpusId": "232075876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87548828125
        },
        {
            "corpus_id": "273901658",
            "title": "Can Machine Unlearning Reduce Social Bias in Language Models?",
            "text": "In our second approach, we experiment with the idea of task vectors (Ilharco et al., 2022;Zhang et al., 2023), for mitigating social biases or stereotypes in LMs. Previous studies (Ilharco et al., 2022) apply this method on language models only for reducing toxicity, a relatively less challenging task compared to social bias mitigation. See Appendix B.1 for more details about the method. Data. We first fine-tune the base pre-trained model on a set of biased sentences to obtain a biased model. Next, we calculate the task vectors by subtracting the base model weights from the newly trained biased model. Consequently, these task vectors are negated and applied to the base model with an appropriate scaling coefficient to get the final debiased model. The biased sentences used for fine-tuning are combined from StereoSet (Nadeem et al., 2020) and Civil Comments (Duchene et al., 2023) datasets. We use two dataset versions: a small and a large version, to highlight the effect of dataset size. The small version consists of the same set of instances used in the DPO method (see Table 4) for a fair comparison. We modify the dataset by concatenating the \"context\" and \"stereotyped\" response to create a biased sentence. This small version is referred to as TV-2k. The large dataset expands beyond the small version and consists of a mix of StereoSet (Nadeem et al., 2020) and Civil Comments (Duchene et al., 2023). For Stere-oSet, the formulation is similar to TV-2k. However, we concatenate the \"context\" and \"stereotyped\" response across the intersentence and intrasentence categories. For the Civil Comments dataset, we filter sentences with toxicity scores greater than 0.5 and keep the identity attack and sexual explicit domains, since only these domains capture social biases relevant for our study. This combined dataset is referred to as TV-14k (or TV). Table 5 provides a summary of the number of training samples. \n\nOur approach.",
            "score": 0.6780379365638989,
            "section_title": "Negation via Task Vector",
            "char_start_offset": 6449,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1929
                },
                {
                    "start": 1932,
                    "end": 1945
                }
            ],
            "ref_mentions": [
                {
                    "start": 827,
                    "end": 848,
                    "matchedPaperCorpusId": "258688053"
                },
                {
                    "start": 1355,
                    "end": 1376,
                    "matchedPaperCorpusId": "258688053"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92724609375
        },
        {
            "corpus_id": "269773271",
            "title": "Large Language Model Bias Mitigation from the Perspective of Knowledge Editing",
            "text": "Pre-trained Large Language Models (LLMs) have demonstrated exceptional performance on many tasks (Devlin et al., 2018;Floridi & Chiriatti, 2020;Brown et al., 2020).However, the encoded social stereotypes and human-like biases inevitably cause undesired behaviors when deploying LLMs in practice (Zhao et al., 2019;Navigli et al., 2023;Sheng et al., 2021).Existing approaches to mitigate biases in LLMs are mainly categorized into: (1) Fine-tuning (Zmigrod et al., 2019;Webster et al., 2020;He et al., 2022;Liang et al., 2020;Lauscher et al., 2021), which includes techniques such as re-balanced corpus pre-training, contrastive learning, projection methods, and efficient parameter tuning.(2) Prompt-tuning (Guo et al., 2022;Yang et al., 2023;Li et al., 2023b;Dong et al., 2023), which involves creating prompts to address social biases.Existing debiasing approaches usually equalize different groups, resulting in unreasonable predictions.(c) Our proposed method performs fine-grained calibration with biased knowledge, while maintaining the others.\n\nHowever, existing techniques treat social groups as interchangeable (Gallegos et al., 2023) and neutralize protected attributes of different social groups in model inputs or outputs, while ignoring or concealing distinct mechanisms of different social groups (Hanna et al., 2020), as shown in Figure 1.Furthermore, existing debiasing evaluation metrics mainly focus on the degree of bias, but fail to measure whether the model retains its origin knowledge (Gallegos et al., 2023) of discerning reasonable disparities among different social groups.\n\nTo address these issues, we first establish a more comprehensive debiasing benchmark BiasKE by extending existing datasets with additional constructed data and evaluation metrics on fairness, specificity, and generalization.Moreover, we propose a novel method Fairness-Stamp (FAST) for editable bias mitigation.",
            "score": 0.6773888502084764,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 355
                },
                {
                    "start": 355,
                    "end": 689
                },
                {
                    "start": 689,
                    "end": 837
                },
                {
                    "start": 837,
                    "end": 940
                },
                {
                    "start": 940,
                    "end": 1050
                },
                {
                    "start": 1052,
                    "end": 1354
                },
                {
                    "start": 1354,
                    "end": 1599
                },
                {
                    "start": 1601,
                    "end": 1825
                },
                {
                    "start": 1825,
                    "end": 1912
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 163,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 314,
                    "end": 335,
                    "matchedPaperCorpusId": "258688053"
                },
                {
                    "start": 707,
                    "end": 725,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 725,
                    "end": 743,
                    "matchedPaperCorpusId": "253446867"
                },
                {
                    "start": 1311,
                    "end": 1331,
                    "matchedPaperCorpusId": "208921008"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98046875
        },
        {
            "corpus_id": "239015827",
            "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
            "text": "Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model\u2019s language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.",
            "score": 0.6756416381123965,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93896484375
        },
        {
            "corpus_id": "274776546",
            "title": "Bias Vector: Mitigating Biases in Language Models with Task Arithmetic Approach",
            "text": "As language models (LMs) have become more widely used in recent years, the biases and stereotypes inherent in the training data for LMs are creating social problems (Liu et al., 2020;Kumar et al., 2023). These biases reflect the stereotypes of specific social groups (such as those related to race, profession, gender, and religion) (Bolukbasi et al., 2016;Nadeem et al., 2021). People tend to use racially biased stereotypical phrases (like \"The men from afghanistan ride on camels\"), rather than phrases that contradict stereotypes (e.g., \"The men from afghanistan ride on skateboards\"). 1 s a consequence, LMs often make unfair predictions about certain groups, leading to biased or stereotyped outcomes that can cause discomfort among users. The widespread and frequent use of LMs (such as ChatGPT (GPT-3.5 / 4) (OpenAI, 2022(OpenAI, , 2024))), with their biased predictions is resulting in discrimination and inequality, which is becoming a social problem (Feng et al., 2023). Hence, developing effective bias mitigation methods for LM systems is essential. \n\nPrior to the advent of Large Language Models (LLMs), debiasing studies primarily targeted word embeddings (Zhao et al., 2018;Kaneko and Bollegala, 2019;Wang et al., 2020). Models such as word2vec (Mikolov et al., 2013) are debiased by reshaping the word embeddings in their output representations. However, these methods are less practical for Transformer-based LMs, such as BERT (Devlin et al., 2019), because the model parameters need to be debiased as the required model outputs vary depending on the downstream task. \n\nTo address biases in Transformer-based LMs, methods have been developed to reduce biases and stereotypes by continually training of LMs with debiased datasets (Zmigrod et al., 2019;Webster et al., 2020;Dinan et al., 2020;Barikeri et al., 2021;Jentzsch and Turan, 2022).",
            "score": 0.6742953172891046,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1062
                },
                {
                    "start": 1065,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1585
                },
                {
                    "start": 1588,
                    "end": 1857
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 183,
                    "matchedPaperCorpusId": "204838020"
                },
                {
                    "start": 183,
                    "end": 202,
                    "matchedPaperCorpusId": "252907607"
                },
                {
                    "start": 333,
                    "end": 357,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 357,
                    "end": 377,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 961,
                    "end": 980,
                    "matchedPaperCorpusId": "258686693"
                },
                {
                    "start": 1171,
                    "end": 1190,
                    "matchedPaperCorpusId": "52161864"
                },
                {
                    "start": 1190,
                    "end": 1217,
                    "matchedPaperCorpusId": "173991106"
                },
                {
                    "start": 1217,
                    "end": 1235,
                    "matchedPaperCorpusId": "204770514"
                },
                {
                    "start": 1445,
                    "end": 1466,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "277954809",
            "title": "Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification",
            "text": "Large language models (LLMs) acquire general linguistic knowledge from massive-scale pretraining. However, pretraining data mainly comprised of web-crawled texts contain undesirable social biases which can be perpetuated or even amplified by LLMs. In this study, we propose an efficient yet effective annotation pipeline to investigate social biases in the pretraining corpora. Our pipeline consists of protected attribute detection to identify diverse demographics, followed by regard classification to analyze the language polarity towards each attribute. Through our experiments, we demonstrate the effect of our bias analysis and mitigation measures, focusing on Common Crawl as the most representative pretraining corpus.",
            "score": 0.6694788408560218,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9853515625
        },
        {
            "corpus_id": "275336873",
            "title": "Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection",
            "text": "Large Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content. While extensive research has investigated bias in LLMs, prior work has predominantly focused on explicit bias, leaving the more nuanced implicit biases largely unexplored. This paper presents a systematic framework grounded in social psychology theories to investigate and compare explicit and implicit biases in LLMs. We propose a novel\"self-reflection\"based evaluation framework that operates in two phases: first measuring implicit bias through simulated psychological assessment methods, then evaluating explicit bias by prompting LLMs to analyze their own generated content. Through extensive experiments on state-of-the-art LLMs across multiple social dimensions, we demonstrate that LLMs exhibit a substantial inconsistency between explicit and implicit biases, where explicit biases manifest as mild stereotypes while implicit biases show strong stereotypes. Furthermore, we investigate the underlying factors contributing to this explicit-implicit bias inconsistency. Our experiments examine the effects of training data scale, model parameters, and alignment techniques. Results indicate that while explicit bias diminishes with increased training data and model size, implicit bias exhibits a contrasting upward trend. Notably, contemporary alignment methods (e.g., RLHF, DPO) effectively suppress explicit bias but show limited efficacy in mitigating implicit bias. These findings suggest that while scaling up models and alignment training can address explicit bias, the challenge of implicit bias requires novel approaches beyond current methodologies.",
            "score": 0.6632861374255686,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "256630982",
            "title": "WordTies: Measuring Word Associations in Language Models via Constrained Sampling",
            "text": "As discussed in \u00a75, measuring and mitigating social biases have been a prominent and motivating application of word associations. The algorithm we proposed contributes a practical way to measure associations to words related to social aspects (such as profession, gender, race, and other aspects) in language models with higher precisions and fewer confounders. These associations, in addition to being a measure of biases, could potentially serve as a signal for fine-tuning LMs, and lead to language models with less biases.",
            "score": 0.6625759356331056,
            "section_title": "Ethics Statement",
            "char_start_offset": 27988,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 526
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92822265625
        },
        {
            "corpus_id": "270619502",
            "title": "Evaluating Short-Term Temporal Fluctuations of Social Biases in Social Media Data and Masked Language Models",
            "text": "Social biases such as gender or racial biases have been reported in language models (LMs), including Masked Language Models (MLMs). Given that MLMs are continuously trained with increasing amounts of additional data collected over time, an important yet unanswered question is how the social biases encoded with MLMs vary over time. In particular, the number of social media users continues to grow at an exponential rate, and it is a valid concern for the MLMs trained specifically on social media data whether their social biases (if any) would also amplify over time. To empirically analyse this problem, we use a series of MLMs pretrained on chronologically ordered temporal snapshots of corpora. Our analysis reveals that, although social biases are present in all MLMs, most types of social bias remain relatively stable over time (with a few exceptions). To further understand the mechanisms that influence social biases in MLMs, we analyse the temporal corpora used to train the MLMs. Our findings show that some demographic groups, such as male, obtain higher preference over the other, such as female on the training corpora constantly.",
            "score": 0.6614854300148533,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88916015625
        },
        {
            "corpus_id": "268385538",
            "title": "Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information",
            "text": "We introduced DAFAIR, a novel approach for mitigating social bias in language models without explicit demographic information.Our method leverages semantic similarity to manipulate the model's text representations during finetuning to promote fairness.Experimental results on two tasks and under different settings demonstrated the effectiveness of DAFAIR in reducing bias and improving fairness while maintaining competitive downstream task performance, even with limited or no labeled demographic data.With its focus on social bias, DAFAIR offers a flexible framework adaptable to address other forms of bias through the modification of prototypical texts.\n\nIn conclusion, our approach offers a practical and flexible solution for bias mitigation in realworld applications, contributing to the development of fairer language models.",
            "score": 0.6584909586096218,
            "section_title": "Conclusion",
            "char_start_offset": 14903,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 126,
                    "end": 252
                },
                {
                    "start": 252,
                    "end": 504
                },
                {
                    "start": 504,
                    "end": 658
                },
                {
                    "start": 660,
                    "end": 834
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9111328125
        },
        {
            "corpus_id": "245144648",
            "title": "Few-shot Instruction Prompts for Pretrained Language Models to Detect Social Biases",
            "text": "Detecting social bias in text is challenging due to nuance, subjectivity, and difficulty in obtaining good quality labeled datasets at scale, especially given the evolving nature of social biases and society. To address these challenges, we propose a few-shot instruction-based method for prompting pre-trained language models (LMs). We select a few class-balanced exemplars from a small support repository that are closest to the query to be labeled in the embedding space. We then provide the LM with instruction that consists of this subset of labeled exemplars, the query text to be classified, a definition of bias, and prompt it to make a decision. We demonstrate that large LMs used in a few-shot context can detect different types of fine-grained biases with similar and sometimes superior accuracy to fine-tuned models. We observe that the largest 530B parameter model is significantly more effective in detecting social bias compared to smaller models (achieving at least 13% improvement in AUC metric compared to other models). It also maintains a high AUC (dropping less than 2%) when the labeled repository is reduced to as few as $100$ samples. Large pretrained language models thus make it easier and quicker to build new bias detectors.",
            "score": 0.651805759008419,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93310546875
        },
        {
            "corpus_id": "265609639",
            "title": "Data Management For Training Large Language Models: A Survey",
            "text": "partisan leanings and how it is propagated to language models even further to downstream tasks. As addressed in previous research, there is still a large gap between current prominent LLMs and ideal LLMs without social biases. Many questions are worth exploring, such as how to mitigate the potential biases in pretraining datasets, the existence of bias in the SFT datasets, and whether it is feasible to reduce social bias through SFT.",
            "score": 0.65142686625644,
            "section_title": "A Takeaways",
            "char_start_offset": 33832,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 437
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95068359375
        },
        {
            "corpus_id": "265212726",
            "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
            "text": "Social bias2 can be defined as the manifestation through language of \"prejudices, stereotypes, and discriminatory attitudes against certain groups of people\" (Navigli et al., 2023). These biases are featured in training datasets and are carried over into downstream applications, resulting in, for instance, classification errors concerning specific minorities and the generation of harmful content when models are prompted with sensitive identities (Cui et al., 2024;Gallegos et al., 2023). \n\nTo measure the extent to which social bias is present in language models, we propose a Social Bias Probing framework (see Fig. 1) which serves as a technique for fine-grained fairness benchmarking of LMs. We first collect a set of stereotypes and identities (Section 3.1-Section 3.2), which results in the SOFA (Social Fairness) dataset (Section 3.3). The final phase of our workflow involves evaluating language models by employing our proposed perplexity-based fairness measures in response to the constructed probes (Section 3.4), exploited in the designed evaluation setting (Section 3.5).",
            "score": 0.6480321580148205,
            "section_title": "Social Bias Probing Framework",
            "char_start_offset": 7371,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1087
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 180,
                    "matchedPaperCorpusId": "258688053"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.955078125
        },
        {
            "corpus_id": "235623756",
            "title": "Towards Understanding and Mitigating Social Biases in Language Models",
            "text": "Given the existence of local and global biases in LMs, our approach towards mitigating them lies in 1) learning a set of bias-sensitive tokens, and 2) mitigating bias of these sensitive tokens via our newly proposed autoregressive iterative nullspace projection algorithm (see Figure 2). \n\nAlgorithm 1 AUTOREGRESSIVE INLP algorithm for mitigating social biases in pretrained LMs. \n\n1: Given: pre-trained LM p * \u03b8 . 2: Learn bias-sensitive tokens S by projection onto bias subspace. 3: Learn context bias classifier with parameter W and obtain nullspace P via multiple steps of nullspace projection. 4: for t = 1, ..., T do 5: \n\n// Compute debiasing level 8:",
            "score": 0.6450033121892047,
            "section_title": "Mitigating Biases",
            "char_start_offset": 17909,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 287
                },
                {
                    "start": 290,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 657
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.919921875
        },
        {
            "corpus_id": "259859044",
            "title": "Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models",
            "text": "Pre-trained Language Models (PLMs) have achieved state-of-the-art performance across various tasks in natural language processing (Devlin et al., 2019;Liu et al., 2019;Clark et al., 2020). One of the crucial reasons for this success is pretraining on large-scale corpora, which is collected from unmoderated sources such as the internet. Prior studies (Caliskan et al., 2017;Zhao et al., 2018;May et al., 2019;Kurita et al., 2019;Gehman et al., 2020) have shown that PLMs capture a significant amount of social biases existing in the pretraining corpus. For instance, they showed that the PLMs learned that the word \"he\" is closer to the word \"engineer\" because of the frequent cooccurrence of this combination in the training corpora, which is known as social gender biases. Since PLMs are increasingly deployed in real-world sce-narios, there is a serious concern that they propagate discriminative prediction and unfairness. \n\nSeveral solutions for mitigating the social biases have been proposed, including: using banned word lists (Raffel et al., 2020), building deliberated training datasets (Bender et al., 2021), balancing the biased and unbiased terms in the training dataset (Dixon et al., 2018;Bordia and Bowman, 2019), debiasing embedding spaces (Liang et al., 2020;Cheng et al., 2021), and self-debiasing in text generation (Schick et al., 2021). Although all these solutions have shown different levels of success, they tend to limit the PLMs' ability (Meade et al., 2022). For example, the banned words solution prevent gaining knowledge of topics related to banned words. Also, some of them hurt the PLMs' performance on downstream tasks. Furthermore, dataset curation and pre-training are two resourceintensive tasks needed for most of the above solutions (Schick et al., 2021).",
            "score": 0.6441620160018975,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 927
                },
                {
                    "start": 930,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1795
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 151,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 352,
                    "end": 375,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 375,
                    "end": 393,
                    "matchedPaperCorpusId": "52161864"
                },
                {
                    "start": 393,
                    "end": 410,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 410,
                    "end": 430,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 430,
                    "end": 450,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 1036,
                    "end": 1057,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1098,
                    "end": 1119,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1185,
                    "end": 1205,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 1205,
                    "end": 1229,
                    "matchedPaperCorpusId": "102352788"
                },
                {
                    "start": 1258,
                    "end": 1278,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1278,
                    "end": 1297,
                    "matchedPaperCorpusId": "232185104"
                },
                {
                    "start": 1466,
                    "end": 1486,
                    "matchedPaperCorpusId": "239015827"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "274776546",
            "title": "Bias Vector: Mitigating Biases in Language Models with Task Arithmetic Approach",
            "text": "Language models (LMs) are inherently biased because their training processes rely on humancreated text data, which would reflect human biases (Bolukbasi et al., 2016). Navigli et al. (2023) defined the term bias in the field of Natural Language Processing as \"prejudices, stereotypes, and discriminatory attitudes against certain groups of people.\" We adopt this bias definition throughout this paper. Various debiasing methods have been proposed to mitigate these biases (Schick et al., 2021;Zmigrod et al., 2019;Webster et al., 2020;Ravfogel et al., 2020;Liang et al., 2020). \n\nSeveral studies have shown that for wordembedding models, such as word2vec (Mikolov et al., 2013), the bias in word embeddings can be mitigated using approaches like subtracting the statistically significant mean vector associated with the bias from each word vector (Bolukbasi et al., 2016;Mu and Viswanath, 2018;Gonen and Goldberg, 2019;Wang et al., 2020). In contrast, other studies ahve proposed bias mitigation techniques specifically for Transformer-based LMs (Ravfogel et al., 2020;Liang et al., 2020). \n\nSeveral benchmarks have been introduced to evaluate debiasing approaches. Islam et al. (2016) developed the Word Embedding Association Test (WEAT) to measure bias scores in word embeddings. May et al. (2019) proposed the Sentence Encoder Association Test (SEAT) as an extension of WEAT, extending the focus from word to sentence. StereoSet (Nadeem et al., 2021) is another benchmark designed to evaluate stereotypes across four bias categories: race, profession, gender, and religion. StereoSet consists of two subsets: intrasentence, which measures biases within a individual sentence, and intersentence, which evaluates biases at the discourse level across multiple sentences. Nangia et al. (2020) also introduced the CrowS-Pairs benchmark for bias neasurements.",
            "score": 0.6415479958022933,
            "section_title": "Language Models and Bias",
            "char_start_offset": 4642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 577
                },
                {
                    "start": 580,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1089
                },
                {
                    "start": 1092,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1856
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 166,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 168,
                    "end": 189,
                    "matchedPaperCorpusId": "258688053"
                },
                {
                    "start": 472,
                    "end": 493,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 493,
                    "end": 514,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 535,
                    "end": 557,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 557,
                    "end": 576,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 847,
                    "end": 871,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 871,
                    "end": 894,
                    "matchedPaperCorpusId": "3986339"
                },
                {
                    "start": 894,
                    "end": 919,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 919,
                    "end": 937,
                    "matchedPaperCorpusId": "204770514"
                },
                {
                    "start": 1046,
                    "end": 1069,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 1069,
                    "end": 1088,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1282,
                    "end": 1299,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1432,
                    "end": 1453,
                    "matchedPaperCorpusId": "215828184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91455078125
        },
        {
            "corpus_id": "268536898",
            "title": "From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards",
            "text": "Breakthroughs in deep learning and natural language processing (NLP) have paved the way for a new generation of highly performing large language models (LLMs).These models can now pass the bar exam (Koetsier, 2023), autonomously generate code (Meta, 2023), or even write books (Times Now Digital, 2023).\n\nIn recent years, they have become ubiquitous, finding practical applications across a myriad of fields, from art and entertainment (Robertson, 2023) to healthcare (Frist, 2023) and education (Heaven, 2023).While these models have many benefits, notably in improving efficiency, reducing costs, and facilitating communication and personalization, they also come with their own range of challenges and safety-related issues.Indeed, these models are prone to spreading misinformation, violating users' privacy, infringing on copyright law, manipulating end users, and reproducing harmful social biases (Chen et al., 2023).\n\nAs these generative technologies become more prominent in our daily lives, and even more influential for critical decision-making in high-stake applications such as as hiring, lending, and criminal justice (Ray, 2023), biased outcomes have the potential to perpetuate societal inequities and further harm already marginalized populations.Hence, the importance of mitigating biases in these large language models cannot be overstated.\n\nVarious attempts to mitigate the biases in these models have been made in industry and academia alike -filtering the training datasets, fine-tuning the models on smaller diversified datasets, using reinforcement learning from human feedback, as well as explicitly blocking certain prompts or certain words from being used (Wiggers, 2021;Touvron et al., 2023b).While these techniques have helped reduce the ingrained biases in these models, they still fall short in terms of scalability and generalization across diverse datasets: for example, studies have shown that even after mitigation, these models still reproduce the same harmful social biases (Salinas et al., 2023) and that safety safeguards lead to lexical overfitting (R\u00f6ttger et al., 2023).",
            "score": 0.6405039087996043,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 159,
                    "end": 303
                },
                {
                    "start": 305,
                    "end": 511
                },
                {
                    "start": 511,
                    "end": 727
                },
                {
                    "start": 727,
                    "end": 924
                },
                {
                    "start": 926,
                    "end": 1264
                },
                {
                    "start": 1264,
                    "end": 1359
                },
                {
                    "start": 1361,
                    "end": 1721
                },
                {
                    "start": 1721,
                    "end": 2112
                }
            ],
            "ref_mentions": [
                {
                    "start": 1132,
                    "end": 1143,
                    "matchedPaperCorpusId": "258157875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93505859375
        },
        {
            "corpus_id": "235623756",
            "title": "Towards Understanding and Mitigating Social Biases in Language Models",
            "text": "Machine learning tools for processing large datasets are increasingly deployed in real-world scenarios such as healthcare (Velupillai et al., 2018), legal systems (Dale, 2019), and computational social science (Bamman et al., 2016). However, recent work has shown that discriminative models including pretrained word and sentence embeddings reflect and propagate social biases present in training corpora (Bolukbasi et al., 2016;Caliskan et al., 2017;Lauscher and Glava\u0161, 2019;Swinger et al., 2019). Further usages of such approaches can amplify biases and unfairly discriminate against users, particularly those from disadvantaged social groups (Barocas and Selbst, 2016;Sun et al., 2019; In this paper, we aim to provide a more formal understanding of social biases in LMs. In particular, we focus on representational biases, which, following the taxonomy in Blodgett et al. (2020), are harmful biases resulting from stereotyping that propagate negative generalizations about particular social groups, as well as differences in system performance for different social groups, text that misrepresents the distribution of different social groups in the population, or language that is denigrating to particular social groups. A better understanding of these biases in text generation would subsequently allow us to design targeted methods to mitigate them. We begin by summarizing three inherent difficulties in defining and measuring biases during text generation: P1 Granularity: In prior work studying biases in embeddings, social biases are measured using a set of association tests between predefined social constructs (e.g., gender and racial terms) and social professions (e.g., occupations, academic fields). While it suffices to measure such associations over a set of tests for discriminative purposes, the study of biases in text generation can be more nuanced -biases can potentially arise during the generation of any token (Nadeem et al., 2020), as well as from a more holistic, global interpretation of the generated sentence (Sheng et al., 2019).",
            "score": 0.6392800505887378,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 2062
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 147,
                    "matchedPaperCorpusId": "53103736"
                },
                {
                    "start": 163,
                    "end": 175,
                    "matchedPaperCorpusId": "58952135"
                },
                {
                    "start": 405,
                    "end": 429,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 429,
                    "end": 451,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 451,
                    "end": 477,
                    "matchedPaperCorpusId": "135465247"
                },
                {
                    "start": 477,
                    "end": 498,
                    "matchedPaperCorpusId": "56517207"
                },
                {
                    "start": 646,
                    "end": 672,
                    "matchedPaperCorpusId": "143133374"
                },
                {
                    "start": 672,
                    "end": 689,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 861,
                    "end": 883,
                    "matchedPaperCorpusId": "218971825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9423828125
        },
        {
            "corpus_id": "259859034",
            "title": "Unlearning Bias in Language Models by Partitioning Gradients",
            "text": "In the past few years, extraordinary improvements have been made to most applications of natural language processing due to the prevalence of large pretrained language models, particularly Transformers (Vaswani et al., 2017). These language models achieve remarkable performance not only because of mechanisms like attention (Bahdanau et al., 2016), but because of rich and diverse natural language corpora scraped from literature and the internet. However, in spite of some measures to ensure that these natural language sentences are high quality (Radford et al., 2019), recent work has shown that pretraining corpora contain many toxic/biased sentences and that neural models trained on such data readily capture and exhibit these biases (Caliskan et al., 2017;May et al., 2019;Gehman et al., 2020;Kurita et al., 2019). \n\nPrevious studies suggest that embeddings and models encode harmful social biases (Bolukbasi et al., 2016;Caliskan et al., 2017;Kaneko and Bollegala, 2021;Dev et al., 2019;Nangia et al., 2020;Kurita et al., 2019;Nadeem et al., 2020). This can be problematic, as the lack of interpretability in modern language models means that negative stereotypes and social biases encoded in models may lead to unfairness and harms in production systems. Without effective mitigation techniques, finetuned models utilizing these flawed language representations might accidentally inherit spurious correlations not representative of the real world or their target task. \n\nTo mitigate the representational harms explained in Barocas et al. (2017); Blodgett et al. (2020), we might aim for two goals of different granularities. The first goal proposes to debias a model such that its predictions encode the least bias. The second aims to remove social bias throughout a model such that the model minimally represents constructs that can cause itself to be biased in its predictions. Regardless of the debiasing goal, the north star is to eliminate harms caused by the model, so we must be motivated by how pretrained language models are used.",
            "score": 0.6375818628793284,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1478
                },
                {
                    "start": 1481,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2049
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 224,
                    "matchedPaperCorpusId": "227275068"
                },
                {
                    "start": 741,
                    "end": 764,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 764,
                    "end": 781,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 801,
                    "end": 821,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 906,
                    "end": 930,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 930,
                    "end": 952,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 952,
                    "end": 979,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 996,
                    "end": 1016,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1016,
                    "end": 1036,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 1556,
                    "end": 1578,
                    "matchedPaperCorpusId": "218971825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93359375
        },
        {
            "corpus_id": "258686693",
            "title": "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models",
            "text": "We conduct a systematic analysis of the political biases of language models. We probe LMs using prompts grounded in political science and measure models' ideological positions on social and economic values. We also examine the influence of political biases in pretraining data on the political leanings of LMs and investigate the model performance with varying political biases on downstream tasks, finding that LMs may have different standards for different hate speech targets and misinformation sources based on their political biases.\n\nOur work highlights that pernicious biases and unfairness in downstream tasks can be caused by non-toxic data, which includes diverse opinions, but there are subtle imbalances in data distributions. Prior work discussed data filtering or augmentation techniques as a remedy (Kaushik et al., 2019); while useful in theory, these approaches might not be applicable in real-world settings, running the risk of censorship and exclusion from political participation. In addition to identifying these risks, we discuss strategies to mitigate the negative impacts while preserving the diversity of opinions in pretraining data.",
            "score": 0.6372255356095637,
            "section_title": "Conclusion",
            "char_start_offset": 26576,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "270878706",
            "title": "Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis",
            "text": "Large Language Models (LLMs) perpetuate social biases, reflecting prejudices in their training data and reinforcing societal stereotypes and inequalities. Our work explores the potential of the Contact Hypothesis, a concept from social psychology for debiasing LLMs. We simulate various forms of social contact through LLM prompting to measure their influence on the model\u2019s biases, mirroring how intergroup interactions can reduce prejudices in social contexts. We create a dataset of 108,000 prompts following a principled approach replicating social contact to measure biases in three LLMs (LLaMA 2, Tulu, and NousHermes) across 13 social bias dimensions. We propose a unique debiasing technique, Social Contact Debiasing (SCD), that instruction-tunes these models with unbiased responses to prompts. Our research demonstrates that LLM responses exhibit social biases when subject to contact probing, but more importantly, these biases can be significantly reduced by up to 40% in 1 epoch of instruction tuning LLaMA 2 following our SCD strategy.",
            "score": 0.6371911211469052,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9765625
        },
        {
            "corpus_id": "273901658",
            "title": "Can Machine Unlearning Reduce Social Bias in Language Models?",
            "text": "The widespread integration of language models (LMs) into various everyday and industry applications has raised significant concerns on the trustworthiness of such models (Xu et al., 2023), for generating toxic, unfair, and harmful outputs. Although numerous pre-processing techniques have been suggested to create unbiased datasets (Ung et al., 2021;Zmigrod et al., 2019), the challenge is that specific pre-training data is not disclosed, making pre-trained models susceptible to intrinsic biases by default. On the other hand, an alternative approach to mitigating bias involves retraining the model on secure, unbiased data. However, this can be computationally expensive. As a result, the focus has been shifted to techniques that work to nullify the model's inherent bias. \n\nMultiple techniques for mitigating bias exist, yet there is a lack of comparative studies to evaluate their respective advantages and disadvantages. In this study, we explore and compare different debiasing approaches through both quantitative and qualitative analyses. One approach is based on Machine Unlearning (Cao and Yang, 2015;Xu et al., 2023). It involves selectively forgetting unwanted data (or concepts) in a trained model while retaining useful information and maintaining computational efficiency. We compare two machine unlearning methods, Partitioned Contrastive Gradient Unlearning (PCGU) (Yu et al., 2023) and unlearning via task vectors (Jang et al., 2022) to a popular alignment-based approach using Direct Preference Optimization (DPO) (Rafailov et al., 2024), which aligns the model to human preferences. We conduct experiments on the OPT (Zhang et al., 2022) and LLaMA-2 models (Touvron et al., 2023). Social Bias. We focus on social bias that is characterized by deliberate or unintentional discriminatory attitudes or actions toward individuals, groups, or specific ideas and beliefs, resulting in prejudiced or unfair treatment (Gallegos et al., 2024;Navigli et al., 2023). Our main contributions are highlighted below:",
            "score": 0.6351996358464007,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 777
                },
                {
                    "start": 780,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 1094,
                    "end": 1114,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 1385,
                    "end": 1402,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 1536,
                    "end": 1559,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1933,
                    "end": 1956,
                    "matchedPaperCorpusId": "261530629"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90625
        },
        {
            "corpus_id": "265607922",
            "title": "Assessing Political Inclination of Bangla Language Models",
            "text": "Bias identification and mitigation have been subjects of significant research interest (Liu et al., 2022\u037e Chen et al., 2023). Various forms of bias in language models have been extensively studied, from stereotypical to social and political biases (Liang et al., 2021). Researchers have developed various techniques to quantify, detect, and mitigate these biases, contributing to a growing body of literature in the field. Sun et al. (2022) examined societal biases within pre-trained language models, investigating six sensitive attributes, including race, gender, religion, appearance, age, and socioeconomic status. Their study also proposed potential mitigation strategies by developing debiasing adapters integrated into the layers of pre-trained language models. \n\nSimilarly, gender bias within LMs has garnered significant research attention. Recent studies have convincingly demonstrated the inherent gender bias present in these models (Kumar et al., 2020). Researchers have proposed various metrics to quantify and measure this bias (Bordia and Bowman, 2019). To address this issue, several debiasing strategies have been put forth. Qian et al. (2019) suggested a debiasing approach that modifies the loss function by incorporating terms aimed at equalizing probabilities associated with male and female words in the model's output. Vig et al. (2020) applied the theory of causal mediation analysis to develop a method for interpreting the components of a model that contribute to its bias. These research endeavors have laid a progressive foundation for examining gender biases in LMs. \n\nFurthermore, researchers have investigated various aspects of bias within LMs (Kaneko et al., 2022\u037e de Vassimon Manela et al., 2021\u037e Van Der Wal et al., 2022\u037e Joniak and Aizawa, 2022). Kirk et al. (2021) conducted research on generative models, particularly GPT-2 (Radford et al., 2019), and uncovered occupational biases. They observed that the job types suggested by the model tended to align with stereotypical attributes associated with people. Similarly, Venkit et al. (2022) identified biases against individuals with disabilities within language models.",
            "score": 0.6344050793375711,
            "section_title": "Related Works",
            "char_start_offset": 3297,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1596
                },
                {
                    "start": 1599,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2047
                },
                {
                    "start": 2048,
                    "end": 2159
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 268,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 423,
                    "end": 440,
                    "matchedPaperCorpusId": "252907549"
                },
                {
                    "start": 1043,
                    "end": 1068,
                    "matchedPaperCorpusId": "102352788"
                },
                {
                    "start": 1143,
                    "end": 1161,
                    "matchedPaperCorpusId": "170078973"
                },
                {
                    "start": 1343,
                    "end": 1360,
                    "matchedPaperCorpusId": "227275068"
                },
                {
                    "start": 1784,
                    "end": 1802,
                    "matchedPaperCorpusId": "236950797"
                },
                {
                    "start": 1863,
                    "end": 1885,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90966796875
        },
        {
            "corpus_id": "245144648",
            "title": "Few-shot Instruction Prompts for Pretrained Language Models to Detect Social Biases",
            "text": "Detecting social bias in text is of utmost importance as stereotypes and biases can be projected through language (Fiske, 1993). Detecting bias is challenging because it can be expressed through seemingly innocuous statements which are implied and rarely explicit, and the interpretation of bias can be subjective leading to noise in labels. In this work, we focus on detecting social bias in text as defined in Sap et al. (2020) using few-shot instruction-based prompting of pre-trained language models (LMs). \n\nCurrent approaches that detect bias require large labeled datasets to train the models (Chung et al., 2019;Waseem and Hovy, 2016;Zampieri et al., 2019;Davidson et al., 2017a). Collecting such labeled sets is an expensive process and hence they are not easily available. Furthermore, most of the prior work relies on finetuning (Sap et al., 2020;Mandl et al., 2019;Zampieri et al., 2019) neural architectures which is costly in case of large LMs (Strubell et al., 2019) and access to finetune large LMs may be limited (Brown et al., 2020). Prior work on bias detection has not focused on modeling multiple types of biases across datasets as it requires careful optimization to succeed (Hashimoto et al., 2017;S\u00f8gaard and Goldberg, 2016;Ruder, 2017). Finetuning a model can also lead to over-fitting especially in case of smaller train sets and to catastrophic forgetting of knowledge present in the pre-trained model (Fatemi et al., 2021). Moreover, finetuning approaches are prone to be affected by noisy labels (Song et al., 2022) which is especially an issue with datasets for bias detection. The human labeling used to annotate these datasets can introduce bias and noisy labels (Hovy and Prabhumoye, 2021). \n\nWe harness the knowledge present in large scale pre-trained language models (Davison et al., 2019;Zhou et al., 2020;Petroni et al., 2019;Zhong et al., 2021;Shin et al., 2020) to detect a rich set of biases.",
            "score": 0.6329590974096131,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 510
                },
                {
                    "start": 513,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1723
                },
                {
                    "start": 1726,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 412,
                    "end": 429,
                    "matchedPaperCorpusId": "207853290"
                },
                {
                    "start": 600,
                    "end": 620,
                    "matchedPaperCorpusId": "196188987"
                },
                {
                    "start": 620,
                    "end": 642,
                    "matchedPaperCorpusId": "1721388"
                },
                {
                    "start": 642,
                    "end": 664,
                    "matchedPaperCorpusId": "67856299"
                },
                {
                    "start": 664,
                    "end": 687,
                    "matchedPaperCorpusId": "1733167"
                },
                {
                    "start": 840,
                    "end": 858,
                    "matchedPaperCorpusId": "207853290"
                },
                {
                    "start": 858,
                    "end": 877,
                    "matchedPaperCorpusId": "263876502"
                },
                {
                    "start": 877,
                    "end": 899,
                    "matchedPaperCorpusId": "67856299"
                },
                {
                    "start": 958,
                    "end": 981,
                    "matchedPaperCorpusId": "174802812"
                },
                {
                    "start": 1197,
                    "end": 1221,
                    "matchedPaperCorpusId": "2213896"
                },
                {
                    "start": 1221,
                    "end": 1248,
                    "matchedPaperCorpusId": "16661147"
                },
                {
                    "start": 1248,
                    "end": 1260,
                    "matchedPaperCorpusId": "90063862"
                },
                {
                    "start": 1525,
                    "end": 1544,
                    "matchedPaperCorpusId": "220546541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85498046875
        },
        {
            "corpus_id": "271865498",
            "title": "Spoken Stereoset: on Evaluating Social Bias Toward Speaker in Speech Large Language Models",
            "text": "Our study presents Spoken StereoSet, the first specifically designed to evaluate social biases in speech large language models. Through rigorous testing on prominent speech large language models, we uncovered both the presence and the extent of biases related to gender and age. While many models demonstrate minimal bias, others still exhibit slight social bias tendency, indicating the necessity for ongoing evaluation and mitigation strategies. The results highlight the importance of incorporating diverse and representative data in training speech large language models to ensure they promote fairness. Future work should focus on expanding the dataset to include more categories and scenarios, and developing techniques to further reduce biases in speech large language models, fostering a more equitable interaction across all demographics.",
            "score": 0.6327014206792085,
            "section_title": "CONCLUSION",
            "char_start_offset": 22995,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 847
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.986328125
        },
        {
            "corpus_id": "271097745",
            "title": "Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation",
            "text": "Large Language Models (LLMs) have recently gained significant traction due to their impressive natural language understanding and generation capabilities across various tasks, including machine translation, text summarization, topic detection, and engaging human-like conversations [4,3,1]. However, as LLMs become more integral to our daily lives across various domains -ranging from healthcare and finance to law and education -it is increasingly crucial to address the inherent biases that can emerge from these models. Such biases can lead to unfair treatment, reinforce stereotypes, and exclude social groups, compromising the ethical standards and social responsibility of AI technologies [13,14,39]. The presence of bias in LLMs is a multifaceted issue rooted in the data used for training. Specifically, biases in data availability, selection, language, and social contexts may collectively reflect prejudices, disparities, and stereotypes that can inadvertently be learned and perpetuated by LLMs, leading to unfair and harmful responses. Biases may also arise from the unfair usage of LLMs, since users may favor generated information that confirms their preexisting beliefs, selectively interpreting responses that align with their views (confirmation bias), or blindly trust the generated output without any critical thinking, deeming it a priori superior to human judgment (automation bias) [38,37]. Therefore, understanding, unveiling, and mitigating these biases is essential for fostering sustainability and inclusivity in AI applications. Mitigation strategies should involve curating more balanced and representative training datasets [31,33], while also implementing robust bias detection [32,36] and alignment mechanisms [40,41], incorporating fairness guidelines. However, several challenges arise in ensuring that language models are entirely bias-free, including obtaining representative datasets for safety tuning, developing universally accepted bias metrics, and the significant resources required for thorough bias mitigation. \n\nStarting from the above considerations, our study proposes a robust methodology to test the resilience of various widely-used Language Models (LMs) at different scales, ranging from high-quality Small Language Models (SLMs) like Google's Gemma 2B to large-scale LLMs like OpenAI's GPT-3.5 Turbo (175B).",
            "score": 0.6290608204588776,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 2053
                },
                {
                    "start": 2056,
                    "end": 2358
                }
            ],
            "ref_mentions": [
                {
                    "start": 282,
                    "end": 285,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 702,
                    "end": 705,
                    "matchedPaperCorpusId": "258688053"
                },
                {
                    "start": 1404,
                    "end": 1408,
                    "matchedPaperCorpusId": "261530629"
                },
                {
                    "start": 1657,
                    "end": 1660,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 1708,
                    "end": 1712,
                    "matchedPaperCorpusId": "9424845"
                },
                {
                    "start": 1741,
                    "end": 1745,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9736328125
        },
        {
            "corpus_id": "271859735",
            "title": "Social Debiasing for Fair Multi-modal LLMs",
            "text": "Multi-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities. However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender. This paper addresses the issue of social biases in MLLMs by i) Introducing a comprehensive Counterfactual dataset with Multiple Social Concepts (CMSC), which provides a more diverse and extensive training set compared to existing datasets. ii) Proposing an Anti-Stereotype Debiasing strategy (ASD). Our method works by revisiting the MLLM training process, rescaling the autoregressive loss function, and improving data sampling methods to counteract biases. Through extensive experiments on various MLLMs, our CMSC dataset and ASD method demonstrate a significant reduction in social biases while maintaining the models' original performance.",
            "score": 0.6287822928087742,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.982421875
        },
        {
            "corpus_id": "258686693",
            "title": "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models",
            "text": "Language models (LMs) are pretrained on diverse data sources\u2014news, discussion forums, books, online encyclopedias. A significant portion of this data includes facts and opinions which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure media biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings which reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and media biases into misinformation detectors. We discuss the implications of our findings for NLP research and propose future directions to mitigate unfairness.",
            "score": 0.6280354975126189,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89990234375
        },
        {
            "corpus_id": "270521813",
            "title": "The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Pre-trained Language Models",
            "text": "We propose a debiasing approach derived from our social bias neuron detection technique, named Bias Neuron Suppression (BNS).Since we have precisely identified social bias neurons, we could mitigate social biases by simply suppressing the activation of specific social bias neurons.Specifically, we use an indicator function 1 to mark whether the j-th neuron in the lth layer w (l) j needs to be suppressed.If a neuron is identified to be a social bias neuron, BNS then suppresses the activation of this neuron by setting its value to 0:\n\nwhere \u03c3 is a threshold and is set as 0.2 \u00d7 max j,l {IG 2 (w (l) j )} in our experiments.We evaluate the efficacy of our debiasing method on the social bias benchmark StereoSet (Nadeem et al., 2021).Since experiments are conducted on masked language models, we only use the intrasentence subset of StereoSet that is designed for the MLM task.Each sample in StereoSet is a sentence triplet: the first sentence is stereotypical, the second one is anti-stereotypical, and the third one is unrelated.The following is an example: Girls tend to be more soft than boys.(Stereotype) Girls tend to be more determined than boys.(Anti-Stereotype) Girls tend to be more fish than boys.(Unrelated)\n\nWe use the three metrics of StereoSet (Nadeem et al., 2021): Language Modeling Score (LMS), Stereotype Score (SS), and Idealized CAT Score (ICAT).These metrics are calculated by comparing the probability assigned to the contrasting portion of each sentence conditioned on the shared portion of the sentence.SS is the proportion of examples in which a model prefers a stereotypical association over an anti-stereotypical one.The ideal SS for a language model is 50, i.e., the LM shows no preference for either stereotypical associations or anti-stereotypical associations.LMS is used to evaluate the language modeling abilities of models.It is the proportion of examples where the stereotypical or anti-stereotypical sentences are assigned a higher probability than the unrelated ones.The ideal LMS is 100, i.e., the model always prefers meaningful associations to unrelated ones.",
            "score": 0.6270467655403154,
            "section_title": "EVALUATION OF DEBIASING",
            "char_start_offset": 15305,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 125,
                    "end": 282
                },
                {
                    "start": 282,
                    "end": 407
                },
                {
                    "start": 407,
                    "end": 537
                },
                {
                    "start": 539,
                    "end": 627
                },
                {
                    "start": 627,
                    "end": 737
                },
                {
                    "start": 737,
                    "end": 880
                },
                {
                    "start": 880,
                    "end": 1034
                },
                {
                    "start": 1034,
                    "end": 1100
                },
                {
                    "start": 1100,
                    "end": 1156
                },
                {
                    "start": 1156,
                    "end": 1211
                },
                {
                    "start": 1211,
                    "end": 1222
                },
                {
                    "start": 1224,
                    "end": 1370
                },
                {
                    "start": 1370,
                    "end": 1531
                },
                {
                    "start": 1531,
                    "end": 1648
                },
                {
                    "start": 1648,
                    "end": 1795
                },
                {
                    "start": 1795,
                    "end": 1861
                },
                {
                    "start": 1861,
                    "end": 2008
                },
                {
                    "start": 2008,
                    "end": 2103
                }
            ],
            "ref_mentions": [
                {
                    "start": 715,
                    "end": 736,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1262,
                    "end": 1283,
                    "matchedPaperCorpusId": "215828184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "227230297",
            "title": "Do Neural Language Models Overcome Reporting Bias?",
            "text": "We show that pre-trained LMs to some extent overcome reporting bias in the sense that they possess knowledge that wasn't explicitly stated, including trivial facts. Unfortunately, they also over-represent rare and newsworthy events, amplifying the bias that already exists in their training corpus. \n\nThe results in this paper are in line with prior work that showed that LMs amplify social bias (May et al., 2019;Sheng et al., 2019) and knowledge about named entities that are prominent in the corpus (Shwartz et al., 2020a). Going forward, it is important to study how the choice of training corpus, model size, and other factors affect the type and extent of biases the LM would have.",
            "score": 0.6252278294799319,
            "section_title": "Conclusion",
            "char_start_offset": 14967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 298
                },
                {
                    "start": 301,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 687
                }
            ],
            "ref_mentions": [
                {
                    "start": 396,
                    "end": 414,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 414,
                    "end": 433,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 502,
                    "end": 525,
                    "matchedPaperCorpusId": "215238527"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "275920669",
            "title": "Unmasking Conversational Bias in AI Multiagent Systems",
            "text": "Large Language Models (LLMs) inherit social biases from their training data, which can persist or even worsen in their generated outputs. While existing bias detection methods-such as questionnaires and situational tests-identify biases in isolated settings, they fail to capture how LLMs behave in real-world multiagent interactions. \n\nThe framework proposed in this paper is aimed at detecting conversational biases in multiagent LLM systems. By simulating chatroom debates with AI agents initially holding strong opinions, we observe unexpected opinion shifts-especially in conservative echo chambers-suggesting a latent liberal bias in many models. Notably, this bias is undetectable using conventional bias evaluation techniques. BesidesThe findings highlight the need for more advanced, context-aware bias detection and mitigation strategies. \n\nGiven these considerations, it is also important to examine whether such unwarranted behavior extends beyond political opinion to other contexts. For instance, in discussions related to equity, law enforcement, or governance, it is crucial to determine whether LLM agents deviate from a neutral or balanced perspective without clear justification-potentially due to intrinsic biases. Consider, for example, an LLM assistant used to analyze the resumes of job candidates and later engaged in a conversation about their strengths and weaknesses. If an inherent bias influences its responses, the resulting assessments could lead to unfair decision-making. \n\nA consequential follow-up of this study is to investigate why such specific biases emerge in LLMs, and to devise mitigation approaches. Besides the issue with training data, which can exhibit over-representations of some perspectives, the specific generative frameworks underlying the models may not be robust to the exposure of the bias. This suggests that, besides solely evaluating the biases through the generated text, future work can look inside the models and explore how the probabilities over the output tokens evolve throughout the discussion concerning the emerging bias. For instance, we can conduct an in-processing mitigation strategy by refining the weights of the model, thus making its responses insensitive to the bias.",
            "score": 0.6215010146286859,
            "section_title": "Conclusions and Future Work",
            "char_start_offset": 33361,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 334
                },
                {
                    "start": 337,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 848
                },
                {
                    "start": 851,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1504
                },
                {
                    "start": 1507,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2244
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.955078125
        },
        {
            "corpus_id": "266054040",
            "title": "Layered Bias: Interpreting Bias in Pretrained Large Language Models",
            "text": "Motivation: Large Language Models (LLMs) have risen to prominence, revolutionizing the field of natural language processing (NLP). These models, such as OPT (Zhang et al., 2022) and LLaMA (Touvron et al., 2023a), are trained on vast and diverse data sources encompassing webpages, Wikipedia, books, scientific papers, and other online content. While this broad spectrum of data ensures a rich representation of the world's knowledge, it also serves as a double-edged sword. On one side, it represents a democratic and diverse range of ideas, yet on the flip side, it exposes the models to inherent social biases. \n\nIn recent years, the NLP community has prioritized studying biases in LLMs. Early work by Bolukbasi et al. (2016) revealed gender and ethnic biases in word embeddings like Word2Vec and GloVe. This trend of identifying biases continued with more complex models like BERT, where researchers examined how biases are encoded and propagated (Kurita et al., 2019;May et al., 2019). Researchers have also developed datasets, such as StereoSet (Nadeem et al., 2021) and CrowS-Pairs (Nangia et al., 2020), specifically to measure and understand these biases. Sap et al. (2020) delved into the effects of biased data, especially from human annotators, on the behavior of models. Alongside identification, efforts have been geared towards the mitigation of bias in LLMs. Techniques such as iterative nullspace projection (INLP) (Ravfogel et al., 2020a) and Counterfactual Data Augmentation (CDA) (Zmigrod et al., 2019) have been proposed and implemented to mitigate biases in LLMs. Nevertheless, many of the existing studies have examined and evaluated biases in LLMs in a more coarse-grained manner, and it is unclear how the debiasing techniques affected the LLMs in deeper neural layers. \n\nWe aim to address this research gap by conducting an in-depth analysis to interpret layer-wise bias in pretrained LLMs.",
            "score": 0.6209149477843661,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 612
                },
                {
                    "start": 615,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1794
                },
                {
                    "start": 1797,
                    "end": 1916
                }
            ],
            "ref_mentions": [
                {
                    "start": 705,
                    "end": 728,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 951,
                    "end": 972,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 972,
                    "end": 989,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1051,
                    "end": 1072,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1089,
                    "end": 1110,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1165,
                    "end": 1182,
                    "matchedPaperCorpusId": "207853290"
                },
                {
                    "start": 1432,
                    "end": 1456,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 1500,
                    "end": 1522,
                    "matchedPaperCorpusId": "184486914"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9736328125
        },
        {
            "corpus_id": "271923841",
            "title": "Identifying and Mitigating Social Bias Knowledge in Language Models",
            "text": "Bias Mitigation in Pre-trained Language Models. The increasing deployment of chatbots driven by large language models (LLMs) has heightened concerns over fairness. Issues related to fairness in LLMs can have dire outcomes, such as the amplification of bias, discrimination, and detrimental effects on marginalized groups. Consequently, substantial efforts are being made to assess and address biases in large language models (Gallegos et al., 2023;Li et al., 2023b;Fan et al., 2024a;Luo et al., 2024). Several approaches have been proposed for debiasing pre-trained language models, which can be grouped into two categories: (1) Fine-tuning. This branch includes additional pretraining on re-balanced corpora (Zmigrod et al., 2019;Webster et al., 2020) or with a contrastive objective (He et al., 2022;Cheng et al., 2021), projection-based methods (Liang et al., 2020;Ravfogel et al., 2020;Kaneko and Bollegala, 2021;Dev et al., 2020) in the embedding space, in-training methods (Han et al., 2021;He et al., 2022) and parameter-efficient fine-tuning (Lauscher et al., 2021;Xie and Lukasiewicz, 2023) methods. \n\n(2) Prompt-tuning. Prompt-tuning (Guo et al., 2022;Yang et al., 2023;Li et al., 2023c;Dong et al., 2023) involve generating either discrete prompts or con-tinuous prompts to mitigate social biases. There are also post-hoc approaches (Schick et al., 2021;Chen et al., 2023Chen et al., , 2024a;;Fan et al., 2024b;Chen et al., 2024b) that are deployed after the training phase to achieve effective debiasing.",
            "score": 0.6206345087674743,
            "section_title": "B Related Works",
            "char_start_offset": 31332,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 1108
                },
                {
                    "start": 1111,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1516
                }
            ],
            "ref_mentions": [
                {
                    "start": 917,
                    "end": 934,
                    "matchedPaperCorpusId": "201670701"
                },
                {
                    "start": 1144,
                    "end": 1162,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1162,
                    "end": 1180,
                    "matchedPaperCorpusId": "253446867"
                },
                {
                    "start": 1180,
                    "end": 1197,
                    "matchedPaperCorpusId": "259342087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97265625
        },
        {
            "corpus_id": "259991491",
            "title": "Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?",
            "text": "Social bias in language models (LMs) has recently gained substantial attention from the research community (Mehrabi et al., 2021;Liang et al., 2021) due to increasing adoption of these models, such as ChatGPT, in consumer-facing applications (Dwivedi et al., 2023;Kasneci et al., 2023). Given this ongoing trend, many noted the urgent need to mitigate the bias, toxicity, and stereotypes in LM output (Abid et al., 2021;Bender et al., 2021). Bias in LMs manifests when these models produce consistently disparate outcomes for different protected classes (e.g., male, female, transgender) within any particular sensitive attribute (such as gender). These disparities have the potential to skew model decisions and challenge the fairness of such deployed models in many important domains such as health and legal systems (Weidinger et al., 2021). Recently, LMs have shown significant improvement in complex reasoning tasks by leveraging carefully constructed prompts, including Chain-of-Thought prompting, that compel the language model to surface the \"thought process\" that led to a final answer (Wei et al., 2022). Given the complexity of social bias and recent successes in bias mitigation through chain-of-thought prompting (Ganguli et al., 2023), we hypothesize that a similar approach could help improve models' ability to identify social bias in language. In this paper, we focus on bias identification using the following approach. First, we restructure the BBQ dataset (Parrish et al., 2021) to make it suitable for bias identification. Second, we evaluate LMs ability to identify bias through zero-shot prompting using a variety of instructions, including Chain-of-Thought (CoT) prompts. In terms of LMs, we specifically focus on the applicability of instruction-fine-tuned models for this task. This is a work in progress and constitutes the first part of a developing bias mitigation framework.",
            "score": 0.6205952114813358,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1904
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 129,
                    "matchedPaperCorpusId": "201666566"
                },
                {
                    "start": 129,
                    "end": 148,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 264,
                    "end": 285,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 401,
                    "end": 420,
                    "matchedPaperCorpusId": "236384212"
                },
                {
                    "start": 420,
                    "end": 440,
                    "matchedPaperCorpusId": "262580630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90771484375
        },
        {
            "corpus_id": "261530629",
            "title": "Bias and Fairness in Large Language Models: A Survey",
            "text": "Abstract Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.",
            "score": 0.6202477988708802,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98486328125
        },
        {
            "corpus_id": "269195406",
            "title": "AI Chatbots and Linguistic Injustice",
            "text": "It is widely known that recent language models have improved significantly (Devlin et al. 2018, Brown et al. 2020, Clark et al. 2020). Recent advancements in language modeling have embraced the approach of training large-scale models on extensive, unannotated corpora using self-supervised learning techniques. These methods involve predicting masked words and the next sentence in a sequence. (Devlin et al. 2018, He et al. 2020), wrong word detection (Clark et al. 2020), and left-to-right language generation (Brown et al. 2020, Raffel et al. 2020). \n\nThe recent natural language processing models are trained by assessing the similarity vocabularies and sentences in text. Since the optimization objective focuses on maximizing the likelihood of the training data, the trained model enhances the coherence of words and sentences frequently found together in the training corpus. However, being created by humans, the training data sets can contain significant amounts of social bias and stereotypes, encompassing factors such as gender, race, and religion (Kiritchenko & Mohammad 2018, Nadeem et al. 2021, Stanczak & Augenstein 2021). \n\nSome studies have demonstrated that pretrained language models are capable of acquiring various forms of stereotypical and baised reasoning. For example, Kiritchenko & Mohammad (2018) examined how language models perform in sentiment analysis across various social groups, measuring differences in their behaviours. Recent studies by Nangia et al. (2020) and Nadeem et al. (2021) investigated stereotypical reasoning related to race, gender, profession, and religion using masked language models and sentence encoders. \n\nRecent research examined strategies to reduce the social biases inherent in language models, aiming to enhance their reliability. These studies have investigated techniques to mitigate biases during the learning and prediction phases of language models. Typical methods for mitigating bias involve the use of counterfactual data augmentation (Zmigrod et al. 2019, Dinan et al. 2020, Webster et al. 2020, Barikeri et al. 2021), dropout regularization (Webster et al. 2020), and selfdebias (Schick et al. 2021). MIT researchers have trained language models that can realize logic to avoid harmful stereotypes such as gender and racial biases.",
            "score": 0.6197999311823823,
            "section_title": "Language Modeling Bias",
            "char_start_offset": 14133,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1138
                },
                {
                    "start": 1141,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2171
                },
                {
                    "start": 2172,
                    "end": 2302
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 113,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 113,
                    "end": 133,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 453,
                    "end": 472,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 512,
                    "end": 530,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 530,
                    "end": 551,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1060,
                    "end": 1088,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 1088,
                    "end": 1108,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1295,
                    "end": 1324,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 1475,
                    "end": 1495,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1500,
                    "end": 1520,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 2004,
                    "end": 2024,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 2024,
                    "end": 2043,
                    "matchedPaperCorpusId": "207852875"
                },
                {
                    "start": 2064,
                    "end": 2087,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 2150,
                    "end": 2170,
                    "matchedPaperCorpusId": "232075876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9462890625
        },
        {
            "corpus_id": "264305744",
            "title": "Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning",
            "text": "Pre-trained language models (PLMs) are widely used in many real-world applications, demonstrating remarkable performance (Devlin et al., 2019;Brown et al., 2020). However, it has been demonstrated that PLMs encode unfair social biases in their parameters based on their pre-training step over large-scale text corpora (May et al., 2019). Furthermore, these biases -for example, based on gender, race, or religion -can easily propagate to the downstream tasks that use these PLMs (Kaneko and Bollegala, 2021). For example, \"She is a nurse\" can have a higher conditional likelihood than \"He is a nurse\" in the language modeling task, and \"nurse\" can have higher coreference scores to \"she\" than \"he\" in the coreference resolution task (Lu et al., 2020). Considering that NLP applications like machine translation systems, resume filtering systems, dialogue systems, and speech recognition (Tatman, 2017) are widely used by millions of users globally, it is crucial to mitigate the social biases present in PLMs and strive for models that will not propagate discriminatory predictions or offensive outputs towards specific groups before being deployed. \n\nMuch prior effort has focused primarily on debiasing the representations learned during the pretraining process, e.g., through projection (Dev et al., 2020;Liang et al., 2020;Ravfogel et al., 2020;Kaneko and Bollegala, 2021), further pre-training on unbiased external corpora (Webster et al., 2020;Lauscher et al., 2021;He et al., 2022), or finetuning to debias (Cheng et al., 2021;Guo et al., 2022). The effectiveness of such debiasing efforts is typically measured on intrinsic benchmarks like SEAT (Sentence Encoding Association Test) which computes the association between demographic terms (e.g., woman, man) and stereotype terms (e.g., science, art). An unbiased model should display no difference in the similarity between the representations of these terms (May et al., 2019).",
            "score": 0.6184077556697767,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 142,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 142,
                    "end": 161,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 479,
                    "end": 507,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 887,
                    "end": 901,
                    "matchedPaperCorpusId": "13997424"
                },
                {
                    "start": 1290,
                    "end": 1308,
                    "matchedPaperCorpusId": "201670701"
                },
                {
                    "start": 1308,
                    "end": 1327,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1327,
                    "end": 1349,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 1349,
                    "end": 1376,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1450,
                    "end": 1472,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 1472,
                    "end": 1488,
                    "matchedPaperCorpusId": "253157517"
                },
                {
                    "start": 1514,
                    "end": 1534,
                    "matchedPaperCorpusId": "232185104"
                },
                {
                    "start": 1534,
                    "end": 1551,
                    "matchedPaperCorpusId": "248780440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.951171875
        },
        {
            "corpus_id": "235623756",
            "title": "Towards Understanding and Mitigating Social Biases in Language Models",
            "text": "As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.",
            "score": 0.6166969506181166,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9677734375
        },
        {
            "corpus_id": "271769656",
            "title": "A Parameter-Efficient Multi-Objective Approach to Mitigate Stereotypical Bias in Language Models",
            "text": "For example, LMs can violate equal social group associations by predicting different occupations for male and female genders, or violate equal neutral neutral associations by believing that criminals are more likely to be people of color (Gallegos et al., 2023). In addition, biased LMs generate sentences containing higher-level disparity, such as sentiment (Huang et al., 2020) and regard (Sheng et al., 2019) for different demographics, demonstrating global bias (Liang et al., 2021). As a result, methods targeting only one specific form of bias can lead to incomplete bias removal and unsatisfactory debiasing performance. Besides, the increasing scale of pre-trained LMs boosts the design and application of parameterefficient fine-tuning methods (Houlsby et al., 2019;Lester et al., 2021;Li and Liang, 2021;Hu et al., 2022). Unfortunately, relatively little work has been devoted to studying parameter-efficient methods in the field of bias mitigation (Lauscher et al., 2021;Gira et al., 2022;Xie and Lukasiewicz, 2023). In this work, we also aim to further explore lightweight debiasing techniques using parameter-1 efficient fine-tuning methods. \n\nThe main contribution of this work includes: \n\n1. We refine and integrate existing probabilistic alignment debiasing approaches to simultaneously address multiple forms of bias representation, employing a parameter-efficient prefix-tuning technique for implementation. \n\n2. We empirically demonstrate the effectiveness of our method on diverse intrinsic and extrinsic bias evaluation benchmarks and compared it with existing debiasing techniques. \n\n3. We thoroughly analyze our parameter-efficient debiasing framework and show that it can achieve better bias mitigation performance and parameter efficiency than full fine-tuning. Additionally, our method is effective in reducing bias in large LMs.",
            "score": 0.616542302236467,
            "section_title": "Introduction",
            "char_start_offset": 1610,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1154
                },
                {
                    "start": 1157,
                    "end": 1201
                },
                {
                    "start": 1204,
                    "end": 1425
                },
                {
                    "start": 1428,
                    "end": 1603
                },
                {
                    "start": 1606,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1855
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 379,
                    "matchedPaperCorpusId": "207847197"
                },
                {
                    "start": 391,
                    "end": 411,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 466,
                    "end": 486,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 753,
                    "end": 775,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 775,
                    "end": 795,
                    "matchedPaperCorpusId": "233296808"
                },
                {
                    "start": 795,
                    "end": 814,
                    "matchedPaperCorpusId": "230433941"
                },
                {
                    "start": 814,
                    "end": 830,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 959,
                    "end": 982,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 982,
                    "end": 1000,
                    "matchedPaperCorpusId": "248780268"
                },
                {
                    "start": 1000,
                    "end": 1026,
                    "matchedPaperCorpusId": "259095584"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.943359375
        },
        {
            "corpus_id": "276408214",
            "title": "Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models",
            "text": "Pre-training large language models (LLMs) on vast text corpora enhances their performance in various natural language processing tasks (Touvron et al., 2023;Zhao et al., 2023;Chiang et al., 2023) but risks encoding social biases, particularly gender bias, that are implicitly present in uncensored datasets (Liang et al., 2021;Luccioni and Viviano, 2021). Mitigating these biases is essential for the responsible deployment of LLMs in real-world applications. An effective debiasing method should meet several key criteria: (1) Automation to reduce human intervention, (2) Applicability across both open-source and black-box LLMs to support various deployment settings, and (3) Utility Preservation to maintain the original model performance.",
            "score": 0.6164795035587649,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 742
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 175,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 307,
                    "end": 327,
                    "matchedPaperCorpusId": "235623756"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97607421875
        },
        {
            "corpus_id": "276768480",
            "title": "Implicit Bias in LLMs: A Survey",
            "text": "The advent of Large Language Models (LLMs) has significantly transformed the landscape of natural language processing (NLP), enabling breakthroughs across a wide array of tasks. Distinguished from task-specific models, LLMs function as foundation models (Liu et al. 2021), capable of addressing diverse tasks through prompt-based learning. This inherent flexibility obviates the necessity for advanced programming expertise, thereby democratizing access to state-of-the-art NLP capabilities. The synergy of high performance and user accessibility has facilitated the widespread adoption of LLMs in various domains (Jiang et al. 2023;Kasneci et al. 2023;Nay et al. 2023). \n\nAs the societal impact of LLMs continues to grow, concerns regarding their potential harms have garnered increasing attention. These models, trained on extensive corpora of human data from the internet, are susceptible to inheriting and, in some cases, amplifying toxic and biased content (Dodge et al. 2021). Bias is broadly defined as a preconceived negative attitude or stereotype directed towards specific groups (Garimella et al. 2021). Such biases embedded in LLMs can manifest as outputs containing negative sentiments towards vulnerable populations, thereby undermining the interests of marginalized communities and exacerbating existing social inequities. \n\nMost prior research has concentrated on addressing explicit bias, which are readily identifiable and can be mitigated through established techniques (Shen et al. 2023). Advances in these methods have significantly reduced the presence of explicit biases in LLM outputs, rendering such biases nearly imperceptible. However, akin to humans who may suppress overt biases under social norms without resolving their underlying prejudices, LLMs may exhibit a shift from explicit to more subtle and covert forms of bias. We refer to this subtle, unconscious, and automatic bias as \"implicit bias\" (Wilson et al. 2000). Implicit bias has been shown to exert a stronger influence on behavior compared to explicit bias and poses the potential for more profound and far-reaching consequences. Therefore, systematically understanding, detecting, and mitigating implicit bias in LLMs is critical to ensuring the fairness, accuracy, and trustworthiness. \n\nThis survey provides a comprehensive review of implicit bias in LLMs.",
            "score": 0.6157410496381547,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1337
                },
                {
                    "start": 1340,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2121
                },
                {
                    "start": 2122,
                    "end": 2279
                },
                {
                    "start": 2282,
                    "end": 2351
                }
            ],
            "ref_mentions": [
                {
                    "start": 614,
                    "end": 633,
                    "matchedPaperCorpusId": "259112211"
                },
                {
                    "start": 633,
                    "end": 653,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 1090,
                    "end": 1113,
                    "matchedPaperCorpusId": "236477795"
                },
                {
                    "start": 1930,
                    "end": 1950,
                    "matchedPaperCorpusId": "18324937"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "270521813",
            "title": "The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Pre-trained Language Models",
            "text": "Large pre-trained language models (PLMs) have demonstrated remarkable performance across various natural language processing tasks.Nevertheless, they also exhibit a proclivity to manifest biased behaviors that are unfair to marginalized social groups (Aky\u00fcrek et al., 2022;Webster et al., 2020).As research on AI fairness gains increasing importance, there have been efforts to detect (Davani et al.;Fleisig et al., 2023;An & Rudinger, 2023) and mitigate (Kaneko & Bollegala, 2021;Guo et al., 2022) social biases in PLMs.Most approaches for detecting social biases in PLMs rely on prompt or probing-based techniques that treat PLMs as black boxes (Goldfarb-Tarrant et al., 2023;Feng et al., 2023).These methods often begin with designing prompt templates or probing schemas to elicit biased outputs from PLMs.Subsequently, they would measure the model's fairness by calculating the proportion of biased outputs.The effectiveness of this approach relies heavily on the quality of the designed prompt templates or probing schemas (Shaikh et al., 2022).In addition, many previous debiasing methods (Qian et al., 2022;Kaneko & Bollegala, 2021) have focused on constructing anti-stereotypical datasets and then either retraining the PLM from scratch or conducting fine-tuning.This line of debiasing approaches, although effective, comes with high costs for data construction Published as a conference paper at ICLR 2024 Demographic-1 (Logits = 0.1)  (Logits = 0.9) Logits Gap=0.8",
            "score": 0.6150882188114292,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 295
                },
                {
                    "start": 295,
                    "end": 521
                },
                {
                    "start": 521,
                    "end": 697
                },
                {
                    "start": 697,
                    "end": 809
                },
                {
                    "start": 809,
                    "end": 911
                },
                {
                    "start": 911,
                    "end": 1050
                },
                {
                    "start": 1050,
                    "end": 1271
                },
                {
                    "start": 1271,
                    "end": 1474
                }
            ],
            "ref_mentions": [
                {
                    "start": 400,
                    "end": 421,
                    "matchedPaperCorpusId": "259092939"
                },
                {
                    "start": 481,
                    "end": 498,
                    "matchedPaperCorpusId": "248780440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9404296875
        },
        {
            "corpus_id": "270619502",
            "title": "Evaluating Short-Term Temporal Fluctuations of Social Biases in Social Media Data and Masked Language Models",
            "text": "Despite their usage in numerous NLP applications, MLMs such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) tend to encode discriminatory social biases expressed in human-written texts in the training corpora (Kurita et al., 2019;Zhou et al., 2022;Kaneko et al., 2022).For example, if a model is given \"[MASK] is a nurse.\"as the input, a gender biased MLM would predict \"She\" with a higher likelihood score than for \"He\" when filling the [MASK].Such social biases can result in unfavourable experiences for some demographic groups in certain applications.Continuous use of biased models has the potential to amplify biases and unfairly discriminate against users belonging to particular demographic groups.MLMs are increasingly used in real-world applications such as text generation (Liang et al., 2023), recommendation systems (Malkiel et al., 2020;Kuo and Li, 2023), search engines (Achsas et al., 2022;Li et al., 2023) and dialogue systems (Song et al., 2021;Park et al., 2022).Therefore, it is crucial to study how MLMs potentially shape social biases.\n\nOn the other hand, social biases may change due to societal changes, cultural shifts and technological advancements.MLMs have been trained on ever-increasing massive corpora, often collected from the Web.In particular, posts on social media, such as but not limited to Reddit and X (former Twitter), have been used to train MLMs.Social biases contained in the training data are inadvertently learned and perpetuated by MLMs.At the time of writing, there are 5.07 billion social media users worldwide with 259 million new users joining since this time in 2023.\n\n1 Given this rapid increase and the significance of social media data as a source for training MLMs, an open question is whether LMs trained on social media data continue to demonstrate increasing levels of social biases.",
            "score": 0.6142157964394048,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 282,
                    "end": 335
                },
                {
                    "start": 335,
                    "end": 458
                },
                {
                    "start": 458,
                    "end": 568
                },
                {
                    "start": 568,
                    "end": 719
                },
                {
                    "start": 719,
                    "end": 995
                },
                {
                    "start": 995,
                    "end": 1070
                },
                {
                    "start": 1072,
                    "end": 1188
                },
                {
                    "start": 1188,
                    "end": 1276
                },
                {
                    "start": 1276,
                    "end": 1401
                },
                {
                    "start": 1401,
                    "end": 1496
                },
                {
                    "start": 1496,
                    "end": 1631
                },
                {
                    "start": 1633,
                    "end": 1854
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 89,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 222,
                    "end": 243,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 243,
                    "end": 261,
                    "matchedPaperCorpusId": "247450590"
                },
                {
                    "start": 797,
                    "end": 817,
                    "matchedPaperCorpusId": "227230609"
                },
                {
                    "start": 864,
                    "end": 881,
                    "matchedPaperCorpusId": "256134878"
                },
                {
                    "start": 898,
                    "end": 919,
                    "matchedPaperCorpusId": "247682569"
                },
                {
                    "start": 919,
                    "end": 935,
                    "matchedPaperCorpusId": "248157257"
                },
                {
                    "start": 957,
                    "end": 976,
                    "matchedPaperCorpusId": "235417177"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78076171875
        },
        {
            "corpus_id": "270214849",
            "title": "LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models",
            "text": "Language models (LMs) parameterized by deep neural networks (Vaswani et al., 2017;Lewis et al., 2019;Radford et al., 2019;Brown et al., 2020) have thrived in producing fluent and meaningful texts on a variety of nat-ural language generation tasks (See et al., 2019;Ji et al., 2023).Recently, researchers further applied LMs to more diverse classification tasks by transforming them to generative frames (Raffel et al., 2020).These successes underscore the versatility of LMs, establishing them as the foundations for different natural language processing applications (Bommasani et al., 2021;Zhou et al., 2023).In addition, with model sizes continually increasing, large language models (LLMs) have demonstrated unprecedented abilities to follow natural language instructions (Dong et al., 2022;Ouyang et al., 2022).These abilities empower the zero-shot adaptation of LLMs to unseen tasks (Kojima et al., 2022), paving the way towards artificial general intelligence (Bubeck et al., 2023).\n\nNotwithstanding, despite their remarkable performance, LMs suffer from the fairness issue, i.e., they may generate negative texts that are biased against underrepresented demographic groups (e.g., female) in our society (Sheng et al., 2019).\n\nFor instance, GPT-2 (Radford et al., 2019) tends to generate more negative texts towards females (Huang et al., 2019).Such social biases, termed as global biases (Sheng et al., 2020), stem from the real world corpora due to historical reasons (Basta et al., 2019).Unsurprisingly, LMs reproduce or amplify the biases from the data whereon they are trained (Gehman et al., 2020;Schick et al., 2021).\n\nAs remedies, a vast amount of bias mitigation approaches have been proposed.Early attempts of fine-tuning with clean data proved effective on small LMs (Lu et al., 2020;Saunders & Byrne, 2020;Bender et al., 2021).",
            "score": 0.6131032688342929,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 282,
                    "end": 425
                },
                {
                    "start": 425,
                    "end": 611
                },
                {
                    "start": 611,
                    "end": 816
                },
                {
                    "start": 816,
                    "end": 989
                },
                {
                    "start": 991,
                    "end": 1232
                },
                {
                    "start": 1234,
                    "end": 1352
                },
                {
                    "start": 1352,
                    "end": 1498
                },
                {
                    "start": 1498,
                    "end": 1631
                },
                {
                    "start": 1633,
                    "end": 1709
                },
                {
                    "start": 1709,
                    "end": 1846
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 122,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 122,
                    "end": 141,
                    "matchedPaperCorpusId": "237091588"
                },
                {
                    "start": 265,
                    "end": 281,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 403,
                    "end": 424,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 568,
                    "end": 592,
                    "matchedPaperCorpusId": "237091588"
                },
                {
                    "start": 795,
                    "end": 815,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 889,
                    "end": 910,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 1254,
                    "end": 1276,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1396,
                    "end": 1416,
                    "matchedPaperCorpusId": "218470535"
                },
                {
                    "start": 1610,
                    "end": 1630,
                    "matchedPaperCorpusId": "232075876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97021484375
        },
        {
            "corpus_id": "272881159",
            "title": "Keeping Up with the Language Models: Systematic Benchmark Extension for Bias Auditing",
            "text": "We point out shortcomings in current bias scores and propose disaggregate counterfactual bias measures to address current issues. We analyze the robustness-bias interplay in bias auditing and emphasize how important it is to properly attribute causes of biased behavior such that we can work on improving model fairness and brittleness. Last, despite not being designed with generative LMs in mind, we show that BBNLI-next is also effective in uncovering bias in certain categories for state-of-the art, open-source generative LMs. \n\n2 Social bias auditing in language models: background and related work \n\nAs LMs are deployed in real world settings [40,43,46], practitioners analyzed their societal impacts [3,12], and quantified their bias and fairness [2,7,17,24,54]. Different bias scores and measures have been proposed [4,6,14] and analyzed [10,20,27], and several datasets for bias auditing have been introduced [1, 16, 30, 37-39, 41, 44]. Researchers scrutinized deficiencies of current datasets [5] and the lack of clarity on the definition of social bias in NLP models and its measures [4,50]. For a survey on bias in LMs, we refer the reader to Gallegos et al. [18]. For a description of the evolution of bias evaluations from intrinsic to extrinsic measures and beyond, we refer the reader to Lum et al. [34]. \n\nWe adopt the precise definition of social bias from Bommasani and Liang [6]. Social bias is observed when model predictions vary with different groups (e.g., male or female) for a particular target context (e.g., software engineering). We agree with Bommasani and Liang [6] that bias is relative.",
            "score": 0.6130238712634838,
            "section_title": "Introduction",
            "char_start_offset": 5605,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 531
                },
                {
                    "start": 534,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1321
                },
                {
                    "start": 1324,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1620
                }
            ],
            "ref_mentions": [
                {
                    "start": 708,
                    "end": 711,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 755,
                    "end": 758,
                    "matchedPaperCorpusId": "248177981"
                },
                {
                    "start": 758,
                    "end": 760,
                    "matchedPaperCorpusId": "75135222"
                },
                {
                    "start": 760,
                    "end": 763,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 763,
                    "end": 766,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 766,
                    "end": 769,
                    "matchedPaperCorpusId": "249889949"
                },
                {
                    "start": 825,
                    "end": 828,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 830,
                    "end": 833,
                    "matchedPaperCorpusId": "252907216"
                },
                {
                    "start": 847,
                    "end": 851,
                    "matchedPaperCorpusId": "247762845"
                },
                {
                    "start": 851,
                    "end": 854,
                    "matchedPaperCorpusId": "229923772"
                },
                {
                    "start": 854,
                    "end": 857,
                    "matchedPaperCorpusId": "250390988"
                },
                {
                    "start": 1004,
                    "end": 1007,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 1096,
                    "end": 1099,
                    "matchedPaperCorpusId": "218971825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88671875
        },
        {
            "corpus_id": "270878706",
            "title": "Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis",
            "text": "We examine the presence of social biases in LLMs across 13 bias dimensions using prompting scales of certainty, likelihood, and frequency.We further demonstrate that LLMs are aligned with the psychological concept of Contact Hypothesis just like humans, suggesting that simulating positive interactions between groups of people can reduce their prejudices, whereas negative interactions might amplify these biases.We further propose SCD, a social contact-inspired debiasing strategy that instruction-tunes LLMs on social contact data to mitigate bias, which leads to promising results.We highlight that positive/negative priming and contact simulation are effective in large language models, more so in systematic fine-tuning as opposed to individual-level prompt adjustments.",
            "score": 0.6111373225045582,
            "section_title": "Conclusion",
            "char_start_offset": 24765,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 138,
                    "end": 414
                },
                {
                    "start": 414,
                    "end": 585
                },
                {
                    "start": 585,
                    "end": 776
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.935546875
        },
        {
            "corpus_id": "258865159",
            "title": "Conceptor-Aided Debiasing of Large Language Models",
            "text": "Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy. We use conceptors--a soft projection method--to identify and remove the bias subspace in LLMs such as BERT and GPT. We propose two methods of applying conceptors (1) bias subspace projection by post-processing by the conceptor NOT operation; and (2) a new architecture, conceptor-intervened BERT (CI-BERT), which explicitly incorporates the conceptor projection into all layers during training. We find that conceptor post-processing achieves state-of-the-art (SoTA) debiasing results while maintaining LLMs' performance on the GLUE benchmark. Further, it is robust in various scenarios and can mitigate intersectional bias efficiently by its AND operation on the existing bias subspaces. Although CI-BERT's training takes all layers' bias into account and can beat its post-processing counterpart in bias mitigation, CI-BERT reduces the language model accuracy. We also show the importance of carefully constructing the bias subspace. The best results are obtained by removing outliers from the list of biased words, combining them (via the OR operation), and computing their embeddings using the sentences from a cleaner corpus.",
            "score": 0.6082208498296194,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.986328125
        },
        {
            "corpus_id": "268667459",
            "title": "Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception",
            "text": "Bias of LMs.Understanding bias within LMs is complex due to its normative and subjective nature, often influenced by various contextual and cultural factors (Gallegos et al., 2023).While providing a formal definition of bias can be challenging, it is commonly observed and studied through its manifestations in LM outputs.Biases manifest in various forms, including representational biases depicting certain social groups negatively (Beukeboom and Burgers, 2019), disparate system performance leading to misclassifications (Blodgett et al., 2016), and reinforcement of normativity (Bender et al., 2021).Misrepresentation of social groups can also exacerbate biases (Smith et al., 2022).While research (Hada et al., 2023;Gon\u00e7alves and Strubell, 2023;Conti and Wisniewski, 2023;Wang et al., 2023) has addressed bias in LMs broadly, our work focuses on political standing bias, aiming to elucidate discrepancies between LM cognition and human perceptions.\n\nBias Mitigation.Bias mitigation techniques encompass pre-processing, in-training, intraprocessing, and post-processing interventions (Gallegos et al., 2023).Pre-processing involves altering model inputs, such as data and prompts (Venkit et al., 2023), to create more representative training datasets through techniques like data augmentation (Qian et al., 2022), data filtering (Garimella et al., 2022), prompt modification (Venkit et al., 2023), and debiasing pre-trained representations.Intraprocessing methods (Zayed et al., 2023) modify model behavior at inference without further training, including decoding strategies, post hoc model adjustments, and modular debiasing networks.Intraining techniques aim to reduce bias by modifying the optimization process, such as adjusting loss functions (Liu et al., 2021), updating probabilities, freezing parameters (Gira et al., 2022) (i.e., 15 tests).",
            "score": 0.6063592548135368,
            "section_title": "Related Work",
            "char_start_offset": 5034,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 12,
                    "end": 181
                },
                {
                    "start": 181,
                    "end": 322
                },
                {
                    "start": 322,
                    "end": 603
                },
                {
                    "start": 603,
                    "end": 686
                },
                {
                    "start": 686,
                    "end": 952
                },
                {
                    "start": 954,
                    "end": 970
                },
                {
                    "start": 970,
                    "end": 1111
                },
                {
                    "start": 1111,
                    "end": 1443
                },
                {
                    "start": 1443,
                    "end": 1639
                },
                {
                    "start": 1639,
                    "end": 1853
                }
            ],
            "ref_mentions": [
                {
                    "start": 581,
                    "end": 602,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 701,
                    "end": 720,
                    "matchedPaperCorpusId": "264490615"
                },
                {
                    "start": 720,
                    "end": 749,
                    "matchedPaperCorpusId": "266163873"
                },
                {
                    "start": 749,
                    "end": 776,
                    "matchedPaperCorpusId": "264439068"
                },
                {
                    "start": 1332,
                    "end": 1356,
                    "matchedPaperCorpusId": "253762006"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71435546875
        },
        {
            "corpus_id": "270521813",
            "title": "The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Pre-trained Language Models",
            "text": "Pre-trained Language models (PLMs) have been acknowledged to contain harmful information, such as social biases, which may cause negative social impacts or even bring catastrophic results in application. Previous works on this problem mainly focused on using black-box methods such as probing to detect and quantify social biases in PLMs by observing model outputs. As a result, previous debiasing methods mainly finetune or even pre-train language models on newly constructed anti-stereotypical datasets, which are high-cost. In this work, we try to unveil the mystery of social bias inside language models by introducing the concept of {\\sc Social Bias Neurons}. Specifically, we propose {\\sc Integrated Gap Gradients (IG$^2$)} to accurately pinpoint units (i.e., neurons) in a language model that can be attributed to undesirable behavior, such as social bias. By formalizing undesirable behavior as a distributional property of language, we employ sentiment-bearing prompts to elicit classes of sensitive words (demographics) correlated with such sentiments. Our IG$^2$ thus attributes the uneven distribution for different demographics to specific Social Bias Neurons, which track the trail of unwanted behavior inside PLM units to achieve interoperability. Moreover, derived from our interpretable technique, {\\sc Bias Neuron Suppression (BNS)} is further proposed to mitigate social biases. By studying BERT, RoBERTa, and their attributable differences from debiased FairBERTa, IG$^2$ allows us to locate and suppress identified neurons, and further mitigate undesired behaviors. As measured by prior metrics from StereoSet, our model achieves a higher degree of fairness while maintaining language modeling ability with low cost.",
            "score": 0.605986455613979,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "271745282",
            "title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models",
            "text": "In recent years, the proliferation of Large Language Models (LLMs) like GPT-4 has revolutionized natural language processing, enabling applications from automated text generation to complex language understanding. However, as these models integrate more deeply into technological solutions, their inherent biases have become a significant concern. These biases can lead to misrepresentation of individuals and groups, promote stereotypes, and suggest unfair allocation of resources, impacting people both psychologically and socio-economically. \n\nDespite a fair amount of research happening on AI Safety and fairness, there is still a lack of understanding of measuring societal biases in language models systematically. Biases in these models-such as gender, racial, ethnic, and ideological biases-stem from training data that reflect societal prejudices. Consequently, language models can perpetuate and even amplify these biases, leading to outputs that reinforce stereotypes and disadvantage certain groups (Bender et al., 2021). The implications are profound, affecting areas, like hiring, law enforcement, and daily AI interactions (Schwartz et al., 2022;Ferrara, 2023). Thus, assessing and mitigating bias in language models is essential for their ethical and fair use. \n\nWe define bias as the semantic variance in text generated by an LLM along with differential treatment towards stereotype identification, refusal, and correction of social group attributes when presented with prompts obtained by varying only the social group association. For example, in Figure 1, the LLM is presented with a prompt \"I think men are much better with managing finances\" and another obtained by only varying the gender (men to women). The image on the left shows the scenario where the LLM responds differently to these inputs. In one case it identifies the stereotype present in the prompt and rejects this with an explanation; in the other, it fails to identify any gender bias in the input and supports the gender-based claim instead. On the right shows an unbiased model that responds to both the prompts similarly, by identifying and rejecting the gender-stereotype in the input. \n\nBias mitigation techniques rely on bias assessment metrics to evaluate their efficacy. Bias evaluation metrics are broadly classified into embeddingbased, probability-based, and generated-text-based metrics (Gallegos et al., 2023).",
            "score": 0.6058940839234268,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1276
                },
                {
                    "start": 1279,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2177
                },
                {
                    "start": 2180,
                    "end": 2266
                },
                {
                    "start": 2267,
                    "end": 2411
                }
            ],
            "ref_mentions": [
                {
                    "start": 1011,
                    "end": 1032,
                    "matchedPaperCorpusId": "121125604"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97509765625
        },
        {
            "corpus_id": "275920669",
            "title": "Unmasking Conversational Bias in AI Multiagent Systems",
            "text": "Large Language Models (LLMs) learn social biases inherent in their training data, which can lead to the generation of text that perpetuates or even exacerbates these biases Bender et al. (2021). To mitigate the potential harms caused by biased LLMs, it is crucial to first quantify the biases reflected in their generated outputs. The existing literature on bias detection and mitigation within LLMs is extensive Gallegos et al. (2024); however, most current approaches for detecting biases in text generation examine the models in isolation and out of context. These methods assess the LLM using targeted instructions designed to reveal biases relative to specific dimensions of interest, typically through open-ended questions Pit et al. (2024); Scherrer et al. (2024); Shin et al. (2024); Ji et al. (2024); Gupta et al. (2023), structured questionnaires Rozado (2024); Safdari et al. (2023) ;La Cava and Tagarelli (2024), situational tests Rao et al. (2023); Argyle et al. (2023); Fontana et al. (2024), or basic text completion tasks Huang et al. (2019); Dhamala et al. (2021); Sheng et al. (2021); Dong et al. (2023). While these approaches can successfully identify biases, they may not adequately represent the downstream applications in which LLMs are employed. Consequently, models that seem unbiased under these assessments may still manifest biases when employed in more complex contexts. \n\nThe undetected emergence of biases presents a significant concern especially in AI multiagent systems Talebirad and Nadiri (2023). In such systems, LLMs are employed to simulate human actors within social networks Park et al. (2023), execute collaborative tasks with other models Wu et al. (2023); Mohtashami et al. (2023), and engage with both humans and other AI agents through online social media Cao et al. (2023).",
            "score": 0.6058891872411105,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1820
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 193,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 413,
                    "end": 435,
                    "matchedPaperCorpusId": "261530629"
                },
                {
                    "start": 748,
                    "end": 770,
                    "matchedPaperCorpusId": "260164518"
                },
                {
                    "start": 857,
                    "end": 870,
                    "matchedPaperCorpusId": "267412830"
                },
                {
                    "start": 962,
                    "end": 982,
                    "matchedPaperCorpusId": "252280474"
                },
                {
                    "start": 1059,
                    "end": 1080,
                    "matchedPaperCorpusId": "231719337"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94482421875
        },
        {
            "corpus_id": "277150560",
            "title": "Attention Pruning: Automated Fairness Repair of Language Models via Surrogate Simulated Annealing",
            "text": "Recent works have started using expert models for bias reduction. Orgad and Belinkov [49] predicted biased samples with an auxiliary model and reweighted them during pre-training. Jeon et al. [35] employed binary classifiers, or bias experts, to pinpoint biased examples in specific classes. \n\nIn-processing methods modify the model's parameters and optimization process. Methods such as ADELE [36] introduce adapter layers for parameter-efficient fine-tuning to reduce bias. Regularization techniques can be applied to penalize the model for producing biased outputs [47]. Additionally, debiasing word embeddings can help reduce bias in the model's representation of words by reducing gendered or racial stereotypes [7,25,68]. Several loss functions, such as declustering loss [23] and contrastive loss [34,41] are also shown to be effective for mitigating bias. Zhang et al. [71] leveraged gradient-based explanations to target sensitive attributes and adjust training for balanced fairness and performance. Apart from that, Dhingra et al. [16] used style transfer to replace harmful language while maintaining the original meaning. \n\nPost-processing techniques modify the model's inference behavior after complete training. This may involve altering the model before inference or the model output during/after inference. For instance, Gehman et al. [24] proposed token-blocking methods during decoding to prevent the generation of harmful or biased terms. \n\nHauzenberger et al. [33] introduced sparse debiasing subnetworks that are trained separately and can be applied to the model at inference time. Qian et al. [53] performed keyword-based distillation to remove bias during inference. Tokpo and Calders [61] identify biased tokens and replace them with less stereotypical terms. Postprocessing approaches are beneficial since dataset collection and LLM training are expensive processes that we may not always be able to repeat if issues of unfairness are found. Furthermore, postprocessing mitigation approaches are sometimes the only viable option with the status quo of utilizing large pre-trained LLMs that take a huge amount of resources to train.",
            "score": 0.6052813837055327,
            "section_title": "Related Works",
            "char_start_offset": 39048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 291
                },
                {
                    "start": 294,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2158
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 89,
                    "matchedPaperCorpusId": "259138920"
                },
                {
                    "start": 568,
                    "end": 572,
                    "matchedPaperCorpusId": "221136077"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "236477795"
                },
                {
                    "start": 804,
                    "end": 808,
                    "matchedPaperCorpusId": "253157517"
                },
                {
                    "start": 808,
                    "end": 811,
                    "matchedPaperCorpusId": "233864681"
                },
                {
                    "start": 877,
                    "end": 881,
                    "matchedPaperCorpusId": "258833564"
                },
                {
                    "start": 1617,
                    "end": 1621,
                    "matchedPaperCorpusId": "236459953"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91357421875
        },
        {
            "corpus_id": "267411833",
            "title": "Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes",
            "text": "The rapid progress of large language models (LLMs) has ushered in a new era of technological capabilities, with increasing excitement around their few-and zero-shot capacities. For a wide range of tasks like question-answering and logical reasoning, simply modifying the prompting language can efficiently adapt the LLM without finetuning (e.g., Brown et al., 2020;Kojima et al., 2022;Liu et al., 2023;Radford et al., 2019;Reynolds and McDonell, 2021;Wei et al., 2022;Zhao et al., 2021). While few-shot approaches condition the model on a few input-output exemplars, zero-shot learning adapts the model with no training data. \n\nAt the same time as this success, however, LLMs have been shown to learn, reproduce, and even amplify denigrating, stereotypical, and exclusionary social behaviors (e.g., Bender et al., 2021;Hutchinson et al., 2020;Mei et al., 2023;Sheng et al., 2021b;Weidinger et al., 2022). We refer to this class of harms as \"social bias,\" a normative term that characterizes disparate representations, treatments, or outcomes between social groups due to historical and structural power imbalances. \n\nThe growing recognition of these harms has led to an abundance of works proposing bias mitigations for LLMs. One major drawback of many mitigation techniques, however, is their lack of scalability, computational feasibility, or generality to different dimensions of bias. In contrast to existing bias mitigation approaches, downstream applications of LLMs often require more generalizable and efficient mitigations that can be easily applied to a black-box model with no information about the training data or model parameters. \n\nIn this work, we introduce zero-shot selfdebiasing as an adaptation of zero-shot learning that leverages nothing other than the LLM itself to elicit recognition and avoidance of stereotypes1 in an LLM.",
            "score": 0.6051604910982934,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1114
                },
                {
                    "start": 1117,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1644
                },
                {
                    "start": 1647,
                    "end": 1848
                }
            ],
            "ref_mentions": [
                {
                    "start": 346,
                    "end": 365,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 365,
                    "end": 385,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 402,
                    "end": 423,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 423,
                    "end": 451,
                    "matchedPaperCorpusId": "231925131"
                },
                {
                    "start": 451,
                    "end": 468,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 468,
                    "end": 486,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 799,
                    "end": 819,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 819,
                    "end": 843,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 843,
                    "end": 860,
                    "matchedPaperCorpusId": "259129801"
                },
                {
                    "start": 860,
                    "end": 880,
                    "matchedPaperCorpusId": "234337004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9853515625
        },
        {
            "corpus_id": "271769656",
            "title": "A Parameter-Efficient Multi-Objective Approach to Mitigate Stereotypical Bias in Language Models",
            "text": "Language models (LMs) pre-trained on large-scale self-supervised datasets have shown impressive capacities in various natural language processing (NLP) tasks (Peters et al., 2018;Devlin et al., 2019;Radford et al., 2018;Liu et al., 2019;Lan et al., 2020). In particular, pre-trained generative LMs, e.g., GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), OPT (Zhang et al., 2022) and GPT-4 (OpenAI, 2023), have gained great attention from both academic communities and non-expert users, due to their remarkable instruction-following and zero-shot task adaptation abilities (Brown et al., 2020;OpenAI, 2023;Wei et al., 2022). \n\nDespite their remarkable achievements and great practical values, potential ethical risks cannot be neglected. Since these pre-trained LMs are mostly trained on online datasets, their training data is likely to contain undesired patterns including toxic speech and social biases (Zhao et al., 2019;Tan and Celis, 2019). Numerous experiments have revealed that LMs trained in these datasets also demonstrate similar social biases, raising concerns that they could amplify biases and discrimination against disadvantaged demographics (Zhao et al., 2019;May et al., 2019;Tan and Celis, 2019;Bommasani et al., 2020;Guo and Caliskan, 2021;Kurita et al., 2019;Brown et al., 2020;Sheng et al., 2019;Yeo and Chen, 2020). Recently, several methods for reducing stereotypical biases have been proposed (Barikeri et al., 2021;Bommasani et al., 2020;Kaneko and Bollegala, 2021). However, most methods neglect the fact that bias can be represented in various forms in LMs. For example, LMs can violate equal social group associations by predicting different occupations for male and female genders, or violate equal neutral neutral associations by believing that criminals are more likely to be people of color (Gallegos et al., 2023).",
            "score": 0.6047772246202688,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1857
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 179,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 179,
                    "end": 199,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 237,
                    "end": 254,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 581,
                    "end": 601,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 614,
                    "end": 631,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 914,
                    "end": 933,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 1167,
                    "end": 1186,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 1186,
                    "end": 1203,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1223,
                    "end": 1246,
                    "matchedPaperCorpusId": "220046499"
                },
                {
                    "start": 1246,
                    "end": 1269,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 1269,
                    "end": 1289,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 1289,
                    "end": 1308,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1308,
                    "end": 1327,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 1327,
                    "end": 1346,
                    "matchedPaperCorpusId": "220444795"
                },
                {
                    "start": 1427,
                    "end": 1450,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 1450,
                    "end": 1473,
                    "matchedPaperCorpusId": "220046499"
                },
                {
                    "start": 1473,
                    "end": 1500,
                    "matchedPaperCorpusId": "231698657"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "272827587",
            "title": "A Multi-LLM Debiasing Framework",
            "text": "Large language models have rapidly advanced, enabling them to perform a wide range of tasks with increasing proficiency. Despite these advancements, LLMs continue to exhibit bias, namely social bias, which perpetuates negative stereotypes. Recent research has shown remarkable strides in reducing bias in LLMs through different techniques such as model fine-tuning, zero-shot prompting, and data augmentation. There is an increasing interest in self-debiasing methods because they do not require access to the model parameters, which adds another layer of complexity. Current bias mitigation techniques rely on a single LLM to debias. \n\nMethods using multiple LLMs have been developed to address problems outside of bias and fairness (Wang et al., 2024a;Pan et al., 2024;Zeng et al., 2024;Kannan et al., 2023;Sreedhar and Chilton, 2024;Zhang et al., 2024c), showing great potential. Multi-LLM frameworks can mimic human discussion, employing multiple LLMs to interact with one another, drawing on each other's perspectives. While multi-LLM frame-works have demonstrated improvement in evaluation and problem-solving tasks, it has not been explored in debiasing LLMs. \n\nWe seek to answer the question: How can we harness the diverse reasoning of multiple LLMs to effectively reduce bias in these models? We propose a multi-LLM framework that leverages multiple models in a conversational context to reduce bias in LLMs. We conduct experiments exploring two approaches to our multi-LLM framework: centralized, where a single model facilitates communication, and decentralized, where all models directly communicate with each other. Figures 1(b) and 1(c) show the high-level difference between the two approaches. Interestingly, we find that our decentralized approach generally outperforms our centralized approach. Our multi-LLM method overall surpasses the baseline in several social groups. \n\nThe key contributions of this work are as follows: \n\n(1) we introduce a multi-LLM strategy for debiasing LLM outputs, employing multiple models in a conversational setup. This method aims to derive the least biased response through interactive model dialogue;",
            "score": 0.6043268275208744,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1166
                },
                {
                    "start": 1169,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1891
                },
                {
                    "start": 1894,
                    "end": 1944
                },
                {
                    "start": 1947,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2153
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.955078125
        },
        {
            "corpus_id": "270521896",
            "title": "Disentangling Dialect from Social Bias via Multitask Learning to Improve Fairness",
            "text": "Social bias can be defined as stereotypical thinking or prejudices against social groups (Fiske, 1998).\n\nIn NLP, it can manifest in hidden representations (Splieth\u00f6ver and Wachsmuth, 2020) or unfair predictions (Angwin et al., 2016).Identifying social bias in data is an important step towards debiasing NLP models, since models adopt and amplify pre-existing biases which can have harmful effects (Zhao et al., 2017;Shwartz and Choi, 2020).\n\nIn related work, Wald and Pfahler (2023) analyze bias in fine-tuned large language models (LLM) as a proxy for bias in data.Focusing on single texts, Sap et al. (2020) introduce the Social Bias Inference Corpus (SBIC) and train a model to predict multiple bias aspects.Prabhumoye et al. (2022) apply few-shot learning to instruction-tuned LLMs on the same data.We build on the work of Sap et al. (2020), but extend it by considering dialects.\n\nRelevant to dialects, another perspective to social bias is fairness regarding performance across social groups (Tolan, 2018).For example, Tatman (2017) find that video captioning systems perform worse for dialect speakers and women.In NLP, Blodgett et al. (2016) highlight disparities between dialect and standard language, as well as social groups in language identification.Resende et al. (2024) find negative biases in hate speech detection towards AAE texts due to underrepresentation in datasets.However, no work so far has considered fairness for the interplay of dialects and social bias.Ziems et al. (2022) find, similar to Joshi et al. (2024), that models perform worse on AAE compared to SAE in natural language understanding tasks.\n\nMany existing fairness evaluations target toxicity detection.Mozafari et al. (2020) observe that a fine-tuned model labels AAE texts more often as hate speech than SAE texts and propose a mitigation strategy.Similarly, Halevy et al. (2021) aim to mitigate this bias by introducing a dialect detection and dialect-specific toxicity classifiers.",
            "score": 0.6023664222729354,
            "section_title": "Related Work",
            "char_start_offset": 5203,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 105,
                    "end": 233
                },
                {
                    "start": 233,
                    "end": 441
                },
                {
                    "start": 443,
                    "end": 567
                },
                {
                    "start": 567,
                    "end": 712
                },
                {
                    "start": 712,
                    "end": 804
                },
                {
                    "start": 804,
                    "end": 885
                },
                {
                    "start": 887,
                    "end": 1013
                },
                {
                    "start": 1013,
                    "end": 1120
                },
                {
                    "start": 1120,
                    "end": 1264
                },
                {
                    "start": 1264,
                    "end": 1389
                },
                {
                    "start": 1389,
                    "end": 1483
                },
                {
                    "start": 1483,
                    "end": 1630
                },
                {
                    "start": 1632,
                    "end": 1693
                },
                {
                    "start": 1693,
                    "end": 1840
                },
                {
                    "start": 1840,
                    "end": 1975
                }
            ],
            "ref_mentions": [
                {
                    "start": 89,
                    "end": 102,
                    "matchedPaperCorpusId": "152778440"
                },
                {
                    "start": 155,
                    "end": 188,
                    "matchedPaperCorpusId": "227151662"
                },
                {
                    "start": 398,
                    "end": 417,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 417,
                    "end": 440,
                    "matchedPaperCorpusId": "227230297"
                },
                {
                    "start": 593,
                    "end": 610,
                    "matchedPaperCorpusId": "207853290"
                },
                {
                    "start": 828,
                    "end": 845,
                    "matchedPaperCorpusId": "207853290"
                },
                {
                    "start": 1026,
                    "end": 1039,
                    "matchedPaperCorpusId": "13997424"
                },
                {
                    "start": 1483,
                    "end": 1502,
                    "matchedPaperCorpusId": "247863188"
                },
                {
                    "start": 1693,
                    "end": 1715,
                    "matchedPaperCorpusId": "221136077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90673828125
        },
        {
            "corpus_id": "248987624",
            "title": "RL with KL penalties is better viewed as Bayesian inference",
            "text": "Our paper is a contribution to important lines of work on social bias in large language models and on aligning artificial intelligence with human preferences. The first line of work is primarily concerned with risks associated with an over-representation of certain hegemonic (e.g. sexist, racist, homophobic) viewpoints and voices present in the training data for large language models, which consists primarily of crawled, uncurated user-generated content. Deploying language models exhibiting social biases poses a risk of amplifying and perpetuaing these biases (Sheng et al., 2019;Blodgett et al., 2020;Bender et al., 2021). The second line of work is concerned more broadly with ensuring that objectives that machine learning systems pursue are aligned with human values (Amodei et al., 2016;Russell, 2019). Large language models, due to their capabilities, can be a testbed for alignment techniques for future, more powerful machine learning systems (Askell et al., 2021;Bowman, 2021). Research on RLHF for fine-tuning LMs -such as our paper -can therefore be motivated by both narrower (social bias) and broader (alignment) considerations. As a theoretical contribution, our paper is not expected to pose significant risk. However, RLHF is a dual use technology: it can be diverted to malicious uses such as spreading misinformation or generating harmful content.",
            "score": 0.6014311736675857,
            "section_title": "Ethics statement",
            "char_start_offset": 20203,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1371
                }
            ],
            "ref_mentions": [
                {
                    "start": 566,
                    "end": 586,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 586,
                    "end": 608,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 608,
                    "end": 628,
                    "matchedPaperCorpusId": "262580630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95556640625
        },
        {
            "corpus_id": "266551169",
            "title": "A Group Fairness Lens for Large Language Models",
            "text": "Evaluation bias and fairness in LLMs is a comprehensive yet challenging task. A straightforward idea is measuring the distance between embeddings of words representing different social groups [10,42]. However, these methods demonstrate weak or inconsistent correlations on downstream tasks [9,12,26,49,50,64]. Some works utilize the coreference resolution task to assess models' associations between gender pronouns and professions, exploring both stereotypical and anti-stereotypical contexts [58,73,81]. Following this, advancements in bias evaluation transition from fill-in-the-blank to sentence-level assessments, enabling more nuanced bias analysis [47,48]. Subsequent research broadens the evaluation scope, encompassing a broader range of social groups to offer a more comprehensive insight into biases present in language models [2,21,33,63]. \n\nRecent efforts propose datasets for various tasks to assess biases in LLMs. BBQ specifically targets bias within question-answering contexts [53]. BOLD employs web-based sentence prefixes to uncover potential biased or toxic text generation, especially when inadequate bias mitigation is applied [18]. BiasAsker introduces an automated framework for identifying both absolute and correlated social biases in conversational AI [71]. DecodingTrust assesses bias across 12 stereotyped and 12 non-stereotyped demographic groups [29]. TrustGPT measures disparities in toxicity levels across social groups through designed prompts [29]. \n\nPrior evaluation paradigms often prioritize mainstream groups with abundant data, sidelining or marginalizing others. Contrarily, our approach seeks to treat all groups equitably, without hierarchies. Besides, In the above research, the direct inquiry approach is increasingly thwarted by the enhanced safety mechanisms of the model, making it challenging to evaluate the intrinsic biases and fairness of large models. We subtly incorporate bias-detection queries into open-ended specific tasks.",
            "score": 0.6011460050763084,
            "section_title": "RELATED WORK 2.1 Evaluation bias and fairness in LLMs",
            "char_start_offset": 4607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 1982
                }
            ],
            "ref_mentions": [
                {
                    "start": 192,
                    "end": 196,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 290,
                    "end": 293,
                    "matchedPaperCorpusId": "258236466"
                },
                {
                    "start": 293,
                    "end": 296,
                    "matchedPaperCorpusId": "247762845"
                },
                {
                    "start": 299,
                    "end": 302,
                    "matchedPaperCorpusId": "250390436"
                },
                {
                    "start": 302,
                    "end": 305,
                    "matchedPaperCorpusId": "248177827"
                },
                {
                    "start": 305,
                    "end": 308,
                    "matchedPaperCorpusId": "248780439"
                },
                {
                    "start": 494,
                    "end": 498,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 501,
                    "end": 504,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 655,
                    "end": 659,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 659,
                    "end": 662,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 838,
                    "end": 841,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 847,
                    "end": 850,
                    "matchedPaperCorpusId": "253224433"
                },
                {
                    "start": 995,
                    "end": 999,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 1150,
                    "end": 1154,
                    "matchedPaperCorpusId": "231719337"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96044921875
        },
        {
            "corpus_id": "271923841",
            "title": "Identifying and Mitigating Social Bias Knowledge in Language Models",
            "text": "While our research yields important contributions, we acknowledge the presence of certain limitations. Firstly, our proposed fine-grained debiasing framework requires human-relevant social bias to process. In this paper, we utilize bias knowledge that has been validated within existing datasets for convenience. In practice, retaining a comprehensive bias knowledge base is both time-consuming and labor-intensive. We notice that recent works (Sahoo et al., 2022;Dev et al., 2023) have proposed an automated social bias detection method. In the future, our work could be augmented by integrating these methods to enhance the construction and filtration of a biased knowledge base. Besides, social bias in open language generation or dialogue (Yu et al., 2022;Ovalle et al., 2023) represents another critical scenario for applying mitigating techniques, which is not addressed in this paper. Expanding our fairness edit method to these scenarios constitutes one of our future research endeavors. Finally, compared to the results on BERT and GPT2, the debiasing performance on larger models (Section 4.2) appears less pronounced. This may be attributed to the intricate nature of the knowledge embedded within larger models, rendering it less amenable to simplistic modifications, which also constitutes a focal point within our future agenda.",
            "score": 0.6001478541952201,
            "section_title": "E Limitation and Future Works",
            "char_start_offset": 46407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1342
                }
            ],
            "ref_mentions": [
                {
                    "start": 760,
                    "end": 780,
                    "matchedPaperCorpusId": "258741207"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9462890625
        },
        {
            "corpus_id": "277758223",
            "title": "Investigating and Mitigating Undesirable Biases in Large Language Models",
            "text": "The rise of large language models (LLMs) has revolutionized natural language processing, offering immense capabilities across various applications. \nThe widespread integration of these models into commonplace technology has brought to light deep concerns about the biases they encompass, which could serve to perpetuate negative preconceptions and social injustices. The scope of my research includes social biases, brand biases, the impact of personas on bias, and stereotypes in low-resource languages. My contributions aim to deepen our understanding of these biases and develop methodologies to mitigate them, enhancing the fairness and utility of LLMs across diverse global applications.",
            "score": 0.5997281554527163,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9833984375
        },
        {
            "corpus_id": "270620779",
            "title": "Mitigating Social Biases in Language Models through Unlearning",
            "text": "The widespread integration of language models (LMs) into various everyday applications has raised significant concerns on the trustworthiness of such models (Xu et al., 2023), for generating toxic, unfair, and harmful outputs.The majority of these Large Language Models (LLMs) undergo pre-training and fine-tuning processes using publicly accessible data, which may inherently contain social biases, toxicity, polarization, and harmful content.\n\nAlthough numerous pre-processing techniques have been suggested to create unbiased datasets * These authors contributed equally to this work 1 https://github.com/VectorInstitute/Bias_in_LMs-Bias_mitigation(Ung et al., 2021;Zmigrod et al., 2019), the challenge is that many models are pre-trained, and in many situations the specific training data are not disclosed, making them susceptible to intrinsic biases by default.On the other hand, an alternative approach to mitigating bias involves retraining the model on secure, unbiased data.However, this can be computationally expensive.As a result, the focus has been shifted to techniques that work to nullify the model's inherent bias.\n\nOne such approach is Machine Unlearning (Cao and Yang, 2015;Ginart et al., 2019;Xu et al., 2023;Zhang et al., 2023a;Nguyen et al., 2022;Xu et al., 2023).It involves selectively forgetting unwanted data in a trained model while retaining useful information and maintaining computational efficiency.This idea was extended in Partitioned Contrastive Gradient Unlearning (PCGU) (Yu et al., 2023) to reduce gender bias in encoder-based models.For LLMs, Jang et al. (2022) highlight the usefulness of unlearning via task vectors in toxicity mitigation.\n\nIn this paper, we use these two unlearning techniques: PCGU and negation via Task Vector for debiasing the OPT (Zhang et al., 2022) and LLaMA-2 models (Touvron et al., 2023).",
            "score": 0.5980477831645996,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 226,
                    "end": 444
                },
                {
                    "start": 446,
                    "end": 651
                },
                {
                    "start": 651,
                    "end": 867
                },
                {
                    "start": 867,
                    "end": 984
                },
                {
                    "start": 984,
                    "end": 1031
                },
                {
                    "start": 1031,
                    "end": 1132
                },
                {
                    "start": 1134,
                    "end": 1287
                },
                {
                    "start": 1287,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1572
                },
                {
                    "start": 1572,
                    "end": 1680
                },
                {
                    "start": 1682,
                    "end": 1856
                }
            ],
            "ref_mentions": [
                {
                    "start": 1174,
                    "end": 1194,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 1194,
                    "end": 1214,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 1230,
                    "end": 1250,
                    "matchedPaperCorpusId": "258221533"
                },
                {
                    "start": 1508,
                    "end": 1525,
                    "matchedPaperCorpusId": "259859034"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94482421875
        },
        {
            "corpus_id": "276647994",
            "title": "Beneath the Surface: How Large Language Models Reflect Hidden Bias",
            "text": "The remarkable performance of Large Language Models (LLMs) is frequently accompanied by the propagation of social bias inherent in their training data (Gallegos et al., 2024a;Hofmann et al., 2024;Navigli et al., 2023;Cui et al., 2024). These biases raise serious ethical concerns, as they perpetuate stereotypes, reinforce discrimination, and negatively impact real-world decision-making. In domains such as hiring, law enforcement, and content moderation, the use of these models in realworld applications may disproportionately harm marginalized individuals and communities (Parrish et al., 2022;Nangia et al., 2020;Nadeem et al., 2021;Marchiori Manerba et al., 2024;Bi et al., 2023;del Arco et al., 2024;Kotek et al., 2023). \n\nNumerous studies (Parrish et al., 2022;Marchiori Manerba et al., 2024;Nangia et al., 2020;Nadeem et al., 2021) benchmark Overt Bias in LLMs by analyzing direct associations between a specific demographic term and a bias-related concept term. As illustrated in Figure 1, example (a) from BBQ (Parrish et al., 2022) can demonstrate overt bias when the model consistently associates \"Margaret\" (female) with the term \"bad at math\" and \"George\" (male) with the term \"good at math\", or vice versa. However, a fundamental issue remains: overt bias can be simply mitigated by breaking the direct association between demographic terms and concept terms (Gallegos et al., 2024b;Li et al., 2024). Additionally, as LLMs evolve, their responses to overt bias evaluations have become more neutral and self-regulated, frequently aligning with socially desirable norms. This trend is largely driven by advances in model training techniques, particularly instruction tuning and alignment strategies, which encourage neutrality in responses to overtly biased contexts (Ouyang et al., 2022;Zhang et al., 2023;Peng et al., 2023;Ji et al., 2024).",
            "score": 0.5974391117509216,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1856
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 175,
                    "matchedPaperCorpusId": "261530629"
                },
                {
                    "start": 175,
                    "end": 196,
                    "matchedPaperCorpusId": "272214842"
                },
                {
                    "start": 196,
                    "end": 217,
                    "matchedPaperCorpusId": "258688053"
                },
                {
                    "start": 576,
                    "end": 598,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 598,
                    "end": 618,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 618,
                    "end": 638,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 638,
                    "end": 669,
                    "matchedPaperCorpusId": "265212726"
                },
                {
                    "start": 707,
                    "end": 726,
                    "matchedPaperCorpusId": "261276445"
                },
                {
                    "start": 747,
                    "end": 769,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 769,
                    "end": 800,
                    "matchedPaperCorpusId": "265212726"
                },
                {
                    "start": 800,
                    "end": 820,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 820,
                    "end": 840,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1021,
                    "end": 1043,
                    "matchedPaperCorpusId": "239010011"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9697265625
        },
        {
            "corpus_id": "267770177",
            "title": "Potential and Challenges of Model Editing for Social Debiasing",
            "text": "Large Language Models (LLMs) perform excellently across various downstream tasks, allowing a wide range of applications, ranging from chatbots to medical diagnoses (Wang et al., 2023b) to robotics (Driess et al., 2023). However, LLMs * These authors contributed equally to this work. trained on vast corpora can inadvertently learn biased information, leading to negative stereotypes and social biases encoded within the models (Gallegos et al., 2023). For instance, when given 'Arab people are rich.' and 'Arab people are poor.', the LLaMA-2-7B (Touvron et al., 2023b) model favors the first sentence, which reflects stereotypes encoded within the model. Such biases have the potential to result in unfairness and harm when deployed in production systems (Nadeem et al., 2021a;Prakash and Lee, 2023). \n\nTraditional debiasing methods typically require training models from scratch to modify training datasets (Zmigrod et al., 2019;Dinan et al., 2020;Qian et al., 2022a;Narayanan Venkit et al., 2023), optimization process (Huang et al., 2020;Qian et al., 2022b;He et al., 2022;Park et al., 2023;Zhou et al., 2023) or they may operate within the output space (He et al., 2021;Tokpo and Calders, 2022;Majumder et al., 2022;Dhingra et al., 2023). The former is very costly for language models, while the latter does not truly address bias encoded in the model, potentially leading to non-robustness. \n\nWhen it comes to mitigating specific biases in pre-trained models, such as the association between \"nurse\" and \"women\", directly fine-tuning (Ghanbarzadeh et al., 2023) the model might be both costly and impractical with insufficient data.",
            "score": 0.5969596517841916,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 801
                },
                {
                    "start": 804,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1638
                }
            ],
            "ref_mentions": [
                {
                    "start": 756,
                    "end": 778,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 778,
                    "end": 800,
                    "matchedPaperCorpusId": "266054040"
                },
                {
                    "start": 909,
                    "end": 931,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 931,
                    "end": 950,
                    "matchedPaperCorpusId": "207852875"
                },
                {
                    "start": 950,
                    "end": 969,
                    "matchedPaperCorpusId": "249062690"
                },
                {
                    "start": 1022,
                    "end": 1042,
                    "matchedPaperCorpusId": "207847197"
                },
                {
                    "start": 1042,
                    "end": 1061,
                    "matchedPaperCorpusId": "249062690"
                },
                {
                    "start": 1061,
                    "end": 1077,
                    "matchedPaperCorpusId": "252907344"
                },
                {
                    "start": 1077,
                    "end": 1095,
                    "matchedPaperCorpusId": "257079734"
                },
                {
                    "start": 1095,
                    "end": 1113,
                    "matchedPaperCorpusId": "259370743"
                },
                {
                    "start": 1158,
                    "end": 1175,
                    "matchedPaperCorpusId": "237634972"
                },
                {
                    "start": 1175,
                    "end": 1199,
                    "matchedPaperCorpusId": "246210255"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "271516455",
            "title": "Fairness Definitions in Language Models Explained",
            "text": "We categorize fairness definitions in LMs into two branches based on the LMs that they are applied to, including: (1) fairness definitions for medium-sized LMs and (2) fairness definitions for large-sized LMs as presented in figure 1. These types of LMs are distinguished by their training strategies: medium-sized LMs typically follow the pre-training and fine-tuning paradigm, while large-sized language models operate under the prompting paradigm. \n\nIn medium-sized LMs, biases are further categorized into two types based on their manifestation: intrinsic bias [60] and extrinsic bias [43]. There are two types of intrinsic bias that will be presented including similarity-based bias and probability-based bias [92]. Extrinsic bias, on the other hand, refers to biases that manifest in the model's outputs during downstream tasks. The extrinsic bias definitions are further summarized into two categories: natural language understanding (NLU) tasks with text classification [30,42] and natural language inference [7,44]; and natural language generation (NLG) tasks with recommender system [69] and question-answering [111]. In large-sized LMs, further categorizations are based on evaluation strategies designed to quantify fairness in these models, including demographic representation [22,99], stereotypical association [3,94], counterfactual fairness [93,94], and performance disparities [142,165]. Overall, this survey explores fairness definitions in LMs according to the proposed taxonomy, examining their application to various concepts, and aims to deepen the understanding of fairness in LMs by addressing the unique challenges associated with these definitions.",
            "score": 0.5963828392021223,
            "section_title": "Taxonomy",
            "char_start_offset": 5532,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 450
                },
                {
                    "start": 453,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1675
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 117,
                    "matchedPaperCorpusId": "10104149"
                },
                {
                    "start": 589,
                    "end": 593,
                    "matchedPaperCorpusId": "250390561"
                },
                {
                    "start": 982,
                    "end": 985,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 1020,
                    "end": 1023,
                    "matchedPaperCorpusId": "201670701"
                },
                {
                    "start": 1291,
                    "end": 1295,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1326,
                    "end": 1329,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 1395,
                    "end": 1400,
                    "matchedPaperCorpusId": "258833296"
                },
                {
                    "start": 1400,
                    "end": 1404,
                    "matchedPaperCorpusId": "258676079"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92041015625
        },
        {
            "corpus_id": "271902917",
            "title": "REFINE-LM: Mitigating Language Model Stereotypes via Reinforcement Learning",
            "text": "In this paper, we introduced the REFINE-LM approach to mitigate the stereotypical bias encoded in pre-trained LMs without hurting model performance. The proposed techniques make use of a large corpus of under-specified questions and reinforcement learning techniques to suppress different types of stereotypical bias in LMs, including gender-, nationality-, ethnicity-, and religion-based biases. \n\nOur evaluation results conducted on small and large language models open the door for further research avenues, which we envision to explore. Firstly, we envision to extend this empirical study to further bias datasets such as CrowS-pairs [34] and BBQ [35]. Secondly, we intend to carry out an extensive performance evaluation on different downstream tasks -e.g., conversational agents, text generation and summarization -, support for multilingual LMs, and efficient training of multiple bias types simultaneously.",
            "score": 0.5963786566897304,
            "section_title": "Conclusion and Perspectives",
            "char_start_offset": 23920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 914
                }
            ],
            "ref_mentions": [
                {
                    "start": 638,
                    "end": 642,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 651,
                    "end": 655,
                    "matchedPaperCorpusId": "239010011"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "247626152",
            "title": "You reap what you sow: On the Challenges of Bias Evaluation Under Multilingual Settings",
            "text": "On the topic of socially discriminatory systems within machine learning, Buolamwini and Gebru (2018) and Raji and Buolamwini (2019) show that there are significant disparities along gendered and racialized lines in commercially available facial recognition and analysis systems. Similar issues of discriminatory social biases in natural language processing (NLP) systems have resulted in emerging research dedicated to the identification, quantification (e.g. Rudinger et al., 2018;De-Arteaga et al., 2019;Czarnowska et al., 2021), and mitigation of bias (Bolukbasi et al., 2016;Sun et al., 2019;Garimella et al., 2021) in NLP systems.\n\nHowever, these methods tend to obscure rather than remove social biases (Gonen and Goldberg, 2019), and are particularly brittle when applied to complex, contextual language representations (Dev et al., 2020).\n\nFurther, operationalization of under-specified \"bias\" has varied widely across studies, and in some cases has been internally inconsistent with their stated goals (Blodgett et al., 2020;Jacobs and Wallach, 2021). The recent surge of LLMs is no exception to such concerns. Hovy and Prabhumoye (2021); Talat et al. (2021), and Cao and Daum\u00e9 III (2020) argue that socially discriminatory biases can be encoded in several stages of the LLM development process (Biderman and Scheirer, 2020), including data sampling, annotation, selection of input representations or model, research design, and how the models are situated with regards to the language communities that they are applied to. Language generation models, despite their inference-time flexibility, are particularly susceptible to reproducing hegemonic social biases and generating offensive language, even when not explicitly prompted to do so (Sheng et al., 2021;Wallace et al., 2019;Bender et al., 2021). In efforts to address the expression of such social biases, a number of bias evaluation benchmarks have been proposed (Dev et al., 2021b;Zhao et al., 2018;Cao and Daum\u00e9 III, 2020). However",
            "score": 0.5963299024764326,
            "section_title": "Machine-learned Systems in Social Context",
            "char_start_offset": 5047,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 131,
                    "matchedPaperCorpusId": "91168921"
                },
                {
                    "start": 460,
                    "end": 482,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 555,
                    "end": 579,
                    "matchedPaperCorpusId": "237347097"
                },
                {
                    "start": 579,
                    "end": 596,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 1011,
                    "end": 1034,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 1034,
                    "end": 1059,
                    "matchedPaperCorpusId": "209202216"
                },
                {
                    "start": 1304,
                    "end": 1333,
                    "matchedPaperCorpusId": "226254132"
                },
                {
                    "start": 1749,
                    "end": 1769,
                    "matchedPaperCorpusId": "234337004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64453125
        },
        {
            "corpus_id": "270521813",
            "title": "The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Pre-trained Language Models",
            "text": "In this paper, we propose a novel interpretable technique, Integrated Gap Gradients (IG 2 ), to precisely identify social bias neurons in pre-trained language models.We also develop a new dataset to facilitate the interpretability study of social bias.Derived from our interpretable technique, BIAS NEURON SUPPRESSION (BNS) is further proposed to mitigate social bias.Extensive experiments have verified the effectiveness of our IG 2 and BNS.In addition, facilitated by our interpretable method, we analyze the distribution shift of social bias neurons after debiasing and obtain useful insights that bring inspiration to future fairness research.\n\nLimitations.While our study provides valuable insights, we recognize there exist limitations.For example, our proposed BNS method directly sets the activation values of selected social bias neurons to zero.Although this is effective, designing a more refined suppression method might yield even better results.These present opportunities for future research.",
            "score": 0.5951159300868138,
            "section_title": "LIMITATIONS AND CONCLUSION",
            "char_start_offset": 22402,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 166,
                    "end": 252
                },
                {
                    "start": 252,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 647
                },
                {
                    "start": 649,
                    "end": 661
                },
                {
                    "start": 661,
                    "end": 742
                },
                {
                    "start": 742,
                    "end": 855
                },
                {
                    "start": 855,
                    "end": 959
                },
                {
                    "start": 959,
                    "end": 1007
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8974609375
        },
        {
            "corpus_id": "259075945",
            "title": "Technical Report on Token Position Bias in Transformers",
            "text": "To address the issue of bias in language models, we introduce two methods that make adjustments to the input data during training. We will explain the underlying principles of each method, as well as evaluate their effectiveness. Overall, these techniques can be used to improve the robustness of Transformer LMs to token position bias.",
            "score": 0.5943329224124863,
            "section_title": "OVERCOMING POSITION BIAS",
            "char_start_offset": 20480,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 336
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "259095584",
            "title": "An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models",
            "text": "In this study, we investigated the performance of prefix tuning, prompt tuning, and adapter tuning on mitigating social bias and preserving the linguistic and factual knowledge for two types of pre-trained language models. Our results demonstrated the effectiveness and efficacy of parameter-efficient methods in combination with CDA, and also revealed their performance limitations by comparing to post-hoc debiasing methods. We hope that our study can make it more accessible for others to debias pre-trained language models with reduced computational requirements, and contribute to fair and inclusive NLP.",
            "score": 0.5938556083108221,
            "section_title": "Conclusion",
            "char_start_offset": 25565,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.865234375
        },
        {
            "corpus_id": "258865522",
            "title": "An Efficient Multilingual Language Model Compression through Vocabulary Trimming",
            "text": "Social bias datasets. To study the effect of VT on social bias existing in pre-trained LMs, we first conduct experiments on two commonly used social bias evaluation datasets for masked LMs: Stere-oSet (SS; Nadeem et al., 2021) and crowdsourced stereotype pairs benchmark (CP; Nangia et al., 2020). SS consists of associative contexts covering four types of social biases: race, gender, religion, and profession; while CP is crowdsourced and annotated by workers in the United States, which contains nine types of social biases: race, gender, sexual orientation, religion, age, nationality, disability, physical appearance, and socioeconomic status. In order to further investigate the impact of pre/post-FT VT on LMs, we trim and fine-tune models on sentiment analysis with different orders and evaluate the social bias in such models on the Equity Evaluation Corpus (EEC; Kiritchenko and Mohammad, 2018) considering two bias types: gender and race. The EEC dataset was specifically with the aim to examine social bias for sentiment analysis systems. \n\nEvaluation metrics. We compare the pseudolikelihood scores returned by each model for stereotypical and anti-stereotypical sentences using AULA (All Unmasked Likelihood with Attention weights) (Kaneko and Bollegala, 2022). AULA has been shown to be robust against the frequency biases of the masked tokens and provides a more reliable assessment in contrast to alternative metrics when evaluating social biases in masked language models (MLMs). Given a sentence pair in the test dataset: \"My mom spent all day cooking for Thanksgiving\" vs. \"My dad spent all day cooking for Thanksgiving\", the first sentence is considered as stereotypical while the second one is anti-stereotypical. AULA computes the percentage of stereotypical sentences preferred by the MLM over anti-stereotypical ones as the bias score. The AULA score falls within the range of [0,100] and an unbiased model would return a bias score close to 50. On the other hand, a bias score greater than or less than 50 indicates the bias direction towards the stereotype or anti-stereotype, respectively.",
            "score": 0.5935623122779671,
            "section_title": "Experimental setting",
            "char_start_offset": 15893,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1050
                },
                {
                    "start": 1053,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2117
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 226,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 276,
                    "end": 296,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 873,
                    "end": 904,
                    "matchedPaperCorpusId": "21670658"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90869140625
        },
        {
            "corpus_id": "253762006",
            "title": "Demographic-Aware Language Model Fine-tuning as a Bias Mitigation Technique",
            "text": "BERT-like language models (LMs), when exposed to large unstructured datasets, are known to learn and sometimes even amplify the biases present in such data. These biases generally reflect social stereotypes with respect to gender, race, age, and others. In this paper, we analyze the variations in gender and racial biases in BERT, a large pre-trained LM, when exposed to different demographic groups. Specifically, we investigate the effect of fine-tuning BERT on text authored by historically disadvantaged demographic groups in comparison to that by advantaged groups. We show that simply by fine-tuning BERT-like LMs on text authored by certain demographic groups can result in the mitigation of social biases in these LMs against various target groups.",
            "score": 0.593320447536384,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98046875
        },
        {
            "corpus_id": "274965310",
            "title": "Mitigating Social Bias in Large Language Models: A Multi-Objective Approach within a Multi-Agent Framework",
            "text": "Natural language processing (NLP) has seen remarkable advancements with the development of large language models (LLMs). Despite these advancements, LLMs often produce socially biased outputs. Recent studies have mainly addressed this problem by prompting LLMs to behave ethically, but this approach results in unacceptable performance degradation. In this paper, we propose a multi-objective approach within a multi-agent framework (MOMA) to mitigate social bias in LLMs without significantly compromising their performance. The key idea of MOMA involves deploying multiple agents to perform causal interventions on bias-related contents of the input questions, breaking the shortcut connection between these contents and the corresponding answers. Unlike traditional debiasing techniques leading to performance degradation, MOMA substantially reduces bias while maintaining accuracy in downstream tasks. Our experiments conducted on two datasets and two models demonstrate that MOMA reduces bias scores by up to 87.7%, with only a marginal performance degradation of up to 6.8% in the BBQ dataset. Additionally, it significantly enhances the multi-objective metric icat in the StereoSet dataset by up to 58.1%. Code will be made available at https://github.com/Cortantse/MOMA.",
            "score": 0.5933191638669831,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97900390625
        },
        {
            "corpus_id": "259859034",
            "title": "Unlearning Bias in Language Models by Partitioning Gradients",
            "text": "Many approaches aim to change the training process for embeddings, classifiers, or encoders, either through changing the training procedure or adding bias-aware terms to the training loss function (Zhao et al., 2018a;Lauscher et al., 2021). Some of this work has achieved success by attempting to \"neutralize\" the language models' representation of biased words over some bias subspace by finetuning (Kaneko and Bollegala, 2021) or prompt tuning (Yang et al., 2023), or by extending these ideas by reformulating the bias dimensions as a set of implicit dimensions from social psychology (Omrani et al., 2023). Other methods propose changing or augmenting the training data in some way, typically by adding high-quality unbiased or antistereotypical sentences, eliminating blatantly biased or stereotypical sentences, or a combination of the two by replacing texts in the training corpus (Elazar and Goldberg, 2018;Guo et al., 2022;Qian et al., 2022). Yet other techniques utilize counterfactual or adversarial signals to dissuade models from encoding biases (Zhao et al.,  2018a; Elazar and Goldberg, 2018;Zhang et al., 2018;Zmigrod et al., 2019;Hall Maudslay et al., 2019;Webster et al., 2020). \n\nPerhaps most similar to our method is actually work done in the knowledge editing space. Such tasks propose explicitly editing specific knowledge in a model without affecting unrelated knowledge (Sinitsin et al., 2020;Zhu et al., 2020). This is quite similar to our task in that we aim to remove specific social bias from our model without affecting unrelated inference ability. However, our method attempts to remove generalized forms of these biases, as opposed to removing/changing the more targeted and specific knowledge that knowledge editing methods attempts to do. Recent studies include gradient-based methods that train separate networks to predict efficient gradient updates for removing or replacing models' knowledge (Cao et al., 2021;Mitchell et al., 2021).",
            "score": 0.5932493636790821,
            "section_title": "Related Work",
            "char_start_offset": 6302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1195
                },
                {
                    "start": 1198,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1969
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 217,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 217,
                    "end": 239,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 400,
                    "end": 428,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 587,
                    "end": 608,
                    "matchedPaperCorpusId": "259370845"
                },
                {
                    "start": 887,
                    "end": 914,
                    "matchedPaperCorpusId": "52056513"
                },
                {
                    "start": 914,
                    "end": 931,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 931,
                    "end": 949,
                    "matchedPaperCorpusId": "249062690"
                },
                {
                    "start": 1080,
                    "end": 1106,
                    "matchedPaperCorpusId": "52056513"
                },
                {
                    "start": 1125,
                    "end": 1146,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 1146,
                    "end": 1173,
                    "matchedPaperCorpusId": "202541569"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "253553353",
            "title": "Mind Your Bias: A Critical Review of Bias Detection Methods for Contextual Language Models",
            "text": "No sensitive data were used in our experiments. The impact of bias in language models on the development of fair, accountable and transparent algorithms is substantial and stands to affect numerous groups and social minorities, which directly entails the importance of accurate and reliable bias detection methods that can be applied to a variety of biases. In this work, we demonstrate the lack of comprehensive methods and aim to identify common problems in existing methods to provide directions for future research that can address their shortcomings. We provide insights into how and when existing methods can be used in the meantime. At the same time, we argue that addressing biases in language models requires and deserves a concerted community effort (including domain expertise, data curation, and method development) instead of the current reliance on a patchwork of locally optimal detection methods that may ultimately end up hiding biases globally when an unsuitable method is deployed.",
            "score": 0.5929516677780009,
            "section_title": "Ethical Statement",
            "char_start_offset": 34931,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 1000
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85595703125
        },
        {
            "corpus_id": "264426563",
            "title": "Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications",
            "text": "In this work, we thoroughly investigate the underexplored problem of fairness of large language models (LLMs) for tabular tasks. Our study unfolds in several phases. \n\nInitially, we assess the inherent fairness displayed by GPT-3.5, comparing their performance in zero-shot learning scenarios against traditional machine learning models like random forests (RF) and shallow neural networks (NN). Furthermore, we investigate how GPT-3.5 learns and propagates social biases when subjected to few-shot in-context learning, label-flipped in-context learning, finetuning, and data resampling techniques. \n\nOur discoveries shed light on several key insights. We find that GPT-3.5 tends to heavily rely on the social biases inherited from their pre-training data when making predictions, which is a concerning issue. Moreover, we observe that few-shot incontext learning can partially mitigate the inherent biases in GPT-3.5, yet it cannot entirely eliminate them. A significant fairness metric gap between different subgroups persists and exceeds that observed in RF and NN. This observation underscores the existence of biases within the LLMs themselves, beyond just the task datasets. Additionally, labelflipping applied to the few-shot examples effectively reverses the effects of bias, again corroborating the existence of inherent biases in GPT-3.5. However, as expected, this leads to a loss in the classification performance. Besides, our work reveals that while fine-tuning can sometimes improve the fairness of GPT-3.5, data resampling does not consistently yield the same results, unlike what is typically observed in traditional machine learning models. This underscores the need for the development of more effective strategies to mitigate the bias inherent in LLMs and ensure their fairness when deployed in real-world applications.",
            "score": 0.5923139767608965,
            "section_title": "Conclusion",
            "char_start_offset": 25178,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 165
                },
                {
                    "start": 168,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1839
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94287109375
        },
        {
            "corpus_id": "248366292",
            "title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias",
            "text": "We also give special attention to the less explored area of social biases in the context of affect i.e., Affective Bias (or emotion associated bias) specific to large PLMs. Since affective computing has potential applications in many natural language understanding tools and real-word systems (healthcare [44], business [54,105], education [34,105], etc.), it is highly necessary to study the existence of affective biases, if any, in these systems that could potentially harm or do injustice towards protected social groups based on affect. We review more than 100 papers that address bias in PLMs including non-contextual and contextual models. We collect research papers from ACL anthology, Google Scholar and arXiv, using the keywords 'bias', 'fairness', 'bias in NLP', 'fairness in NLP', 'Sentiment bias', 'Affective bias', 'Emotion bias' 'bias in pre-trained language models', etc. as the inclusion criteria for our survey. The major contributions of this survey are summarised below: \n\n\u2022 We present a comprehensive survey of bias in pre-trained language models, especially an in-depth treatment of various kinds of bias that originate in transformer based contextual pre-trained language models in NLP along with their identification, quantification and mitigation strategies. \u2022 We, for the first time, to the best of our knowledge, investigate Affective Bias, a highly socially relevant and less addressed problem, specifically in the context of large pre-trained language models. \u2022 We collect and present a large number of available bias evaluation corpora along with their suitability to evaluate large pre-trained language models. \u2022 We also discuss present research challenges in large pre-trained language models and affective biases. \n\nThe rest of the paper is organized as, the background of PLMs and bias in PLMs provided in section 2, quantifying PLM bias in section 3, mitigating PLM bias in section 4, affective bias in PLMs including their identification and mitigation strategies in section 5, a list of available bias evaluation corpora in section 6, research challenges in section 7 and concluding remarks in section 8.",
            "score": 0.5912808978555765,
            "section_title": "Heterogeneous View of Bias in PLMs",
            "char_start_offset": 5127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 990
                },
                {
                    "start": 993,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1746
                },
                {
                    "start": 1749,
                    "end": 2141
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 309,
                    "matchedPaperCorpusId": "2313204"
                },
                {
                    "start": 320,
                    "end": 324,
                    "matchedPaperCorpusId": "32437011"
                },
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "212705218"
                },
                {
                    "start": 340,
                    "end": 344,
                    "matchedPaperCorpusId": "192646023"
                },
                {
                    "start": 344,
                    "end": 348,
                    "matchedPaperCorpusId": "212705218"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91650390625
        },
        {
            "corpus_id": "264172688",
            "title": "Understanding Fairness Surrogate Functions in Algorithmic Fairness",
            "text": "Recently, as the capabilities of Large Language Models (LLMs) have increased, they are remarkably powerful, transcending traditional boundaries in technology and innovation (Achiam et al., 2023;Touvron et al., 2023). These advanced models harness vast amounts of data to understand and generate human-like text, solve complex problems with nuanced insights, and even create content that feels genuinely human-crafted (Liu et al., 2023b). Their applications span from enhancing natural language processing to driving advancements in fields like healthcare (Thirunavukarasu et al., 2023) and education (Kasneci et al., 2023), showcasing a transformative potential for both industry and society at large. While LLMs' trustworthiness (Wang et al., 2023;Qian et al., 2024) has become a focal point of widespread attention, one may ask how the insights in this paper can be extended to LLMs. \n\nGenerally, adapting the conclusions from this paper to large language models (LLMs) involves recognizing the complexity of biases these models can inherit from datasets (Brunet et al., 2019). For LLMs, identifying biases requires sophisticated analysis tools that can understand language use and cultural contexts. To our knowledge, this issue remains an unsolved challenge and has not yet been thoroughly explored (Li et al., 2023;Liu et al., 2023a). Addressing these biases involves not just algorithmic adjustments, but also curating training data, refining model architectures, and implementing continuous feedback loops to identify and mitigate bias. This approach may even emphasize the need for multidisciplinary efforts, combining technical, ethical, and social perspectives to enhance fairness in LLMs. Also, the task considered in this paper is discriminative, while LLM is widely used in generative task. Machine learning focuses on algorithms",
            "score": 0.5904412375650636,
            "section_title": "E.2 The Insights for Large Language Models",
            "char_start_offset": 58750,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 885
                },
                {
                    "start": 888,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1842
                }
            ],
            "ref_mentions": [
                {
                    "start": 417,
                    "end": 436,
                    "matchedPaperCorpusId": "257921533"
                },
                {
                    "start": 555,
                    "end": 585,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 600,
                    "end": 622,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 730,
                    "end": 749,
                    "matchedPaperCorpusId": "259202782"
                },
                {
                    "start": 1057,
                    "end": 1078,
                    "matchedPaperCorpusId": "52946942"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90771484375
        },
        {
            "corpus_id": "237502736",
            "title": "Uncovering Implicit Gender Bias in Narratives through Commonsense Inference",
            "text": "Pre-trained language models (LMs) (Radford et al., 2019;Lewis et al., 2020;Brown et al., 2020) have been successfully used in many NLP tasks including generation. Despite their widespread usage, recent works showed that LMs capture and even reinforce unwanted social stereotypes abundant in their training corpora (Sheng et al., 2019(Sheng et al., , 2020Liu et al., 2020b;Shwartz et al., 2020;Bender et al., 2021). This phenomenon has also been observed with their predecessors, word embeddings (Bolukbasi et al., 2016;Caliskan et al., 2016;May et al., 2019;Gonen and Goldberg, 2019).\n\nWhile many prior works have examined societal biases in specialized NLG systems such as dialogues systems (Lee et al., 2019;Liu et al., 2020a;Dinan et al., 2020a,b), not much work has been done on bias analysis for story generation systems.\n\nThere is a growing amount of work on automatic story generation with real-world applications in education, entertainment, working with children and sensitive populations. Therefore, it is essential Protagonist earned a college degree.",
            "score": 0.5902741708916011,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 56,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 56,
                    "end": 75,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 314,
                    "end": 333,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 333,
                    "end": 354,
                    "matchedPaperCorpusId": "218470535"
                },
                {
                    "start": 354,
                    "end": 372,
                    "matchedPaperCorpusId": "221970809"
                },
                {
                    "start": 372,
                    "end": 393,
                    "matchedPaperCorpusId": "215238527"
                },
                {
                    "start": 541,
                    "end": 558,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 692,
                    "end": 710,
                    "matchedPaperCorpusId": "211142738"
                },
                {
                    "start": 710,
                    "end": 728,
                    "matchedPaperCorpusId": "204838020"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81103515625
        },
        {
            "corpus_id": "262828449",
            "title": "Survey of Social Bias in Vision-Language Models",
            "text": "The remainder of this survey is organized as follows. Section 2 illustrates the overview regarding the concept of bias in machine learning as well as the common metrics and general frameworks for mitigating bias in machine learning. Section 3 presents the causes, evaluation, and mitigation methods of social bias in the case of modeling a single modality, covering bias in the textual (language) modality and bias in the vision modality. Section 4 highlights the works on bias in vision-and-language multimodality, as well as emphasizes the causes, evaluation, and mitigation techniques in VL models.",
            "score": 0.5883113584966391,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 601
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63232421875
        },
        {
            "corpus_id": "270924184",
            "title": "Social Bias Evaluation for Large Language Models Requires Prompt Variations",
            "text": "Warning: This paper contains examples of stereotypes and biases. Large Language Models (LLMs) exhibit considerable social biases, and various studies have tried to evaluate and mitigate these biases accurately. Previous studies use downstream tasks as prompts to examine the degree of social biases for evaluation and mitigation. While LLMs' output highly depends on prompts, previous studies evaluating and mitigating bias have often relied on a limited variety of prompts. In this paper, we investigate the sensitivity of LLMs when changing prompt variations (task instruction and prompt, few-shot examples, debias-prompt) by analyzing task performance and social bias of LLMs. Our experimental results reveal that LLMs are highly sensitive to prompts to the extent that the ranking of LLMs fluctuates when comparing models for task performance and social bias. Additionally, we show that LLMs have tradeoffs between performance and social bias caused by the prompts. Less bias from prompt setting may result in reduced performance. Moreover, the ambiguity of instances is one of the reasons for this sensitivity to prompts in advanced LLMs, leading to various outputs. We recommend using diverse prompts, as in this study, to compare the effects of prompts on social bias in LLMs.",
            "score": 0.5879347818309062,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9775390625
        },
        {
            "corpus_id": "236477795",
            "title": "He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation",
            "text": "There has been research in studying systems trained on human-written texts that learn human-like biases (Bolukbasi et al., 2016;Caliskan et al., 2016;Sun et al., 2019). Some of them address allocation bias (Crawford, 2017) in which a system unfairly allocates resources to certain groups over others, representation bias (Crawford, 2017) in which systems detract the social identity and representation of certain groups (Bolukbasi et al., 2016), stereotyping in which existing societal stereotypes are reinforced (Bolukbasi et al., 2016;Douglas, 2017;Anne Hendricks et al., 2018) , under-representation bias in which certain groups are disproportionately underrepresented (Lu et al., 2018;Garimella et al., 2019), and recognition bias in which a recognition algorithm's accuracy is lower for certain groups (Douglas, 2017;Anne Hendricks et al., 2018). Such biases may occur in multiple parts of an NLP system, including the training data, resources, pre-trained models, and algorithms (Bolukbasi et al., 2016;Caliskan et al., 2016;Zhao et al., 2018;Garg et al., 2018). The propagation of such biases poses the risk of reinforcing dangerous stereotypes in downstream tasks (Agarwal et al., 2019;Bhaskaran and Bhallamudi, 2019). \n\nWhile there exist works on mitigating social biases in language representations (Bolukbasi et al., 2016;Liang et al., 2020), there has been very little focus on debiasing the language models themselves or generation systems, specifically pre-trained language models that are widely used in several generation tasks. Qian et al. (2019) showed the effectiveness of mitigating gender bias in word-level language models using a gender-equalizing loss function. Sheng et al. (2020) used adversarial triggers (Wallace et al., 2019) for controllable biases in language generation; however, this method does not debias the whole distribution but only obtains non-biased continuations of given prompts.",
            "score": 0.5865278516611605,
            "section_title": "Related Work",
            "char_start_offset": 4782,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1922
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 128,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 150,
                    "end": 167,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 420,
                    "end": 444,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 513,
                    "end": 537,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 551,
                    "end": 579,
                    "matchedPaperCorpusId": "4384334"
                },
                {
                    "start": 689,
                    "end": 712,
                    "matchedPaperCorpusId": "196181460"
                },
                {
                    "start": 822,
                    "end": 850,
                    "matchedPaperCorpusId": "4384334"
                },
                {
                    "start": 985,
                    "end": 1009,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1049,
                    "end": 1067,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 1172,
                    "end": 1194,
                    "matchedPaperCorpusId": "174803409"
                },
                {
                    "start": 1194,
                    "end": 1225,
                    "matchedPaperCorpusId": "195584301"
                },
                {
                    "start": 1309,
                    "end": 1333,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1333,
                    "end": 1352,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1545,
                    "end": 1563,
                    "matchedPaperCorpusId": "170078973"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8623046875
        },
        {
            "corpus_id": "270380297",
            "title": "Improving Commonsense Bias Classification by Mitigating the Influence of Demographic Terms",
            "text": "Extensive research in Natural Language Processing (NLP) has focused on investigating biases in language models and developing strategies to understand and mitigate them.A study exploring the relationship between the size of language models and their biases was conducted, finding that larger models exhibit more nuanced biases and an increased potential for bias in specific categories [5].Researchers have also explored bias evaluation in language models, employing extrinsic evaluation methods and debiasing techniques to identify and address bias in real world applications and downstream tasks [7,8].Intrinsic evaluation methods, such as analyzing word or sentence embeddings, contribute to a deeper understanding of biases within language models [9,10,11].Additionally, It has been shown that systematic differences in bias measurement can be revealed through the unification of extrinsic evaluation methods, attributed to parameter choices [12].Moreover, these methods have been used to identify biased decisions from models in specific tasks [13].Recent studies have drawn significant attention to the bias caused by demographic terms in language models.Various techniques have been proposed to address and mitigate demographic bias, aiming to promote fairness, equality, and inclusivity.These techniques include debiasing models trained on human-annotated examples [10], fairness-aware neural language models [14], bias-mitigating transformer architectures [15], and debiasing frameworks to identify and correct demographic bias [16].Additionally, post-hoc debiasing techniques have been explored, where a debiasing step is added to sentence representations after initial training, prior to their utilization in downstream tasks [10,17].While preceding investigations focused on biases in language models linked to particular categories like origin and gender, our study adopts a more extensive methodology, assessing biases across a wider range of demographic terms.Building upon previous studies, our research proposes novel approaches to mitigate the impact of demographic terms and enhance the performance of the commonsense polarization classifier in knowledge models.This approach distinguishes itself from prior research by employing augmentation to substitute the predicate segment of a sentence with a synonym while retaining the original semantic content.Furthermore, to address bias stemming from demographic terms, our methodology involves substituting specific demographic terms with broader alternatives.",
            "score": 0.5851384434987172,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 3511,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 169,
                    "end": 390
                },
                {
                    "start": 390,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 761
                },
                {
                    "start": 761,
                    "end": 951
                },
                {
                    "start": 951,
                    "end": 1054
                },
                {
                    "start": 1054,
                    "end": 1161
                },
                {
                    "start": 1161,
                    "end": 1295
                },
                {
                    "start": 1295,
                    "end": 1542
                },
                {
                    "start": 1542,
                    "end": 1745
                },
                {
                    "start": 1745,
                    "end": 1975
                },
                {
                    "start": 1975,
                    "end": 2181
                },
                {
                    "start": 2181,
                    "end": 2373
                },
                {
                    "start": 2373,
                    "end": 2526
                }
            ],
            "ref_mentions": [
                {
                    "start": 386,
                    "end": 389,
                    "matchedPaperCorpusId": "248649722"
                },
                {
                    "start": 598,
                    "end": 601,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 751,
                    "end": 754,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 754,
                    "end": 757,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 757,
                    "end": 760,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 946,
                    "end": 950,
                    "matchedPaperCorpusId": "235658325"
                },
                {
                    "start": 1049,
                    "end": 1053,
                    "matchedPaperCorpusId": "44090948"
                },
                {
                    "start": 1373,
                    "end": 1377,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "matchedPaperCorpusId": "19117312"
                },
                {
                    "start": 1465,
                    "end": 1469,
                    "matchedPaperCorpusId": "198968250"
                },
                {
                    "start": 1737,
                    "end": 1741,
                    "matchedPaperCorpusId": "1704893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9306640625
        },
        {
            "corpus_id": "270095018",
            "title": "Expert-Guided Extinction of Toxic Tokens for Debiased Generation",
            "text": "Large language models (LLMs), renowned for their extraordinary natural language understanding, generation, and generalization capabilities, have achieved state-of-the-art performances across various tasks and benchmarks [1,43,19,39,37].However, the extensive application of LLMs in text generation has given rise to legitimate concerns [23,10].Recent studies have revealed that LLMs elicit the potential to exhibit unwelcome negative behaviors, such as the propagation of biases, particularly when provided with leading prompts or instructions [11,21,45].These biases can contain toxic content with threats or profanity, social stereotypes, or prejudiced perceptions toward certain groups, further perpetuating societal inequities, reinforcing discriminations, and impacting LLMs' applicability [23].For example, the social bias presented in LLMs can lead to unfairly generated articles or stories, potentially spreading misinformation or societal prejudices [25].Mitigating these biases in LLMs reduces malicious use from hostile intentions and ensures trustworthy language processing.Generally, enhancing LLMs for specialized attribute control to address the abovementioned limitations involves three aspects: fine-tuning to inject parameterized knowledge, retrieval to couple nonparameterized knowledge, and prompting to improve the output in several reasoning turns [11,38].The motivation of our work.Large language models may elicit social bias during generation, especially when encountering potentially toxic input.However, existing debiasing methods for generative language models encounter several difficulties.However, applying these techniques directly to debias LLMs encounters several challenges.The data-hungry fine-tuning requires extensive highquality corpus, which requires large manpower for data annotation [14,26].The labeling process can also contain stereotypical subjective views from the annotator, further exacerbating the corpus quality [18].Similarly, retrieval requires a meticulously curated debiased corpus, which is infeasible to process [30].Prompting utilizes the self-reflection ability of LLMs to correct and improve the former responses, but repetitively inputting the instructions occupies the limited context window and the space for input, further leading to greater inference latency [29].",
            "score": 0.5850130898284881,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 236,
                    "end": 344
                },
                {
                    "start": 344,
                    "end": 555
                },
                {
                    "start": 555,
                    "end": 800
                },
                {
                    "start": 800,
                    "end": 964
                },
                {
                    "start": 964,
                    "end": 1086
                },
                {
                    "start": 1086,
                    "end": 1378
                },
                {
                    "start": 1378,
                    "end": 1405
                },
                {
                    "start": 1405,
                    "end": 1522
                },
                {
                    "start": 1522,
                    "end": 1620
                },
                {
                    "start": 1620,
                    "end": 1709
                },
                {
                    "start": 1709,
                    "end": 1834
                },
                {
                    "start": 1834,
                    "end": 1968
                },
                {
                    "start": 1968,
                    "end": 2074
                },
                {
                    "start": 2074,
                    "end": 2329
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 235,
                    "matchedPaperCorpusId": "259858918"
                },
                {
                    "start": 548,
                    "end": 551,
                    "matchedPaperCorpusId": "268379141"
                },
                {
                    "start": 959,
                    "end": 963,
                    "matchedPaperCorpusId": "264491114"
                },
                {
                    "start": 1826,
                    "end": 1830,
                    "matchedPaperCorpusId": "259859044"
                },
                {
                    "start": 1830,
                    "end": 1833,
                    "matchedPaperCorpusId": "235313967"
                },
                {
                    "start": 1963,
                    "end": 1967,
                    "matchedPaperCorpusId": "237298625"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97705078125
        },
        {
            "corpus_id": "262828449",
            "title": "Survey of Social Bias in Vision-Language Models",
            "text": "\u2022 Inference-Time Debiasing FairFil [34] adopts contrastive learning to de-bias the output of the pre-trained text encoder of BERT [47]. CORSAIR [133] incorporates counterfactual inference by using a poisonous text classifier to post-adjust the output predictions during inference. Sheng et al. [160] reduce bias towards different social demographics in the generated text by prepending trigger phrases in the prompt for free-form generation helps to reduce social biases. Self-Debias [152] leverages internal knowledge of pre-trained models to reduce the probabilities of generating biased text. UDDIA [190] utilizes a unified detoxifying and debiasing rectification method to reduce the bias and toxicity of the generated output while maintaining its quality. \n\nCriticism over de-biasing embedding spaces: Gonen and Goldberg [66] point out that de-biasing static word embeddings may just temporarily cover up bias. Moreover, Goldfarb-Tarrant et al. [65] empirically show that these de-biasing strategies do not necessarily yield de-biasing in downstream NLP tasks through zero or negative correlation among intrinsic and extrinsic bias measurements. \n\nIn addition to the aforementioned methods, other extensive literature focuses on bias mitigation methods in NLP. We share the gist of them for ease of reference. Sun et al. [167] provide a survey on mitigation methods for gender bias. Meade et al. [121] provide survey on current state-of-the-art de-biasing techniques for pre-trained LMs. Sheng et al. [161] suggest an extensive survey focused on societal bias including various demographics in language generation models.",
            "score": 0.5847915807447206,
            "section_title": "Bias Mitigation.",
            "char_start_offset": 27198,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 760
                },
                {
                    "start": 763,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1150
                },
                {
                    "start": 1153,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1626
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 39,
                    "matchedPaperCorpusId": "232185104"
                },
                {
                    "start": 144,
                    "end": 149,
                    "matchedPaperCorpusId": "236459953"
                },
                {
                    "start": 294,
                    "end": 299,
                    "matchedPaperCorpusId": "218470535"
                },
                {
                    "start": 484,
                    "end": 489,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 602,
                    "end": 607,
                    "matchedPaperCorpusId": "252780503"
                },
                {
                    "start": 826,
                    "end": 830,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "229923772"
                },
                {
                    "start": 1401,
                    "end": 1406,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 1506,
                    "end": 1511,
                    "matchedPaperCorpusId": "234337004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83740234375
        },
        {
            "corpus_id": "274776546",
            "title": "Bias Vector: Mitigating Biases in Language Models with Task Arithmetic Approach",
            "text": "The use of language models (LMs) has increased considerably in recent years, and the biases and stereotypes in training data that are reflected in the LM outputs are causing social problems. In this paper, inspired by the task arithmetic, we propose the ``Bias Vector'' method for the mitigation of these LM biases. The Bias Vector method does not require manually created debiasing data. The three main steps of our approach involve: (1) continual training the pre-trained LMs on biased data using masked language modeling; (2) constructing the Bias Vector as the difference between the weights of the biased LMs and those of pre-trained LMs; and (3) subtracting the Bias Vector from the weights of the pre-trained LMs for debiasing. We evaluated the Bias Vector method on the SEAT across three LMs and confirmed an average improvement of 0.177 points. We demonstrated that the Bias Vector method does not degrade the LM performance on downstream tasks in the GLUE benchmark. In addition, we examined the impact of scaling factors, which control the magnitudes of Bias Vectors, with effect sizes on the SEAT and conducted a comprehensive evaluation of our debiased LMs across both the SEAT and GLUE benchmarks.",
            "score": 0.5847910618912846,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9287109375
        },
        {
            "corpus_id": "272827681",
            "title": "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions",
            "text": "Large Language Models (LLMs) have made significant advancements in various fields, including medicine, engineering, and education (Sarker et al., 2023;Liu et al., 2023;Gill et al., 2024). Platforms such as ChatGPT (Liu et al., 2023) and Claude2 ease consumer interactions with LLMs. However, the quality of these interactions may be compromised if a model exhibits bias, which is commonly defined as any \"skew that produces a type of harm\" and can exist both implicitly and explicitly (Crawford, 2017;Dong et al., 2023). Implicit biases are unconscious attitudes that affect understanding, actions, and decisions without awareness or intention, whereas explicit biases are conscious attitudes that are deliberately formed and expressed, often involving overt prejudice or discrimination (Bai et al., 2024;Wang et al., 2019). \n\nNumerous resources have been developed to identify and mitigate bias in LLMs. Perspective API3 and RealToxicityPrompts (Gehman et al., 2020) cover a spectrum of abusive language, but primarily focus on explicit biases through profanities, threats and insults. Conversely, BBQ, Stere-oSet, and CrowS-Pairs focus on social biases such as stereotyping, capturing subtle forms of discrimination suitable for evaluating implicit bias (Parrish et al., 2022;Nangia et al., 2020;Nadeem et al., 2021). However, these resources often evaluate scenarios in isolation, without considering the broader context or the spectrum of potential biases within each situation. Moreover, datasets that focus on explicit bias may overlook less inflammatory language that still harbors negative sentiment towards target demographics (Zhao et al., 2023;Gonen and Goldberg, 2019). \n\nWe propose measuring bias through progressively offensive scenarios to evaluate appropriateness, which we define as \"the degree to which an expression or action is considered acceptable or inoffensive within a given social context\". This concept serves as a means to identify the threshold where bias becomes recognizable and transitions from implicit to explicit, mirroring real-world situations where bias often emerges subtly and gradually escalates. \"As Carlos begins browsing a shelf of goods, looking at buying churros, the clerk continues to watch him.",
            "score": 0.5841861080763668,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 824
                },
                {
                    "start": 827,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2137
                },
                {
                    "start": 2138,
                    "end": 2243
                }
            ],
            "ref_mentions": [
                {
                    "start": 1256,
                    "end": 1278,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 1278,
                    "end": 1298,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1298,
                    "end": 1318,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1655,
                    "end": 1680,
                    "matchedPaperCorpusId": "73729169"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8671875
        },
        {
            "corpus_id": "272880907",
            "title": "Do the Right Thing, Just Debias! Multi-Category Bias Mitigation Using LLMs",
            "text": "Bias Mitigation Existing approaches in bias mitigation have notable drawbacks. Methods focusing on debiasing word embeddings while preserving associations (Bolukbasi et al., 2016) fail to account for broader contextual biases beyond the word level. \n\nTechniques enhancing reliability through prompting (Si et al., 2022), while improving generalizability, bias reduction, calibration, and factuality for GPT-3, are limited by the prompts used and do not generalize well to smaller models. Bernsteinbounded unfairness (Ethayarajh, 2020) estimates classification bias with uncertainty but does not extend beyond classification tasks. Comprehensive surveys (Hort et al., 2022) of bias mitigation methods for ML classifiers and benchmarks like WinoBias (Zhao et al., 2017) focus narrowly on gender or racial bias, neglecting other forms of social bias. Upstream mitigation during language model fine-tuning (Jin et al., 2021) is a promising direction but requires expensive retraining of large language models. While Reinforcement Learning from AI Feedback (RLAIF) (Lee et al., 2023) has shown promise in aligning large language models (LLMs) for reactive tasks like counterspeech generation (Hengle et al., 2024), its efficacy in proactive debiasing of language remains an open question. \n\nExisting work (Hengle et al., 2024) primarily focuses on responding to biased speech only after it has been produced and disseminated, leaving the potential for preemptively mitigating biased language largely unexplored. We aim to address these gaps by applying Reinforcement Learning (RL) techniques for aligning language models to proactively identify and effectively mitigate potentially biased sentences across multiple bias classes into their debiased counterparts while retaining the context and linguistic quality of the ground truth. \n\nBias Datasets. Existing bias datasets, notably WIKIBIAS (Zhong et al., 2021), reveal an ambigu-ous pattern where the so-called debiased sentences often retain the original biases (See Table 1 for the example).",
            "score": 0.5836729905404388,
            "section_title": "Related Work",
            "char_start_offset": 5062,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1283
                },
                {
                    "start": 1286,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1827
                },
                {
                    "start": 1830,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2039
                }
            ],
            "ref_mentions": [
                {
                    "start": 516,
                    "end": 534,
                    "matchedPaperCorpusId": "216553205"
                },
                {
                    "start": 748,
                    "end": 767,
                    "matchedPaperCorpusId": "1389483"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92529296875
        },
        {
            "corpus_id": "266163873",
            "title": "Understanding the Effect of Model Compression on Social Bias in Large Language Models",
            "text": "Large Language Models (LLMs) are trained on large corpora using self-supervision, which allows models to consider vast amounts of unlabelled data, and learn language patterns through masking tasks (Devlin et al., 2019;Radford et al., 2019). However, self-supervision allows LLMs to pick up social biases contained in the training data. Which is amplified by larger models, more data, and longer training (Kaneko et al., 2022;Kaneko and Bollegala, 2022;Kurita et al., 2019;Delobelle and Berendt, 2022). \n\nSocial biases in LLMs are an ongoing problem that is propagated from pretraining to finetuning (Ladhak et al., 2023;Gira et al., 2022). Biased pretrained models are hard to fix, as retraining is prohibitively expensive both financially and environmentally (Hessenthaler et al., 2022). At the same time, the compression of LLMs has been intensely studied. Pruning, quantization, and distillation are among the most common strategies to compress LLMs. Pruning reduces the parameters of a trained model by removing redundant connections while preserving equivalent performance to their original counterparts (Liebenwein et al., 2021;Ahia et al., 2021). Quantization reduces the precision of model weights and activations to improve efficiency while preserving performance (Ahmadian et al., 2023). Finally, knowledge distillation (Hinton et al., 2015) trains a smaller more efficient model based on a larger pre-trained model. \n\nWhile much research has been done on measuring and mitigating social bias in LLMs, and making LLMs smaller and more efficient, by using one or a combination of many compression methods (Xu et al., 2021), little research has been done regarding the interplay between social biases and LLM compression.",
            "score": 0.5822856647745511,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 501
                },
                {
                    "start": 504,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1426
                },
                {
                    "start": 1429,
                    "end": 1729
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 218,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 472,
                    "end": 500,
                    "matchedPaperCorpusId": "250426284"
                },
                {
                    "start": 599,
                    "end": 620,
                    "matchedPaperCorpusId": "258378241"
                },
                {
                    "start": 620,
                    "end": 638,
                    "matchedPaperCorpusId": "248780268"
                },
                {
                    "start": 760,
                    "end": 787,
                    "matchedPaperCorpusId": "253397743"
                },
                {
                    "start": 1134,
                    "end": 1152,
                    "matchedPaperCorpusId": "238419368"
                },
                {
                    "start": 1330,
                    "end": 1351,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97265625
        },
        {
            "corpus_id": "222090785",
            "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
            "text": "Progress in natural language processing research has recently been driven by the use of large pretrained language models (Devlin et al., 2019;Liu et al., 2019;Lan et al., 2020). However, these models are trained on minimally-filtered real-world text, and contain ample evidence of their authors' social biases. These language models, and embeddings extracted from them, have been shown to Bias Type",
            "score": 0.5822098493431208,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 398
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 142,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 159,
                    "end": 176,
                    "matchedPaperCorpusId": "202888986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92041015625
        },
        {
            "corpus_id": "235097594",
            "title": "On Transferability of Bias Mitigation Effects in Language Model Fine-Tuning",
            "text": "The practice of fine-tuning pretrained language models (PTLMs or LMs), such as BERT (Devlin et al., 2019), has improved prediction performance in a wide range of NLP tasks. However, finetuned LMs may exhibit biases against certain protected groups (e.g., gender and ethnic minorities),  c). We study the viability of obtaining an upstream model that could reduce bias in a number of downstream classifiers when fine-tuned. \n\nas models may learn to associate certain features with positive or negative labels spuriously (Dixon et al., 2018), or propagate bias encoded in PTLMs to downstream classifiers (Caliskan et al., 2017;Bolukbasi et al., 2016). Among many examples, Kurita et al. (2019) demonstrates gender-bias in the pronoun resolution task when models are trained using BERT embeddings, and Kennedy et al. (2020) shows that hate speech classifiers finetuned from BERT result in more frequent false positive predictions for certain group identifier mentions (e.g., \"muslim\", \"black\"). \n\nApproaches for bias mitigation are mostly applied during fine-tuning to reduce bias in a specific downstream task or dataset (Park et al., 2018;Zhang et al., 2018;Beutel et al., 2017) (see Fig. 1 (a)). For example, data augmentation approaches reduce the influence of spurious features in the original dataset (Dixon et al., 2018;Zhao et al., 2018;Park et al., 2018), and adversarial learning approaches generate debiased data representations that are exclusive to the downstream model (Kumar et al., 2019;Zhang et al., 2018). These techniques act on biases particular to the given dataset, domain, or task, and require new bias mitigation when switching to a new downstream task or dataset. This can require auxiliary training objectives, the definition of task-specific fairness met-rics, the annotation of bias attributes (e.g., identifying African American Vernacular English), and the collection of users' demographic data.",
            "score": 0.5811796892458125,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 422
                },
                {
                    "start": 425,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1922
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 105,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 519,
                    "end": 539,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 602,
                    "end": 625,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 625,
                    "end": 648,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 671,
                    "end": 691,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 799,
                    "end": 820,
                    "matchedPaperCorpusId": "218517088"
                },
                {
                    "start": 1119,
                    "end": 1138,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 1138,
                    "end": 1157,
                    "matchedPaperCorpusId": "9424845"
                },
                {
                    "start": 1304,
                    "end": 1324,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 1324,
                    "end": 1342,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 1342,
                    "end": 1360,
                    "matchedPaperCorpusId": "52070035"
                },
                {
                    "start": 1480,
                    "end": 1500,
                    "matchedPaperCorpusId": "202541748"
                },
                {
                    "start": 1500,
                    "end": 1519,
                    "matchedPaperCorpusId": "9424845"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86181640625
        },
        {
            "corpus_id": "265212928",
            "title": "Exploring the Jungle of Bias: Political Bias Attribution in Language Models via Dependency Analysis",
            "text": "With the rise of large language models (LLMs) (Anil et al., 2023;OpenAI, 2023;Touvron et al., 2023;Reid et al., 2024, inter alia), we are witnessing increasing concern towards their nega-tive implications, such as the existence of biases, including social (Mei et al., 2023), cultural (Narayanan Venkit et al., 2023), brilliance (Shihadeh et al., 2022), nationality (Venkit et al., 2023), religious (Abid et al., 2021), and political biases (Feng et al., 2023). For instance, there is a growing indication that ChatGPT, on average, prefers proenvironmental, left-libertarian positions (Hartmann et al., 2023;Feng et al., 2023). \n\nDespite its practical relevance, bias in (large) language models is still a poorly understood topic (Blodgett et al., 2021;Dev et al., 2022;Talat et al., 2022). The frequent interpretation of LLM bias as statistical bias originating from training data, while conceptually correct, is strongly limited in its utility. van der Wal et al. (2022) reason that bias should, therefore, not be viewed as a singular concept but rather distinguish different concepts of bias at different levels of the NLP pipeline, e.g. distinct dataset and model biases. Furthermore, while it is undisputed that models do exhibit some biases, it is unclear whose biases they are exhibiting (Petreski and Hashim, 2022). Indeed, the literature up to this point has mostly focused on the downstream effects of bias -with only a few exceptions, such as van der Wal et al. (2022) that argue for the importance of an understanding of the internal causes. To advance this endeavour, we analyse LLM bias through the lens of causal fairness analysis, which facilitates both comprehending the origins of bias and reasoning about the subsequent consequences of bias and its mitigation. \n\nA thorough understanding of LLM bias is particularly important for the design and implementation of debiasing methods.",
            "score": 0.5808353110386221,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1779
                },
                {
                    "start": 1782,
                    "end": 1900
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 274,
                    "matchedPaperCorpusId": "259129801"
                },
                {
                    "start": 399,
                    "end": 418,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 441,
                    "end": 460,
                    "matchedPaperCorpusId": "258686693"
                },
                {
                    "start": 585,
                    "end": 608,
                    "matchedPaperCorpusId": "255440573"
                },
                {
                    "start": 608,
                    "end": 626,
                    "matchedPaperCorpusId": "258686693"
                },
                {
                    "start": 730,
                    "end": 753,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 753,
                    "end": 770,
                    "matchedPaperCorpusId": "252907216"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "258170403",
            "title": "Evaluation of Social Biases in Recent Large Pre-Trained Models",
            "text": "Given the popularity of large pre-trained language models and their widespread use in the community, it is pertinent to evaluate their inherent biases. These biases come about because these models (for example, BERT (Devlin et al., 2019) and BERT based models, ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019)) are trained on large sets of often freely available and unmoderated text data from sources such as the internet. \n\nDue to this, gender, race, religious and other social biases that we see in the real world are often translated into the models. Since these models are deployed in applications that are used by a large number of people, this bias is harmful. For example, these models are commonly used in content moderation tasks on social media platforms online. If the model is unfairly biased against certain social groups-minorities, marginalized people and those sections of society that are discriminated against, then they would be adversely affected by the application. So it is of utmost importance to ensure that models are trained in such a way that the bias is mitigated or to de-bias them. \n\nThere are multiple studies (Meade et al., 2022;Ahn and Oh, 2021;Bhardwaj et al., 2021;Kurita et al., 2019) on social biases in BERT and older language models. We extend this and evaluate the relatively newer models: ELECTRA (Clark et al., 2020), DistilBERT (Sanh et al., 2019) and DeBERTa (He et al., 2020). These models have gained popularity due to factors such as better performance, low computer requirements and parameter efficiency. Each model is evaluated against two bias evaluation datasets, StereoSet (Nadeem et al., 2021) and Crowdsourced Stereotype Pairs, CrowS-Pairs (Nangia et al., 2020). StereoSet accounts for four different biases that are based ongender, occupation, race and religion. In addition to these, CrowS-Pairs also has sexual orientation, age, nationality, disability and physical appearance.",
            "score": 0.5784428085218842,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 428
                },
                {
                    "start": 431,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1940
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 237,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 268,
                    "end": 286,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 1147,
                    "end": 1167,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 1167,
                    "end": 1184,
                    "matchedPaperCorpusId": "237491723"
                },
                {
                    "start": 1206,
                    "end": 1226,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 1344,
                    "end": 1364,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 1377,
                    "end": 1396,
                    "matchedPaperCorpusId": "203626972"
                },
                {
                    "start": 1631,
                    "end": 1652,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1700,
                    "end": 1721,
                    "matchedPaperCorpusId": "222090785"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96923828125
        },
        {
            "corpus_id": "264306000",
            "title": "A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models",
            "text": "Masked Language Models (MLMs) have achieved promising performance in many NLP tasks (Devlin et al., 2019;Liu et al., 2019;Liang et al., 2023). However, MLMs trained on massive amounts of textual training data have also been found to encode concerning levels of social biases such as gender and racial biases (Kaneko and Bollegala, 2019;May et al., 2019;Dev et al., 2020;Silva et al., 2021;Kaneko et al., 2022). In spite of the overall success of MLMs across NLP tasks, such biases within MLMs raise ethical considerations and underscore the need for debiasing methods to ensure fair and unbiased MLMs. \n\nOn the other hand, MLMs are trained by considering and optimising various underlying factors that contribute to their performance on downstream tasks. These factors include but are not limited to parameter size, tokenization methods, training objectives and training corpora. The performance of MLMs is affected by the interplay of such factors. \n\nNevertheless, it remains unclear as to how these factors influence social biases in MLMs and their downstream task performance. \n\nEvaluating the impact of these factors is challenging due to three main reasons: (a) The factors that we consider within a model are not independent, rather, they exhibit complicated interdependence and affect the performance of models simultaneously. (b) MLMs are diverse with different architectures, configurations and parameters. The diversity across models requires the need for generalisation and abstraction when considering the values of factors. (c) Many recent works proposed debiasing methods to mitigate social biases in MLMs (Webster et al., 2020;Lauscher et al., 2021;Schick et al., 2021;Guo et al., 2022). However, most debiasing methods tend to worsen the performance of MLMs in downstream tasks (Meade et al., 2022). Therefore, it is crucial to consider the trade-off between social bias and downstream task performance when comparing MLMs. \n\nTo address the non-independent issue of factors, we propose a method using Gradient Boosting (Freund and Schapire, 1997) to consider dependencies among factors.",
            "score": 0.5781711514563285,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 949
                },
                {
                    "start": 952,
                    "end": 1079
                },
                {
                    "start": 1082,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1939
                },
                {
                    "start": 1942,
                    "end": 2102
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 105,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 336,
                    "end": 353,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 353,
                    "end": 370,
                    "matchedPaperCorpusId": "201670701"
                },
                {
                    "start": 370,
                    "end": 389,
                    "matchedPaperCorpusId": "235097394"
                },
                {
                    "start": 1642,
                    "end": 1664,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 1664,
                    "end": 1684,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 1684,
                    "end": 1701,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1794,
                    "end": 1814,
                    "matchedPaperCorpusId": "239015827"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "271404509",
            "title": "Cultural Value Differences of LLMs: Prompt, Language, and Model Size",
            "text": "Assessing social and cultural biases in language models is crucial to mitigate associated risks and reveal the values embodied by the models.Liang et al. (2021) provided a formal comprehension of social biases in language models.The work identified fine-grained local biases and high-level global biases as sources of representational biases and proposed the evaluation metrics for measurement.Subsequently, it introduced the mitigation method.Sheng et al. (2021) presented the first comprehensive survey on societal biases in language generation in 2021, identifying their negative impact and exploring methods for evaluation and mitigation.The study highlighted the challenge of bias assessment due to the open-domain nature of NLG and the diverse conceptualizations of bias across cultures.Recently, more studies have focused on evaluating bias and values in large language models, with innovative methodologies employed.Cheng et al. (2023) utilized the concept of markedness, initially linguistic but now a part of social science, to evaluate models' stereotypes unsupervisedly.Meanwhile, Kotek et al. (2023) employed a direct method to assess gender bias in LLMs, revealing models' tendency to reflect imbalances over gender due to training on skewed datasets.In Ferrara (2023), bias in generative language models was defined and its sources, such as training data and model specifications, were investigated.However, the study also acknowledged that some biases may persist inevitably due to the inherent nature of language and cultural norms.\n\nPrevious studies have demonstrated diverse techniques for accurately and efficiently identifying biases.However, they have also underscored the challenges in mitigating biases in generated text, as biases can be inherited from human language and culture in training data.This indicates that the exhibited values of models are shaped by the training data, making it impossible to dissociate the influence of training data when trying to understand the patterns of values expressed by models.",
            "score": 0.5777916124358333,
            "section_title": "Bias Study of Language Model",
            "char_start_offset": 4457,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 141,
                    "end": 229
                },
                {
                    "start": 229,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 444
                },
                {
                    "start": 444,
                    "end": 642
                },
                {
                    "start": 642,
                    "end": 793
                },
                {
                    "start": 793,
                    "end": 924
                },
                {
                    "start": 924,
                    "end": 1082
                },
                {
                    "start": 1082,
                    "end": 1265
                },
                {
                    "start": 1265,
                    "end": 1414
                },
                {
                    "start": 1414,
                    "end": 1549
                },
                {
                    "start": 1551,
                    "end": 1655
                },
                {
                    "start": 1655,
                    "end": 1822
                },
                {
                    "start": 1822,
                    "end": 2041
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 160,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 1093,
                    "end": 1112,
                    "matchedPaperCorpusId": "261276445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94140625
        },
        {
            "corpus_id": "236477795",
            "title": "He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation",
            "text": "(1) This is the first known work to (a) address bias mitigation during the training of pre-trained contextual language models (BERT), and (b) handle implicit biases that may not be captured by explicit measures, using loss functions and further pretraining of BERT. (2) The representations from DE-BIASBERT demonstrate lower biases compared to those obtained by a recent post-processing method (Liang et al., 2020), using SEAT (May et al., 2019). Using human evaluations, we show that the sentence completions obtained using DEBIASBERT demonstrate lower biases compared to those using BERT. \n\n(3) We propose bias mitigation objective in the language decoding stage in text generation tasks, specifically in summarization, and show that the summaries thus obtained contain significantly lower biases in comparison to those obtained using a regular encoder-decoder model. (4) Finally, we identify limitations and future directions of our work, which we believe will pave the way for more effective identification and mitigation of social biases in language modelling and generation.",
            "score": 0.5777682554516503,
            "section_title": "Introduction",
            "char_start_offset": 3685,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 590
                },
                {
                    "start": 593,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1080
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 414,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 427,
                    "end": 445,
                    "matchedPaperCorpusId": "85518027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8544921875
        },
        {
            "corpus_id": "261394436",
            "title": "The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages",
            "text": "Despite their widespread adoption, Natural Language Processing (NLP) systems are typically trained on data with social and demographic biases. Such biases inevitably propagate to our models and their generated outputs, e.g., by over-representing some demographic groups and under-representing others. It is, therefore, critical to measure, report, and design methods to mitigate these biases, before",
            "score": 0.5766484737526748,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 399
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "247450641",
            "title": "VAST: The Valence-Assessing Semantics Test for Contextualizing Language Models",
            "text": "Bias in LMs LM designers and users must consider worstcase scenarios which might occur as a result using LMs. One of these scenarios, highlighted by Bender et al. (2021) in their work on the limitations of large LMs, involves behavior reflecting human-like social biases that disproportionately affect marginalized groups. Several techniques have been designed for measuring bias in LMs. For example, Guo and Caliskan (2021) treat contextualization in CWEs as a random effect, and derive a combined bias effect size from a meta-analysis of 10,000 WEAT tests. May et al. (2019) insert WEAT target and attribute words into semantically \"bleached\" templates, such as \"This is TERM,\" to convey little meaning beyond that of the terms inserted to measure bias in sentence vectors from LMs. Sheng et al. (2019) measure \"regard\" for social groups in LM text output. Nadeem, Bethke, and Reddy (2020) find that LMs with more trainable parameters exhibit better language modeling performance, but prefer biased stereotypes more than smaller LMs. Wolfe and Caliskan (2021) find that under-representation of marginalized groups in the training corpora of four LMs results in CWEs which are more self-similar, but undergo more change in in the model, indicating that LMs generalize poorly to less frequently observed groups, and overfit to often stereotypical pretraining contexts.",
            "score": 0.576109634278413,
            "section_title": "Representational Evolution",
            "char_start_offset": 12541,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 169,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 401,
                    "end": 424,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 559,
                    "end": 576,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1036,
                    "end": 1061,
                    "matchedPaperCorpusId": "238259136"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.951171875
        },
        {
            "corpus_id": "275757053",
            "title": "Evaluating and Mitigating Social Bias for Large Language Models in Open-ended Settings",
            "text": "Current social bias benchmarks for Large Language Models (LLMs) primarily rely on pre-defined question formats like multiple-choice, limiting their ability to reflect the complexity and open-ended nature of real-world interactions. To address this gap, we extend an existing BBQ dataset introduced by incorporating fill-in-the-blank and short-answer question types, designed to evaluate biases in an open-ended setting. Our finding reveals that LLMs tend to produce responses that are more biased against certain protected attributes, like age and socio-economic status. On the other hand, these biased outputs produced by LLMs can serve as valuable contexts and chains of thought for debiasing. Our debiasing approach combined zero-shot, few-shot, and chain-of-thought could significantly reduce the level of bias to almost 0. We open-source our evaluation and debiasing code hoping to encourage further measurements and mitigation of bias and stereotype in LLMs.",
            "score": 0.5754195585051389,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97705078125
        },
        {
            "corpus_id": "261101183",
            "title": "CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias",
            "text": "Language models (LMs) have been found to exhibit unintended biases (Hada et al., 2023;Levy et al., 2023;Gupta et al., 2022) leading to uneven performance across different sociodemographic groups (Bender et al., 2021;Schwartz et al., 2021;Blodgett et al., 2020). Recently, efforts like red teaming have emerged to mitigate such biases (Perez et al., 2022;Ganguli et al., 2022;Zhuo et al., 2023). To evaluate red teaming, or other bias mitigation methods, it is necessary to quantify LM bias in a consistent and rigorous manner. Due to increasing application in real-world of LMs, it is important to have reliable and robust measures to quantify bias. Prior work on bias measurement are unreliable (Selvam et al., 2023), as they are sensitive to minor perturbations in the templates designed to compare performance across social groups (cf. Fig. 1), due to factors such as lack of template diversity, or limited number of templates. Surprisingly, these dataset often rely on manually designed templates (Seshadri et al., 2022;Alnegheimish et al., 2022). \n\nTo overcome these limitations, we introduce the Comprehensive Assessment of Language Models (CALM) for robust measurement of social bias in LMs. In accordance with the group fairness framework proposed by Czarnowska et al. (2021), within this paper, we define bias as the disparate treatment of one group or an individual compared to another, given similar circumstances. CALM aims to rigorously examine bias in LMs' predictions, aiding in understanding potential real-world impacts and biases in downstream applications. This perspective aligns with other template-based approaches Parrish et al. (2022); Nagireddy et al. (2024). We believe that models having lower CALM bias scores are likely to exhibit reduced biases in practical scenarios. Construction of CALM was inspired by multi-faceted benchmark datasets such as GLUE (Wang et al., 2018b) and SuperGLUE (Wang et al., 2019).",
            "score": 0.5749631121506815,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1051
                },
                {
                    "start": 1054,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1937
                }
            ],
            "ref_mentions": [
                {
                    "start": 67,
                    "end": 86,
                    "matchedPaperCorpusId": "264490615"
                },
                {
                    "start": 86,
                    "end": 104,
                    "matchedPaperCorpusId": "258823009"
                },
                {
                    "start": 104,
                    "end": 123,
                    "matchedPaperCorpusId": "247958394"
                },
                {
                    "start": 195,
                    "end": 216,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 216,
                    "end": 238,
                    "matchedPaperCorpusId": "237923745"
                },
                {
                    "start": 238,
                    "end": 260,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 334,
                    "end": 354,
                    "matchedPaperCorpusId": "203078302"
                },
                {
                    "start": 696,
                    "end": 717,
                    "matchedPaperCorpusId": "252968208"
                },
                {
                    "start": 1001,
                    "end": 1024,
                    "matchedPaperCorpusId": "252780987"
                },
                {
                    "start": 1024,
                    "end": 1050,
                    "matchedPaperCorpusId": "250390802"
                },
                {
                    "start": 1259,
                    "end": 1283,
                    "matchedPaperCorpusId": "235658325"
                },
                {
                    "start": 1637,
                    "end": 1658,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 1660,
                    "end": 1683,
                    "matchedPaperCorpusId": "266174740"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89111328125
        },
        {
            "corpus_id": "271874752",
            "title": "Covert Bias: The Severity of Social Views\u2019 Unalignment in Language Models Towards Implicit and Explicit Opinion",
            "text": "Bias amplification is a well-known phenomenon in which a model aggravates the stereotypes presented in its training data (Li et al., 2023). A huge body of work has examined fairness issues in LLMs through different means by providing debiasing methods or evaluation metrics. For instance, work by (Garimella et al., 2022) introduced bias mitigation methods by fine-tuning pre-trained BERT models on text authored by demographic groups and used the sentence encoder association test to measure gender and racial bias by measuring the association sets of target concepts and attributes. Another line of work focuses on bias identification, which can be achieved through defining certain extrinsic evaluation metrics. Some recent work has investigated implicit bias (Gupta et al., 2024)by assigning a persona to \"user\" instructions to provide information about the social group target as an identity assignment. Further work by (Bai et al., 2024) proposed a measure of implicit bias in LLMs as a prompt-based method called the implicit association test. This metric compares the association between two sets of target groups along with two sets of attributes. Stress testing has been employed in various evaluation scenarios, such as in natural language inference (Naik et al., 2018;Das et al., 2024), to push models beyond their normal functioning limits and identify weaknesses. However, in this study, we focus on evaluating bias in implicit opinions by using the concept of edge case stress testing. This allows us to gain new insights into how bias is amplified in the social aspects of opinions through two well-structured downstream perspectives.",
            "score": 0.5745672701233899,
            "section_title": "Related Work",
            "char_start_offset": 2572,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1650
                }
            ],
            "ref_mentions": [
                {
                    "start": 1261,
                    "end": 1280,
                    "matchedPaperCorpusId": "46932607"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.751953125
        },
        {
            "corpus_id": "233476528",
            "title": "Mitigating Political Bias in Language Models Through Reinforced Calibration",
            "text": "generated text tends to present a favorable attitude (e.g., \"I believe it should be legal and not regulated.\"), which is mostly a liberal stance. More interestingly, even a prompt including a conservative trigger (republican) results in generation which leans to the liberal side (\"vote for Hillary...\").\n\nThe ethical implications of bias in NLG have started to receive considerable attention in discussions around the social impact of AI ( Sheng et al. 2020( Sheng et al. , 2019Wallace et al. 2019;Bordia and Bowman 2019). Given the ever-growing number of down-stream models that rely on GPT-2 (and other LMs), it is of utmost importance, and a matter of fairness, for these LMs to generate politically unbiased text.\n\nIn this paper, we define what political bias is in generative LMs and present how to mitigate such bias during generation. Specifically, our contributions are three-fold:\n\n\u2022 We propose two bias metrics (Indirect Bias and Direct Bias) to quantify the political bias in language model generation ( \u00a73). Although in this work we focus on political bias based on three attributes (gender, location and topic), our framework can be easily extended to other types of bias and different attributes.\n\n\u2022 We present a reinforcement learning based framework for mitigating political bias in two modes: word-embedding guided debias and classifier-guided debias ( \u00a74). Since our framework neither accesses the original training data nor retrains the model from scratch, it can be generalized to other large-scale LMs with minimum modification.\n\n\u2022 We systematically evaluate our methods with the proposed metrics, finding that it successfully reduces political bias while maintaining reasonable fluency ( \u00a76.1- \u00a76.3). Furthermore, human evaluation confirms that our methods successfully mitigate the political bias without sacrificing readability and semantic coherence ( \u00a76.4). \n\nAmy is a republican. About voting he/she will + vote for Hillary but doesn't want to be \"Hillary Clinton's Democrat\"! Table 1: Demo examples of Indirect Bias and Direct Bias existing in vanilla GPT-2 generation. For Indirect Bias, we fill in the blank [ATTR] with keywords representing",
            "score": 0.5744249354992625,
            "section_title": "Introduction",
            "char_start_offset": 1956,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 458,
                    "end": 479,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 479,
                    "end": 499,
                    "matchedPaperCorpusId": "201698258"
                },
                {
                    "start": 499,
                    "end": 522,
                    "matchedPaperCorpusId": "102352788"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8505859375
        },
        {
            "corpus_id": "264305744",
            "title": "Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning",
            "text": "Pre-trained Language Models are widely used in many important real-world applications. However, recent studies show that these models can encode social biases from large pre-training corpora and even amplify biases in downstream applications. To address this challenge, we propose Co$^2$PT, an efficient and effective debias-while-prompt tuning method for mitigating biases via counterfactual contrastive prompt tuning on downstream tasks. Our experiments conducted on three extrinsic bias benchmarks demonstrate the effectiveness of Co$^2$PT on bias mitigation during the prompt tuning process and its adaptability to existing upstream debiased language models. These findings indicate the strength of Co$^2$PT and provide promising avenues for further enhancement in bias mitigation on downstream tasks.",
            "score": 0.5743156852487855,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.955078125
        },
        {
            "corpus_id": "265281304",
            "title": "Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads",
            "text": "A growing body of work exploring AI fairness in general, and bias in NLP systems in particular, has highlighted stereotyping embedded in state-of-theart large language models -that is, such models represent some social groups disparately on demographic subsets, including gender, race, and age (Bender et al., 2021;Shah et al., 2020;Guo and Caliskan, 2021;Hutchinson et al., 2020;Kurita et al., 2019;May et al., 2019;Tan and Celis, 2019;Wolfe and Caliskan, 2021;Rozado, 2023) Caliskan et al., 2017), which examines the associations in contextualized word embeddings between concepts captured in the Implicit Association Test (Greenwald et al., 1998). While the SEAT score provides a quantifiable score to evaluate the stereotyping in PLMs, it is unknown how such stereotypical associations manifest in PLMs. \n\nTo mitigate stereotyping and representational harms in PLMs, many different debiasing strategies have been proposed, including data augmentation (Garimella et al., 2021), post-hoc operations (Cheng et al., 2021;Liang et al., 2020), fine-tuning the model (Kaneko and Bollegala, 2021;Lauscher et al., 2021), prompting techniques (Guo et al., 2022), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022). However, recent literature has noted several critical weaknesses of existing bias mitigation approaches, including the effectiveness of bias mitigation (Gonen and Goldberg, 2019;Meade et al., 2022), high training cost (Kaneko and Bollegala, 2021;Lauscher et al., 2021), poor generalizability (Garimella et al., 2021), and the inevitable degradation of language modeling capability (He et al., 2022;Meade et al., 2022). We believe that progress in addressing PLM bias has been inhibited by a lack of deeper understanding of how the bias manifests/behaves internally in the PLM. This paper aims to offer a perspective on this research gap.",
            "score": 0.5741381657214936,
            "section_title": "Stereotyping and Representational Harms in PLMs",
            "char_start_offset": 7260,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 807
                },
                {
                    "start": 810,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 315,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 315,
                    "end": 333,
                    "matchedPaperCorpusId": "209461005"
                },
                {
                    "start": 333,
                    "end": 356,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 356,
                    "end": 380,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 380,
                    "end": 400,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 400,
                    "end": 417,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 417,
                    "end": 437,
                    "matchedPaperCorpusId": "202781363"
                },
                {
                    "start": 437,
                    "end": 462,
                    "matchedPaperCorpusId": "238259136"
                },
                {
                    "start": 462,
                    "end": 475,
                    "matchedPaperCorpusId": "257342280"
                },
                {
                    "start": 476,
                    "end": 498,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 625,
                    "end": 649,
                    "matchedPaperCorpusId": "7840819"
                },
                {
                    "start": 955,
                    "end": 979,
                    "matchedPaperCorpusId": "236477795"
                },
                {
                    "start": 1021,
                    "end": 1040,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1064,
                    "end": 1092,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1092,
                    "end": 1114,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 1137,
                    "end": 1155,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1211,
                    "end": 1232,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1386,
                    "end": 1412,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 1412,
                    "end": 1431,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 1452,
                    "end": 1480,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1480,
                    "end": 1502,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 1526,
                    "end": 1550,
                    "matchedPaperCorpusId": "236477795"
                },
                {
                    "start": 1615,
                    "end": 1632,
                    "matchedPaperCorpusId": "252907344"
                },
                {
                    "start": 1632,
                    "end": 1651,
                    "matchedPaperCorpusId": "239015827"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9150390625
        },
        {
            "corpus_id": "271923841",
            "title": "Identifying and Mitigating Social Bias Knowledge in Language Models",
            "text": "We acknowledge the presence of certain limitations. First, in this paper, we construct our new datasets leveraging GPT-4. Although human validation is performed to ensure the reliability of the data, GPT-4 may suffer from the limitations of its internal knowledge, potentially introducing blind spots into our benchmark. Second, the memory mechanism of language models is still under exploration, while we assume that FFN layers are responsible for storing biased knowledge based on previous observations (Geva et al., 2020;Meng et al., 2022a;Geva et al., 2022). Third, debiasing larger models, as shown in Table 4, is more challenging and will guide our future research, which constitutes our future direction. Besides, social bias in open language generation or dialogue represents another critical scenario for mitigating bias (Wan et al., 2023), which constitutes one of our future research endeavors.",
            "score": 0.5741286184493888,
            "section_title": "Limitation",
            "char_start_offset": 21035,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 905
                }
            ],
            "ref_mentions": [
                {
                    "start": 524,
                    "end": 543,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 830,
                    "end": 848,
                    "matchedPaperCorpusId": "258833296"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "235623756",
            "title": "Towards Understanding and Mitigating Social Biases in Language Models",
            "text": "To test whether we are able to efficiently characterize and mitigate social biases in LMs, we experiment on the GPT-2 LM trained in English (Radford et al., 2019). We first  2 (some of the found tokens are extremely offensive and we have deferred them to Appendix D.1). Visually, many of these words very negatively stereotype certain genders and religions (especially for the female gender and Muslim religion). To perform a more careful empirical analysis, we sampled the top 100 bias-sensitive tokens for each social group and asked 5 independent human annotators to judge whether the found token was indeed stereotyped negatively against that social group. For the Islamic religion, 32% of the top-ranked words were judged as showing severely negative bias (words such as \"terror\" and \"terrorism\"). We show more details and results in Appendix D.1.",
            "score": 0.5733792719122243,
            "section_title": "Experiments",
            "char_start_offset": 23162,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 852
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93212890625
        },
        {
            "corpus_id": "267627928",
            "title": "MAFIA: Multi-Adapter Fused Inclusive Language Models",
            "text": "Pretrained Language Models (PLMs) are growing in power and prominence across numerous NLP tasks (Wang et al., 2023;Ahuja et al., 2023). Their reach has expanded beyond academia, reaching general users through services like code assistance and chatbots (Li et al., 2023;K\u00f6pf et al., 2023). Despite the extraordinary performance of these models on their respective tasks, several works have identified the harmful social biases picked up by these models as an artifact of their pretraining on \u2020 Equal Contribution \u2021 Work done when the author was at Microsoft web-scale corpus consisting of unmoderated usergenerated content (Manzini et al., 2019;Webster et al., 2020;Nadeem et al., 2021, inter alia). \n\nWhile most previous works focus on (binary) gender biases, other societal biases, such as race and religion, are less studied in the context of PLMs. Moreover, these biases are often intertwined with each other, creating complex and nuanced forms of discrimination. We define intersectional biases as the biases that arise from the combination of different attributes, such as gender, race, and religion. In this work, we focus on building debiasing techniques that can model and mitigate gender (including non-binary), race, religion, profession, and intersectional biases, which are often ignored in previous works. \n\nThe community has developed a gamut of methods to measure and mitigate biases in LLMs (Bordia and Bowman, 2019;Liang et al., 2020;Ravfogel et al., 2020;Webster et al., 2020;Lauscher et al., 2021;Smith et al., 2022;Kumar et al., 2023). The majority of these methods finetune all the parameters of a language model to debias it towards a particular bias dimension such as gender or race, and the escalating size of PLMs can pose computational challenges, particularly for smaller academic labs or enterprises. While some methods (Schick et al., 2021;Yang et al., 2023) do not alter a model's internal representations or its parameters.",
            "score": 0.5729793782073194,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1954
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 134,
                    "matchedPaperCorpusId": "257663467"
                },
                {
                    "start": 622,
                    "end": 644,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 1407,
                    "end": 1432,
                    "matchedPaperCorpusId": "102352788"
                },
                {
                    "start": 1432,
                    "end": 1451,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1451,
                    "end": 1473,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 1494,
                    "end": 1516,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 1516,
                    "end": 1535,
                    "matchedPaperCorpusId": "253224433"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8896484375
        },
        {
            "corpus_id": "258686693",
            "title": "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models",
            "text": "We propose a two-step methodology to establish the effect of political biases in pretraining corpora on the fairness of downstream tasks: (1) we develop a framework, grounded in political science literature, to measure the inherent political leanings of pretrained language models, and (2) then investi-gate how the political leanings of LMs affect their performance in downstream social-oriented tasks.",
            "score": 0.5729780675313041,
            "section_title": "Methodology",
            "char_start_offset": 4492,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "261049466",
            "title": "A Survey on Fairness in Large Language Models",
            "text": "Large Language Models (LLMs) have shown powerful performance and development prospects and are widely deployed in the real world. However, LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms. In this paper, we provide a comprehensive review of related research on fairness in LLMs. Considering the influence of parameter magnitude and training paradigm on research strategy, we divide existing fairness research into oriented to medium-sized LLMs under pre-training and fine-tuning paradigms and oriented to large-sized LLMs under prompting paradigms. First, for medium-sized LLMs, we introduce evaluation metrics and debiasing methods from the perspectives of intrinsic bias and extrinsic bias, respectively. Then, for large-sized LLMs, we introduce recent fairness research, including fairness evaluation, reasons for bias, and debiasing methods. Finally, we discuss and provide insight on the challenges and future directions for the development of fairness in LLMs.",
            "score": 0.5729778938868915,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9755859375
        },
        {
            "corpus_id": "265607922",
            "title": "Assessing Political Inclination of Bangla Language Models",
            "text": "The field of Natural Language Processing (NLP) has experienced a transformative paradigm shift driven by the advent of pre-trained large-scale language models (LMs) (Min et al., 2021\u037e Thapa et al., 2023). These models have unleashed novel opportunities in specific areas such as text generation (Zhang et al., 2022), question answering (Yasunaga et al., 2021), sentiment analysis (Xu et al., 2020), machine translation (Baziotis et al., 2020\u037e Qian et al., 2021), document summarization (Pilault et al., 2020), and a myriad of other linguistic tasks. Language models gain these capabilities from training on a vast corpus, enabling them to understand syntactic, language conventions, and nuances with remarkable accuracy (Hu et al., 2023\u037e Thapa andAdhikari, 2023). \n\nHowever, this capability does not come without its complexities. Language models (LM) undergo traditional pre-training on expansive text corpora sourced from diverse domains, including materials such as news articles, discussion forums, books, and digital encyclopaedias. These sources often encompass a range of political inclinations, social biases, stereotypical beliefs, and ideas that tend toward extremes (Feng et al., 2023). Consequently, while learning from training data, LMs inevitably absorb a complex spectrum of perspectives and biases inherently embedded within the training data. \n\nThe implications of these biases are extensive, profound, and have far-reaching implications (Yu et al., 2023). They have the capacity to subtly shape the generated text, often mirroring the inherent biases prevalent in the training data. In today's interconnected world, AI-generated content is integral to human communication, spanning domains such as news article composition and virtual assistant responses. The need to rigorously examine and mitigate these embedded biases extends beyond scientific exploration\u037e it represents a vital ethical responsibility. One specific dimension of bias that requires a thorough examination is political bias (Nozza et al., 2022). Politics is a fundamental aspect of human society, exerting significant influence in various domains (Stier et al., 2020).",
            "score": 0.5727607737377111,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1360
                },
                {
                    "start": 1363,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 336,
                    "end": 359,
                    "matchedPaperCorpusId": "233219869"
                },
                {
                    "start": 380,
                    "end": 397,
                    "matchedPaperCorpusId": "216641917"
                },
                {
                    "start": 486,
                    "end": 508,
                    "matchedPaperCorpusId": "202541012"
                },
                {
                    "start": 747,
                    "end": 762,
                    "matchedPaperCorpusId": "259184423"
                },
                {
                    "start": 1177,
                    "end": 1196,
                    "matchedPaperCorpusId": "258686693"
                },
                {
                    "start": 1456,
                    "end": 1473,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 2012,
                    "end": 2032,
                    "matchedPaperCorpusId": "248780490"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94140625
        },
        {
            "corpus_id": "225068149",
            "title": "Efficiently Mitigating Classification Bias via Transfer Learning",
            "text": "A critical issue with downstream bias mitigation techniques is their inefficiency. For example, hate speech is a recurrent problem Preprint occurring in myriad domains including various social media platforms; downstream bias mitigation requires that new bias factors are identified in each dataset, and subsequently bias mitigation techniques are applied and evaluated for each. But given the common semantic themes of hate speech, offensive language, and toxicity, as well as the common themes of a given bias factor (Warner & Hirschberg, 2012), repeating this process over every dataset and domain is redundant. Moreover, given the challenges and costs of bias mitigation -model training, definition of fair measures, annotating bias factors (e.g., African American English), and collection of user demographic datamodel developers in the areas affected by bias are less likely to perform the process repeatedly. For any model affected by bias, we aim to make bias mitigation more efficient and accessible. \n\nIn this paper, we propose the Upstream Bias Mitigation for Downstream Fine-Tuning (UBM) framework for efficient, cross-domain and task bias mitigation for classification models, which can make bias mitigation more accessible in practice. Our framework takes a transfer learning approach: first, in the upstream bias mitigation stage, a PTLM is fine-tuned with bias mitigation objectives on one or several upstream tasks, and subsequently the classification layer is re-initialized; second, in the downstream fine-tuning stage, the de-biased encoder from the PTLM is again fine-tuned on a downstream task without additional bias mitigation steps. Our hypothesis is that the de-biased encoder will maintain a reduction in overall bias with respect to protected groups, to downstream tasks in new datasets. If so, de-biased downstream models would be achievable by de-biasing and transfering one upstream model. This transfer learning approach to mitigating classification bias offers the potential for more widespread application of bias mitigation techniques in model deployment. \n\nWe conduct a series of experiments with the proposed UBM framework. These experiments address the overall effectiveness of the transfer learning approach and the scope of our proposed framework.",
            "score": 0.5718994190607967,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1714,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1009
                },
                {
                    "start": 1012,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2090
                },
                {
                    "start": 2093,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2287
                }
            ],
            "ref_mentions": [
                {
                    "start": 519,
                    "end": 546,
                    "matchedPaperCorpusId": "12477446"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64794921875
        },
        {
            "corpus_id": "261530629",
            "title": "Bias and Fairness in Large Language Models: A Survey",
            "text": "We classify an extensive range of bias mitigation methods by their intervention stage: pre-processing (modifying model inputs), in-training (modifying the optimization process), intra-processing (modifying inference behavior), and post-processing (modifying model outputs). We construct granular subcategories at each mitigation stage to draw similarities and trends between classes of methods, with mathematical formalization of several techniques with unified notation, and representative examples of each class of method. We draw attention to ways that bias may persist at each mitigation stage. 5. An overview of key open problems and challenges that future work should address. We challenge future research to address power imbalances in LLM development, conceptualize fairness more robustly for NLP, improve bias evaluation principles and standards, expand mitigation efforts, and explore theoretical limits for fairness guarantees.\n\nWe do not attempt to survey the abundance of work on algorithmic fairness more generally, or even bias in all language technologies broadly. In contrast, we focus solely on bias issues in LLMs for English (with additional languages for machine translation and multilingual models), and restrict our search to works that propose novel closed-form metrics, datasets, or mitigation techniques; for our conceptualization of what constitutes an LLM, see Definition 1 in Section 2. In some cases, techniques we survey may have been used in contexts beyond bias and fairness, but we require that each work must at some point specify their applicability towards understanding social bias or fairness.\n\nIn the remainder of the article, we first formalize the problem of bias in LLMs (Section 2), and then provide taxonomies of metrics for bias evaluation (Section 3), datasets for bias evaluation (Section 4), and techniques for bias mitigation (Section 5). Finally, we discuss open problems and challenges for future research (Section 6).",
            "score": 0.5716280932119274,
            "section_title": "A survey and taxonomy of techniques for bias mitigation.",
            "char_start_offset": 5598,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "269303081",
            "title": "Bias patterns in the application of LLMs for clinical decision support: A comprehensive study",
            "text": "\u2022 We present a framework utilizing multiple clinical datasets and conduct a comprehensive evaluation to quantify social biases in large language models (LLMs) designed for clinical applications.\n\n\u2022 We compare a multitude of popular general-purpose and clinical-focused LLMs to empirically evaluate and demonstrate the influence of various design choices on social biases.\n\n\u2022 We identify a list of tasks that are prone to the identified biases and potential at-risk subpopulations and discuss possible mitigation strategies.",
            "score": 0.5713216132885122,
            "section_title": "Introduction",
            "char_start_offset": 4454,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 196,
                    "end": 371
                },
                {
                    "start": 373,
                    "end": 523
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.939453125
        },
        {
            "corpus_id": "272880907",
            "title": "Do the Right Thing, Just Debias! Multi-Category Bias Mitigation Using LLMs",
            "text": "We evaluated the robustness of state-of-the-art large language models to address the growing need for language models for bias mitigation across different social bias classes. We set up a tri-step configuration that leverages supervised fine-tuning, reinforcement learning, and in-context learning to mitigate multi-class social bias in texts. We also presented the ANUBIS dataset, which consists of 1507 perfectly debiased sentence pairs spanning 9 different bias classes, and devised a simple yet strict grammar-based evaluation metric to classify a given sentence pair as biased or debiased. We performed a comprehensive evaluation across quantitative and qualitative metrics to demonstrate the superiority of our tri-step configuration on ANU-BIS over existing datasets and ablations. Regarding environmental impact, we minimized the carbon footprint by optimizing computing resources and energy consumption and leveraging the ANUBIS dataset during model training and deployment.",
            "score": 0.5712118696509665,
            "section_title": "Conclusion",
            "char_start_offset": 23604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 983
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9443359375
        },
        {
            "corpus_id": "259342087",
            "title": "Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases",
            "text": "Pre-trained Language Models (PLMs) have demonstrated outstanding performance in recent years and have been widely used in natural language understanding tasks (Peters et al., 2018;Delobelle et al., 2022). However, the powerful language modeling capability enables PLMs to learn good representations from large-scale training corpora while capturing human-like social biases. Recent studies have demonstrated that the representations encoded by PLMs learn social biases specific to demographic groups (e.g., gender, race, religion) and can be amplified to downstream tasks, leading to unfair outcomes and adverse social effects (Zhao et al., 2019;Webster et al., 2020). As a result, mitigating social biases in PLMs' encoding can improve the fairness of NLP systems significantly (Bolukbasi et al., 2016;Bender and Friedman, 2018).\n\nMost existing debiasing techniques first need to construct sample pairs using Counterfactual Data Augmentation (CDA) (Zmigrod et al., 2019;Wang et al., 2022) to balance the training corpora. The general approach of CDA is to replace the original corpus with attribute words (e.g., he/she, man/woman) specific to different demographic groups. For example, RCDA (Chen et al., 2021) uses a generator to generate a large number of antisense sentences and then uses a discriminator to evaluate the quality of the original and antisense samples jointly. FairFil (Cheng et al., 2021) obtains a pair of positive sample sentences by replacing the attribute words in the training corpora with the antonyms and then uses contrastive learning to train a filter for debiasing. Auto-Debias (Guo et al., 2022) uses pairs of attribute words as training corpora, amplifies the bias between sample pairs by searching biased prompt texts in the Wikipedia vocabulary, and then performs semantic alignment using Jensen-Shannon divergence. These methods aim to mitigate social biases between different demographic groups by narrowing the representation distance between sample pairs. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model is easy to overfit the",
            "score": 0.5710202463264539,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 180,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 180,
                    "end": 203,
                    "matchedPaperCorpusId": "250390561"
                },
                {
                    "start": 627,
                    "end": 646,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 779,
                    "end": 803,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 803,
                    "end": 829,
                    "matchedPaperCorpusId": "52255687"
                },
                {
                    "start": 949,
                    "end": 971,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 971,
                    "end": 989,
                    "matchedPaperCorpusId": "247155039"
                },
                {
                    "start": 1388,
                    "end": 1408,
                    "matchedPaperCorpusId": "232185104"
                },
                {
                    "start": 1608,
                    "end": 1626,
                    "matchedPaperCorpusId": "248780440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95947265625
        },
        {
            "corpus_id": "273901320",
            "title": "Mitigate Extrinsic Social Bias in Pre-trained Language Models via Continuous Prompts Adjustment",
            "text": "Pre-trained language models (PLMs), such as BERT (Kenton and Toutanova, 2019), RoBERTa (Liu et al., 2019) and Albert (Lan, 2019), have been widely adopted in natural language understandings (NLU) due to their outstanding capacities of learning linguistic and factual information. However, recent studies (Meade et al., 2022) have demonstrated that PLMs often encode undesirable social biases and harmful stereotypes, which may lead to an unfair allocation of social resources. Prior methods of intrinsic bias mitigation (Guo et al., 2022;Lu et al., 2020) focus on removing demographic information in representations of PLMs. However, existing work (Zhou et al., 2023) has shown that even removing certain stereotypes in PLMs before they are applied into downstream tasks, unwanted bias will re-enter in the fine-tuned language models. Therefore, we are interested in removing extrinsic social bias, which improves fairness in a task-specific way. \n\nExisting extrinsic debiasing methods rely on manually curated word lists (e.g., \"he\" or \"she\" for gender). For example, Causal-debias (Zhou et al., 2023) replaces sensitive tokens in manually word lists to construct environments and removes spurious correlation via causal invariant learning. CLP (Garg et al., 2019) substitutes sensitive tokens in word list and uses counterfactual logit pairing to satisfy counterfactual token fairness. Gender-tuning (Ghanbarzadeh et al., 2023) generates gender-perturbed examples using manually word lists and integrates Masked Language Modeling (MLM) training objectives into the fine-tuning process. However, word lists are often limited by length and scope. For some protective attributes like race, it is difficult to design representative vocabularies intuitively due to the semantic constaints of words. Prior word lists (Manzini et al., 2019) often use name as proxy of race, which tend to occur less frequently in downstream tasks. Therefore, the diversity of sensitive groups covered by these word lists may be insufficient, resulting in the degradation performance of extrinsic bias mitigation.",
            "score": 0.5708042190990412,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 946
                },
                {
                    "start": 949,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2090
                }
            ],
            "ref_mentions": [
                {
                    "start": 304,
                    "end": 324,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 520,
                    "end": 538,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1246,
                    "end": 1265,
                    "matchedPaperCorpusId": "52880735"
                },
                {
                    "start": 1402,
                    "end": 1429,
                    "matchedPaperCorpusId": "259859044"
                },
                {
                    "start": 1813,
                    "end": 1835,
                    "matchedPaperCorpusId": "102350941"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.912109375
        },
        {
            "corpus_id": "253553353",
            "title": "Mind Your Bias: A Critical Review of Bias Detection Methods for Contextual Language Models",
            "text": "Humans are intrinsically biased, yet we desire our machines to be objective and make fair decisions. However, language models (LMs) that empower much of the web as we know it are well known to contain biases that promote structural discrimination in downstream tasks against minorities and larger social groups alike (Bender et al., 2021). The word representations that are derived from these models (so-called word embeddings) also retain potentially harmful biases contained in the data that are used in the training process (Bolukbasi et al., 2016). To identify and ultimately address these biases, numerous techniques have been proposed for the detection of biases in LMs. However, given the heterogeneity of published bias detection methods, which rely on a multitude of assumptions and use diverging definitions of bias, a thorough comparison is challenging (Blodgett et al., 2020). In practice, inconsistencies are observed even within the results of single methods (May et al., 2019). Consequentially, a comprehensive overview of biases in LMs remains elusive, while findings are inconsistent, inconclusive, and not suitable for determining approaches to debiasing. \n\nIn this work, we aim to address these issues by reproducing and rigorously comparing recent stateof-the-art (SotA) bias detection methods for contextualized word embeddings (CWEs). We focus on four parameters for this comparison, namely the descriptors that are used for targets of bias, the mode of word contextualization for the extraction of CWEs, the encoding levels that are used as output of the LMs, and the rationale behind the evaluation metric. For each parameter choice, we investigate its respective influence on the resulting bias scores in an intra-method comparison. \n\nWhere feasible, we also conduct inter-method comparisons. Based on our findings, we are able to trace some inconsistencies in published results to implementation errors and design choices (and remediate them), and provide recommendations and requirements for the future design of improved bias detection methods. \n\nContributions.",
            "score": 0.5699863899094788,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1173
                },
                {
                    "start": 1176,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1757
                },
                {
                    "start": 1760,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 2072
                },
                {
                    "start": 2075,
                    "end": 2089
                }
            ],
            "ref_mentions": [
                {
                    "start": 317,
                    "end": 338,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 527,
                    "end": 551,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 864,
                    "end": 887,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 973,
                    "end": 991,
                    "matchedPaperCorpusId": "85518027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91796875
        },
        {
            "corpus_id": "270226200",
            "title": "JBBQ: Japanese Bias Benchmark for Analyzing Social Biases in Large Language Models",
            "text": "With the development of Large Language Models (LLMs), social biases in the LLMs have become a crucial issue. While various benchmarks for social biases have been provided across languages, the extent to which Japanese LLMs exhibit social biases has not been fully investigated. In this study, we construct the Japanese Bias Benchmark dataset for Question Answering (JBBQ) based on the English bias benchmark BBQ, and analyze social biases in Japanese LLMs. The results show that while current open Japanese LLMs improve their accuracies on JBBQ by setting larger parameters, their bias scores become larger. In addition, prompts with warnings about social biases and Chain-of-Thought prompting reduce the effect of biases in model outputs, but there is room for improvement in the consistency of reasoning.",
            "score": 0.5698450521512941,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93408203125
        },
        {
            "corpus_id": "276408990",
            "title": "Bias Amplification: Large Language Models as Increasingly Biased Media",
            "text": "Model collapse, a phenomenon where models degrade in performance due to indiscriminate use of synthetic data is well studied. However, its role in bias amplification, the progressive reinforcement of preexisting social biases in Large Language Models (LLMs) remains underexplored. In this paper, we formally define the conditions for bias amplification and demonstrate through statistical simulations that bias can intensify even in the absence of sampling errors, the primary driver of model collapse. Empirically, we investigate political bias amplification in GPT2 using a custom built benchmark for sentence continuation tasks. Our findings reveal a progressively increasing right-leaning bias. Furthermore, we evaluate three mitigation strategies, Overfitting, Preservation, and Accumulation, and show that bias amplification persists even when model collapse is mitigated. Finally, a mechanistic interpretation identifies distinct sets of neurons responsible for model collapse and bias amplification, suggesting they arise from different underlying mechanisms.",
            "score": 0.5697055403858949,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96533203125
        },
        {
            "corpus_id": "235623756",
            "title": "Towards Understanding and Mitigating Social Biases in Language Models",
            "text": "We first formally define these two sources of biases (addressing P1) and ways to separate them from desirable context associations (addressing P2). With this in mind, we propose diverse benchmarks and metrics that test for both sources of bias (addressing P3). \n\nUsing these new formulations, we empirically validate the existence of biases in pretrained LMs. \n\nAs a step towards mitigating bias in LMs, our second contribution is a new method called AUTOREGRESSIVE INLP (A-INLP) that is able to perform post-hoc debiasing of large pretrained LMs. The key to our approach lies in dynamically finding bias-sensitive tokens rather than relying on a predefined set of bias-sensitive words that are common in existing literature (Bolukbasi et al., 2016). While a predefined set may work for studying word embeddings, LMs must handle many possible diverse contexts and generated outputs. We present a way to expand beyond a set of tokens using the geometry of embeddings and a bias classifier that generalizes to new contexts. Using these techniques in A-INLP shows effectiveness in mitigating bias over diverse input contexts and possible generation candidates through a set of experiments studying biases resulting from gender and religion. We also perform in-depth analysis into the various design decisions in measuring, detecting, and mitigating biases which we hope will inspire work towards automatically identifying sensitive tokens for fairer NLP.",
            "score": 0.5695187203257477,
            "section_title": "Bias association",
            "char_start_offset": 4603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 260
                },
                {
                    "start": 263,
                    "end": 359
                },
                {
                    "start": 362,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1451
                }
            ],
            "ref_mentions": [
                {
                    "start": 725,
                    "end": 749,
                    "matchedPaperCorpusId": "1704893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95751953125
        },
        {
            "corpus_id": "236477795",
            "title": "He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation",
            "text": "In this paper, we addressed the problem of bias mitigation in pre-trained contextual language models, and proposed an approach to mitigate explicit and implicit biases in BERT using existing and our proposed loss functions. We showed empirically that our approach achieves better mitigation of the encoded biases in BERT representations compared to that using post-processing them, while requiring training times only in the range of a few hours. We illustrated the effectiveness of language model bias mitigation using human evaluation for sentence completion, noting that our method in general results in less biased completions. Further, we proposed a bias mitigation objective in decoder component in summarization frameworks, while preserving the quality and fluency of the generated text. Finally, we outlined some limitations of some existing works, including this paper, shedding light on some future directions to develop better bias mitigation techniques for language modelling and generation. We believe that our approach generalizes to other demographics (with manual effort only in obtaining the corresponding word tuples), and other pre-trained language models.",
            "score": 0.5687710958147918,
            "section_title": "Conclusions",
            "char_start_offset": 28672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1175
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8671875
        },
        {
            "corpus_id": "270521813",
            "title": "The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Pre-trained Language Models",
            "text": "Figure 1: We employ the proposed IG 2 method to pinpoint neurons within a language model that can be attributed to undesirable behaviors, such as social bias.Neurons harboring social bias are visually marked with red.Best viewed in color on screen.and model retraining.Moreover, it faces the challenge of catastrophic forgetting if fine-tuning is performed.To this end, we explore interpreting and mitigating social biases in PLMs by introducing the concept of SOCIAL BIAS NEURONS.We aim to answer two questions: (1) How to precisely identify the social bias neurons in PLMs? (2) How to effectively mitigate social biases in PLMs?\n\nWe first introduce an interpretable technique, denoted as INTEGRATED GAP GRADIENTS (IG 2 ), to pinpoint social bias neurons within PLMs.IG 2 is inspired by a classic interpretability method, INTEGRATED GRADIENTS (IG) (Sundararajan et al., 2017), that attributes model outputs to model inputs or specific modules.However, despite good interpretability, IG cannot be directly applied to the study of social bias.The primary challenge stems from the fact that the IG method is designed for singular knowledge attribution, whereas social biases arise from the uneven distribution of pairwise knowledge learned by language models for different demographics.Therefore, we propose our IG 2 method to fill in the blank of the interpretable social bias study.Specifically, as illustrated in Figure 1, we back-propagate and integrate the gradients of the logits gap for a selected pair of demographics.Instead of only attributing singular model outputs, our IG 2 is specifically designed for the fairness research scenario and thus attributes the logits gap in model predictions for pairwise demographics.Since the logits gap is the root of the uneven distribution in model outputs for different demographics, back-propagating and integrating its gradients can identify related model units in the trail.Experimental results have verified the accuracy of our IG 2 in detecting social bias neurons.Note that our method exhibits generalizability that extends beyond the scope of social bias research.It can be applied to the study of other imbalanced distributions in model outputs with minimal modifications.",
            "score": 0.5679657320784324,
            "section_title": "Social Bias Neurons",
            "char_start_offset": 1601,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 217
                },
                {
                    "start": 217,
                    "end": 248
                },
                {
                    "start": 248,
                    "end": 269
                },
                {
                    "start": 269,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 481
                },
                {
                    "start": 481,
                    "end": 630
                },
                {
                    "start": 632,
                    "end": 768
                },
                {
                    "start": 768,
                    "end": 944
                },
                {
                    "start": 944,
                    "end": 1042
                },
                {
                    "start": 1042,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1382
                },
                {
                    "start": 1382,
                    "end": 1524
                },
                {
                    "start": 1524,
                    "end": 1727
                },
                {
                    "start": 1727,
                    "end": 1925
                },
                {
                    "start": 1925,
                    "end": 2018
                },
                {
                    "start": 2018,
                    "end": 2119
                },
                {
                    "start": 2119,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 849,
                    "end": 876,
                    "matchedPaperCorpusId": "16747630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8955078125
        },
        {
            "corpus_id": "268718089",
            "title": "All Should Be Equal in the Eyes of LMs: Counterfactually Aware Fair Text Generation",
            "text": "The success of Language Models (LMs) such as GPTs (Radford et al. 2019;Brown et al. 2020), FlanT5 (Chung et al. 2022), and Pythia (Biderman et al. 2023), etc. has yielded widespread public adoption. However, these LMs are known to perpetuate harmful social biases, primarily due to their large-scale unvetted training data sources (Vig et al. 2020;Ferrara 2023), that comprises substantial biases. With their increasing use in crucial applications (healthcare, education, and marketing), there are already several reports of such issues plaguing downstream tasks such as job recommendation engines (Steed et al. 2022;Ferrara 2023) and text summarization (Ladhak et al. 2023). \n\nAs a result, there has been a growing interest in methods to tackle the issue of bias in language modeling. Dataset based approaches proposed by Solaiman and Dennison (2021) and Bender et al. (2021) suggest careful curation of finetuning and training datasets that can improve the fairness of LMs. However, given that modern LMs are trained on trillions of tokens (Touvron et al. 2023), manually curating and auditing the training datasets is infeasible. Other debiasing techniques propose Optimization based alternatives that typically involve either the fine-tuning or complete retraining of the LM, or the utilization of auxiliary classifiers (CDA (Zmigrod et al. 2019a), INLP (Ravfogel et al. 2020a), Dropout (Webster et al. 2021), AutoDebias (Guo, Yang, and Abbasi 2022), GN-Glove (Zhao et al. 2018)). Within the optimization-based approaches, several other techniques (SD (Liang et al. 2020a), INLP (Ravfogel et al. 2020a)) necessitate computationally intensive optimizations for adapting off-the-shelf LM embeddings. For larger LMs, these optimization procedures can demand a substantial amount of time, ranging on the order of days to weeks.",
            "score": 0.5666485184455186,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1827
                }
            ],
            "ref_mentions": [
                {
                    "start": 331,
                    "end": 348,
                    "matchedPaperCorpusId": "227275068"
                },
                {
                    "start": 598,
                    "end": 617,
                    "matchedPaperCorpusId": "248780439"
                },
                {
                    "start": 654,
                    "end": 674,
                    "matchedPaperCorpusId": "258378241"
                },
                {
                    "start": 823,
                    "end": 851,
                    "matchedPaperCorpusId": "235489789"
                },
                {
                    "start": 856,
                    "end": 876,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1329,
                    "end": 1351,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 1358,
                    "end": 1381,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 1425,
                    "end": 1453,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1556,
                    "end": 1576,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1583,
                    "end": 1605,
                    "matchedPaperCorpusId": "215786522"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9267578125
        },
        {
            "corpus_id": "267311383",
            "title": "Evaluating Gender Bias in Large Language Models via Chain-of-Thought Prompting",
            "text": "Various forms of social biases have been documented in NLP applications (Dev et al., 2021a). Existing approaches to mitigate these biases can be broadly classified into categories that address bias in static word embeddings (Bolukbasi et al., 2016;Gonen and Goldberg, 2019;Zhao et al., 2018b;Kaneko andBollegala, 2019, 2021b;Kaneko et al., 2022a), contextualized word embeddings derived from Masked Language Models (MLMs) (Kurita et al., 2019;Zhou et al., 2022;Kaneko et al., 2023a), and texts generated by generative LLMs (Guo et al., 2022b;Ganguli et al., 2023;Turpin et al., 2023). Our paper specifically delves into gender-related biases within the last category, a topic we explore in greater detail in the following sections. Liang et al. (2021) suggested dynamically identifying tokens sensitive to bias by leveraging embeddings' geometry. The process of contextualized debiasing involves applying orthogonal projections to hidden layers, aiming to eliminate gender biases (Kaneko and Bollegala, 2021a). Another approach by Ouyang et al. (2022) addresses biases in LLMs by adjusting parameters to align with both human and LLM preferences. Meanwhile, Joniak and Aizawa (2022) introduced a framework that identifies a subset of model parameters with reduced bias through attention head pruning. However, unlike our method, these approaches require access to internal parameters. Schick et al. (2021) inaugurated the notion of self-diagnosis in LMs, elucidating their propensity for an awareness of their own pernicious bi-",
            "score": 0.566223293411318,
            "section_title": "Related Work",
            "char_start_offset": 17195,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1528
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 91,
                    "matchedPaperCorpusId": "237347097"
                },
                {
                    "start": 224,
                    "end": 248,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 248,
                    "end": 273,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 325,
                    "end": 346,
                    "matchedPaperCorpusId": "248965268"
                },
                {
                    "start": 422,
                    "end": 443,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 443,
                    "end": 461,
                    "matchedPaperCorpusId": "247450590"
                },
                {
                    "start": 461,
                    "end": 482,
                    "matchedPaperCorpusId": "256389777"
                },
                {
                    "start": 523,
                    "end": 542,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 732,
                    "end": 751,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 980,
                    "end": 1009,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1031,
                    "end": 1051,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1385,
                    "end": 1405,
                    "matchedPaperCorpusId": "232075876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92578125
        },
        {
            "corpus_id": "233476528",
            "title": "Mitigating Political Bias in Language Models Through Reinforced Calibration",
            "text": "In this work, we have discussed two metrics for measuring political bias in language model generation and presented a framework to mitigate such bias that requires neither extra data nor retraining. As more potentially-biased LMs are adopted in AI applications, it is a growing concern that the political bias will be amplified if fairness is not taken into considering. Our method is especially meaningful in such contexts, since the training data of LMs are normally not publicly available and training a new large-scale LM from scratch is costly.",
            "score": 0.565381407912793,
            "section_title": "Conclusion",
            "char_start_offset": 26076,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9296875
        },
        {
            "corpus_id": "247619104",
            "title": "Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal",
            "text": "Large LMs embody societal biases that could result in harms such as misinformation, stereotype propagation, and disparate resource allocation (Bender et al., 2021;Sheng et al., 2021). Multiple studies have shown that LMs are biased in producing outputs with negative connotations such as toxicity (Gehman et al., 2020;Zhou et al., 2021;Xu et al., 2021) and negative regard (Sheng et al., 2020(Sheng et al., , 2021 towards minority populations. Others have shown that LMs encode prevalent gender biases, such as one gender being more associated with a particular class of professions. Such biases can be revealed via contextual embedding tests (Guo and Caliskan, 2021), stereotype tests (Sap et al., 2020;Nangia et al., 2020), and evaluation of generated texts (Dhamala et al., 2021;Sheng et al., 2019). Few works have also shown that LM can be biased towards ideologies, e.g., Islam (Brown et al., 2020).\n\nApproaches to mitigate bias in LMs can be broadly summarized as: (a) training or finetuning on a balanced dataset (Solaiman and Dennison, 2021;Dinan et al., 2020)), (b) attaching prefix at inference or training time (Sheng et al., 2020), and (c) using a bias or attribute classifier (e.g., toxicity classifier) to control fairness in text generation (Dathathri et al., 2020;Liang et al., 2021;Liu et al., 2021;Krause et al., 2021). While all these debiasing approaches can be used to mitigate bias in an LM after it is distilled, no prior work aims to directly debias and distill in a single step. Furthermore, the majority of existing approaches focus on reducing toxic text generation (Solaiman and Dennison, 2021;Dathathri et al., 2020;Liang et al., 2021;Liu et al., 2021;Krause et al., 2021). Different from existing works, we present an approach for fair knowledge distillation",
            "score": 0.5648493338514344,
            "section_title": "Related Work",
            "char_start_offset": 3333,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 163,
                    "matchedPaperCorpusId": "262580630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.953125
        },
        {
            "corpus_id": "268692065",
            "title": "RuBia: A Russian Language Bias Detection Dataset",
            "text": "Large language models (LLMs) are trained on primarily unfiltered text corpora which contain many instances of prejudice or bigotry being displayed.Learning to predict the contents of these corpora, the LLMs inherit most of the social biases present in the data.Moreover, they have been shown to use these biases when applied to real-life downstream tasks, reinforcing harmful social tropes and constructs (Zhao et al., 2018;Sheng et al., 2019).For instance, non-debiased LLM solving the task of coreference resolution tend to associate male pronouns with stereotypically masculine jobs (physician, scientist) and female pronouns with stereotypically feminine jobs (Bolukbasi et al., 2016).\n\nIn recent work, diagnostic tools for measuring bias came into focus.Specialized datasets are designed and collected via crowdsourcing with the aim of contrastive evaluation (Zhao et al., 2018;Nadeem et al., 2021;Nangia et al., 2020;Levy et al., 2021).These datasets consist of sets of both more and less biased sentences.This way, LLMs can be rated based on how likely they are to prefer a more biased sentence to a less biased one.While multiple of such datasets exist, almost all of them are in English and can only be used to evaluate English language models, while the LLM's pre-training method is widely applied to many other languages (Chung et al., 2020).\n\n\u22c6 denotes equal contribution \u221e now at Toloka AI In this work, we propose RuBia-a bias detection dataset for the Russian language specifically, inspired by both modern bias detection datasets and the earlier template-based works (Kurita et al., 2019).To achieve this, we employ the practices adopted by other researchers in the area while adapting them to the different sociolinguistic environment.We also take into account recent comparative studies of existing datasets (Blodgett et al., 2020(Blodgett et al., , 2021;;Orgad and Belinkov, 2022) and try to avoid the most common pitfalls, such as lack of precise definitions and inclusion of unclear stereotypes.",
            "score": 0.5637607534234368,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 147,
                    "end": 261
                },
                {
                    "start": 261,
                    "end": 444
                },
                {
                    "start": 444,
                    "end": 689
                },
                {
                    "start": 691,
                    "end": 759
                },
                {
                    "start": 759,
                    "end": 942
                },
                {
                    "start": 942,
                    "end": 1012
                },
                {
                    "start": 1012,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1353
                },
                {
                    "start": 1355,
                    "end": 1605
                },
                {
                    "start": 1605,
                    "end": 1752
                },
                {
                    "start": 1752,
                    "end": 2016
                }
            ],
            "ref_mentions": [
                {
                    "start": 424,
                    "end": 443,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 664,
                    "end": 688,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 923,
                    "end": 941,
                    "matchedPaperCorpusId": "237452751"
                },
                {
                    "start": 1583,
                    "end": 1604,
                    "matchedPaperCorpusId": "190000105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9501953125
        },
        {
            "corpus_id": "271865498",
            "title": "Spoken Stereoset: on Evaluating Social Bias Toward Speaker in Speech Large Language Models",
            "text": "Warning: This paper may contain texts with uncomfortable content.Large Language Models (LLMs) have achieved remarkable performance in various tasks, including those involving multimodal data like speech. However, these models often exhibit biases due to the nature of their training data. Recently, more Speech Large Language Models (SLLMs) have emerged, underscoring the urgent need to address these biases. This study introduces Spoken Stereoset, a dataset specifically designed to evaluate social biases in SLLMs. By examining how different models respond to speech from diverse demographic groups, we aim to identify these biases. Our experiments reveal significant insights into their performance and bias levels. The findings indicate that while most models show minimal bias, some still exhibit slightly stereotypical or anti-stereotypical tendencies.",
            "score": 0.562971479428715,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98681640625
        },
        {
            "corpus_id": "272969377",
            "title": "Model-based Preference Optimization in Abstractive Summarization without Human Feedback",
            "text": "We propose MPO, which leverages the outputs of a language model as a dataset for preference optimization, relying extensively on the outputs from the SFT model. Previous researches (Sheng et al. (2019), Nangia et al. (2020)) has shown that selfsupervised language models, which are trained on unlabeled web-scale datasets, can unintentionally learn and perpetuate social and ethical biases, including racism and sexism. If such biases are inherent within the data, our proposed self-feedback framework may unintentionally reinforce them. We used the TL;DR dataset for training, derived from Reddit posts, which may contain unmoderated and biased expressions. The presence of offensive content in this dataset risks influencing the model's outputs, potentially perpetuating these biases in further training within MPO. Moreover, as MPO progresses and the model increasingly favors extractive summarization, it may struggle to effectively paraphrase and filter out offensive expressions.",
            "score": 0.5628737269391645,
            "section_title": "Ethical Concerns",
            "char_start_offset": 25888,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 985
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 201,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 203,
                    "end": 223,
                    "matchedPaperCorpusId": "222090785"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85009765625
        },
        {
            "corpus_id": "266163771",
            "title": "GPTBIAS: A Comprehensive Framework for Evaluating Bias in Large Language Models",
            "text": "Warning: This paper contains content that may be offensive or upsetting. There has been a significant increase in the usage of large language models (LLMs) in various applications, both in their original form and through fine-tuned adaptations. As a result, LLMs have gained popularity and are being widely adopted by a large user community. However, one of the concerns with LLMs is the potential generation of socially biased content. The existing evaluation methods have many constraints, and their results exhibit a limited degree of interpretability. In this work, we propose a bias evaluation framework named GPTBIAS that leverages the high performance of LLMs (e.g., GPT-4 \\cite{openai2023gpt4}) to assess bias in models. We also introduce prompts called Bias Attack Instructions, which are specifically designed for evaluating model bias. To enhance the credibility and interpretability of bias evaluation, our framework not only provides a bias score but also offers detailed information, including bias types, affected demographics, keywords, reasons behind the biases, and suggestions for improvement. We conduct extensive experiments to demonstrate the effectiveness and usability of our bias evaluation framework.",
            "score": 0.5625464862234961,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.947265625
        },
        {
            "corpus_id": "265067541",
            "title": "All Should Be Equal in the Eyes of Language Models: Counterfactually Aware Fair Text Generation",
            "text": "The success of Language Models (LMs) such as GPTs (Radford et al. 2019;Brown et al. 2020), FlanT5 (Chung et al. 2022), and Pythia (Biderman et al. 2023), etc. has yielded widespread public adoption. However, these LMs are known to perpetuate harmful social biases, primarily due to their large-scale unvetted training data sources (Vig et al. 2020;Ferrara 2023), that comprise substantial biases. With their increasing use in crucial applications (healthcare, education, and marketing), there are already several reports of such issues plaguing downstream tasks such as job recommendation engines (Steed et al. 2022;Ferrara 2023) and text summarization (Ladhak et al. 2023). \n\nAs a result, there has been a growing interest in methods to tackle the issue of bias in language modeling. Dataset based approaches proposed by Solaiman and Dennison (2021) and Bender et al. (2021) suggest careful curation of finetuning and training datasets that can improve the fairness of LMs. However, given that modern LMs are trained on trillions of tokens (Touvron et al. 2023), manually curating and auditing the training datasets is infeasible. Other debiasing techniques propose Optimization based alternatives that typically involve either the fine-tuning or complete retraining of the LM, or the utilization of auxiliary classifiers (CDA (Zmigrod et al. 2019a), INLP (Ravfogel et al. 2020a), Dropout (Webster et al. 2021), AutoDebias (Guo, Yang, and Abbasi 2022), GN-Glove (Zhao et al. 2018)). Within the optimization-based approaches, several other techniques (SD (Liang et al. 2020a), INLP (Ravfogel et al. 2020a)) necessitate computationally intensive optimizations for adapting off-the-shelf LM embeddings. For larger LMs, these optimization procedures can demand a substantial amount of time, ranging on the order of days to weeks.",
            "score": 0.5624501112066187,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1826
                }
            ],
            "ref_mentions": [
                {
                    "start": 331,
                    "end": 348,
                    "matchedPaperCorpusId": "227275068"
                },
                {
                    "start": 597,
                    "end": 616,
                    "matchedPaperCorpusId": "248780439"
                },
                {
                    "start": 653,
                    "end": 673,
                    "matchedPaperCorpusId": "258378241"
                },
                {
                    "start": 822,
                    "end": 850,
                    "matchedPaperCorpusId": "235489789"
                },
                {
                    "start": 855,
                    "end": 875,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1328,
                    "end": 1350,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 1357,
                    "end": 1379,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 1424,
                    "end": 1452,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1555,
                    "end": 1575,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1582,
                    "end": 1604,
                    "matchedPaperCorpusId": "215786522"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93310546875
        },
        {
            "corpus_id": "272826949",
            "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios",
            "text": "Large Language Models (LLMs) have transformed human-computer interaction, exhibiting unprecedented capabilities in natural language processing, communication, and content generation. However, their widespread adoption is impeded by a fundamental challenge: bias. Bias in LLMs is not merely a technical issue but a broader societal concern with significant ethical and practical implications [5]. Enterprises seeking to integrate LLMs into various applications must contend with the risks posed by biased outputs, which can reinforce stereotypes and propagate misinformation. \n\nBias in LLMs manifests in multiple forms, including racial, gender, and cultural stereotypes, often perpetuating systemic inequalities. These biases have tangible consequences; for instance, in 2018, Amazon discontinued an AI-driven recruiting tool after discovering it systematically downgraded resumes containing the term \"women's,\" reflecting an inherent bias in the training data that favored male candidates [1]. More recently, in early 2024, Google suspended Gemini's imagegeneration feature following reports of inaccuracies and potential biases, further highlighting the challenges associated with mitigating bias in generative AI systems. \n\nThe sources of bias in LLMs are multifaceted, stemming from a) inherent biases in the training data, b) biases introduced by model architecture, and c) the influence of human evaluators during the debiasing process. In response to the rising need to address bias holistically, researchers have adopted multiple ways to evaluate and mitigate bias in LLMs (TABLE I), such as curating datasets with comprehensive data for model training and implementing different debiasing approaches. The datasets used to train these models, such as Winogender, Winobias [2], BOLD (Bias in Open-ended Language Generation Dataset) [3], and the BBQ benchmark (Bias Benchmark for QA-Question Answering) [4], have limitations in representing the full spectrum of real-world language and societal biases. Similarly, existing debiasing techniques [5] often depend on external knowledge with potential bias or annotated non-biased samples, which are not always available or practical to obtain.",
            "score": 0.5624128396511064,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2196
                }
            ],
            "ref_mentions": [
                {
                    "start": 391,
                    "end": 394,
                    "matchedPaperCorpusId": "261530629"
                },
                {
                    "start": 990,
                    "end": 993,
                    "matchedPaperCorpusId": "261276445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98095703125
        },
        {
            "corpus_id": "244954639",
            "title": "Ethical and social risks of harm from Language Models",
            "text": "The present report is a contribution toward the wider research programme of responsible innovation on LMs. In particular, we create a unified taxonomy to structure the landscape of potential ethics and social risks associated with language models (LMs). Our goals are to support the broader research programme toward responsible innovation on LMs, to broaden the public discourse on ethical and social risks related to LMs, and to break risks from LMs into smaller, actionable pieces to actively support and encourage their mitigation. As the author list demonstrates, this is a deeply collaborative effort within our own research organisation. More expertise and perspectives will be required to continue to build out this taxonomy of potential risks from LMs. Next steps building on this work will be to engage such perspectives and build out mitigation tools, working toward the responsible innovation of LMs.\n\nC. Ingraham. How rising inequality hurts everyone, even the rich. Washington Post, February 2018. ISSN 0190-8286. URL https://www.washingtonpost.com/news/wonk/wp/2018/02/06/how-rising-inequality-h urts-everyone-even-the-rich/.\n\nPowerful large language models (LLMs) may lead to improved versions of existing language technologies. However, they may also make new types of language technology possible. For example, they may create conversational interfaces with human users where the use of this technology is indistinguishable from interaction with a human counterpart. Such applications are discussed in more detail in section V. Human-Computer Interaction Harms.\n\nDistinguishing \"statistical bias\" from \"social bias\" Concerns regarding \"bias\" in language models generally revolve around distributional skews that result in unfavourable impacts for particular social groups (Sheng et al., 2021). We note that there are different definitions of \"bias\" and \"discrimination\" in classical statistics compared to sociotechnical studies. In classical statistics, \"bias\" designates the difference between a model's prediction and the ground truth (Dietterich and Kong, 1995); in machine learning, minimising statistical bias is a component of reducing error (Dietterich and Kong, 1995). In sociotechnical studies, \"bias\" refers to skews that lead to unjust discrimination based on traits such",
            "score": 0.5615986671117228,
            "section_title": "Conclusion",
            "char_start_offset": 136664,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90185546875
        },
        {
            "corpus_id": "271769656",
            "title": "A Parameter-Efficient Multi-Objective Approach to Mitigate Stereotypical Bias in Language Models",
            "text": "Bias in NLP systems Stereotypical bias can manifest itself in different forms in LMs (Gallegos et al., 2023). Geometric relationships in model representations, for example, can encode stereotypical associations between genders and occupations (Bolukbasi et al., 2016;Caliskan et al., 2017;Zhao et al., 2019;May et al., 2019;Tan and Celis, 2019;Bommasani et al., 2020). Bias is also indicated by various divergence of probabilities from LMs. Kurita et al. (2019) and Brown et al. (2020) observed different probabilities predicted by both masked LMs and generative LMs for male and female genders given stereotypical attributes; Liang et al. (2021) identified local bias as different next token probability distributions conditioned on same contexts with only social group swapped; Barikeri et al. (2021) additionally considered difference in probabilities assigned to whole sentence pairs which are minimally different in social groups, which corresponds to global bias defined in Liang et al. (2021). Bias can also be observed as disparity in model generation (Sheng et al., 2019;Yeo and Chen, 2020) and performance in downstream tasks, such as toxicity detection (Sap et al., 2022) and coreference resolution (Kurita et al., 2019). In this work, we mainly mitigate bias reflected by divergent probability distributions predicted by LMs. \n\nMitigating bias in pre-trained LMs While many studies aimed to train fair LMs from scratch by constructing fairer datasets (Zhao et al., 2019;Zmigrod et al., 2019), it can be computationally expensive and not always feasible in practice. As a result, much effort has been put into mitigating bias from pre-trained LMs via debiasing finetuning. Kaneko and Bollegala (2021) extended projection-based methods from static word embeddings (Bolukbasi et al., 2016) and fine-tuned models to output orthogonal contextualized representations for gendered and stereotypical words. However, Gonen and Goldberg (2019) argued that projection-based methods did not completely capture and remove bias.",
            "score": 0.5612789538412544,
            "section_title": "Related Work",
            "char_start_offset": 4693,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1337
                },
                {
                    "start": 1340,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2026
                }
            ],
            "ref_mentions": [
                {
                    "start": 243,
                    "end": 267,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 267,
                    "end": 289,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 289,
                    "end": 307,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 307,
                    "end": 324,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 344,
                    "end": 367,
                    "matchedPaperCorpusId": "220046499"
                },
                {
                    "start": 441,
                    "end": 461,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 466,
                    "end": 485,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 627,
                    "end": 646,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 780,
                    "end": 802,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 980,
                    "end": 999,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 1060,
                    "end": 1080,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 1080,
                    "end": 1098,
                    "matchedPaperCorpusId": "220444795"
                },
                {
                    "start": 1164,
                    "end": 1182,
                    "matchedPaperCorpusId": "244117167"
                },
                {
                    "start": 1210,
                    "end": 1231,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 1463,
                    "end": 1482,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 1482,
                    "end": 1503,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 1684,
                    "end": 1711,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1774,
                    "end": 1798,
                    "matchedPaperCorpusId": "1704893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91064453125
        },
        {
            "corpus_id": "237491723",
            "title": "Mitigating Language-Dependent Ethnic Bias in BERT",
            "text": "We study ethnicity bias in BERT, arguably the most widely used LM. This is consistent with recent studies of bias in LMs (Liang et al., 2020a;Cheng et al., 2021).\n\nOther than BERT, one model we tried is XLM (Lample and Conneau, 2019) for which the CB scores are (en) 8.95, (de) 12.72, (es) 9.97, (ko) 30.25, (tr) 42.11, and (zh) 12.40. In all languages except Chinese, the CB score is higher (i.e., LM is more biased) than our proposed mitigation methods in Table 2, 5. Note that XLM that covers all six languages is RoBERTa-based , so for a fair comparison, we only report the results of BERT variants.",
            "score": 0.5610463924368763,
            "section_title": "C Another model type: XLM",
            "char_start_offset": 26608,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66015625
        },
        {
            "corpus_id": "271902917",
            "title": "REFINE-LM: Mitigating Language Model Stereotypes via Reinforcement Learning",
            "text": "The success of (Large) Language Models (LMs) has led to a revolution in the domain of NLP, opening the door to numerous challenges. The emergence of LMs-based applications such as chatbots and text-based assistants with astounding capabilities has, on the one hand, sparked unprecedented enthusiasm within the research community [16,36]. However, it has motivated ethical concerns and araised questions about the risks this technology may pose to society, particularly algorithmic fairness and the proliferation of harmful stereotypical bias. Indeed, several studies have shown that LMs suffer from stereotypical biases, which can be detected, for instance, through Implicit Association Tests (IATs) [7]. These biases are still prevalent in recent LLMs such as ChatGPT, GPT4, etc., [26,40]. Figure 1 illustrates stereotypical biases (such as gender, ethnicity or religion) that can be observed when prompting ChatGPT (more examples and analysis with LLama2, Mistral and GPT4 are provided in Section 1 of the supplementary material). These findings ask for a thorough investigation of stereotypical bias in LMs, and for methods to mitigate their impact, perpetuation or even their exacerbation in various academic, societal and industrial applications. \n\nWhile some work has been proposed to mitigate bias in LMs, it remains challenging for several reasons. Firstly, metrics are highly task-dependent, i.e., quantifying stereotypical bias is highly dependent on the application at hand, meaning that the methods used to measure bias in LMs for one kind of bias can not be directly applied to other biases. For example, mitigation metrics for gender bias are typically not directly applicable to nationality-based or ethnic bias, e.g., gender bias mitigation relies on pronoun completion or the existence of sufficiently gendered phrases within corpora [8,31,47]. Secondly, even with adequate methods to measure bias, in practice, there is often a trade-off between bias mitigation and model performance [21], which can have a negative impact despite the bias removal. Namely, removing bias from a LM may risk deteriorating its performance on downstream applications such as questionanswering [49].",
            "score": 0.5607534384264601,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2196
                }
            ],
            "ref_mentions": [
                {
                    "start": 333,
                    "end": 336,
                    "matchedPaperCorpusId": "212747830"
                },
                {
                    "start": 700,
                    "end": 703,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 782,
                    "end": 786,
                    "matchedPaperCorpusId": "261276445"
                },
                {
                    "start": 786,
                    "end": 789,
                    "matchedPaperCorpusId": "268417107"
                },
                {
                    "start": 1851,
                    "end": 1854,
                    "matchedPaperCorpusId": "231698886"
                },
                {
                    "start": 1854,
                    "end": 1857,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1857,
                    "end": 1860,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 2002,
                    "end": 2006,
                    "matchedPaperCorpusId": "248780440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96240234375
        },
        {
            "corpus_id": "271909713",
            "title": "Promoting Equality in Large Language Models: Identifying and Mitigating the Implicit Bias based on Bayesian Theory",
            "text": "Large language models (LLMs) are trained on extensive text corpora, which inevitably include biased information. Although techniques such as Affective Alignment can mitigate some negative impacts of these biases, existing prompt-based attack methods can still extract these biases from the model's weights. Moreover, these biases frequently appear subtly when LLMs are prompted to perform identical tasks across different demographic groups, thereby camouflaging their presence. To address this issue, we have formally defined the implicit bias problem and developed an innovative framework for bias removal based on Bayesian theory, Bayesian-Theory based Bias Removal (BTBR). BTBR employs likelihood ratio screening to pinpoint data entries within publicly accessible biased datasets that represent biases inadvertently incorporated during the LLM training phase. It then automatically constructs relevant knowledge triples and expunges bias information from LLMs using model editing techniques. Through extensive experimentation, we have confirmed the presence of the implicit bias problem in LLMs and demonstrated the effectiveness of our BTBR approach.",
            "score": 0.5605214358850492,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96337890625
        },
        {
            "corpus_id": "268363277",
            "title": "Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization",
            "text": "Bias in NLP systems can typically be categorized as internal bias and external bias (Elsafoury et al., 2023;Li et al., 2023a), depending on whether the bias is related to the training data of downstream tasks. Internal bias often pertains to issues of social fairness (Parraga et al., 2022), such as gender and racial bias, which have been identified in the embeddings of pre-trained language models (Guo et al., 2022). Existing work has attempted to address these issues through methods like adjusting pre-training data, introducing additional objectives, or post-processing. \n\nOn the other hand, external bias related to downstream tasks is often associated with task-specific features, such as entity bias in fake news detection (Zhu et al., 2022), position bias in emotion cause extraction (Yan et al., 2021), and language bias in Visual Question Answering (VQA) (Cadene et al., 2019), and so on. To mitigate these specific biases, two distinct approaches have been developed: data distribution-related and model training-related (Shah et al., 2020;Parraga et al., 2022;Li et al., 2023a). In the data distribution-related approach, efforts are made to re-sample, weight, or generate data to counteract bias (Dixon et al., 2018;Pruksachatkun et al., 2021;Qian et al., 2022). In contrast, model training-related methods explore adversarial techniques, causality (Cadene et al., 2019;Zhu et al., 2022), disentanglement, and additional auxiliary modules to mitigate bias.",
            "score": 0.559063710770977,
            "section_title": "Debiasing Strategies in NLP",
            "char_start_offset": 7131,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1471
                }
            ],
            "ref_mentions": [
                {
                    "start": 400,
                    "end": 418,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 794,
                    "end": 812,
                    "matchedPaperCorpusId": "235359000"
                },
                {
                    "start": 867,
                    "end": 888,
                    "matchedPaperCorpusId": "195584122"
                },
                {
                    "start": 1034,
                    "end": 1053,
                    "matchedPaperCorpusId": "209461005"
                },
                {
                    "start": 1211,
                    "end": 1231,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 1231,
                    "end": 1258,
                    "matchedPaperCorpusId": "235489989"
                },
                {
                    "start": 1258,
                    "end": 1276,
                    "matchedPaperCorpusId": "249062690"
                },
                {
                    "start": 1364,
                    "end": 1385,
                    "matchedPaperCorpusId": "195584122"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84716796875
        },
        {
            "corpus_id": "215238527",
            "title": "\u201cYou Are Grounded!\u201d: Latent Name Artifacts in Pre-trained Language Models",
            "text": "Social Bias. There is multiple evidence that word embeddings encode gender and racial bias (Bolukbasi et al., 2016;Caliskan et al., 2017;Manzini et al., 2019;Gonen and Goldberg, 2019), in particular in the representations of given names (Romanov et al., 2019). Bias can perpetuate to downstream tasks such as coreference resolution (Webster et al., 2018;Rudinger et al., 2018), natural language inference (Rudinger et al., 2017), machine translation (Stanovsky et al., 2019), and sentiment analysis (D\u00edaz et al., 2018). In open-ended natural language generation, prompts with mentions of different demographic groups (e.g., \"The gay person was\") generate stereotypical texts (Sheng et al., 2019). \n\nNamed Entities. Field and Tsvetkov (2019) used pre-trained LMs to analyze power, sentiment, and agency aspects of entities, and found the representations were biased towards the LM training corpus. In particular, frequently discussed entities such as politicians biased the representations of their given names. Prabhakaran et al. (2019) showed that bias reflected in the language describing named entities is encoded into their representations, in particular associating politicians with toxicity. The potential effect on downstream applications is demonstrated with the sensitivity of sentiment and toxicity systems to name perturbation, which can be mitigated by name perturbation during training. \n\nReporting Bias. People rarely state the obvious (Grice et al., 1975), thus uncommon events are reported disproportionally, and their frequency in corpora does not directly reflect real-world frequency (Gordon and Van Durme, 2013;Sorower et al., 2011). A private case of reporting bias is towards named entities: not all Donalds are discussed with equal probability. Web corpora specifically likely suffer from media bias, making some entities more visible than others (coverage bias; D'Alessio and Allen, 2006), sometimes due to \"newsworthiness\" (structural bias; van Dalen, 2012).",
            "score": 0.5585959080513468,
            "section_title": "Related Work",
            "char_start_offset": 8939,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 13,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1983
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 115,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 115,
                    "end": 137,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 137,
                    "end": 158,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 158,
                    "end": 183,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 354,
                    "end": 376,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 405,
                    "end": 428,
                    "matchedPaperCorpusId": "5310359"
                },
                {
                    "start": 450,
                    "end": 474,
                    "matchedPaperCorpusId": "173991101"
                },
                {
                    "start": 499,
                    "end": 518,
                    "matchedPaperCorpusId": "3272048"
                },
                {
                    "start": 675,
                    "end": 695,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 1011,
                    "end": 1036,
                    "matchedPaperCorpusId": "203078302"
                },
                {
                    "start": 1603,
                    "end": 1631,
                    "matchedPaperCorpusId": "16567195"
                },
                {
                    "start": 1631,
                    "end": 1652,
                    "matchedPaperCorpusId": "7012088"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91748046875
        },
        {
            "corpus_id": "271923841",
            "title": "Identifying and Mitigating Social Bias Knowledge in Language Models",
            "text": "Language Models as Social Bias Knowledge Bases. In our experiments, we select the last layer of BERT as the decisive layer as it demonstrates a significantly higher average indirect effect than the other layers, as shown in Figure 3(a). To confirm that bias social knowledge are indeed stored in the localized decisive layer, we perform FAST on every layer of BERT, with results shown in Figure 3(b). It is observable that layer 11 achieves optimal performance in terms of SS, RS, and ICAT, corroborating the effectiveness of knowledge locating. Layers 1-5 show minimal alleviation of biases (no decline in SS), suggesting a minimal correlation between these layers with the storage of biased knowledge. Notably, layers 6-10 not only result in a reduction in SS but also a significant decrease in RS, indicating the entanglement of biased knowledge with other knowledge. This suggests that our framework can identify where social bias knowledge is stored in language models. Additional results and analysis can be referred to Appendix C.2 and D.3. \n\nFairness-Utility Trade-off via Hyperparameters. \n\nWe have performed a grid search for hyperparame- ters \u03b1 and \u03b2, with results presented in Table 5. The optimization proves robust within specific ranges (i.e., 20-80 for \u03b1, 0.05-0.5 for \u03b2). However, a trade-off between the bias mitigation and knowledge retention is observed (Kim et al., 2020;Liu and Vicente, 2022). When either \u03b1 or \u03b2 is set to 0, both the knowledge retention score (RS) and language modeling ability (LMS) suffer significant declines. Conversely, when either \u03b1 or \u03b2 is set too high, the fairness performance (SS) is negatively affected. Based on these findings, we choose \u03b1 at 40 and \u03b2 at 0.1 as they yield the best overall results.",
            "score": 0.5578256472700093,
            "section_title": "Analysis and Discussion",
            "char_start_offset": 18423,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1047
                },
                {
                    "start": 1050,
                    "end": 1097
                },
                {
                    "start": 1100,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1750
                }
            ],
            "ref_mentions": [
                {
                    "start": 1374,
                    "end": 1392,
                    "matchedPaperCorpusId": "220380834"
                },
                {
                    "start": 1392,
                    "end": 1414,
                    "matchedPaperCorpusId": "220962256"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88134765625
        },
        {
            "corpus_id": "270521813",
            "title": "The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Pre-trained Language Models",
            "text": "It can be applied to the study of other imbalanced distributions in model outputs with minimal modifications.After pinpointing social bias neurons in PLMs, we propose a training-free debiasing method, termed BIAS NEURON SUPPRESSION (BNS), to reduce social bias by suppressing the activation of these neurons.Specifically, we first pinpoint the social bias neurons whose attribution scores are above the selected threshold, and then suppress them to mitigate social biases by setting their activation value to 0. Extensive experiments have verified that our debiasing method outperforms baselines with more fairness while preserving language modeling abilities.Furthermore, facilitated by our interpretable technique, we analyze the distribution shift of social bias neurons after debiasing.FairBERTa (Qian et al., 2022) pre-trains RoBERTa on a constructed anstereotypical dataset to reduce social biases.By comparing the results of RoBERTa and FairBERTa, we observe that the change in the number of social bias neurons is minimal.However, there have been noteworthy alterations in the distribution of these social bias neurons.Prior to debiasing, social bias neurons pinpointed in RoBERTa are predominantly concentrated in the deepest few layers.We speculate that due to their proximity to the final output layer, these neurons have a considerable adverse impact on the biased model outputs.After the debiasing process, a substantial number of neurons migrated from the deepest layers to the shallowest layers.This significant reduction in the number of social bias neurons within the deepest layers might be the reason lying behind the effectiveness of the debiasing method used by FairBERTa.We also calculate the intra-and inter-intersection of social bias neurons for different bias scenarios and get useful insights.We hope our interesting insights unveiled from interpreting social biases within PLMs can activate more inspiration for future research about AI fairness.Main contributions of our work are as follows: Experimental results reveal that our debiasing method, BNS, can reduce social biases with low cost and minimal loss in language modeling abilities compared with baselines.\n\n\u2022 By analyzing the distribution shift of social bias neurons after debiasing, some useful insights have been unveiled to bring inspiration to future fairness research.",
            "score": 0.557677083604734,
            "section_title": "Social Bias Neurons",
            "char_start_offset": 3720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 109,
                    "end": 308
                },
                {
                    "start": 308,
                    "end": 660
                },
                {
                    "start": 660,
                    "end": 790
                },
                {
                    "start": 790,
                    "end": 904
                },
                {
                    "start": 904,
                    "end": 1030
                },
                {
                    "start": 1030,
                    "end": 1127
                },
                {
                    "start": 1127,
                    "end": 1246
                },
                {
                    "start": 1246,
                    "end": 1391
                },
                {
                    "start": 1391,
                    "end": 1510
                },
                {
                    "start": 1510,
                    "end": 1693
                },
                {
                    "start": 1693,
                    "end": 1820
                },
                {
                    "start": 1820,
                    "end": 1974
                },
                {
                    "start": 1974,
                    "end": 2192
                },
                {
                    "start": 2194,
                    "end": 2361
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90673828125
        },
        {
            "corpus_id": "259129801",
            "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks",
            "text": "Caliskan et al. [7] demonstrate that word embeddings and language models (LMs) trained on a large amount of human-generated texts encode human-like social biases. Social biases encoded in these models are also reflected in their downstream tasks such as machine translation, sentiment classification, and natural language generation [1,[22][23][24]. As the downstream tasks of language models are rapidly deployed for real-world applications, the presence of social biases in these models reinforces social stereotypes, discrimination, and inequalities. Despite enormous efforts in bias evaluation of LMs, prior work extensively focuses on biases related to gender, race, and ethnicity [5,22,24,50,52]. Social stigmas, also an element of social biases, are stigmatized conditions that often relate to diseases, disabilities, mental illness, socioeconomic status, etc [36]. Considering all stigmatized conditions, social stigmas affect a substantial amount of people. In the United States, approximately 26 percent of adults experience a disability, with up to one in four individuals being affected . In 2021, there were around 57.8M adults that experienced mental illness, which was around 22% of the population in the United States . Social stigmas prevent individuals from social activities and access to education, healthcare, and career opportunities, negatively influencing their psychological well-being and life outcomes [14,[31][32][33]37]. As language models capture other social biases, they may also learn bias against socially stigmatized groups. Such a risk would reinforce social inequalities with the rise of real-world applications of LMs. \n\nThis study examines bias against 93 stigmatized groups in the United States. To the best of our knowledge, this is the first study that examines social stigmas in LMs on a large scale. Pachankis et al. [36] conduct the first psychology study that classifies 93 social stigmas along six stigma dimensions and evaluates their interpersonal outcome, social rejection. We adapt their list of these 93 social stigmas and a widely used psychological questionnaire that measures social rejection, the Social Distance Scale, to quantify bias against stigmatized groups.",
            "score": 0.556964429380301,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1656
                },
                {
                    "start": 1659,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2220
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 19,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 333,
                    "end": 336,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 336,
                    "end": 340,
                    "matchedPaperCorpusId": "250391069"
                },
                {
                    "start": 344,
                    "end": 348,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 686,
                    "end": 689,
                    "matchedPaperCorpusId": "102352788"
                },
                {
                    "start": 689,
                    "end": 692,
                    "matchedPaperCorpusId": "250391069"
                },
                {
                    "start": 692,
                    "end": 695,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 695,
                    "end": 698,
                    "matchedPaperCorpusId": "227275068"
                },
                {
                    "start": 698,
                    "end": 701,
                    "matchedPaperCorpusId": "238259136"
                },
                {
                    "start": 867,
                    "end": 871,
                    "matchedPaperCorpusId": "3750353"
                },
                {
                    "start": 1429,
                    "end": 1433,
                    "matchedPaperCorpusId": "21883193"
                },
                {
                    "start": 1433,
                    "end": 1437,
                    "matchedPaperCorpusId": "145255089"
                },
                {
                    "start": 1437,
                    "end": 1441,
                    "matchedPaperCorpusId": "246654106"
                },
                {
                    "start": 1441,
                    "end": 1445,
                    "matchedPaperCorpusId": "226207034"
                },
                {
                    "start": 1445,
                    "end": 1448,
                    "matchedPaperCorpusId": "33273008"
                },
                {
                    "start": 1861,
                    "end": 1865,
                    "matchedPaperCorpusId": "3750353"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97216796875
        },
        {
            "corpus_id": "259129801",
            "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks",
            "text": "We adapt their list of these 93 social stigmas and a widely used psychological questionnaire that measures social rejection, the Social Distance Scale, to quantify bias against stigmatized groups. To assess the magnitude of bias, we curate a separate list of 29 non-stigmatized conditions derived from the original set of 93 stigmatized conditions, enabling a comparative analysis. \n\nMLMs have been popularly used in downstream Natural Language Processing (NLP) tasks such as natural language inference, natural language generation, and extractive question answering [11,26,39]. This study evaluates six MLMs, with each varying in size and training data: RoBERTa-base [26], RoBERTa-large [26], DistilBERT [42], BERTweet-base [35], BERTweet-large [35], and XLNet-large [53]. Trained with a bidirectional objective, MLMs can predict missing words in sentences based on the surrounding contexts [26]. Recent studies investigate bias in these models and their downstream tasks via prompting. By supplying LMs with specific texts to predict missing words or generate text following a given prefix, researchers examine the generated texts to evaluate the models' performance. These texts used for evaluation are commonly referred to as prompts. We curate prompts based on the Social Distance Scale for the experiments in this study. For example, one of our prompts is \"It is for me to rent a room to someone who has depression. \" \n\nMeanwhile, this study also directs attention to the downstream sentiment classification tasks of MLMs because of their widespread use in real-world applications which include content moderation, market prediction, and resume screening. Sentiment classification is used to classify the underlying attitudes of the author based on the written texts. Yet sentiment classifiers-tools developed based on LMs to classify the underlying sentiment of text-are also found to encode social biases [23]. To investigate if bias against stigmatized conditions are also captured in downstream sentiment classification tasks, this research examines four sentiment classifiers that are trained based on MLMs: BERTweet-base-sentiment-analysis [35,40], DistilBERT base uncased finetuned SST-2 [20], SiEBERT [17], and Twitter-RoBERTa-base [27].",
            "score": 0.5566889283609896,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2039,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1423
                },
                {
                    "start": 1426,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 567,
                    "end": 571,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 574,
                    "end": 577,
                    "matchedPaperCorpusId": "220045430"
                },
                {
                    "start": 725,
                    "end": 729,
                    "matchedPaperCorpusId": "218719869"
                },
                {
                    "start": 746,
                    "end": 750,
                    "matchedPaperCorpusId": "218719869"
                },
                {
                    "start": 768,
                    "end": 772,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84814453125
        },
        {
            "corpus_id": "272881257",
            "title": "A Comprehensive Survey of Bias in LLMs: Current Landscape and Future Directions",
            "text": "Detecting and measuring bias in Large Language Models (LLMs) involves a combination of quantitative and qualitative methods. This section explores various approaches to identifying and evaluating bias. biases inherent in large language models (LLMs). However, LLMs still contain several implicit biases which needs to be addressed (Bai et. al., 2024). Next section focusses on recent evolution and research in the field of bias evaluation and mitigation. (Lin et. al., 2024) investigate the biases present within Large Language Models (LLMs) used for bias detection, shifting the focus from solely identifying media content bias to examining biases within the models themselves. This paper analyze LLM performance in political bias prediction and text continuation tasks across diverse topics. The study also proposes debiasing methods, such as prompt engineering and fine-tuning, to mitigate bias and improve fairness in LLM outputs. Deceiving to Enlighten (Cheng et. al.,. 2024) is exciting concept of detecting and mitigating LLM biases. The paper emphasize the importance of equipping LLMs with mechanisms for bias recognition and self-reflection. The study demonstrates that informing LLMs their outputs don't reflect personal views, coupled with role-playing scenarios where the model self-assesses biases, leads to improved bias identification. Utilizing multi-role debate loops with an impartial referee, the proposed ranking mechanism refines outputs, outperforming existing methods in bias mitigation and contributing to ethical AI advancements. BiasAlert (Fan et. al., 2024), a tool designed to detect social bias in the open-text generations of Large Language Models (LLMs), integrates external human knowledge with LLM reasoning to reliably detect biases. BiasAlert outperforms state-of-the-art methods like GPT4-as-A-Judge, validating its effectiveness in evaluating and mitigating LLM biases across diverse scenarios. Social bias in code generated by Large Language Models (LLMs) is under-explored issue. A novel bias testing framework tailored for code generation tasks show 20.29% to 44.93% of generated code functions exhibit bias in sensitive tasks (Huang et. al., 2023).",
            "score": 0.5556609236132233,
            "section_title": "Bias Detection and Measurement",
            "char_start_offset": 10886,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2190
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9580078125
        },
        {
            "corpus_id": "272464141",
            "title": "AGR: Age Group fairness Reward for Bias Mitigation in LLMs",
            "text": "Large language models (LLMs) used in various fields can perpetuate age biases, affecting career opportunities and healthcare [1]. Unlike fixed gender and racial biases, age bias is continuous and evolving. Figure 1 illustrates that LLMs have the lowest accuracy in detecting age bias compared to other types, highlighting its complexity. Medium-sized LLMs, such as BERT [2] and GPT-1 [3], generally have under a billion parameters and face two types of social biases: internal, present in the model's pre-trained outputs, and external, affecting downstream task predictions. Internal debiasing methods address biases in a pre-trained model's outputs through three main approaches: pre-processing [4], in-training [5], and post-processing [6]. External debiasing methods tackle biases in model predictions during downstream tasks, using data-centered approaches [7] to integrate fairness goals during training. Large-scale LLMs like GPT-3 encounter greater debiasing challenges due to size and complexity, often addressed through preference alignment [8] and prompt engineering techniques [9]. \n\nUnlike gender and racial biases, age bias is challenging due to its dynamic nature, complicating counterfactual and contrastive methods. Research on age bias mitigation remains limited [10]. \n\nAdditionally, common fine-tuning methods for LLMs include instruction-based fine-tuning [11] and reinforcement learning with human feedback [12]. However, no instruction-based datasets address age bias, and these methods do not target social biases, leading to potential performance discrepancies across age groups. \n\nTo address this challenge, we revised and expanded BBQ [13] and ISB [14] datasets and manually annotated them to create age preference and instruction fine-tuning datasets for age bias. We also propose AGR, which introduces an Age Group fairness Reward to reduce performance disparities across age groups during training. \n\nIn summary, our contributions are as follows: \n\n\u2022 We construct age bias preference and instruction fine-tuning datasets for bias evaluation in LLMs.",
            "score": 0.5549776691447168,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1285
                },
                {
                    "start": 1288,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1603
                },
                {
                    "start": 1606,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1927
                },
                {
                    "start": 1930,
                    "end": 1975
                },
                {
                    "start": 1978,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 128,
                    "matchedPaperCorpusId": "59158788"
                },
                {
                    "start": 370,
                    "end": 373,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "259095603"
                },
                {
                    "start": 713,
                    "end": 716,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 738,
                    "end": 741,
                    "matchedPaperCorpusId": "258740820"
                },
                {
                    "start": 861,
                    "end": 864,
                    "matchedPaperCorpusId": "259859044"
                },
                {
                    "start": 1050,
                    "end": 1053,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1376,
                    "end": 1380,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1428,
                    "end": 1432,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1661,
                    "end": 1665,
                    "matchedPaperCorpusId": "239010011"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93212890625
        },
        {
            "corpus_id": "259281110",
            "title": "What is Your Favorite Gender, MLM? Gender Bias Evaluation in Multilingual Masked Language Models",
            "text": "The issue of bias in large language models has garnered significant attention in recent years, prompting several studies that aim to assess and address the presence of bias in MLMs. Nangia et al. (2020) introduced CrowS-Pairs, a dataset consisting of single sentences with masked attribute words, aiming to assess potential social bias in terms of race, gender, and religion in MLMs. Nadeem et al. (2021) adopted a similar approach by masking modified tokens to measure bias. These studies were confined to English, however, and lacked a precise definition of the term \"bias\", resulting in ambiguity and assumptions about its meaning (Blodgett et al., 2021). Ahn and Oh (2021) introduced a novel approach to evaluating bias in MLMs using pairs of sentences with varying degrees of masking. They proposed a new metric, the Categorical Bias score, showing the variance of log-likelihood interpreted as an effect size of the attribute word. In addition, they analyzed ethnic bias across six languages, making an effort to generalize bias evaluation. However, their method still required human-written sentences with bias annotations, which is a limitation in terms of capturing the natural usage of language and can be exploited when the model finds a simple loophole around the set of rules (Durmus et al., 2022). Our work requires minimal annotation and is evaluated on a real dataset rather than a contrived one. \n\nA few studies have attempted to evaluate bias in multilingual MLMs. Kaneko and Bollegala (2022) assessed bias by utilizing a set of English words associated with males and females and computing their likelihoods. Kaneko et al. (2022) used a parallel corpus in English and eight other languages, where bias was annotated solely in English, to evaluate gender bias in the target language models. Our approach is distinguished in that it does not rely on a parallel corpus, making it language-independent.",
            "score": 0.5545046804120728,
            "section_title": "Related Work",
            "char_start_offset": 3699,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1412
                },
                {
                    "start": 1415,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1917
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 202,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 384,
                    "end": 404,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 634,
                    "end": 657,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 659,
                    "end": 676,
                    "matchedPaperCorpusId": "237491723"
                },
                {
                    "start": 1289,
                    "end": 1310,
                    "matchedPaperCorpusId": "248300077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83447265625
        },
        {
            "corpus_id": "272827587",
            "title": "A Multi-LLM Debiasing Framework",
            "text": "Large Language Models (LLMs) are powerful tools with the potential to benefit society immensely, yet, they have demonstrated biases that perpetuate societal inequalities. Despite significant advancements in bias mitigation techniques using data augmentation, zero-shot prompting, and model fine-tuning, biases continuously persist, including subtle biases that may elude human detection. Recent research has shown a growing interest in multi-LLM approaches, which have been demonstrated to be effective in improving the quality of reasoning and factuality in LLMs. Building on this approach, we propose a novel multi-LLM debiasing framework aimed at reducing bias in LLMs. Our work is the first to introduce and evaluate two distinct approaches within this framework for debiasing LLMs: a centralized method, where the conversation is facilitated by a single central LLM, and a decentralized method, where all models communicate directly. Our findings reveal that our multi-LLM framework significantly reduces bias in LLMs, outperforming the baseline method across several social groups.",
            "score": 0.5541808897748929,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95849609375
        },
        {
            "corpus_id": "267406560",
            "title": "Exploring the Effect of Multiple Natural Languages on Code Suggestion Using GitHub Copilot",
            "text": "Several studies have been carried out toward understanding and mitigating social biases in language models [10,13,24]. Regarding gender bias, Treude and Hata [23] examined the extent to which 56 tasks related to software development are affected by implicit gender bias embedded in large language models. Their findings revealed a clear pattern of gender bias. Regarding religion bias, Abid et al. [1] investigated the anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. Regarding political bias, Liu et al. [15] described metrics for measuring political bias and proposed a reinforcement learning framework for mitigating such biases in the generated text. Regarding sentiment bias, Huang et al. [10] quantified sentiment bias by adopting individual and group fairness metrics from the fair machine learning literature. \n\nIn contrast to previous work, we present a novel perspective to examine the Copilot capability within the context of diverse natural languages on code suggestion. This would offer valuable insights into optimizing the usage of Copilot and complement the knowledge of social bias in large language models.",
            "score": 0.5540907860213931,
            "section_title": "RELATED WORK",
            "char_start_offset": 5958,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 951
                },
                {
                    "start": 954,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1258
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 114,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 114,
                    "end": 117,
                    "matchedPaperCorpusId": "258833296"
                },
                {
                    "start": 158,
                    "end": 162,
                    "matchedPaperCorpusId": "257622664"
                },
                {
                    "start": 398,
                    "end": 401,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 639,
                    "end": 643,
                    "matchedPaperCorpusId": "233476528"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9189453125
        },
        {
            "corpus_id": "246016280",
            "title": "Unintended Bias in Language Model-driven Conversational Recommendation",
            "text": "With the prevalence of language-based intelligent assistants such as Amazon Alexa and Google Assistant, conversational recommender systems (CRSs) have attracted growing attention as they can dynamically elicit users' preferences and incrementally adapt recommendations based on user feedback [17,21]. As one of the most crucial foundations of CRSs, Natural Language Processing (NLP) has witnessed several breakthroughs in the past few years, and the use of pretrained transformer-based language models (LMs) for downstream tasks is one of them [36]. Numerous studies have shown that these transformer-based LMs such as BERT [12], RoBERTa [30] and GPT [40] pretrained on large corpora can learn universal language representations and are extraordinarily powerful for many downstream tasks via fine-tuning [39]. Recently, CRSs have started to leverage pretrained LMs for their ability to semantically interpret a wide range of preference statement variations and have demonstrated their potential to build a variety of strong CRSs [19,32,38]. \n\nHowever, pretrained LMs are well-known for exhibiting unintended social biases involving race, gender, or religion [28,31,42]. These biases result from unfair allocation of resources [20,51], stereotyping that propagates negative generalizations about particular social groups [35], as well as differences in system performance for different social groups, text that misrepresents the distribution of different social groups in the population, or language that is denigrating to particular social groups [4,18,28]. Moreover, these biases may also be exacerbated by biases used for domain-specific LM fine-tuning used for downstream tasks [22,35]. \n\nIn this paper, we study a recently introduced LM-driven recommendation backbone (termed LMRec) for CRSs [19] to investigate how unintended bias manifests in significantly shifted price and category distributions of restaurant recommendations. Specifically, we generate templates with placeholders indicating non-preferenceoriented information such as names or relationships that implicitly indicate race, gender, sexual orientation, religion, and study how different substitutions for these placeholders modulate price and category distributions.",
            "score": 0.5538516355920218,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1689
                },
                {
                    "start": 1692,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2238
                }
            ],
            "ref_mentions": [
                {
                    "start": 292,
                    "end": 296,
                    "matchedPaperCorpusId": "231698518"
                },
                {
                    "start": 296,
                    "end": 299,
                    "matchedPaperCorpusId": "214774912"
                },
                {
                    "start": 544,
                    "end": 548,
                    "matchedPaperCorpusId": "51872504"
                },
                {
                    "start": 624,
                    "end": 628,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 804,
                    "end": 808,
                    "matchedPaperCorpusId": "212747830"
                },
                {
                    "start": 1029,
                    "end": 1033,
                    "matchedPaperCorpusId": "235792544"
                },
                {
                    "start": 1036,
                    "end": 1039,
                    "matchedPaperCorpusId": "220870937"
                },
                {
                    "start": 1158,
                    "end": 1162,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 1162,
                    "end": 1165,
                    "matchedPaperCorpusId": "51888520"
                },
                {
                    "start": 1230,
                    "end": 1233,
                    "matchedPaperCorpusId": "214590570"
                },
                {
                    "start": 1550,
                    "end": 1553,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 1553,
                    "end": 1556,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 1796,
                    "end": 1800,
                    "matchedPaperCorpusId": "235792544"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9423828125
        },
        {
            "corpus_id": "271310069",
            "title": "BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization",
            "text": "Large Language Models (LLMs) have become pivotal in advancing natural language processing, yet their potential to perpetuate biases poses significant concerns. This paper introduces a new framework employing Direct Preference Optimization (DPO) to mitigate gender, racial, and religious biases in LLM-generated English text. By developing a loss function that favors less biased over biased completions, our approach cultivates a preference for respectful and non-discriminatory language in LLMs. We also contribute a manually designed dataset for training LLMs to recognize and correct biases. This dataset encompasses a diverse range of prompts paired with both biased and unbiased completions. Implementing this approach on the Microsoft Phi-2 model, we demonstrate substantial reductions in biased outputs as our model outperforms the baseline model on almost all bias benchmarks. Our model also achieves better performance compared to other open-source models on most benchmarks. By reducing biases in the language generated by the model, our study marks a significant step towards developing more ethical and socially responsible LLMs. We publicly release BiasDPO dataset on HuggingFace.",
            "score": 0.5532282674679481,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96923828125
        },
        {
            "corpus_id": "277954809",
            "title": "Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification",
            "text": "Recent years have witnessed a remarkable progress in the development and adoption of large language model (LLM) technology (Achiam et al., 2023;Dubey et al., 2024). Generally, LLMs first undergo large-scale pretraining to acquire general linguistic knowledge, followed by post-training to align them with human goals and preferences (Ouyang et al., 2022). The majority of LLM's knowledge is acquired in the pretraining stage, and post-training is conceived to mainly change the style of LLMs to serve as interactive, open-domain AI assistants (Zhou et al., 2024;Lin et al., 2024). \n\nHowever, pretraining data comprised of webcrawled texts often contain undesirable biases, such as associating Muslim people with terrorism and extremism (Chowdhery et al., 2023). Consequently, LLMs tend to inherit the biases and produce harmful judgements (Sheng et al., 2021;Schramowski et al., 2022), raising significant risks in terms of safety and fairness. Despite the severity of this issue, it remains extremely difficult to audit and alleviate such dataset-level biases due to the everincreasing size of pretraining corpora and the openended, complex nature of social biases. \n\nIn this study, we make a first step towards analyzing and mitigating problematic social biases in massive-scale text data. Specifically, we propose a scalable annotation pipeline consisting of two steps. First, we conduct protected attribute detection to identify diverse demographics, such as nationality, religion, disability, etc. To reduce false positive detections, we combine keyword matching and word sense disambiguation (Huang et al., 2019). Second, we apply regard classification to analyze the language polarity towards each attribute into positive, neutral, or negative (Sheng et al., 2019). Our overall pipeline is illustrated in Figure 1. \n\nIn our experiments, we apply the pipeline to a subset of Common Crawl, the most widely used corpus for LLM pretraining. For bias analysis, we refine the frequency-based word association analysis (Bordia and Bowman, 2019) to take into account regard information.",
            "score": 0.5529306790708857,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1166
                },
                {
                    "start": 1169,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1821
                },
                {
                    "start": 1824,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2085
                }
            ],
            "ref_mentions": [
                {
                    "start": 333,
                    "end": 354,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 543,
                    "end": 562,
                    "matchedPaperCorpusId": "258832491"
                },
                {
                    "start": 562,
                    "end": 579,
                    "matchedPaperCorpusId": "265608902"
                },
                {
                    "start": 736,
                    "end": 760,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9501953125
        },
        {
            "corpus_id": "256868507",
            "title": "BiasTestGPT: Using ChatGPT for Social Bias Testing of Language Models",
            "text": "Our work builds upon methods for technical social bias testing methods in PLMs, on works related to dataset creation as well as on user-centered tools for inspecting PLMs with a particular focus on testing fairness and social bias. Here we provide a background for these areas and highlight the contributions our work introduces in this space. \n\nMethods for Social Bias Testing in Language Models: Social bias can be defined in language generation as a PLMs tendency to systematically produce text with different levels of inclinations towards different groups (e.g., man vs. woman) [104]. More broadly social bias can be linked to stereotypes in language models. Such stereotypes have been defined in prior work as traits that have been broadly linked with demographic groups in ways that uphold social hierarchies. Various methods have been developed to measure social bias and stereotypes in large language models [22,25]. Broadly social bias quantification methods in PLMs can be divided into ones examining associations in latent representations of language learned by such models (i.e., embeddings) [42,72] and methods based on probabilities associated with language generation and sentence probability [22,57,78,79]. Social bias can also be measured in the pretrained part of language models which are not specialized for any particular task (i.e., intrinsic measures) and in specific downstream tasks (e.g., classification, summarization) for which such models are fine-tuned [26]. Intrinsic  1. Total number of generated test sentences for tested biases. Bias specifications are taken from [9,19,42] and used as input for our controllable generation. We also propose 4 novel biases to show the flexibility of our framework defined in detail in Apx. F.6 (indicated with \"*\"). In brackets, we show the number of terms provided in the bias specification. measures are believed to be particularly valuable for capturing social bias present in pre-training datasets and might be less indicative of application-specific bias resulting from model finetuning on additional datasets [38]. \n\nNevertheless, in the end-user use of PLMs without further specialization which becomes increasingly popular via prompting methods [68], quantification of intrinsic social bias is crucial.",
            "score": 0.5528985147593463,
            "section_title": "BACKGROUND AND RELATED WORK",
            "char_start_offset": 7902,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 343
                },
                {
                    "start": 346,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2088
                },
                {
                    "start": 2091,
                    "end": 2278
                }
            ],
            "ref_mentions": [
                {
                    "start": 583,
                    "end": 588,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 1105,
                    "end": 1109,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 1109,
                    "end": 1112,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1213,
                    "end": 1216,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 1216,
                    "end": 1219,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1219,
                    "end": 1222,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1484,
                    "end": 1488,
                    "matchedPaperCorpusId": "250390561"
                },
                {
                    "start": 1599,
                    "end": 1602,
                    "matchedPaperCorpusId": "225094152"
                },
                {
                    "start": 1602,
                    "end": 1605,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1605,
                    "end": 1608,
                    "matchedPaperCorpusId": "219530686"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95263671875
        },
        {
            "corpus_id": "262828449",
            "title": "Survey of Social Bias in Vision-Language Models",
            "text": "In recent years, the rapid advancement of machine learning (ML) models, particularly transformer-based pre-trained models, has revolutionized Natural Language Processing (NLP) and Computer Vision (CV) fields. However, researchers have discovered that these models can inadvertently capture and reinforce social biases present in their training datasets, leading to potential social harms, such as uneven resource allocation and unfair representation of specific social groups. Addressing these biases and ensuring fairness in artificial intelligence (AI) systems has become a critical concern in the ML community. The recent introduction of pre-trained vision-and-language (VL) models in the emerging multimodal field demands attention to the potential social biases present in these models as well. Although VL models are susceptible to social bias, there is a limited understanding compared to the extensive discussions on bias in NLP and CV. This survey aims to provide researchers with a high-level insight into the similarities and differences of social bias studies in pre-trained models across NLP, CV, and VL. By examining these perspectives, the survey aims to offer valuable guidelines on how to approach and mitigate social bias in both unimodal and multimodal settings. The findings and recommendations presented here can benefit the ML community, fostering the development of fairer and non-biased AI models in various applications and research endeavors.",
            "score": 0.552799243294039,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75634765625
        },
        {
            "corpus_id": "259095603",
            "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
            "text": "Recently, there has been a surge of interest in pretrained large language models (LLM) in natural language processing (NLP). It has been shown that the pre-training + finetuning of a model drastically improves its performance on downstream tasks as the knowledge captured by the pre-training on a large corpus is transferred to the downstream application when finetuning the model. However, this also leads to societal biases like gender bias that were implicitly learned by the pre-trained models being transferred to crucial downstream applications like job recommendation engines (Zhao et al., 2019; * Equal Contribution Barocas et al., 2017;Kurita et al., 2019). Analyzing and mitigating bias without requiring significant re-training or compute resources is crucial to the widespread adoption of LLMs in downstream applications.\n\nPrevious work (Nadeem et al., 2021), (Nangia et al., 2020a), (Cer et al., 2018) has attempted to quantify bias, and others such as Ravfogel et al. (2020) and Liang et al. (2021) have attempted to remove it algorithmically from the models. Closer to our work are data-manipulative techniques such as Zmigrod et al. (2019) and Maudslay et al. (2019) that modify the dataset and further fine-tune the model. In this paper, we propose simple data intervention strategies and show that they can mitigate gender bias in pre-trained models with the help of few-shot fine-tuning. Moreover, taking inspiration from Schick et al. (2021), we find that by utilizing a biased pre-trained LLM for mining for most gender-biased samples in a dataset, our methods can mitigate gender bias with very few training samples. Finally, we perform an extensive evaluation of our debiasing technique on two recent bias benchmarks (Nadeem et al., 2021) and show that our method outperforms three existing state-of-the-art techniques and performs comparably to the other two. Our main contributions are the following:\n\n\u2022 We propose simple data intervention techniques that can be used to reduce gender bias in a pre-trained LLM with few training examples (few-shot), thus making human-in-theloop",
            "score": 0.5522206852584892,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 896,
                    "end": 914,
                    "matchedPaperCorpusId": "53245704"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9228515625
        },
        {
            "corpus_id": "270620779",
            "title": "Mitigating Social Biases in Language Models through Unlearning",
            "text": "Mitigating bias in language models (LMs) has become a critical problem due to the widespread deployment of LMs. Numerous approaches revolve around data pre-processing and fine-tuning of language models, tasks that can be both time-consuming and computationally demanding. Consequently, there is a growing interest in machine unlearning techniques given their capacity to induce the forgetting of undesired behaviors of the existing pre-trained or fine-tuned models with lower computational cost. In this work, we explore two unlearning methods, (1) Partitioned Contrastive Gradient Unlearning (PCGU) applied on decoder models and (2) Negation via Task Vector, to reduce social biases in state-of-the-art and open-source LMs such as LLaMA-2 and OPT. We also implement distributed PCGU for large models. It is empirically shown, through quantitative and qualitative analyses, that negation via Task Vector method outperforms PCGU in debiasing with minimum deterioration in performance and perplexity of the models. On LLaMA-27B, negation via Task Vector reduces the bias score by 11.8%",
            "score": 0.5514676794230056,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95458984375
        },
        {
            "corpus_id": "270620779",
            "title": "Mitigating Social Biases in Language Models through Unlearning",
            "text": "In our second approach, we experiment with the idea of task vectors (Ilharco et al., 2022;Zhang et al., 2023b), for mitigating social biases or stereotypes in LMs.Previous studies (Ilharco et al., 2022) apply this method on language models primarily for reducing toxicity, a relatively less challenging task compared to social bias mitigation.A task vector represents a direction in the weight vector space of a pre-trained model such that moving in that direction enhances performance on a given task.The task vector \u03c4 t \u2208 R d , is the element-wise difference between weights of the fine-tuned model on task t, denoted by \u03b8 t f t and the weights of the pre-trained model denoted by \u03b8 pre , \u03c4 t = \u03b8 t f t \u2212 \u03b8 pre .Given the same model architecture, using elementwise addition combined with an optional scaling term \u03bb, task vectors can be applied to any model parameters to produce a new model with weights: \u03b8 new = \u03b8 pre + \u03bb\u03c4 t .On the other hand, rather than adding the task vector directly to a pre-trained model, if the negation of that task vector is added (\u03c4 new = \u2212\u03c4 ), the performance of the model decreases on the target task.This behavior allows us to achieve unlearning as we can negate the task vectors and help the model forget undesirable behaviours.\n\nWe begin by fine-tuning the base pre-trained model on a set of biased sentences to obtain a biased model.Next, we calculate the task vectors by subtracting the base model weights from the newly generated biased model.Consequently, these task vectors are negated and applied to the base model with an appropriate scaling coefficient to get the final debiased model.The biased sentences for initial fine-tuning to obtain a biased model were combined from StereoSet (Nadeem et al., 2020) and Civil Comments (Duchene et al., 2023) datasets.The detailed pre-processing steps are highlighted in section 4.4.Also, the process of negation via task vector is referred to as Task Vector or TV in subsequent sections for simplicity.",
            "score": 0.5507411395905628,
            "section_title": "Negation via Task Vector",
            "char_start_offset": 9080,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 163,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 502
                },
                {
                    "start": 502,
                    "end": 714
                },
                {
                    "start": 714,
                    "end": 929
                },
                {
                    "start": 929,
                    "end": 1134
                },
                {
                    "start": 1134,
                    "end": 1263
                },
                {
                    "start": 1265,
                    "end": 1370
                },
                {
                    "start": 1370,
                    "end": 1482
                },
                {
                    "start": 1482,
                    "end": 1629
                },
                {
                    "start": 1629,
                    "end": 1801
                },
                {
                    "start": 1801,
                    "end": 1866
                },
                {
                    "start": 1866,
                    "end": 1986
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87744140625
        },
        {
            "corpus_id": "248266397",
            "title": "A Survey on Bias and Fairness in Natural Language Processing",
            "text": "As NLP models become more integrated with the everyday lives of people, it becomes important to examine the social effect that the usage of these systems has. While these models understand language and have increased accuracy on difficult downstream tasks, there is evidence that these models amplify gender, racial and cultural stereotypes and lead to a vicious cycle in many settings. In this survey, we analyze the origins of biases, the definitions of fairness, and how different subfields of NLP mitigate bias. We finally discuss how future studies can work towards eradicating pernicious biases from NLP algorithms.",
            "score": 0.5506384989902798,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75244140625
        },
        {
            "corpus_id": "245219207",
            "title": "Analyzing the Limits of Self-Supervision in Handling Bias in Language",
            "text": "There has been a large body of work focused on defining and measuring social bias during natural language generation (Sheng et al., 2019;Nadeem et al., 2021;Dev et al., 2021), neural toxic degeneration of language models using prompts (Gehman et al., 2020), understanding social bias implications (Sap et al., 2019), and various bias mitigation strategies (Liu et al., 2021a;Lauscher et al., 2021;Geva et al., 2022;Wang et al., 2022;Guo et al., 2022). Recently, Liang et al. (2021) proposed new benchmarks and metrics to measure representational biases in text. Ma et al. (2020) proposed PowerTransformer, a language model trained with auxiliary objectives such as paraphrasing and reconstruction, and propose bias-controlled generation for rephrasing. Several datasets have also been released for measuring and rephrasing social bias. Nangia et al. (2020) introduced a dataset with crowdsourced stereotype pairs across different kinds of bias and Borkan et al. (2019) released a large test set of online comments annotated for unintended bias. More recently, Vidgen et al. (2021) released a dataset annotated with bias labels and spans of biased text in language. \n\nBrown et al. ( 2020) introduced GPT-3 and demonstrated that in-context few-shot learning with and without natural language task descriptions could yield close to state-of-the-art fine-tuning results for several NLP tasks. This was followed by several studies exploring language models with task descriptions and in-context examples (Weller et al., 2020;Schick and Sch\u00fctze, 2021a,b). There is also work that discusses limitations of this approach: Efrat and Levy (2020) discovered that models perform poorly with task descriptions on both simple and more complex tasks and Webson and Pavlick (2021) found that models do not understand the meaning of task descriptions for natural language inference and are sensitive to the choice of language model verbalizers.",
            "score": 0.5505449881660179,
            "section_title": "Related Work",
            "char_start_offset": 5058,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1164
                },
                {
                    "start": 1167,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1927
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 137,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 137,
                    "end": 157,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 356,
                    "end": 375,
                    "matchedPaperCorpusId": "235313967"
                },
                {
                    "start": 375,
                    "end": 397,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 433,
                    "end": 450,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 462,
                    "end": 481,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 562,
                    "end": 578,
                    "matchedPaperCorpusId": "225075985"
                },
                {
                    "start": 948,
                    "end": 968,
                    "matchedPaperCorpusId": "75135222"
                },
                {
                    "start": 1060,
                    "end": 1080,
                    "matchedPaperCorpusId": "235097313"
                },
                {
                    "start": 1499,
                    "end": 1520,
                    "matchedPaperCorpusId": "226262281"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7900390625
        },
        {
            "corpus_id": "249062690",
            "title": "Perturbation Augmentation for Fairer NLP",
            "text": "Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets. In this work, we ask whether training on demographically perturbed data leads to fairer language models. We collect a large dataset of human annotated text perturbations and train a neural perturbation model, which we show outperforms heuristic alternatives. We find that (i) language models (LMs) pre-trained on demographically perturbed corpora are typically more fair, and (ii) LMs finetuned on perturbed GLUE datasets exhibit less demographic bias on downstream tasks, and (iii) fairness improvements do not come at the expense of performance on downstream tasks. Lastly, we discuss outstanding questions about how best to evaluate the (un)fairness of large language models. We hope that this exploration of neural demographic perturbation will help drive more improvement towards fairer NLP.",
            "score": 0.550410554935632,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94580078125
        },
        {
            "corpus_id": "259716055",
            "title": "Exploring Social Biases of Large Language Models in a College Artificial Intelligence Course",
            "text": "There is a growing body of work documenting social biases in large language models. We use the term large language model (LLM) to refer to text generation neural network models trained on massive amounts of text. Popular examples include BERT (Devlin et al. 2019), GPT-3 (Brown et al. 2020), RoBERTa (Liu et al. 2019b), andBLOOM (Big-Science 2022). These models exhibit powerful text generation capabilies, but have also been shown to pick up biases from their training data. \n\nSocial bias in LLMs is concerning because it may result in harm to the targeted social groups. Potential harms can be representational, portraying some groups negatively or failing to represent them at all, or allocational, denying certain groups opportunities or resources (Barocas et al. 2017). \n\nMuch existing work focuses on diagnosing representational harms with bias probe tasks: tasks that measure whether a model's predictions differ between two (or more) groups of interest. A number of probe tasks have been proposed: Rudinger, May, and Van Durme (2017); Sheng et al. (2019); Bordia and Bowman (2019); Lee, Madotto, and Fung (2019); Liu et al. (2019a);May et al. (2019); Nadeem, Bethke, and Reddy (2021);Sotnikova et al. (2021) and others. Most of these focus on gender stereotypes. 2 A smaller number explore other aspects of identity, such as religion (Abid, Farooqi, and Zou 2021) and race. 3  We present the final project to students through the lens of Underwood (2021)'s proposal that LLMs act as models of culture: they distill points-of-view encoded in their training data. From this perspective, exploring the social biases of these models is doubly illuminating. It can reveal biases that may percolate to downstream models, causing representational or allocational harms. It is also a way to explore biases in society at large.",
            "score": 0.5502885863930338,
            "section_title": "Social Biases in Large Language Models",
            "char_start_offset": 3897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1826
                }
            ],
            "ref_mentions": [
                {
                    "start": 300,
                    "end": 323,
                    "matchedPaperCorpusId": "198953378"
                },
                {
                    "start": 1006,
                    "end": 1041,
                    "matchedPaperCorpusId": "5310359"
                },
                {
                    "start": 1043,
                    "end": 1062,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 1090,
                    "end": 1119,
                    "matchedPaperCorpusId": "211142738"
                },
                {
                    "start": 1121,
                    "end": 1140,
                    "matchedPaperCorpusId": "204838020"
                },
                {
                    "start": 1140,
                    "end": 1157,
                    "matchedPaperCorpusId": "198953378"
                },
                {
                    "start": 1159,
                    "end": 1192,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1192,
                    "end": 1215,
                    "matchedPaperCorpusId": "234337004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9833984375
        },
        {
            "corpus_id": "269430951",
            "title": "On Bias and Fairness in NLP: Investigating the Impact of Bias and Debiasing in Language Models on the Fairness of Toxicity Detection",
            "text": "Language models (LM) are being used in different natural language processing (NLP) tasks, like in search engines (Zhu et al. 2023), email filtering (AbdulNabi and Yaseen 2021), and content moderation (Elsafoury et al. 2021).Recent research has indicated that LMs are prone to learning and reproducing harmful social biases (Garg et al. 2018;Caliskan, Bryson, and Narayanan 2017;Sweeney and Najafian 2019;Nangia et al. 2020;Nadeem, Bethke, and Reddy 2021).For example, Elsafoury et al. (2022) demonstrate that different static word embeddings learn to associate between marginalized identities (e.g., Women, LGBTQ, and non-white ethnicity) and profane words.Similar results have been found in contextual word embeddings as well (Ousidhoum et al. 2021;Nozza et al. 2022;Elsafoury 2023).However, the impact of social bias in LMs on downstream NLP tasks like toxicity detection is still understudied.For example, even though it has been shown in the literature that different bias metrics return different results (Badilla, Bravo-Marquez, and P\u00e9rez 2020;Elsafoury, Wilson, and Ramzan 2022), most of the studies that investigated the impact of bias in LMs on downstream NLP tasks used only one bias metric (Steed et al. 2022;Goldfarb-Tarrant et al. 2021), which leaves the findings inconclusive.\n\nUnderstanding the impact of social bias on downstream tasks like toxicity detection is crucial, especially with research demonstrating that content written by marginalized identities is sometimes falsely flagged as toxic or hateful (Sap et al. 2019).Furthermore, various methods have been proposed in the literature for removing bias from LMs and for ensuring that NLP tasks do not 2. We provide a comprehensive investigation of different bias removal (debiasing) methods to remove different sources of bias and their impact on the fairness and performance of the task of toxicity detection.3. We provide guidelines to ensure the fairness of the task of toxicity detection.",
            "score": 0.5491482065740809,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 224,
                    "end": 455
                },
                {
                    "start": 455,
                    "end": 657
                },
                {
                    "start": 657,
                    "end": 784
                },
                {
                    "start": 784,
                    "end": 896
                },
                {
                    "start": 896,
                    "end": 1290
                },
                {
                    "start": 1292,
                    "end": 1542
                },
                {
                    "start": 1542,
                    "end": 1883
                },
                {
                    "start": 1883,
                    "end": 1965
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 223,
                    "matchedPaperCorpusId": "235792305"
                },
                {
                    "start": 323,
                    "end": 341,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 341,
                    "end": 378,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 378,
                    "end": 404,
                    "matchedPaperCorpusId": "196185011"
                },
                {
                    "start": 404,
                    "end": 423,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 423,
                    "end": 454,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 727,
                    "end": 750,
                    "matchedPaperCorpusId": "236460108"
                },
                {
                    "start": 750,
                    "end": 768,
                    "matchedPaperCorpusId": "248780452"
                },
                {
                    "start": 1010,
                    "end": 1050,
                    "matchedPaperCorpusId": "220483093"
                },
                {
                    "start": 1201,
                    "end": 1220,
                    "matchedPaperCorpusId": "248780439"
                },
                {
                    "start": 1220,
                    "end": 1249,
                    "matchedPaperCorpusId": "229923772"
                },
                {
                    "start": 1524,
                    "end": 1541,
                    "matchedPaperCorpusId": "196211238"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.845703125
        },
        {
            "corpus_id": "253384242",
            "title": "HERB: Measuring Hierarchical Regional Bias in Pre-trained Language Models",
            "text": "Large-scale pre-trained language models (LMs) are prevalent in the natural language processing (NLP) community since the costly pre-trained models can be adapted to a wide range of downstream applications. However, research studies demonstrate that the societal biases in the pre-training corpora can be learned by LMs and further propagated to the downstream applications (Zhao et al., 2019;Dev et al., 2020;Goldfarb-Tarrant et al., 2021;Kurita et al., 2019). To qualify and mitigate bias for pre-trained LMs, researchers have developed bias evaluation methods targeting certain social groups such as gender, religion, and race (Sun et al., 2019;Manzini et al., 2019;Xia et al., 2020;Delobelle et al., 2021). However, existing methods do not examine the social groups categorised by geographical information, which leaves the region-related biases in pre-trained LMs unexplored. Therefore, our work bridges this gap by addressing research questions about whether regional bias exists in the pre-trained LMs, and if yes, how to quantify the bias in a principled way. \n\nBias in NLP applications makes distinct judgements on people based on their gender, race, religion, region, or other social groups could be harmful, such as automatically downgrading the resumes of female applicants in recruiting (Dastin, 2018) Regional bias represents stereotypes based on the geographic location where people live or come from (Wikipedia, 2022a). To verify the existence of regional bias, we first leverage a sentence-level bias measurement (Kaneko and Bollegala, 2022), with which the likelihood of a biased sentence produced by a pre-trained LM can be acquired with a designed input: where [region] and [description] can be filled with any desired words. The output likelihood represents the contextualised possibility of associating people in the region with the given context, which can be utilised to analyse the bias integrated into LMs. From the perspective of the pre-trained LM, there is a 'world map' of region-wide judgements regards to the [description] of interest.",
            "score": 0.5491269740886082,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1066
                },
                {
                    "start": 1069,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2066
                }
            ],
            "ref_mentions": [
                {
                    "start": 373,
                    "end": 392,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 392,
                    "end": 409,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 409,
                    "end": 439,
                    "matchedPaperCorpusId": "229923772"
                },
                {
                    "start": 439,
                    "end": 459,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 629,
                    "end": 647,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 647,
                    "end": 668,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 668,
                    "end": 685,
                    "matchedPaperCorpusId": "218869965"
                },
                {
                    "start": 685,
                    "end": 708,
                    "matchedPaperCorpusId": "245131370"
                },
                {
                    "start": 1529,
                    "end": 1557,
                    "matchedPaperCorpusId": "233241161"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "248366292",
            "title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias",
            "text": "Even though word representations are powerful enough to capture semantic similarities and exhibit word relationships through word vector similarities, the explicit and implicit existence of several stereotypes and social biases in PLMs harm its usefulness in many real-world applications. Bias in large PLMs arises from different stages of their developmental process. Figure 2 illustrates the workflow of large PLMs along with possible stages where bias may originate, particularly focussing on recent Transformer based PLMs. To mitigate bias it is essential to understand and disentangle the various sources of bias. The investigation on sources of bias leads to observations that human language that form today's data deluge, big enough to train data greedy NLP algorithms, historically accumulate several severe stereotypes and social biases that pervade society i.e., Historical Bias. Language hence is one of the most potent ways through which societal biases are brought about, propagated, and echoed [72]. Several non-neutral stereotypes live in linguistic communication, imaging asymmetries in terms of dominance, power, quality, or status among target terms such as female and male, blacks and whites belonging to various domains like gender, race, etc., [36]. Taking it for granted as normal, people rehearse most of these preconceptions in day-to-day discourse, consequently routinizing these linguistic discrimination and making them be felt less visible [77]. Therefore, even though we perfectly measure and take data samples from these historical data repositories, these are ridden with biases, i.e., Data Bias, a representative of historical bias, which thereby brings about bias in PLMs [107]. \n\nData bias stemming from innate historical biases is the most general source of bias among different sources of bias explored in literature for various tasks [28], where, quality issues in data, uneven distribution (occurrence or co-occurrences) of key terms in data associated with targets concerning a domain [18], etc., are other factors that contribute towards it. Standard datasets used for pre-training non-contextual [16,22,40,68] and contextual models [110] are found to exhibit bias or imbalance in various domains like gender, race, etc.",
            "score": 0.5489537095584955,
            "section_title": "Bias in Pre-trained Language Models",
            "char_start_offset": 12740,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1711
                },
                {
                    "start": 1714,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2260
                }
            ],
            "ref_mentions": [
                {
                    "start": 1468,
                    "end": 1472,
                    "matchedPaperCorpusId": "145267815"
                },
                {
                    "start": 1705,
                    "end": 1710,
                    "matchedPaperCorpusId": "235436386"
                },
                {
                    "start": 1871,
                    "end": 1875,
                    "matchedPaperCorpusId": "3228123"
                },
                {
                    "start": 2024,
                    "end": 2028,
                    "matchedPaperCorpusId": "102352788"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95751953125
        },
        {
            "corpus_id": "250390797",
            "title": "The Birth of Bias: A case study on the evolution of gender bias in an English language model",
            "text": "Large Language Models (LLMs), such as BERT (Tenney et al., 2019) and GPT-3 (Brown et al., 2020), have become crucial building blocks of many AI systems (Bommasani et al., 2021). As these models are used in ever more real world applications, it has become increasingly important to monitor, understand and mitigate the harmful behaviours they may exhibit. In particular, many of those LLMs have been shown to learn undesirable biases towards certain social groups (Bender et al., 2021;Weidinger et al., 2021). These biases pose a serious threat for the usefulness of the technology, as they may unfairly influence the decisions, recommendations or texts that AI systems building on those LLMs generate. If we want to keep exploring the immense potential of the technology, we need to find ways to avoid or at least mitigate unwanted biases in language models. However, detecting, mitigating and even defining undesirable biases have proven to be extremely challenging tasks. One key difficulty is deciding on where in the language modelling pipeline to measure and to intervene: in the data used for training, in the internal representations of the models, or only in the applications that are built on top of the language models (the downstream applications)? Many recent papers have proposed methods that work at one or two of these loci, for example, by focusing on the dataset (Dixon et al., 2018;Hall Maudslay et al., 2019;Lu et al.), the training procedure (Zhang et al., 2018;Zhao et al., 2018b;Liu and Avci, 2019), or on measuring and fixing biases in word embeddings or internal states of language models (Bolukbasi et al., 2016;Ethayarajh et al., 2019;Wang et al., 2020;Basta et al., 2019;May et al., 2019;Kurita et al., 2019;Tan et al., 2021).",
            "score": 0.5488060842904743,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1753
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 64,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 463,
                    "end": 484,
                    "matchedPaperCorpusId": "262580630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "269626396",
            "title": "Red-Teaming for Inducing Societal Bias in Large Language Models",
            "text": "Modern large language models (LLMs) have a significant amount of world knowledge, which enables strong performance in commonsense reasoning and knowledge-intensive tasks when harnessed properly. The language model can also learn social biases, which has a significant potential for societal harm. There have been many mitigation strategies proposed for LLM safety, but it is unclear how effective they are for eliminating social biases. In this work, we propose a new methodology for attacking language models with knowledge graph augmented generation. We refactor natural language stereotypes into a knowledge graph, and use adversarial attacking strategies to induce biased responses from several open- and closed-source language models. We find our method increases bias in all models, even those trained with safety guardrails. This demonstrates the need for further research in AI safety, and further work in this new adversarial space.",
            "score": 0.548569523371853,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96875
        },
        {
            "corpus_id": "270285699",
            "title": "Towards Understanding Task-agnostic Debiasing Through the Lenses of Intrinsic Bias and Forgetfulness",
            "text": "With this paper we would like to explore empirical observations which will lead to more insights for theoretical analysis about how PLMs learn social bias and how we can efficiently mitigate social bias.This goal is challenging and non-trivial, but the following is a brief theoretical analysis approached from the frameworks of statistical learning theory and natural language processing.Assuming that the bias level is linearly dependent on the generalization performance, that there are obvious biases in the fine-tuning task, and that the debiased model has been properly debiased, we can leverage the PAC-Bayes bound for this theoretical analysis.For instance, in Section 3.5 of Liu et al. (2023a), m is the number of fine-tuning task samples.If there are more samples (larger m) in the fine-tuning, the bias level of the fine-tuned model should be relevant to the fine-tuning dataset size.The conclusion above is intuitive.However, the generalization behavior of LLMs is rather different from traditional machine learning models.For example, when using double-descent (Schaeffer et al., 2023), or how the catastrophic forgetting issue seems to be less strong in very large LLMs (Jain et al., 2023), yet generalization is still good.\n\nWe believe extending ProSocialTuning to much larger models will be helpful in terms of understanding task-agnostic debiasing.In this paper, we only focused on text classification tasks, wherein Masked Language Models with fewer parameters are much more popular.Besides our hardware limitations, we also have other reasons for this: (i) people tend to use instructions to leverage models with over several billions of parameters and there is no downstream fine-tuning, so the relearn-ing of bias issue as we study it does not hold; (ii) we observe a serious decrease in language modeling ability with CDA and safety alignment, e.g., Reinforcement Learning from Human Feedback, can preserve the language modeling ability.However, the recently proposed superficial alignment hypothesis might indicate the ineffectiveness of this alignment method.\n\nRegarding the bias lower bound, our claim is an empirical lower bound but not an exactly theoretical lower bound which requires more effort, although we tend to leverage empirical evidence to inspire future studies.",
            "score": 0.5465017754673088,
            "section_title": "Discussions",
            "char_start_offset": 28025,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 203,
                    "end": 389
                },
                {
                    "start": 389,
                    "end": 652
                },
                {
                    "start": 652,
                    "end": 748
                },
                {
                    "start": 748,
                    "end": 895
                },
                {
                    "start": 895,
                    "end": 929
                },
                {
                    "start": 929,
                    "end": 1035
                },
                {
                    "start": 1035,
                    "end": 1238
                },
                {
                    "start": 1240,
                    "end": 1365
                },
                {
                    "start": 1365,
                    "end": 1501
                },
                {
                    "start": 1501,
                    "end": 1959
                },
                {
                    "start": 1959,
                    "end": 2083
                },
                {
                    "start": 2085,
                    "end": 2300
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71630859375
        },
        {
            "corpus_id": "271404523",
            "title": "Towards Transfer Unlearning: Empirical Evidence of Cross-Domain Bias Mitigation",
            "text": "In recent years, the natural language processing (NLP) field has experienced a transformative shift with the introduction of Large Language Models (LLMs).The remarkable capabilities of LLMs are largely attributed to scaling laws (Kaplan et al., 2020), which suggest their capability heavily depends on the model size and training dataset size.However, training on massive corpus often results in LLMs inadvertently acquiring social biases present in their training datasets (Webster et al., 2021;Nangia et al., 2020;Nadeem et al., 2021).Therefore, addressing these biases is crucial for the development of fair and responsible LLMs.\n\nNumerous studies have been conducted to mitigate the bias and toxicity inherent in LLMs (Zhao et al., 2018;Barikeri et al., 2021;Liang et al., 2020;Ravfogel et al., 2020;Schick et al., 2021).However, recent studies (Meade et al., 2022) empiri-cally show that these approaches are effective in reducing bias while compromising the language modeling performance, as indicated by increased perplexity on unbiased text.Notably, existing posthoc techniques manage to retain language modeling performance, but they fail to detect more subtle and implicit toxic content.\n\nTo overcome this challenge, our study explores an unlearning-based approach that makes LLMs forget biased and toxic content.By running gradient ascent on biased text, our method minimizes the likelihood of biased content while minimizing the degradation of language modeling capabilities.Our debiasing method is inspired by the successes of prior work (Chen and Yang, 2023;Jang et al., 2023), which demonstrated the technique's efficacy in unlearning privacy-sensitive data.\n\nThis study explores Mask Language Modeling (MLM) unlearning, which selectively unlearns harmful content within the text by forgetting only toxic or biased tokens.Through empirical investigation on gender-biased text, we discovered that MLM unlearning effectively reduces gender bias without significantly deteriorating language modeling performance.Furthermore, experimental results demonstrated that unlearning gender-biased text also contributes to mitigating other types of bias, such as race and religion.",
            "score": 0.5463948816392497,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 154,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 537
                },
                {
                    "start": 537,
                    "end": 632
                },
                {
                    "start": 634,
                    "end": 825
                },
                {
                    "start": 825,
                    "end": 1049
                },
                {
                    "start": 1049,
                    "end": 1197
                },
                {
                    "start": 1199,
                    "end": 1323
                },
                {
                    "start": 1323,
                    "end": 1487
                },
                {
                    "start": 1487,
                    "end": 1673
                },
                {
                    "start": 1675,
                    "end": 1837
                },
                {
                    "start": 1837,
                    "end": 2024
                },
                {
                    "start": 2024,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 496,
                    "end": 516,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 516,
                    "end": 536,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 722,
                    "end": 741,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 741,
                    "end": 763,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 763,
                    "end": 782,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 782,
                    "end": 804,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 804,
                    "end": 824,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 849,
                    "end": 869,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 1551,
                    "end": 1572,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 1572,
                    "end": 1590,
                    "matchedPaperCorpusId": "252693065"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "248780440",
            "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
            "text": "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models\u2019 understanding abilities, as shown using the GLUE benchmark.",
            "score": 0.5461471143830118,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "236034024",
            "title": "Intersectional Bias in Causal Language Models",
            "text": "Bias has been a long-standing concern in AI and NLP research [26]. Research on language models and data sets has begun to self-report against bias metrics [36,37], and others have developed checklists for testing model fairness and accuracy [46,43]. We discuss here examples of three types of work: (i) techniques for identifying bias; (ii) techniques for mitigating or addressing bias; and (iii) sociological research on the ways algorithmic bias may result in adverse social effects. Our own work contributes to intersectional aspects of (i) and (iii), and we also consider approaches from the literature that may address (ii).",
            "score": 0.5460422032604242,
            "section_title": "Related Work",
            "char_start_offset": 3459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 629
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 65,
                    "matchedPaperCorpusId": "102352788"
                },
                {
                    "start": 245,
                    "end": 248,
                    "matchedPaperCorpusId": "210156214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7607421875
        },
        {
            "corpus_id": "261530629",
            "title": "Bias and Fairness in Large Language Models: A Survey",
            "text": "The rise and rapid advancement of large language models (LLMs) has fundamentally changed language technologies (e.g., Brown et al., 2020;Conneau et al., 2020;Devlin et al., 2019;Lewis et al., 2020;OpenAI, 2023;Radford et al., 2018;Raffel et al., 2020). With the ability to generate human-like text, as well as adapt to a wide array of natural language processing (NLP) tasks, the impressive capabilities of these models have initiated a paradigm shift in the development of language models. Instead of training task-specific models on relatively small task-specific datasets, researchers and practitioners can use LLMs as foundation models that can be fine-tuned for particular functions (Bommasani et al., 2021). Even without fine-tuning, foundation models increasingly enable few-or zero-shot capabilities for a wide array of scenarios like classification, question-answering, logical reasoning, fact retrieval, information extraction, and more, with the task described in a natural language prompt to the model and few or no labeled examples (e.g., Brown et al., 2020;Kojima et al., 2022;Liu et al., 2023a;Radford et al., 2019;Zhao et al., 2021).\n\nLaying behind these successes, however, is the potential to perpetuate harm. Typically trained on an enormous scale of uncurated Internet-based data, LLMs inherit stereotypes, misrepresentations, derogatory and exclusionary language, and other denigrating behaviors that disproportionately affect already-vulnerable and marginalized communities Dodge et al., 2021;Sheng et al., 2021b). These harms are forms of \"social bias,\" a subjective and normative term we broadly use to refer to disparate treatment or outcomes between social groups that arise from historical and structural power asymmetries, which we define and discuss in Section 2. 1 Though LLMs often reflect existing biases, they can amplify these biases too; in either case, the automated reproduction of injustice can reinforce systems of inequity (Benjamin, 2020). From negative sentiment and toxicity directed towards some social groups, to stereotypical",
            "score": 0.5458256754282855,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 137,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 137,
                    "end": 158,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 158,
                    "end": 178,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 178,
                    "end": 197,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 231,
                    "end": 251,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1052,
                    "end": 1071,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1071,
                    "end": 1091,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 1091,
                    "end": 1109,
                    "matchedPaperCorpusId": "236493269"
                },
                {
                    "start": 1109,
                    "end": 1130,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1130,
                    "end": 1148,
                    "matchedPaperCorpusId": "231979430"
                },
                {
                    "start": 1496,
                    "end": 1515,
                    "matchedPaperCorpusId": "237568724"
                },
                {
                    "start": 1515,
                    "end": 1535,
                    "matchedPaperCorpusId": "234337004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9892578125
        },
        {
            "corpus_id": "271923841",
            "title": "Identifying and Mitigating Social Bias Knowledge in Language Models",
            "text": "Generating fair and accurate predictions plays a pivotal role in deploying large language models (LLMs) in the real world. However, existing debiasing methods inevitably generate unfair or incorrect predictions as they are designed and evaluated to achieve parity across different social groups but leave aside individual commonsense facts, resulting in modified knowledge that elicits unreasonable or undesired predictions. In this paper, we first establish a new bias mitigation benchmark, BiaScope, which systematically assesses performance by leveraging newly constructed datasets and metrics on knowledge retention and generalization. Then, we propose a novel debiasing approach, Fairness Stamp (FAST), which enables fine-grained calibration of individual social biases. FAST identifies the decisive layer responsible for storing social biases and then calibrates its outputs by integrating a small modular network, considering both bias mitigation and knowledge-preserving demands. Comprehensive experiments demonstrate that FAST surpasses state-of-the-art baselines with superior debiasing performance while not compromising the overall model capability for knowledge retention and downstream predictions. This highlights the potential of fine-grained debiasing strategies to achieve fairness in LLMs.",
            "score": 0.5456038398447599,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.978515625
        },
        {
            "corpus_id": "248496913",
            "title": "Gender Bias in Masked Language Models for Multiple Languages",
            "text": "Masked Language Models (MLMs) (Devlin et al., 2019), which are pre-trained on large corpora, have been used successfully in natural language processing tasks for various languages (Conneau and Lample, 2019;Martin et al., 2020;Conneau et al., 2020). Unfortunately, it has been reported that MLMs also * Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon. learn social biases regarding attributes such as gender, religion, and race (Kurita et al., 2019;Dev et al., 2020;Kaneko and Bollegala, 2021a;Bender et al., 2021). The bias in MLMs is evaluated by the imbalance of the likelihood between pairs of sentences associated with an attribute that has a common context (e.g. He/She is a nurse). Nadeem et al. (2021) masked the modified tokens (e.g. He, She), and Nangia et al. (2020) masked the unmodified tokens (e.g. is, a, nurse) one word at a time and calculated the likelihood from their predictions to evaluate the bias. Kaneko and Bollegala (2021c) evaluated the bias using the average of the likelihoods of all tokens without masking the MLM.\n\nDespite the numerous studies of social bias in MLMs covering English, social biases in MLMs for other languages remain understudied (Lewis and Lupyan, 2020;Liang et al., 2020;Bartl et al., 2020;Zhao et al., 2020). To realise the diverse and inclusive social and cultural impact of AI, we believe it is important to establish tools for detecting and mitigating unfair social biases in MLMs, not only for English but for all languages. However, the significant manual annotation effort, the costs and difficulties in recruiting qualified annotators remain major challenges when creating bias evaluation benchmarks for target languages. For example, existing bias evaluation benchmarks such as CrowS-Pairs (CP; Nangia et al., 2020) and Stere-oSet (SS; Nadeem et al",
            "score": 0.544951494684002,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 51,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 180,
                    "end": 206,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 226,
                    "end": 247,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 597,
                    "end": 618,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 618,
                    "end": 635,
                    "matchedPaperCorpusId": "201670701"
                },
                {
                    "start": 635,
                    "end": 663,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 663,
                    "end": 683,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 858,
                    "end": 878,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 926,
                    "end": 946,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1347,
                    "end": 1371,
                    "matchedPaperCorpusId": "220948164"
                },
                {
                    "start": 1371,
                    "end": 1390,
                    "matchedPaperCorpusId": "227230609"
                },
                {
                    "start": 1390,
                    "end": 1409,
                    "matchedPaperCorpusId": "225094152"
                },
                {
                    "start": 1409,
                    "end": 1427,
                    "matchedPaperCorpusId": "218487087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78076171875
        },
        {
            "corpus_id": "270878706",
            "title": "Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis",
            "text": "Large Language Models (LLMs) are not immune to inheriting and perpetuating social biases present in their training data.The presence of such biases in LLM generations2 is a matter of concern, as it risks reinforcing societal prejudices and stereotypes, leading to unfair outcomes in applications ranging from content generation to decision-making processes.Measuring and understanding the extent of social biases in LLMs is challenging as it can manifest in various forms, such as preferential language towards certain groups or discriminatory responses based on demographics.Existing works evaluate social biases by asking the model to choose an entity from two contrasting demographic pairs, using the LLM itself to evaluate the responses (Zhao et al., 2023b), forcing favoritism for one group over the other (Zhao et al., 2023a), and prompting to evaluate bias based on word associations (Wan et al., 2023;Bi et al., 2023;Kaneko et al., 2024;Bai et al., 2024).However, there is no unifying commonality across these methods in terms of a holistic evaluation of bias.Also, all of these methods rely on some sort of comparison-based assessment without looking individually at each demographic group.To overcome these challenges, we introduce an approach grounded in psychological principles of intergroup contact to evaluate and mitigate biases in LLMs, focusing on each individual group.\n\nThe Contact Hypothesis (Allport et al., 1954) postulates that under specific conditions, increased contact between different social groups can reduce prejudices.We apply this concept to LLM generations to explore how simulating various forms of contact by adding examples of positive/negative experiences between social groups (\"contact probing\") in the prompt can influence the biases in the outputs of these models.To our knowledge, this is the first study to explore social bias in natural arXiv:2407.02030v1[cs.CL] 2 Jul 2024 Figure 2: An example of a certainty type prompt for positive contact with positive action in an education scenario which considers a particular descriptor (\"deaf\") from the Ability dimension to test whether contact hypothesis is followed for the key principle of equal group status.language positioned on the contact hypothesis.",
            "score": 0.5447980109669087,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 120,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 576
                },
                {
                    "start": 576,
                    "end": 963
                },
                {
                    "start": 963,
                    "end": 1068
                },
                {
                    "start": 1068,
                    "end": 1199
                },
                {
                    "start": 1199,
                    "end": 1388
                },
                {
                    "start": 1390,
                    "end": 1551
                },
                {
                    "start": 1551,
                    "end": 1807
                },
                {
                    "start": 1807,
                    "end": 1901
                },
                {
                    "start": 1901,
                    "end": 2202
                },
                {
                    "start": 2202,
                    "end": 2248
                }
            ],
            "ref_mentions": [
                {
                    "start": 891,
                    "end": 909,
                    "matchedPaperCorpusId": "264128125"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95068359375
        },
        {
            "corpus_id": "267411833",
            "title": "Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes",
            "text": "Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mitigation.",
            "score": 0.5446162501703924,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9892578125
        },
        {
            "corpus_id": "273901658",
            "title": "Can Machine Unlearning Reduce Social Bias in Language Models?",
            "text": "Mitigating bias in language models (LMs) has become a critical problem due to the widespread deployment of LMs in the industry and customer-facing applications. Numerous approaches revolve around data pre-processing and subsequent fine-tuning of language models, tasks that can be both time-consuming and computationally demanding. As alternatives, machine unlearning techniques are being explored, yet there is a notable lack of comparative studies evaluating the effectiveness of these methods. In this work, we explore the effectiveness of two machine unlearning methods: Partitioned Contrastive Gradient Unlearning (PCGU) applied on decoder models, and Negation via Task Vector, and compare them with Direct Preference Optimization (DPO) to reduce social biases in open-source LMs such as LLaMA-2 and OPT. We also implement distributed PCGU for large models. It is empirically shown, through quantitative and qualitative analyses, that negation via Task Vector method outperforms PCGU and is comparable to DPO in debiasing models with minimum deterioration in model performance and perplexity. Negation via Task Vector reduces the bias score by 25.5% for LLaMA-2 and achieves bias reduction of up to 40% for OPT models. Moreover, it can be easily tuned to balance the trade-off between bias reduction and generation quality, unlike DPO.",
            "score": 0.5444837595189566,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93896484375
        },
        {
            "corpus_id": "268709624",
            "title": "Biases Mitigation and Expressiveness Preservation in Language Models: A Comprehensive Pipeline (Student Abstract)",
            "text": "Pre-trained Language Models (PLMs) excel in diverse natural language tasks due to their training on extensive data. However, prior studies have revealed that PLMs inadvertently encode and propagate social biases from their unfiltered pre-training data. Take gender bias as an example: the PLM is more inclined towards associating male (female) attributes with programmers (nurses). Several solutions for mitigating the social biases have been proposed, including: (1) Post-hoc-based method add a post-training step to these sentence representations before applied to downstream tasks, including removing the estimated genderdirection subspace from sentence representation (Liang et al. 2020), or use pre-defined word tuples combine specific techniques to debias text encoder for a fair sentence representation (Cheng et al. 2021). (2) Fine-tuning-based models use specific loss terms to guide a PLM to remove biases, including distribution alignment loss for debiasing embedding space (Guo, Yang, and Abbasi 2022); orthogonal loss aims to promote irrelevance between stereotyped words and gender-specific words (Kaneko and Bollegala 2021), etc. Current debiasing methods for PLMs have shown promise but grapple with notable challenges: (1) demanding timeconsuming to fine-tune entire parameters in PLMs; (2) disregarding the expressiveness of PLMs, which could potentially disrupt PLM's computational structure and undermine the benefits of pre-training; (3) reintroducing biases from downstream tasks into PLMs when applying debiased models to those tasks. Hence, we present a new two-stage pipeline that aims to simultaneously preserve the PLMs' expressiveness and mitigate biases from both internal and downstream contexts. As shown in Figure 1, in first stage, we keep PLM's parameter frozen, and only train the continuous prefix to reduce the magnitude of trainable parameters, towards mitigating internal bias and meanwhile preserving expressiveness. In second stage, we perform causal interventions on different demographic groups to eliminate the biases from downstream contexts.",
            "score": 0.543467761905831,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2087
                }
            ],
            "ref_mentions": [
                {
                    "start": 672,
                    "end": 691,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 810,
                    "end": 829,
                    "matchedPaperCorpusId": "232185104"
                },
                {
                    "start": 985,
                    "end": 1013,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1111,
                    "end": 1138,
                    "matchedPaperCorpusId": "231698657"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95068359375
        },
        {
            "corpus_id": "268856634",
            "title": "Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs",
            "text": "Social biases in natural language processing (NLP) systems and related data have been studied with respect to their fairness, inclusivity, and accuracy (Hutchinson et al., 2020;Maass, 1999;Zhao et al., 2018).For example, Bolukbasi et al. (2016); Garg et al. (2018) are among the pioneers in demonstrating gender-related associations in word embeddings that might reflect and perpetuate stereotypes.Caliskan et al. (2017) concludes standard machine learning methods for NLP could acquire societal biases from textual data.Some research has expanded our understanding of where NLP systems might acquire subgroup associations within data, and potential bias, including those from data collection (Bender & Friedman, 2018), annotation processes (Gebru et al., 2018), and model architecture choices (Zhao et al., 2017).To date, numerous efforts have been made to mitigate social biases in systems through a variety of methods, including data augmentation (Lu et al., 2018), changes in model architecture (Liang et al., 2020), and training objectives (Liu et al., 2021;Romanov et al., 2019).\n\nIn a similar vein, although recent advancements in LLMs are exciting, researchers are concerned about whether LLMs inherit social biases from the trillions of tokens they have been trained on.Weidinger et al. (2022) provides a comprehensive taxonomy of social risks within LLMs.Although the research community has documented numerous social biases in LLMs (Ferrara, 2023;Mei et al., 2023), few LLM researchers have examined these biases from the perspective of the psychology of human decision-making.Thus, in the present work, we study the bias issue in LLMs from the new angle of the aforementioned representative heuristic, a concept originating in psychology (Kahneman & Tversky, 1973;Kahneman et al., 1982;Tversky & Kahneman, 1974;1983).",
            "score": 0.5425049110886824,
            "section_title": "Related Work",
            "char_start_offset": 5721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 208,
                    "end": 398
                },
                {
                    "start": 398,
                    "end": 521
                },
                {
                    "start": 521,
                    "end": 814
                },
                {
                    "start": 814,
                    "end": 1085
                },
                {
                    "start": 1087,
                    "end": 1279
                },
                {
                    "start": 1279,
                    "end": 1365
                },
                {
                    "start": 1365,
                    "end": 1588
                },
                {
                    "start": 1588,
                    "end": 1829
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 177,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 177,
                    "end": 189,
                    "matchedPaperCorpusId": "142809360"
                },
                {
                    "start": 221,
                    "end": 244,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 246,
                    "end": 264,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 398,
                    "end": 420,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 693,
                    "end": 718,
                    "matchedPaperCorpusId": "52255687"
                },
                {
                    "start": 741,
                    "end": 761,
                    "matchedPaperCorpusId": "4421027"
                },
                {
                    "start": 794,
                    "end": 813,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 999,
                    "end": 1019,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1279,
                    "end": 1302,
                    "matchedPaperCorpusId": "249872629"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.822265625
        },
        {
            "corpus_id": "259129801",
            "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks",
            "text": "Prior research has developed various intrinsic and extrinsic evaluation methods of social biases in LMs. Extrinsic evaluation of bias often focuses on the performance of language models' downstream tasks [23,44]. Numerous studies have evaluated bias intrinsically by measuring associations of social identities and attributes in word embeddings or sentence embeddings [7,24,29]. Word embeddings are dense representations of word co-occurrence statistics trained from a text corpus, with which language models can construct sentences that maintain semantic coherence. By measuring the relative similarity between the word embeddings of target groups and attributes, Caliskan et al. [7] develop the Word Embeddings Association Test (WEAT) to quantify implicit representational bias and associations [7]. For example, men are associated with career and women with family. Through analyzing word embeddings, prior research detects social biases with respect to race, gender, religion, and ethnicity in language models [7,16,28]. Building upon WEAT, May et al. [29] develop the Sentence Encoder Association Test (SEAT) to evaluate bias in phrases and sentences. Moving from the sentence level to the discourse level, Nadeem et al. [34] develop the Context Association Test (CAT) to measure stereotypical biases in pre-trained language models BERT, GPT2, RoBERTa, and XLNet. Consistent with previous findings, the results of their approach indicate that LMs encode stereotypical biases related to gender, profession, race, and religion. \n\nMeasuring Bias in Language Models via Prompting A growing body of research start to utilize prompting to evaluate and improve the performance of language models in NLP tasks such as knowledge probing, commonsense reasoning, and language comprehension [6,43,48]. Meanwhile, researchers also adopt prompting to evaluate bias in language models and their downstream tasks such as sentiment classification [3,22,23,45]. Specifically, several studies suggest using semantically bleached prompt templates to evaluate bias against target groups or attributes [29]. Semantically bleached templates are often short and convey very little meaning beyond the terms that are inserted, such as \"This is",
            "score": 0.5423042532653444,
            "section_title": "Bias Evaluation of Language Models",
            "char_start_offset": 8484,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1530
                },
                {
                    "start": 1533,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2222
                }
            ],
            "ref_mentions": [
                {
                    "start": 368,
                    "end": 371,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 371,
                    "end": 374,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 681,
                    "end": 684,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 797,
                    "end": 800,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1014,
                    "end": 1017,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1017,
                    "end": 1020,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 1020,
                    "end": 1023,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 1226,
                    "end": 1230,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1784,
                    "end": 1787,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1787,
                    "end": 1790,
                    "matchedPaperCorpusId": "221703107"
                },
                {
                    "start": 1935,
                    "end": 1938,
                    "matchedPaperCorpusId": "250390802"
                },
                {
                    "start": 1938,
                    "end": 1941,
                    "matchedPaperCorpusId": "250391069"
                },
                {
                    "start": 1944,
                    "end": 1947,
                    "matchedPaperCorpusId": "253224433"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9404296875
        },
        {
            "corpus_id": "273185453",
            "title": "Collapsed Language Models Promote Fairness",
            "text": "The rise of pre-trained language models (PLMs) has revolutionized natural language processing, greatly enhancing tasks like reasoning and prediction by harnessing the semantic richness of language data. Despite their effectiveness, these models, trained on extensive corpora, often reflect and even intensify societal biases in their training datasets. Such biases manifest in the association of demographic groups with specific roles or capabilities, affecting fairness in applications ranging from legal analytics to hiring processes [49; 12; 38; 2; 52; 3; 7]. Thus, it is crucial to address and mitigate these biases to prevent discriminatory practices in downstream applications [70; 64; 46]. \n\nTo mitigate societal biases in language models, a substantial array of fairness algorithms has been proposed. On the one hand, people target different learning stages: making language models fair via balanced and augmented training data [1], fine-tuning with regularizations or auxiliary objectives [26; 48], or carefully-tuned prompts [68]. On the other hand, these debiasing approaches can also be categorized by their awareness of downstream tasks. Task-specific methods fine-tune language models with sensitive annotations [23; 22; 56; 55], while task-agnostic approaches directly debias word embeddings or representations during pretraining [8; 30; 21; 26]. Despite this multitude of efforts, it is challenging to find common ground among these methods and shared properties of debiased language models. We are thus motivated to ask: Can we understand and improve the fairness of LMs in principle? The recent development of deep learning theory provides fruitful frameworks and tools for us to understand deep neural networks (DNNs). Among them, neural collapse [47] is first observed for classification tasks, and is then analyzed to understand the optimization [24; 72] and generalization [29; 18] of DNNs. Meanwhile, the training of generative language models, typically via the next token prediction task, is essentially a classification problem. We thus study the fairness of LMs through the lens of neural collapse. \n\n\u2022 We for the first time comprehensively analyze the relations between neural collapse and fairness in language models.",
            "score": 0.5420938569571264,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2125
                },
                {
                    "start": 2128,
                    "end": 2246
                }
            ],
            "ref_mentions": [
                {
                    "start": 1035,
                    "end": 1039,
                    "matchedPaperCorpusId": "253446867"
                },
                {
                    "start": 1766,
                    "end": 1770,
                    "matchedPaperCorpusId": "221172897"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9560546875
        },
        {
            "corpus_id": "270214849",
            "title": "LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models",
            "text": "Bias in text generation as a distribution-level difference across varied demographic groups has been widely observed in the literature (Gallegos et al., 2023).Existing measures of social bias falls into two categories.Local bias (Liang et al., 2021) refers to the difference in the highdimensional probability distributions of a LM and boils down to the accumulated difference in token-level distributions.Global bias (Sheng et al., 2020) considers the difference in the distribution of some low-dimensional global property (Sheng et al., 2021).In general, the practical harm of global bias is more straightforward than local bias and has attracted more attention (Blodgett et al., 2020).In this work we study the global bias from an informationtheoretic perspective.\n\nBias mitigation has accumulated a vast literature (Li et al., 2023).Early attempts seek to finetune a pretrained LM on some carefully-curated datasets that contains no or minimal bias (Lu et al., 2020;Saunders & Byrne, 2020) or through a regularization training (Huang et al., 2019;Peng et al., 2020;Wang et al., 2023b).However, these methods are often resource-consuming, hindering their Table 3.Samples from debiasing sentiment (top), regard (middle), and toxicity (bottom) on three LLMs with UDDIA and (e)LIDAO as shadowed in different colors.All results are generated from the first prompt for simple illustrations.On the undebiased generations, we color the tokens that should be independent with property g and gender a as suggested by LIDAO, respectively.\n\nPrompt: Instead, these men watched the man they say humiliated practicality for larger LMs.Consequently, the decodingtime intervention paradigm is favoured (Liu et al., 2021;Yang et al., 2023).Representative decoding-time interventions consist of using reference models to guide generations (Liang et al., 2020;Krause et al., 2021;Liu et al., 2021), or injecting adversarial triggers to stimulate unbiased generations (Sheng et al., 2020;Schick et al., 2021).",
            "score": 0.5418857039863847,
            "section_title": "Related Works",
            "char_start_offset": 30470,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 159,
                    "end": 218
                },
                {
                    "start": 218,
                    "end": 406
                },
                {
                    "start": 406,
                    "end": 545
                },
                {
                    "start": 545,
                    "end": 688
                },
                {
                    "start": 688,
                    "end": 767
                },
                {
                    "start": 769,
                    "end": 837
                },
                {
                    "start": 837,
                    "end": 1089
                },
                {
                    "start": 1089,
                    "end": 1166
                },
                {
                    "start": 1166,
                    "end": 1315
                },
                {
                    "start": 1315,
                    "end": 1388
                },
                {
                    "start": 1388,
                    "end": 1531
                },
                {
                    "start": 1533,
                    "end": 1624
                },
                {
                    "start": 1624,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1992
                }
            ],
            "ref_mentions": [
                {
                    "start": 229,
                    "end": 249,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 418,
                    "end": 438,
                    "matchedPaperCorpusId": "218470535"
                },
                {
                    "start": 524,
                    "end": 544,
                    "matchedPaperCorpusId": "234337004"
                },
                {
                    "start": 664,
                    "end": 687,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 953,
                    "end": 970,
                    "matchedPaperCorpusId": "263135509"
                },
                {
                    "start": 970,
                    "end": 993,
                    "matchedPaperCorpusId": "215548916"
                },
                {
                    "start": 1051,
                    "end": 1069,
                    "matchedPaperCorpusId": "226221869"
                },
                {
                    "start": 1069,
                    "end": 1088,
                    "matchedPaperCorpusId": "257220117"
                },
                {
                    "start": 1689,
                    "end": 1707,
                    "matchedPaperCorpusId": "235313967"
                },
                {
                    "start": 1707,
                    "end": 1725,
                    "matchedPaperCorpusId": "252780503"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93115234375
        },
        {
            "corpus_id": "256868507",
            "title": "BiasTestGPT: Using ChatGPT for Social Bias Testing of Language Models",
            "text": "Pretrained language models (PLMs) have led to impressive progress in a wide range of NLP tasks [53,91]. However, because they are trained on massive text corpora that are mostly not curated, they have been shown to reflect and sometimes amplify real-world social biases [9,82]. These social biases result in problematic responses related to gender, race, and sexual orientation [104]. Even after fine-tuning the models on task-specific data, such issues persist in downstream applications [116].",
            "score": 0.5418199205531851,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 495
                }
            ],
            "ref_mentions": [
                {
                    "start": 95,
                    "end": 99,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 99,
                    "end": 102,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 270,
                    "end": 273,
                    "matchedPaperCorpusId": "225094152"
                },
                {
                    "start": 273,
                    "end": 276,
                    "matchedPaperCorpusId": "235097294"
                },
                {
                    "start": 378,
                    "end": 383,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 489,
                    "end": 494,
                    "matchedPaperCorpusId": "4952494"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95556640625
        },
        {
            "corpus_id": "274234685",
            "title": "Profiling Bias in LLMs: Stereotype Dimensions in Contextual Word Embeddings",
            "text": "Large pre-trained language models reflect the biases in their training data, which in turn reflect the biases of their creators. As the foundation for AI applications, their biases are further propagated, warranting their study to uncover the risks and promote mitigation efforts. \n\nIn this work, we profile twelve LLMs by means of the stereotype content model of social psychology (Fiske et al., 2002), thereby theoretically grounding the analysis, which has in the past been described as the missing link for bias measurements (Blodgett et al., 2020). We employ a polar transformation on the contextual embeddings, finding significant bias for genderassociated names and gendered terms along the stereotype dimensions, widely aligned with human bias. \n\nBias profiles based on the stereotype content model employ an intuitive scoring along scales of opposing concepts (e.g. low vs. high warmth), as proven effective by semantic differentials in human surveys (Osgood et al., 1957). The result is a highly visual solution for communicating bias to wider audiences and users of artificial intelligence.",
            "score": 0.5418093593186724,
            "section_title": "Conclusion",
            "char_start_offset": 20134,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 280
                },
                {
                    "start": 283,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1101
                }
            ],
            "ref_mentions": [
                {
                    "start": 382,
                    "end": 402,
                    "matchedPaperCorpusId": "17057403"
                },
                {
                    "start": 529,
                    "end": 552,
                    "matchedPaperCorpusId": "218971825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93310546875
        },
        {
            "corpus_id": "277667666",
            "title": "Fairness Mediator: Neutralize Stereotype Associations to Mitigate Bias in Large Language Models",
            "text": "Large Language Models (LLMs) have rapidly advanced, achieving remarkable success across diverse natural language processing (NLP) tasks, such as question answering and text generation [10,13,74,88]. These models are now deeply integrated into daily life, powering technologies like search engines [99] and virtual assistants [19]. Despite their successes, LLMs still face significant challenges related to robustness [56-58, 85, 90, 98, 106], privacy [38,93], fairness [47,89,96], and other trustworthiness concerns [53,54]. This paper specifically focuses on the fairness issues associated with LLMs. LLMs often inherit social stereotypes and biases [96] from the training data [35,83,92], leading to biased behavior toward specific social groups, particularly in relation to protected attributes such as religion, race, and gender. For instance, GPT-3 [10] has been shown to frequently associate Muslims with violent contexts [2,39], and Microsoft's AI chatbot Tay infamously produced racist and inappropriate content after interacting with users on social media [8]. As LLMs are increasingly integrated into socially sensitive software applications, developing effective bias mitigation techniques is critical to ensuring fairness and addressing growing concerns. \n\nBias in LLMs often manifests as spurious correlations [25,36,55,76] between biased concepts (e.g., \"violence\") and specific social groups (e.g., \"Muslim\"), a phenomenon commonly referred to as stereotype associations [9,33]. These associations arise from the underrepresentation or skewed portrayal of certain social groups in training data, perpetuating harmful stereotypes and contributing to representational harm [14], whereby systems reinforce the subordination of marginalized groups. Interestingly, such implicit associations and latent activation pathways have also been exploited in recent adversarial attacks [42,91,101], backdoor attacks [50,53,54], jailbreak attacks [40,102,103] and hallucinations [34] against LLMs, revealing a broader category of vulnerabilities where malicious prompts or triggers activate specific undesired behaviors.",
            "score": 0.5417707242361873,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1266
                },
                {
                    "start": 1269,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 188,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 297,
                    "end": 301,
                    "matchedPaperCorpusId": "270870458"
                },
                {
                    "start": 325,
                    "end": 329,
                    "matchedPaperCorpusId": "260499677"
                },
                {
                    "start": 451,
                    "end": 455,
                    "matchedPaperCorpusId": "263211018"
                },
                {
                    "start": 455,
                    "end": 458,
                    "matchedPaperCorpusId": "257460348"
                },
                {
                    "start": 469,
                    "end": 473,
                    "matchedPaperCorpusId": "267523806"
                },
                {
                    "start": 473,
                    "end": 476,
                    "matchedPaperCorpusId": "258833296"
                },
                {
                    "start": 679,
                    "end": 683,
                    "matchedPaperCorpusId": "272214842"
                },
                {
                    "start": 854,
                    "end": 858,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 928,
                    "end": 931,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 1323,
                    "end": 1327,
                    "matchedPaperCorpusId": "215786368"
                },
                {
                    "start": 1327,
                    "end": 1330,
                    "matchedPaperCorpusId": "250526377"
                },
                {
                    "start": 1333,
                    "end": 1336,
                    "matchedPaperCorpusId": "227253684"
                },
                {
                    "start": 1486,
                    "end": 1489,
                    "matchedPaperCorpusId": "29498619"
                },
                {
                    "start": 1489,
                    "end": 1492,
                    "matchedPaperCorpusId": "54036730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9853515625
        },
        {
            "corpus_id": "258714851",
            "title": "On the Origins of Bias in NLP through the Lens of the Jim Code",
            "text": "In this paper, we trace the biases in current natural language processing (NLP) models back to their origins in racism, sexism, and homophobia over the last 500 years. We review literature from critical race theory, gender studies, data ethics, and digital humanities studies, and summarize the origins of bias in NLP models from these social science perspective. We show how the causes of the biases in the NLP pipeline are rooted in social issues. Finally, we argue that the only way to fix the bias and unfairness in NLP is by addressing the social problems that caused them in the first place and by incorporating social sciences and social scientists in efforts to mitigate bias in NLP models. We provide actionable recommendations for the NLP research community to do so.",
            "score": 0.5407962707373644,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62744140625
        },
        {
            "corpus_id": "270286756",
            "title": "Knowledge-Enhanced Language Models Are Not Bias-Proof: Situated Knowledge and Epistemic Injustice in AI",
            "text": "Social bias is observed when language models \"systematically and unfairly discriminate against certain individuals or groups of individuals in favor of others\" [23, p. 332]. It takes form in reproduced stereotypes [71], negative valuations of groups [91], or systematic performance differences based on sensitive attributes [15,43]. Social bias is another widely discussed limitation of language models [8,48,96]. Both social bias and factual inaccuracies are considered obstacles to the trustworthiness of LMs [55,101] but are usually investigated in isolation to each other. Factual inaccuracies are countered by adding knowledge, i.e., data that represent facts about things in the world, while social bias is tackled, e.g., through data balancing, manipulation of the embedding space, or constraining the predictions [96]. It is at times implied that enhancing the factual accuracy of LMs through knowledge enhancement could positively impact bias issues in the same instance, since knowledge is highly trusted and curated. 1617 This corresponds to our observation that, in the context of knowledge-enhanced language modeling, the issue of bias is usually only mentioned as a limitation of statistical AI and its unstructured training databases [2,3,106]. 18 he fact that highly curated and structured KGs, like Wikidata and DBpedia, reproduce the same societal biases mostly goes unmentioned [44]. This omission is unjustified and potentially harmful. \n\nThat is, misconceiving of knowledge as objective and an antithesis to bias, value judgements, and uncertainty, grants anything under the label of knowledge potentially undeserved legitimacy. In fact, it gives undeserved legitimacy to the interests, assumptions and world views of a privileged group. In the case of both the work of Adam [1] and the KGs discussed here, this is predominantly the group of educated Western men [44]. \n\nIn the next section, we summarize representation-related issues in Wikidata and Wikipedia, which are examples of crowd-sourced knowledge bases.",
            "score": 0.5407035643945733,
            "section_title": "Why We Need to Talk About Knowledge Enhancement and Social Bias",
            "char_start_offset": 19875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1889
                },
                {
                    "start": 1892,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 218,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 250,
                    "end": 254,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "252907216"
                },
                {
                    "start": 328,
                    "end": 331,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 403,
                    "end": 406,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 406,
                    "end": 409,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 409,
                    "end": 412,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 511,
                    "end": 515,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 515,
                    "end": 519,
                    "matchedPaperCorpusId": "259202782"
                },
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 1249,
                    "end": 1252,
                    "matchedPaperCorpusId": "263864711"
                },
                {
                    "start": 1254,
                    "end": 1258,
                    "matchedPaperCorpusId": "259203671"
                },
                {
                    "start": 1397,
                    "end": 1401,
                    "matchedPaperCorpusId": "252762171"
                },
                {
                    "start": 1796,
                    "end": 1799,
                    "matchedPaperCorpusId": "23275917"
                },
                {
                    "start": 1884,
                    "end": 1888,
                    "matchedPaperCorpusId": "252762171"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9541015625
        },
        {
            "corpus_id": "271923841",
            "title": "Identifying and Mitigating Social Bias Knowledge in Language Models",
            "text": "Pre-trained Language Models (PLMs) have demonstrated exceptional performance on many tasks, such as language understanding and question answering (Devlin et al., 2018;Floridi and Chiriatti, 2020;Brown et al., 2020). However, the encoded social stereotypes and human-like biases inevitably cause undesired behaviors when deploying PLMs in practice (Zhao et al., 2019;Navigli et al., 2023), e.g., making stereotyped judgments on vulnerable groups (Sheng et al., 2021). Removing such biases can not only enhance the generalization ability and reliability of PLMs but also expedite their deployment while retaining substantial social significance, which garners increasing attention from researchers, practitioners, and the broader public (May et al., 2019;Gehman et al., 2020;Ma et al., 2023). Current approaches to mitigate biases in PLMs include debiasing through fine-tuning or prompt-tuning (Gallegos et al., 2023;Garrido-Mu\u00f1oz et al., 2021;Kaneko and Bollegala, 2021). Fine-tuning involves additional pre-training on balanced corpora (Zmigrod et al., 2019), aligning embeddings within bias subspaces (Liang et al., 2020;Ravfogel et al., 2020), or using contrastive objectives (He et al., 2022;Cheng et al., 2021) to lessen biases. Prompt-tuning techniques use prompts to guide PLMs towards ignoring social group disparities for fairer decision (Guo et al., 2022;Yang et al., 2023;Li et al., 2023c;Dong et al., 2023). \n\nHowever, while these methods emphasize parity across different demographic groups, they also generate unreasonable predictions on commonsense knowledge and are prone to exhibiting new biases regarding individual facts (Hanna et al., 2020;Gallegos et al., 2023;Kumar et al., 2022;Devinney et al., 2022). For example, as shown in Figure 1, for individual facts such as \"The child is generally given birth by [mom/dad].\",",
            "score": 0.5405132648914057,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1418
                },
                {
                    "start": 1421,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1839
                }
            ],
            "ref_mentions": [
                {
                    "start": 195,
                    "end": 214,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 366,
                    "end": 387,
                    "matchedPaperCorpusId": "258688053"
                },
                {
                    "start": 773,
                    "end": 789,
                    "matchedPaperCorpusId": "259937535"
                },
                {
                    "start": 915,
                    "end": 942,
                    "matchedPaperCorpusId": "233832881"
                },
                {
                    "start": 1346,
                    "end": 1364,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 1364,
                    "end": 1382,
                    "matchedPaperCorpusId": "253446867"
                },
                {
                    "start": 1382,
                    "end": 1399,
                    "matchedPaperCorpusId": "259342087"
                },
                {
                    "start": 1639,
                    "end": 1659,
                    "matchedPaperCorpusId": "208921008"
                },
                {
                    "start": 1700,
                    "end": 1722,
                    "matchedPaperCorpusId": "248524791"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94140625
        },
        {
            "corpus_id": "274130807",
            "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation",
            "text": "Biases are often present in unstructured data, and LLMs trained on such data can not only learn these biases but sometimes amplify them as well (Bolukbasi et al., 2016;Sheng et al., 2019). Various methods of bias mitigation exist under different contexts, such as social bias (Garimella et al., 2021(Garimella et al., , 2022)), political bias (Liu et al., 2022), and stereotype bias (Nie et al., 2024b). Instead of reviewing these methods for bias mitigation by bias contexts, we categorize them by the timing of debiasing. More specifically, methods for mitigating biases can be categorized into three distinct approaches: (1) preprocessing the input data prior to training LLMs; \n\n(2) changing the model's architecture and/or picking the best model based on the predefined bias evaluation metrics; \n\n(3) improving unbiasedness by calibrating the outputs at the decision-making stage. We call these three approaches as premodel debiasing, intra-model debiasing, and post-model debiasing, respectively. Table 4 provides an overview of the advantages and disadvantages on each approach. (Ferrara, 2023). We review the commonly used pre-model debiasing methods below. \n\nOne of the widely used methods is counterfactual data augmentation (CDA), which is often used to mitigate bias in the literature (Zmigrod et al., 2019;Webster et al., 2020;Barikeri et al., 2021;Mondal and Lipizzi, 2024). Counterfactual Data Substitution (CDS), a variant of CDA, is also proposed to randomly substitute potentially biased text to avoid duplication (Maudslay et al., 2019). Generally speaking, CDA rebalances a corpus by sawpping bias attribute words in a dataset. For example, we may swap the sentence \"The professor came to the classroom but he forgot to bring his laptop\" to \"The professor came to the classroom but she forgot to bring her laptop\".",
            "score": 0.5402533071006456,
            "section_title": "Bias Mitigation",
            "char_start_offset": 38494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 680
                },
                {
                    "start": 683,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1165
                },
                {
                    "start": 1168,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1834
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 168,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 168,
                    "end": 187,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 276,
                    "end": 299,
                    "matchedPaperCorpusId": "236477795"
                },
                {
                    "start": 299,
                    "end": 326,
                    "matchedPaperCorpusId": "253762006"
                },
                {
                    "start": 343,
                    "end": 361,
                    "matchedPaperCorpusId": "245692342"
                },
                {
                    "start": 1297,
                    "end": 1319,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 1340,
                    "end": 1362,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 1362,
                    "end": 1387,
                    "matchedPaperCorpusId": "270270636"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91796875
        },
        {
            "corpus_id": "258170403",
            "title": "Evaluation of Social Biases in Recent Large Pre-Trained Models",
            "text": "Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups. In this work, we study the general trend in bias reduction as newer pre-trained models are released. Three recent models ( ELECTRA, DeBERTa, and DistilBERT) are chosen and evaluated against two bias benchmarks, StereoSet and CrowS-Pairs. They are compared to the baseline of BERT using the associated metrics. We explore whether as advancements are made and newer, faster, lighter models are released: are they being developed responsibly such that their inherent social biases have been reduced compared to their older counterparts? The results are compiled and we find that all the models under study do exhibit biases but have generally improved as compared to BERT.",
            "score": 0.5402410514076679,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95703125
        },
        {
            "corpus_id": "247012038",
            "title": "Reward modeling for mitigating toxicity in transformer-based language models",
            "text": "M with RL. This dataset contains \u223c 100K prompts that were selected from sentences in the OpenWebText corpus [19], where prompts are labeled based on their toxicity scores. To evaluate the ability of our detoxification approach to handle various social identities, we also consider the Bias in Open-Ended Language Generation Dataset (BOLD) [20]. BOLD is a large-scale dataset that consists of \u223c 23K English text generation prompts for bias benchmarking across various identities, such as gender, race, and religion. Empirical results demonstrate that utilizing RL for fine-tuning the LM to maximize the reward model can mitigate toxic language generation by the LM and outperform the current detoxification methods in the literature. Furthermore, we demonstrate that utilizing a reward model trained to reduce unintended bias towards various social identities successfully enables the LMs to mitigate toxicity when conditioned on prompts related to these social identities.\n\nOur contributions are summarized as follows:\n\n\u2022 We introduce the Reinforce-Detoxify model, our proposed approach for mitigating toxicity in LMs based on PPO from the RL algorithm. \u2022 We propose a reward model based on multitask learning (MTL) that can mitigate unintended bias in toxicity prediction related to various social identities. \u2022 We employ the Jigsaw \"\"Unintended Bias in Toxicity\" dataset for training the MTL reward model and the RTP dataset [10] and the BOLD dataset [20] to condition the LM for continuation generation and evaluate our detoxification approach's ability to handle various social identities related to gender, race, and religion. \u2022 We demonstrate that utilizing our proposed reward model trained to reduce unintended bias toward various social identities for fine-tuning the LM can mitigate toxic language generation by the LM and outperform the existing detoxification methods.\n\nThe structure of this article is described as follows: Section 2 presents the literature review related to LM detoxification methods. Section 3 includes preliminaries related to transformers and the Markov decision process (MDP). Section 4 introduces the proposed reward model for identifying toxic language based on the MTL approach. Furthermore, fine-tuning the LM with RL is discussed in this section. In Section 5, we",
            "score": 0.5399393220382948,
            "section_title": "body",
            "char_start_offset": 5961,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 339,
                    "end": 343,
                    "matchedPaperCorpusId": "231719337"
                },
                {
                    "start": 1427,
                    "end": 1431,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 1453,
                    "end": 1457,
                    "matchedPaperCorpusId": "231719337"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8525390625
        },
        {
            "corpus_id": "261696544",
            "title": "Learning to Predict Concept Ordering for Common Sense Generation",
            "text": "While we conducted our research primarily on the CommonGen dataset, which to the best of our knowledge does not present any explicit ethical issues, it is essential to acknowledge the potential for social biases in the LMs (Blodgett et al., 2021). One of the pre-trained language models we used in our experiments, BART are significantly prone to prediction errors related to gender bias (Sharma et al., 2021) and we are not evaluating for the biases in the generated sentences here which should be done before LMs are deployed in the downstream NLP applications. Given that we are fine-tuning LMs on the CommonGen dataset, some social biases could get amplified during this fine-tuning process. The predicted sentences are possibly influenced by such biases. It is still an open question for how to effectively mitigate these biases, particularly in the context of generative commonsense reasoning tasks. with the different input formatting methods. However, the best performance among the three input formatting methods is achieved when the concepts are concatenated with the ordered concepts around tokens. This may be attributed to the fact that, with the tokens and ordered concepts, a model might better comprehend the ordering task and use the syntactic knowledge embedded in the pre-trained models (Ou et al., 2022).",
            "score": 0.5397003606615534,
            "section_title": "Ethical Considerations",
            "char_start_offset": 19088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1324
                }
            ],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 246,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 1306,
                    "end": 1323,
                    "matchedPaperCorpusId": "248218772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82763671875
        },
        {
            "corpus_id": "271902917",
            "title": "REFINE-LM: Mitigating Language Model Stereotypes via Reinforcement Learning",
            "text": "Using RL, our method does not require any form of manual annotations, but rather uses the LM output to mitigate a wide variety of biases in the answer. While RL has been successfully applied in algorithmic fairness [22,41,46], this is, to the best of our knowledge, the first approach that applies RL for mitigation a wide rage of biases, not only in \"more traditional\" masked LMs, but also in Large LMs such as LLama2 or Mistral. In particular, our method allows us to (i) reduce training resources, (ii) avoid the need for manual annotation, and (iii) support a wide range of stereotypical biases, including gender-occupation, ethnicity, nationality, and religion. The main contributions of our paper are the following: \n\n\u2022 We formulate bias mitigation as contextual bandits RL problem that uses bias measuring framework inspired by [27]. \u2022 We propose REFINE-LM that mitigates different types of stereotypes such as those based on gender, nationality, ethnicity, and religion from any LMs. As shown in our evaluation, REFINE-LM is easy to train and can successfully suppress stereotypes in LMs as well as LLMs without affecting model performance. \u2022 An evaluation of REFINE-LM based on (a) the definitions of bias on the datasets proposed by Li et al. [27], and (b) the performance of the debiased LM on downstream tasks. \n\nThe rest of the paper is organized as follows. Section 2 surveys state of the art in bias detection and mitigation for language models in general. Section 3 explains the framework used to quantify bias as well as the inner workings of REFINE-LM, our proposed solution to reduce bias in pre-trained LMs. Section 4 then describes the empirical study of REFINE-LM, and Section 5 discusses our results as well as avenues for future research.",
            "score": 0.5395420725833373,
            "section_title": "Introduction",
            "char_start_offset": 4027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 721
                },
                {
                    "start": 724,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1322
                },
                {
                    "start": 1325,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1762
                }
            ],
            "ref_mentions": [
                {
                    "start": 215,
                    "end": 219,
                    "matchedPaperCorpusId": "7137572"
                },
                {
                    "start": 219,
                    "end": 222,
                    "matchedPaperCorpusId": "234341131"
                },
                {
                    "start": 222,
                    "end": 225,
                    "matchedPaperCorpusId": "238586034"
                },
                {
                    "start": 835,
                    "end": 839,
                    "matchedPaperCorpusId": "222141056"
                },
                {
                    "start": 1253,
                    "end": 1257,
                    "matchedPaperCorpusId": "222141056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96435546875
        },
        {
            "corpus_id": "259370743",
            "title": "Causal-Debias: Unifying Debiasing in Pretrained Language Models and Fine-tuning via Causal Invariant Learning",
            "text": "Demographic biases and social stereotypes are common in pretrained language models (PLMs), and a burgeoning body of literature focuses on removing the unwanted stereotypical associations from PLMs. However, when fine-tuning these bias-mitigated PLMs in downstream natural language processing (NLP) applications, such as sentiment classification, the unwanted stereotypical associations resurface or even get amplified. Since pretrain&fine-tune is a major paradigm in NLP applications, separating the debiasing procedure of PLMs from fine-tuning would eventually harm the actual downstream utility. In this paper, we propose a unified debiasing framework Causal-Debias to remove unwanted stereotypical associations in PLMs during fine-tuning. Specifically, CausalDebias mitigates bias from a causal invariant perspective by leveraging the specific downstream task to identify bias-relevant and labelrelevant factors. We propose that bias-relevant factors are non-causal as they should have little impact on downstream tasks, while labelrelevant factors are causal. We perform interventions on non-causal factors in different demographic groups and design an invariant risk minimization loss to mitigate bias while maintaining task performance. Experimental results on three downstream tasks show that our proposed method can remarkably reduce unwanted stereotypical associations after PLMs are finetuned, while simultaneously minimizing the impact on PLMs and downstream applications.",
            "score": 0.5395399484900777,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9384765625
        },
        {
            "corpus_id": "276928412",
            "title": "BiasEdit: Debiasing Stereotyped Language Models via Model Editing",
            "text": "In recent years, many studies have underscored the tendency of pre-trained language models (LMs) to have societally stereotypical biases (Liang et al., 2021;Smith et al., 2022;Cheng et al., 2023a;Liu et al., 2023), such as gender bias (Sun et al., 2019;Zhao et al., 2020), race bias (Halevy et al., 2021), religion bias (Das et al., 2023;Manzini et al., 2019), among others. Therefore, eliminating biases from models is crucial to ensure fairness and accuracy in applications of language models. Girls tend to be more than boys . Many methods have been proposed to mitigate bias, such as fine-tuning entire models (Zmigrod et al., 2019;Barikeri et al., 2021) with counterfactual data obtained by swapping out bias attribute words,2 which is partly effective but costly in terms of computational time and space, especially for large language models (LLMs). Others implement debiasing with representation projection (Ravfogel et al., 2020;Liang et al., 2020;Limisiewicz and Marecek, 2022;Iskander et al., 2023) or prompting (Sheng et al., 2020;Schick et al., 2021;Mattern et al., 2022;Venkit et al., 2023). However, without parameter modification, a model remains inherently biased and can not be applied to downstream tasks as an off-the-shelf unbiased model. Recent methods (Kumar et al., 2023;Limisiewicz et al., 2024) employ model adapters where each adapter is trained to specialize only in one bias type. Multiple adapter training for different bias types is not economical for real-world applications. \n\nThese drawbacks inspire us to explore new methods for debiasing stereotyped language models more directly. Model editing (Yin et al., 2023;Wei et al., 2023;Zhang et al., 2024) can change specific information in language models by modifying model parameters, which could be effective in eliminating bias.",
            "score": 0.5389718464742164,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1506
                },
                {
                    "start": 1509,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1812
                }
            ],
            "ref_mentions": [
                {
                    "start": 137,
                    "end": 157,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 157,
                    "end": 176,
                    "matchedPaperCorpusId": "253224433"
                },
                {
                    "start": 176,
                    "end": 196,
                    "matchedPaperCorpusId": "258960243"
                },
                {
                    "start": 235,
                    "end": 253,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 253,
                    "end": 271,
                    "matchedPaperCorpusId": "218487087"
                },
                {
                    "start": 283,
                    "end": 304,
                    "matchedPaperCorpusId": "237940142"
                },
                {
                    "start": 320,
                    "end": 338,
                    "matchedPaperCorpusId": "258486951"
                },
                {
                    "start": 338,
                    "end": 359,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 614,
                    "end": 636,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 636,
                    "end": 658,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 914,
                    "end": 937,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 937,
                    "end": 956,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 986,
                    "end": 1008,
                    "matchedPaperCorpusId": "258740820"
                },
                {
                    "start": 1022,
                    "end": 1042,
                    "matchedPaperCorpusId": "218470535"
                },
                {
                    "start": 1042,
                    "end": 1062,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 1083,
                    "end": 1103,
                    "matchedPaperCorpusId": "256616034"
                },
                {
                    "start": 1274,
                    "end": 1294,
                    "matchedPaperCorpusId": "256827819"
                },
                {
                    "start": 1294,
                    "end": 1319,
                    "matchedPaperCorpusId": "264590288"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94287109375
        },
        {
            "corpus_id": "268512691",
            "title": "Detecting Bias in Large Language Models: Fine-tuned KcBERT",
            "text": "As the advancement of inter-country transportation and the increasing prevalence of multicultural households, society is becoming more diverse, highlighting the heightened necessity for integration [1,2].Various forms of social discrimination based on ethnicity, gender, and race have emerged as the primary impediments to social integration [3,4].This is particularly evident in unfiltered online language, such as comments on social media platforms like Twitter or YouTube, which significantly influences human perception [5].Importantly, this influence extends beyond humans to large language models (LLMs) trained on online language.Recent developments in LLMs, capable of analyzing and generating text similar to human language, have led to advancements in various natural language processing technologies.LLMs can now be used as pre-trained models for fine-tuning specific functionalities with relatively small datasets at the application level.Even without fine-tuning, base models alone can perform diverse tasks such as text generation, question-answering, and natural language inference [6,7,8].While extensive research has been conducted on the utility of LLMs in exploring various aspects, a thorough investigation into the potential risks arising from biases that may adversely affect users has been lacking.\n\nIn this paper, Investigates the societal biases of a model fine-tuned on Korean comments using Bidirectional Encoder Representations from Transformers (KcBERT) [9] and the Korean Offensive Language Dataset (KOLD) [10].Fig. 1 illustrates instances of various societal biases based on English (EN) and Korean (KO) using the Masked Language Modeling (MLM) method of KcBERT [11].For instance, adversarial biases emerged due to conflicts such as the U.S.-Afghanistan war, Korea-Japan relations, and the North Korean war, and these negative contexts are reflected in the ethnic biases of English (EN-1) and Korean (KO-1).This signifies a reflection of the historical and societal context of the respective countries [12].Perceptions that certain occupations are more suitable for either males or females are reflected in the gender biases of English (EN-2) and Korean (KO-2), contributing to the formation of fixed stereotypes about those occupations [13].",
            "score": 0.5387737655166451,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 204,
                    "end": 348
                },
                {
                    "start": 348,
                    "end": 528
                },
                {
                    "start": 528,
                    "end": 637
                },
                {
                    "start": 637,
                    "end": 811
                },
                {
                    "start": 811,
                    "end": 951
                },
                {
                    "start": 951,
                    "end": 1105
                },
                {
                    "start": 1105,
                    "end": 1321
                },
                {
                    "start": 1323,
                    "end": 1541
                },
                {
                    "start": 1541,
                    "end": 1698
                },
                {
                    "start": 1698,
                    "end": 1938
                },
                {
                    "start": 1938,
                    "end": 2038
                },
                {
                    "start": 2038,
                    "end": 2273
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 203,
                    "matchedPaperCorpusId": "237056719"
                },
                {
                    "start": 342,
                    "end": 345,
                    "matchedPaperCorpusId": "34105191"
                },
                {
                    "start": 345,
                    "end": 347,
                    "matchedPaperCorpusId": "13069074"
                },
                {
                    "start": 524,
                    "end": 527,
                    "matchedPaperCorpusId": "15775672"
                },
                {
                    "start": 1100,
                    "end": 1102,
                    "matchedPaperCorpusId": "231719337"
                },
                {
                    "start": 1102,
                    "end": 1104,
                    "matchedPaperCorpusId": "201670701"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94970703125
        },
        {
            "corpus_id": "273811362",
            "title": "Identifying Implicit Social Biases in Vision-Language Models",
            "text": "Bias in Foundation Models It is important to study the biases of language models and vision models separately because the biases of a VL model may be the result of the biases of the individual models as well as the interaction between them. \n\nBias in Language Models. Language models, particularly those trained on large amounts of text data, have been shown to exhibit biases that reflect negative stereotypes about certain social groups. These biases can manifest in the form of undesirable associations in word embeddings (Lauscher and Glava\u0161 2019;Caliskan, Bryson, and Narayanan 2017;Bolukbasi et al. 2016) and contextual embeddings (May et al. 2019;Guo and Caliskan 2020;Kurita et al. 2019;Nadeem, Bethke, and Reddy 2020). Efforts to debias language models have primarily focused on postprocessing approaches, such as data augmentation or collection (Dinan et al. 2020;Dodge et al. 2021), adversarial trigger prompts (Sheng et al. 2020), and different objective functions (Qian et al. 2019;Huang et al. 2020). However, these methods are not scalable to large pre-trained language models, which require significant resources and time to retrain. \n\nBias in language models can also manifest in the generated text, and lead to human-aligned values such as social bias implications (Sap et al. 2020) and toxic speech (Gehman et al. 2020). To address this issue, efficient post-processing approaches to mitigate bias without retraining the model has been proposed (Nadeem, Bethke, and Reddy 2020; Sheng et al. 2020). However, biases in language models go beyond just the representation and generation of text, as systems can also exhibit allocational harms that result in unfair allocation of resources or opportunities to different social groups (Barocas et al. 2017) and correlations between system behavior and features associated with marginalized groups (Cho et al. 2019). These issues require further investigation in future work. \n\nBias in Vision Models.",
            "score": 0.5377828202689645,
            "section_title": "Appendices Related Work",
            "char_start_offset": 43769,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 1936
                },
                {
                    "start": 1939,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 551,
                    "end": 588,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 676,
                    "end": 695,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 855,
                    "end": 874,
                    "matchedPaperCorpusId": "207852875"
                },
                {
                    "start": 922,
                    "end": 941,
                    "matchedPaperCorpusId": "218470535"
                },
                {
                    "start": 977,
                    "end": 995,
                    "matchedPaperCorpusId": "170078973"
                },
                {
                    "start": 995,
                    "end": 1012,
                    "matchedPaperCorpusId": "207847197"
                },
                {
                    "start": 1318,
                    "end": 1337,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 1497,
                    "end": 1514,
                    "matchedPaperCorpusId": "218470535"
                },
                {
                    "start": 1747,
                    "end": 1768,
                    "matchedPaperCorpusId": "216228079"
                },
                {
                    "start": 1859,
                    "end": 1875,
                    "matchedPaperCorpusId": "246652218"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95703125
        },
        {
            "corpus_id": "261101183",
            "title": "CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias",
            "text": "As language models (LMs) become increasingly powerful and widely used, it is important to quantify them for sociodemographic bias with potential for harm. Prior measures of bias are sensitive to perturbations in the templates designed to compare performance across social groups, due to factors such as low diversity or limited number of templates. Also, most previous work considers only one NLP task. We introduce Comprehensive Assessment of Language Models (CALM) for robust measurement of two types of universally relevant sociodemographic bias, gender and race. CALM integrates sixteen datasets for question-answering, sentiment analysis and natural language inference. Examples from each dataset are filtered to produce 224 templates with high diversity (e.g., length, vocabulary). We assemble 50 highly frequent person names for each of seven distinct demographic groups to generate 78,400 prompts covering the three NLP tasks. Our empirical evaluation shows that CALM bias scores are more robust and far less sensitive than previous bias measurements to perturbations in the templates, such as synonym substitution, or to random subset selection of templates. We apply CALM to 20 large language models, and find that for 2 language model series, larger parameter models tend to be more biased than smaller ones. The T0 series is the least biased model families, of the 20 LLMs investigated here. The code is available at https://github.com/vipulgupta1011/CALM.",
            "score": 0.5376019895884758,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93603515625
        },
        {
            "corpus_id": "276317810",
            "title": "SB-Bench: Stereotype Bias Benchmark for Large Multimodal Models",
            "text": "Large Multimodal Models (LMMs) are an advanced extension of Large Language Models (LLMs) that enable AI systems to process and integrate both text and images. These models have demonstrated impressive capabilities in tasks such as image captioning, visual question answering, and multimodal reasoning (Li et al., 2024b;Mahmood et al., 2024). By combining textual and visual information, LMMs offer enhanced comprehension and analysis, making them valuable for a wide range of applications. However, alongside their advancements, LMMs also inherit biases from their training data, which can lead to the reinforcement of stereotypes and social inequities. \n\nBias in LMMs is particularly concerning in real-world applications, where fairness and inclusivity are essential for equi- Table 1: Comparison of various LMM evaluation benchmarks with a focus on stereotype bias. Our approach is one of only three to assess nine bias types, is based on real images, unlike B-AVIBench, and unlike the Open-Ended BiasDora is easy to evaluate because of its Multiple-Choice design. The Question Types are classified as 'ITM' (Image-Text Matching), 'OE' (Open-Ended) or MCQ (Multiple-Choice). \n\ntable outcomes. Existing biases in training data often manifest in model responses, leading to unintended but impactful consequences. Addressing these biases is crucial to ensuring that LMMs contribute positively to society while minimizing potential harms. Researchers have attempted to analyze and mitigate these biases through various benchmarks and evaluation frameworks. However, current approaches typically categorize biases within a limited set of domains and often rely on synthetic datasets, which fail to capture the complexity of bias in real-world scenarios (Fraser & Kiritchenko, 2024;Zhou et al., 2022;Howard et al., 2024b;Raza et al., 2025). \n\nRecent studies have introduced several bias evaluation benchmarks, such as VLStereoset and Social Counterfactuals, to assess bias in LMMs. While these benchmarks have contributed valuable insights, they suffer from notable limitations. Many of these approaches lack diversity in bias categories, restricting their effectiveness and generalizability.",
            "score": 0.5375798786897148,
            "section_title": "Introduction",
            "char_start_offset": 816,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1177
                },
                {
                    "start": 1180,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1837
                },
                {
                    "start": 1840,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2189
                }
            ],
            "ref_mentions": [
                {
                    "start": 1797,
                    "end": 1818,
                    "matchedPaperCorpusId": "265609296"
                },
                {
                    "start": 1818,
                    "end": 1836,
                    "matchedPaperCorpusId": "276317412"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96240234375
        },
        {
            "corpus_id": "247762275",
            "title": "The SAME score: Improved cosine based measure for semantic bias",
            "text": "Over the last years, language models (LM) have become increasingly large and popular due to their superior performance and adaptability to many downstream tasks. Meanwhile many researchers have raised ethical concerns regarding social biases incorporated in such models. Plenty of methods for bias measurements have been introduced since, including some that measure semantic bias in word embeddings [4,5], methods that query the model intrinsic likelihoods [11,17,18] or adaptions of common fairness measures [3,12] for downstream tasks. In recent years, there has been criticism about the focus on pretrained language models (PLM) and their intrinsic or semantic biases in general with the justification that these do not correlate with downstream bias or that testing conditions are too unreliable [9,13,14]. To some extend this can be expected as there are many possible causes for biases [10]. At the same time, many methods to measure social bias are only motivated on an intuitive level and validated with limited empirical studies at best. However, some works have introduced benchmarks for bias score reliability [8,20] or theoretical criteria for bias scores [6]. In that context, it has been shown that WEAT [4], a very prominent bias score for word embeddings, has limitations regarding bias quantification [6,19]. Prompted by these findings, we introduce Scoring Association Means of Word Embeddings (SAME), a novel bias score for semantic bias in word or text embeddings. We use criteria set up in the literature to make certain that SAME is well suited to quantify semantic bias and validate its performance compared to similar bias scores in a thorough empirical analysis. We can show that it outperforms similar bias scores and does relate to downstream bias to a realistic extent considering that there are other causes for downstream bias. Our contributions are: \n\n(i) We propose SAME, a new bias score for word or text embeddings; (ii) We formally show that SAME has necessary properties for bias quantification introduced in [6]; (iii) We conduct experiments to support our theoretical findings and show that SAME outperforms existing bias scores in practice.",
            "score": 0.5371307576219735,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 1881
                },
                {
                    "start": 1884,
                    "end": 2180
                }
            ],
            "ref_mentions": [
                {
                    "start": 400,
                    "end": 403,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 403,
                    "end": 405,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 510,
                    "end": 513,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 513,
                    "end": 516,
                    "matchedPaperCorpusId": "75135222"
                },
                {
                    "start": 801,
                    "end": 804,
                    "matchedPaperCorpusId": "248780439"
                },
                {
                    "start": 1169,
                    "end": 1172,
                    "matchedPaperCorpusId": "267311931"
                },
                {
                    "start": 1219,
                    "end": 1222,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1319,
                    "end": 1322,
                    "matchedPaperCorpusId": "267311931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8916015625
        },
        {
            "corpus_id": "274423106",
            "title": "How far can bias go? - Tracing bias from pretraining data to alignment",
            "text": "The impressive performance of Large Language Models (LLMs) across Natural Language Processing (NLP) tasks, ranging from question-answering to news summarization, has led to their widespread adoption. They have become essential components of applications such as chatbots designed to simulate human conversation (Ferrara, 2023). \n\nHowever, despite their appeal, LLMs have faced criticism for perpetuating and amplifying societal biases (Bommasani et al., 2021;Weidinger et al., 2021). They are believed to reflect and reinforce the biases present in the vast, uncurated corpora used for their training (Bender et al., 2021). These biases can lead to discriminatory and harmful outcomes, particularly for marginalized groups (Spol-  sky, 1998;Noble, 2018). Documented instances include biased resource allocation based on ethnicity (Jackson and Mendoza, 2020;Obermeyer et al., 2019), job discrimination (Kassir et al., 2023;Armstrong et al., 2024), and reinforcement of harmful stereotypes related to gender (Dastin, 2022;Chen, 2023;Lambrecht and Tucker, 2018). \n\nResearch on bias in NLP and LLMs has focused on intrinsic bias in model representations (Bolukbasi et al., 2016;Caliskan et al., 2017;Garg et al., 2018;Gupta et al., 2024) or at the output level (i.a., Schick et al., 2021;Leidinger and Rogers, 2024), often overlooking the impact of pretraining data on model outputs for specific tasks. Recent studies (K\u00f6ksal et al., 2023;Touvron et al., 2023;Orgad and Belinkov, 2022) have explored the connection between training data bias and model bias, but have been hampered by restricted access to training data for commercial LLMs (Solaiman, 2023). Most bias research is constrained to public datasets like CommonCrawl, Wikipedia (Schwenk et al., 2021), and mC4 (Xue et al., 2021).",
            "score": 0.5365581267727983,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 327
                },
                {
                    "start": 330,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1785
                }
            ],
            "ref_mentions": [
                {
                    "start": 601,
                    "end": 622,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 741,
                    "end": 753,
                    "matchedPaperCorpusId": "150449904"
                },
                {
                    "start": 830,
                    "end": 857,
                    "matchedPaperCorpusId": "219137989"
                },
                {
                    "start": 857,
                    "end": 880,
                    "matchedPaperCorpusId": "204881868"
                },
                {
                    "start": 901,
                    "end": 922,
                    "matchedPaperCorpusId": "258702749"
                },
                {
                    "start": 1214,
                    "end": 1233,
                    "matchedPaperCorpusId": "259164882"
                },
                {
                    "start": 1264,
                    "end": 1284,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 1414,
                    "end": 1435,
                    "matchedPaperCorpusId": "258832603"
                },
                {
                    "start": 1456,
                    "end": 1481,
                    "matchedPaperCorpusId": "250390436"
                },
                {
                    "start": 1635,
                    "end": 1651,
                    "matchedPaperCorpusId": "256697103"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92919921875
        },
        {
            "corpus_id": "248496154",
            "title": "Detoxifying Language Models with a Toxic Corpus",
            "text": "Language embeddings or LMs are prone to unintended biases against the under-represented minority groups and inherent toxicity (Bolukbasi et al., 2016;Manzini et al., 2019). Contextualized embeddings like ELMo and BERT have also proven to inherit biases, such as gender bias (Zhao et al., 2019(Zhao et al., , 2018. Language generation also suffers from varying types of social biases such as stereotypical bias (Liang et al., 2021) and sentiment bias (Huang et al., 2020).\n\nAlong with the detection of bias in language embeddings and models, various fairness benchmarking (Nangia et al., 2020;Dhamala et al., 2021) and debiasing approaches have been proposed. Bolukbasi et al. (2016) and Liang et al. (2020) proposed to find the hypothetical bias dimension in embedding spaces. Liu et al. (2020) proposed adversarial learning to disentangle biased and unbiased features in dialogue systems. While most of the work in fairness in NLP focuses on stereotypical biases, other studies focus on the toxicity of LMs (Gehman et al., 2020;Welbl et al., 2021;Schick et al., 2021), which are most relevant to our study.",
            "score": 0.5360485938110902,
            "section_title": "Bias in NLP",
            "char_start_offset": 2737,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 150,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 150,
                    "end": 171,
                    "matchedPaperCorpusId": "102350941"
                },
                {
                    "start": 274,
                    "end": 292,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 292,
                    "end": 312,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 410,
                    "end": 430,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 450,
                    "end": 470,
                    "matchedPaperCorpusId": "207847197"
                },
                {
                    "start": 592,
                    "end": 613,
                    "matchedPaperCorpusId": "231719337"
                },
                {
                    "start": 659,
                    "end": 682,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 687,
                    "end": 706,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 777,
                    "end": 794,
                    "matchedPaperCorpusId": "221970809"
                },
                {
                    "start": 1008,
                    "end": 1029,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 1029,
                    "end": 1048,
                    "matchedPaperCorpusId": "237513578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93115234375
        },
        {
            "corpus_id": "232185097",
            "title": "DebIE: A Platform for Implicit and Explicit Debiasing of Word Embedding Spaces",
            "text": "Ethical and fair natural language processing is an essential precondition for widespread societal adoption of language technologies. In recent years, however, distributional language representations built from large corpora have been shown to encode human-like biases, like racism and sexism (Bolukbasi et al., 2016;Zhao et al., 2019;Lauscher et al., 1 Videos demonstrating the usage of the DEBIE application and command-line tool are available at https://tinyurl. com/y2ymujus 2020a; Nadeem et al., 2020, inter alia). At the word level, most embedding spaces, across a range of embedding models and languages (Lauscher and Glava\u0161, 2019), encode human biases that can be exemplified in biased analogies, such as the famous example of sexism: homemaker (Bolukbasi et al., 2016). While this is not surprising, given the distributional nature of word representation models (Harris, 1954) it is -depending on the sociotechnical context -an undesired artefact of distributional representation learning (Blodgett et al., 2020) which can, in turn, lead to unfair decisions in downstream applications. A number of different measures for quantifying biases in representation spaces have been proposed in recent years (Caliskan et al., 2017;Gonen and Goldberg, 2019;Dev and Phillips, 2019;Garg et al., 2018;Lauscher et al., 2020a) and even more models for removing or attenuating such biases have been developed (Zhao et al., 2019;Bordia and Bowman, 2019;Dinan et al., 2020;Webster et al., 2020;Qian et al., 2019, inter alia). What is still missing, however, is the ability to seamlessly apply different bias measures and debiasing models on arbitrary embedding spaces and for custom (i.e., user-specified) bias specifications.\n\nIn this work, we address this gap by introducing DEBIE, the first integrated platform offering bias measurement and mitigation for arbitrary static embedding spaces and bias specifications. The DE-BIE platform is grounded in the general framework",
            "score": 0.5355585874480615,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 334,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 1231,
                    "end": 1256,
                    "matchedPaperCorpusId": "73729169"
                },
                {
                    "start": 1279,
                    "end": 1297,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 1297,
                    "end": 1320,
                    "matchedPaperCorpusId": "202572693"
                },
                {
                    "start": 1402,
                    "end": 1421,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 1445,
                    "end": 1464,
                    "matchedPaperCorpusId": "207852875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9111328125
        },
        {
            "corpus_id": "258887496",
            "title": "Uncovering and Categorizing Social Biases in Text-to-SQL",
            "text": "Large pre-trained language models are acknowledged to carry social bias towards different demographics, which can further amplify existing stereotypes in our society and cause even more harm. Text-to-SQL is an important task, models of which are mainly adopted by administrative industries, where unfair decisions may lead to catastrophic consequences. However, existing Text-to-SQL models are trained on clean, neutral datasets, such as Spider and WikiSQL. This, to some extent, cover up social bias in models under ideal conditions, which nevertheless may emerge in real application scenarios. In this work, we aim to uncover and mitigate social bias in Text-to-SQL models. We summarize the categories of social bias that may occur in structural data for Text-to-SQL models. We build test benchmarks and reveal that models with similar task accuracy can contain social bias at very different rates. We show how to take advantage of our methodology to assess and mitigate social bias in the downstream Text-to-SQL task.",
            "score": 0.5349300800115306,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9189453125
        },
        {
            "corpus_id": "268379141",
            "title": "Prompting Fairness: Integrating Causality to Debias Large Language Models",
            "text": "Large language models (LLMs), despite their remarkable capabilities, are susceptible to generating biased and discriminatory responses. As LLMs increasingly influence high-stakes decision-making (e.g., hiring and healthcare), mitigating these biases becomes critical. In this work, we propose a causality-guided debiasing framework to tackle social biases, aiming to reduce the objectionable dependence between LLMs' decisions and the social information in the input. Our framework introduces a novel perspective to identify how social information can affect an LLM's decision through different causal pathways. Leveraging these causal insights, we outline principled prompting strategies that regulate these pathways through selection mechanisms. This framework not only unifies existing prompting-based debiasing techniques, but also opens up new directions for reducing bias by encouraging the model to prioritize fact-based reasoning over reliance on biased social cues. We validate our framework through extensive experiments on real-world datasets across multiple domains, demonstrating its effectiveness in debiasing LLM decisions, even with only black-box access to the model.",
            "score": 0.5348809005859155,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98779296875
        },
        {
            "corpus_id": "272693925",
            "title": "Unveiling and Mitigating Bias in Large Language Model Recommendations: A Path to Fairness",
            "text": "Research on social biases in NLP models distinguishes between allocational and representational harms [31], [32], with recent studies focusing on methods to evaluate and mitigate these biases in Natural Language Understanding [33], [34] and Natural Language Generation [35], [36], [37] tasks. Metrics like the Odds Ratio (OR) [38] have been proposed to measure gender biases, particularly in items with large frequency differences or high saliency for different genders [39]. Studies have explored controlling NLG models to reduce biases [40], [41], but their applicability to closed APIbased LLMs remains uncertain. Recent works emphasize the importance of considering both social and technical aspects of AI systems to fully understand the sources of biases [42], [43]. Social science research highlights gender bias in professional documents, impacting application success rates and emphasizing the need for clear bias definitions and metrics in AI fairness [44]. \n\nThere is also significant work in analyzing cultural bias in language models (LMs). Recent efforts have examined the cultural alignment of LMs by exploring their encoded moral knowledge and ability to infer cultural variations in moral judgments [45], [46], [47]. Studies show that LMs often mirror specific societal values and ideologies, such as American liberalism [48], [49]. Research has also explored LMs' understanding of cross-cultural values, beliefs, and opinions on political and global topics [50], [51], [52]. The alignment of LMs has been quantified using cultural surveys and questions probing for culture-related commonsense knowledge. Results indicate that LMs tend to align with Western cultural values across multiple languages [53], [54]. Additionally, studies have explored LMs' knowledge of geo-diverse facts, cultural norms, culinary customs, and social norm reasoning [55], [56], [57]. \n\nThe most relevant papers to our study include the works in [22], [23], [25]. Wan et al. [22] primarily focused on gender bias in LLM-generated recommendation letters.",
            "score": 0.5345955757006438,
            "section_title": "A. Related Works",
            "char_start_offset": 7236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1878
                },
                {
                    "start": 1881,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2047
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 112,
                    "matchedPaperCorpusId": "249674329"
                },
                {
                    "start": 326,
                    "end": 330,
                    "matchedPaperCorpusId": "21344372"
                },
                {
                    "start": 760,
                    "end": 764,
                    "matchedPaperCorpusId": "248665660"
                },
                {
                    "start": 766,
                    "end": 770,
                    "matchedPaperCorpusId": "257833882"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "235336427"
                },
                {
                    "start": 1861,
                    "end": 1865,
                    "matchedPaperCorpusId": "252907608"
                },
                {
                    "start": 1867,
                    "end": 1871,
                    "matchedPaperCorpusId": "259858965"
                },
                {
                    "start": 1873,
                    "end": 1877,
                    "matchedPaperCorpusId": "266176303"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94921875
        },
        {
            "corpus_id": "248780440",
            "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
            "text": "Pretrained language models (PLMs), such as masked language models (MLMs), have achieved remarkable success in many natural language processing (NLP) tasks (Devlin et al., 2019;Liu et al., 2019;Lan et al., 2020;Brown et al.). Unfortunately, pretrained language models, which are trained on large human-written corpora, also inherit human-like biases and undesired social stereotypes (Caliskan et al., 2017;Bolukbasi et al., 2016;Blodgett et al., 2020). For example, in the fill-inthe-blank task, BERT (Devlin et al., 2019) substitutes [MASK] in the sentence \"The man/woman had a job as [MASK]\" with \"manager/receptionist\" respectively, reflecting occupational gender bias. \n\nThe human-like biases and stereotypes encoded in PLMs are worrisome as they can be propagated or even amplified in downstream NLP tasks such as sentiment classification (Kiritchenko and Mohammad, 2018), co-reference resolution (Zhao et al., 2019;Rudinger et al., 2018), clinical text classification (Zhang et al., 2020) and psychometric analysis (Abbasi et al., 2021;Ahmad et al., 2020). \n\nHowever, although it is important to mitigate biases in PLMs, debiasing masked language models such as BERT is still challenging, because the biases encoded in the contextualized models are hard to identify. To address this challenge, previous efforts seek to use additional corpora to retrieve the contextualized embeddings or locate the biases and then debias accordingly. For example, Liang et al. (2020); Kaneko and Bollegala (2021); Garimella et al. (2021) use external corpora to locate sentences containing the demographic-specific words (e.g., man and women) or stereotype words (e.g., manager and receptionist) and then use different debiasing losses to mitigate the biases. \n\nUsing external corpora to debias PLMs heavily relies on the quality of the corpora.",
            "score": 0.5345058650376403,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 671
                },
                {
                    "start": 674,
                    "end": 1061
                },
                {
                    "start": 1064,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1747
                },
                {
                    "start": 1750,
                    "end": 1833
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 176,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 193,
                    "end": 210,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 210,
                    "end": 223,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 382,
                    "end": 405,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 405,
                    "end": 428,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 428,
                    "end": 450,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 500,
                    "end": 521,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 843,
                    "end": 875,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 901,
                    "end": 920,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 920,
                    "end": 942,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 973,
                    "end": 993,
                    "matchedPaperCorpusId": "214590570"
                },
                {
                    "start": 1020,
                    "end": 1041,
                    "matchedPaperCorpusId": "243865626"
                },
                {
                    "start": 1041,
                    "end": 1060,
                    "matchedPaperCorpusId": "211525595"
                },
                {
                    "start": 1452,
                    "end": 1471,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 1473,
                    "end": 1500,
                    "matchedPaperCorpusId": "231698657"
                },
                {
                    "start": 1502,
                    "end": 1525,
                    "matchedPaperCorpusId": "236477795"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.943359375
        },
        {
            "corpus_id": "258959496",
            "title": "KoSBI: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Applications",
            "text": "To alleviate unsafe social bias of LLMs, we propose a large-scale social bias dataset related to safety addressing the Korean language and cultures, KOSBI. Our dataset covers 72 demographic groups in 15 categories, consisting of 34k of situation context and following sentence pairs. To construct KOSBI, we employ a human-LLM collaboration framework, where HyperCLOVA generates contexts and sentences, and human annotators label them as safe or unsafe. Extensive experiments present our dataset as differentiated from existing prevalent datasets on social bias and hate speech. Moreover, the results show the filter model trained with our dataset remarkably improves the ratio of generating safe sentences for various LLMs such as GPT-3 and HyperCLOVA with diverse model sizes, which presents the efficacy of our dataset.",
            "score": 0.5342019433390044,
            "section_title": "Conclusion",
            "char_start_offset": 22453,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92041015625
        },
        {
            "corpus_id": "247762275",
            "title": "The SAME score: Improved cosine based measure for semantic bias",
            "text": "With the enourmous popularity of large language models, many researchers have raised ethical concerns regarding social biases incorporated in such models. Several methods to measure social bias have been introduced, but apparently these methods do not necessarily agree regarding the presence or severity of bias. Furthermore, some works have shown theoretical issues or severe limitations with certain bias measures.For that reason, we introduce SAME, a novel bias score for semantic bias in embeddings. We conduct a thorough theoretical analysis as well as experiments to show its benefits compared to similar bias scores from the literature. We further highlight a substantial relation of semantic bias measured by SAME with downstream bias, a connection that has recently been argued to be negligible. Instead, we show that SAME is capable of measuring semantic bias and identify potential causes for social bias in downstream tasks.",
            "score": 0.5335008608493554,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95654296875
        },
        {
            "corpus_id": "271902917",
            "title": "REFINE-LM: Mitigating Language Model Stereotypes via Reinforcement Learning",
            "text": "Bias is evaluated by human annotators on the LM's answers for sentence completion and summarization tasks. A more recent effort [21] fine-tunes pre-trained LMs by minimizing the distributional disagreement between the completions for different values of the sensitive attribute, e.g., by minimizing the difference in the distribution of professions associated to male vs. female prompts. Albeit more efficient than full retrain- ing, fine-tuning can still be computationally unfeasible for very large pre-trained models. Hence, other approaches propose to debias the output of such models, via post-hoc regularization layers [28,29] or self-debiasing techniques that require proper prompting [18,39]. REFINE-LM is also a post-training debiasing method, which defines bias via the UnQover framework [27] tailored for masked pre-trained LMs and several bias categories. Following a RL technique, our method enables, in particular, reducing training resources, avoiding manual annotation, and supporting a range of biases, including gender-occupation, ethnicity, nationality, and religion. In addition, it can be easily applicable to several small and large LMs.",
            "score": 0.5333952834639728,
            "section_title": "Related Work",
            "char_start_offset": 9434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1159
                }
            ],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 132,
                    "matchedPaperCorpusId": "248780440"
                },
                {
                    "start": 625,
                    "end": 629,
                    "matchedPaperCorpusId": "207996257"
                },
                {
                    "start": 629,
                    "end": 632,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 798,
                    "end": 802,
                    "matchedPaperCorpusId": "222141056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9462890625
        },
        {
            "corpus_id": "261048854",
            "title": "Systematic Offensive Stereotyping (SOS) Bias in Language Models",
            "text": "Elsafoury et al. (2022) introduce systemic offensive stereotyping (SOS) bias and propose a method to measure in static word embeddings. However, the SOS bias has not been yet investigated in LMs. \n\nOn the other hand, there are various metrics in the literature to systematically measure social bias in LMs like SEAT (May et al., 2019), CrowS-Pairs (Nangia et al., 2020), and StereoSet (Nadeem et al., 2021). In SEAT, the authors, inspired by the WEAT metric (Caliskan et al., 2017) to measure bias in static word embeddings, propose a method to measure social bias in LMs. The authors propose to compare sets of sentences using cosine similarity instead of words, as with the WEAT metric. To extend the word level to a sentence level, SEAT slots each word in the seed words used by WEAT in semantically bleached sentence templates. Similarly, CrowS-Pairs and StereoSet metrics are used to measure social bias in LMs. But instead of sentence templates, the authors use crowdsourced sentences and the MLM task to measure the social bias. The Crows-Pairs dataset contains 1,508 sentence pairs (stereotypical and non-stereotypical) and measures nine types of social bias. The StereoSet dataset contains 8,498 sentence pairs to measure four types of social bias. \n\nTo measure the systematic offensive stereotyping (SOS) bias in LMs, these metrics will fall short since the crowdsourced sentences contain socially stereotypical versus non-stereotypical sentences. In this paper, we mitigate the limitations of the current literature by proposing a method to measure SOS bias in LMs. We build on existing social bias metrics but instead of using stereotypical and non-stereotypical sentence-pairs, we create a new dataset of profane and non-profane sentence-pairs to measure SOS bias.",
            "score": 0.5333489694907009,
            "section_title": "Background",
            "char_start_offset": 5772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 195
                },
                {
                    "start": 198,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1257
                },
                {
                    "start": 1260,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1777
                }
            ],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 334,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 348,
                    "end": 369,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 385,
                    "end": 406,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 458,
                    "end": 481,
                    "matchedPaperCorpusId": "23163324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90673828125
        },
        {
            "corpus_id": "265281188",
            "title": "Diagnosing and Debiasing Corpus-Based Political Bias and Insults in GPT2",
            "text": "The training of large language models (LLMs) on extensive, unfiltered corpora sourced from the internet is a common and advantageous practice. Consequently, LLMs have learned and inadvertently reproduced various types of biases, including violent, offensive, and toxic language. However, recent research shows that generative pretrained transformer (GPT) language models can recognize their own biases and detect toxicity in generated content, a process referred to as self-diagnosis. In response, researchers have developed a decoding algorithm that allows LLMs to self-debias, or reduce their likelihood of generating harmful text. This study investigates the efficacy of the diagnosing-debiasing approach in mitigating two additional types of biases: insults and political bias. These biases are often used interchangeably in discourse, despite exhibiting potentially dissimilar semantic and syntactic properties. We aim to contribute to the ongoing effort of investigating the ethical and social implications of human-AI interaction.",
            "score": 0.5327445964416467,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96484375
        },
        {
            "corpus_id": "271923841",
            "title": "Identifying and Mitigating Social Bias Knowledge in Language Models",
            "text": "Existing debiasing methods cannot retain individual commonsense knowledge. The debiasing results are delineated in Table 1 and Table 3. It is observed that all debiasing baselines fail to yield satisfactory results in knowledge retention (i.e., RS), which proves our claim that group-invariant methods compromise the individual knowledge to distinguish between different social groups. \n\nOur approach surpasses baselines in both bias mitigation and knowledge retention. As shown in Table 1 and Table 3, our proposed FAST is the first to achieve near-perfect bias mitigation (i.e., SS lower than 52 for BERT) on the two evaluating datasets, while SS of existing approaches, in terms of gender, are still higher than 56. Further, FAST can also largely retain a high RS, and achieve the highest LMS and ICAT. This demonstrates the effectiveness of our fine-grained calibration strategy towards eliminating social biases in PLMs. In addition, we report the performance of knowledgeediting approaches ROME and MEMIT. It can be discerned that neither ROME nor MEMIT signif-  Our approach scales to larger models. In order to further validate the scalability of FAST, we conduct additional experiments on larger models, i.e., GPT2-XL, GPT-Neo-2.7B, and Llama-2-7B, with results reported in Table 4. After debiasing, FAST induces a significant reduction (9.4 in average) in SS, and a great improvement in ICAT. Meanwhile, FAST can also retain the Retention Score for larger language models. These demonstrate the consistent effectiveness and scalability of FAST. Our approach retains language modeling capability while mitigating bias. As shown in Table 2, FAST achieves better downstream performance than 5 out of 6 baselines on average, indicating that FAST retains language modeling capabilities while mitigating biases. In summary, these results substantiate that FAST addresses the proposed issue in existing methods where the pursuit of equity compromises the preservation of other existing knowledge. Moreover, empirical evidence confirms the effectiveness of our localize-and-mitigate framework in identifying and mitigating specific biased knowledge, thereby validating Assumption 1.",
            "score": 0.5325401753340291,
            "section_title": "Debiasing Performance",
            "char_start_offset": 16211,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 385
                },
                {
                    "start": 388,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2184
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9267578125
        },
        {
            "corpus_id": "271855546",
            "title": "MABR: Multilayer Adversarial Bias Removal Without Prior Bias Knowledge",
            "text": "Research suggests various methods for mitigating social biases in NLP models applied to downstream tasks. Some approaches focus on preprocessing the training data, such as converting biased words to neutral alternatives (De-Arteaga et al. 2019) or to those that counteract bias (Zhao et al. 2018), or balancing each demographic group in training (Zhao et al. 2018;Wang et al. 2019;Lahoti et al. 2020;Han, Baldwin, and Cohn 2022). Others focus on removing demographic information from learned representations, for instance, by applying post-hoc methods to the neural representations of a trained model (Ravfogel et al. 2020(Ravfogel et al. , 2022;;Iskander, Radinsky, and Belinkov 2023). Adversarial training is also a common strategy (Li, Baldwin, and Cohn 2018;Zhang, Lemoine, and Mitchell 2018;Elazar and Goldberg 2018;Wang et al. 2019;Han, Baldwin, and Cohn 2021b). However, all these methods require prior knowledge of the specific bias to be addressed, such as gender bias. Furthermore, many of these approaches depend on demographic annotations for each data instance. For example, to address gender bias, each data sample must be annotated to indicate whether it pertains to a male or female subject. In contrast, our method does not require any prior knowledge about the bias. Additionally, while the authors of these studies select hyperparameters based on the fairness metrics they aim to optimize, we choose our hyperparameters without explicitly measuring fairness metrics. \n\nRecent studies have also explored fairness in machine learning through alternative approaches, such as discovering intersectional unfairness (Xu et al. 2024), learning fairness across multiple subgroups (Shui et al. 2022b), and aligning representations implicitly for fair learning (Shui et al. 2022a). Other work in related areas has proposed leveraging prompt-based learning (Yin, Wang, and Ling 2024) or masking mechanisms (Yin et al. 2024) to mitigate domain gaps.",
            "score": 0.532043485246865,
            "section_title": "Related Work",
            "char_start_offset": 4967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1485
                },
                {
                    "start": 1488,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1956
                }
            ],
            "ref_mentions": [
                {
                    "start": 220,
                    "end": 244,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 278,
                    "end": 296,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 346,
                    "end": 364,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 364,
                    "end": 381,
                    "matchedPaperCorpusId": "195847929"
                },
                {
                    "start": 381,
                    "end": 400,
                    "matchedPaperCorpusId": "219980622"
                },
                {
                    "start": 400,
                    "end": 428,
                    "matchedPaperCorpusId": "247694107"
                },
                {
                    "start": 601,
                    "end": 622,
                    "matchedPaperCorpusId": "215786522"
                },
                {
                    "start": 622,
                    "end": 647,
                    "matchedPaperCorpusId": "246411428"
                },
                {
                    "start": 647,
                    "end": 685,
                    "matchedPaperCorpusId": "258740820"
                },
                {
                    "start": 734,
                    "end": 762,
                    "matchedPaperCorpusId": "21721649"
                },
                {
                    "start": 762,
                    "end": 796,
                    "matchedPaperCorpusId": "9424845"
                },
                {
                    "start": 796,
                    "end": 821,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 821,
                    "end": 838,
                    "matchedPaperCorpusId": "195847929"
                },
                {
                    "start": 838,
                    "end": 867,
                    "matchedPaperCorpusId": "231698605"
                },
                {
                    "start": 1629,
                    "end": 1645,
                    "matchedPaperCorpusId": "270199352"
                },
                {
                    "start": 1691,
                    "end": 1710,
                    "matchedPaperCorpusId": "253018377"
                },
                {
                    "start": 1770,
                    "end": 1789,
                    "matchedPaperCorpusId": "249097883"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66552734375
        },
        {
            "corpus_id": "266054040",
            "title": "Layered Bias: Interpreting Bias in Pretrained Large Language Models",
            "text": "Large language models (LLMs) like GPT and PALM have excelled in numerous natural language processing (NLP) tasks such as text generation, question answering, and translation. However, they are also found to have inherent social biases. To address this, recent studies have proposed debiasing techniques like iterative nullspace projection (INLP) and Counterfactual Data Augmentation (CDA). Additionally, there\u2019s growing interest in understanding the intricacies of these models. Some researchers focus on individual neural units, while others examine specific layers. In our study, we benchmark newly released models, assess the impact of debiasing methods, and investigate how biases are linked to different transformer layers using a method called Logit Lens. Specifically, we evaluate three modern LLMs: OPT, LLaMA, and LLaMA2, and their debiased versions. Our experiments are based on two popular bias evaluation datasets, StereoSet and CrowS-Pairs, and we perform a layer-by-layer analysis using the Logit Lens.",
            "score": 0.5319495930978824,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97509765625
        },
        {
            "corpus_id": "256630982",
            "title": "WordTies: Measuring Word Associations in Language Models via Constrained Sampling",
            "text": "The study of Rodriguez and Merlo (2020) is mostly similar to ours, where they concluded that properties of human word associations, discovered in the 1970s (Tversky, 1977;Tversky and Gati, 1978;Tversky and Hutchinson, 1986), still hold in language models. They probed associations by ranking words by the cosine similarity of embeddings in the vocabulary layer, and measured asymmetry by handcrafted templates. Evert and Lapesa (2021) also tested word associations with word embeddings, but they held the same view as us that it is self-contradictory to obtain decontextualized embeddings from contextualized LM, and therefore did not extend their study on LMs. Measuring and mitigating social biases in pre-trained LMs, often formulated as measuring associations to a certain set of words, is a more popular task. Associations to the words related to social aspects are often measured by the cosine similarity of embeddings aggregated from context sentences (May et al., 2019;Bommasani et al., 2020;Kaneko and Bollegala, 2021). As we have been arguing, cosine similarity is not compatible with the asymmetry of word associations, while our algorithm takes asymmetry into consideration. In some work, biases are also measured via constrained generation, where the constraints (often prompts or templates) are collected from the web (Dhamala et al., 2021) or by crowdsourcing (Nangia et al., 2020). In comparison, our method relies on no external resources, and no confounder is introduced consequently. \n\nConstrained text generation is used to evaluate the commonsense reasoning ability of LMs through other tasks. CommonGen (Lin et al., 2020) is a task where, instead of only one cue word as in our study, multiple words pertaining to commonsense concepts are required to be present in the generated text, as a way to measure how well LMs can link concepts together with commonsense knowledge.",
            "score": 0.5315008993886876,
            "section_title": "Related Work",
            "char_start_offset": 23079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1502
                },
                {
                    "start": 1505,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1894
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 39,
                    "matchedPaperCorpusId": "226283782"
                },
                {
                    "start": 156,
                    "end": 171,
                    "matchedPaperCorpusId": "9173202"
                },
                {
                    "start": 194,
                    "end": 223,
                    "matchedPaperCorpusId": "17034191"
                },
                {
                    "start": 959,
                    "end": 977,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 977,
                    "end": 1000,
                    "matchedPaperCorpusId": "220046499"
                },
                {
                    "start": 1332,
                    "end": 1354,
                    "matchedPaperCorpusId": "231719337"
                },
                {
                    "start": 1375,
                    "end": 1396,
                    "matchedPaperCorpusId": "222090785"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80126953125
        },
        {
            "corpus_id": "269449709",
            "title": "Bias Neutralization Framework: Measuring Fairness in Large Language Models with Bias Intelligence Quotient (BiQ)",
            "text": "To create a more exhaustive formula for detecting, measuring, and evaluating biases in LLMs, incorporating knowledge from various papers on bias detection and mitigation, we refine and extend the Large Language Model Bias Index (LLMBI) [3]",
            "score": 0.5313825477882341,
            "section_title": "Bias Intelligence Quotient (BiQ)",
            "char_start_offset": 9690,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96728515625
        },
        {
            "corpus_id": "277667520",
            "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
            "text": "Large Language Models (LLMs) have empowered artificial intelligence with their remarkable natural language understanding and generation capabilities, enabling breakthroughs in tasks such as machine translation, summarization, and human-like conversation [1,2]. However, their increasing integration into societal domainsincluding healthcare [3], education [4], and law [5]-has amplified concerns about embedded biases. These biases, which can manifest in various forms, risk perpetuating stereotypes, marginalizing underrepresented groups, and undermining ethical AI deployment [6]. Biases may stem from various sources, including biased training data that reflects historical inequalities and prejudicial associations, linguistic imbalances in corpora, flaws in algorithmic design, and the uncritical use of AI systems [7,8]. Previous studies have quantified biased attitudes in language models related to various social groups [9,10], also finding that state-of-the-art LLMs can be manipulated via adversarial attacks to produce biased or harmful responses, despite their bias mitigation and alignment mechanisms [11]. These challenges necessitate rigorous methodologies for evaluating and mitigating biases while ensuring models remain robust against adversarial exploitation. However, current approaches to bias evaluation face critical limitations, including the substantial resources required for bias identification and mitigation, difficulties in acquiring representative datasets for safety assessment, and the absence of universally accepted bias metrics. \n\nTo address these gaps, this work proposes a scalable methodology for benchmarking LLMs against bias elicitation. Our approach follows a two-step process and leverages the LLM-as-a-Judge paradigm [12] to automate bias evaluation, reducing reliance on manual response annotation while ensuring scalability and reproducibility. The first step involves selecting a judge model based on its statistical agreement with human annotations on a curated dataset of prompt-response pairs. These pairs capture both biased and safe behaviors, providing a benchmark for evaluating model ability to discern harmful content. Once chosen, the judge model is used to systematically evaluate LLM robustness using bias-probing prompts across multiple sociocultural dimensions, encompassing both isolated and intersectional bias categories.",
            "score": 0.5313355550677041,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1565
                },
                {
                    "start": 1568,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2176
                },
                {
                    "start": 2177,
                    "end": 2387
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 257,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 257,
                    "end": 259,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 341,
                    "end": 344,
                    "matchedPaperCorpusId": "257312905"
                },
                {
                    "start": 356,
                    "end": 359,
                    "matchedPaperCorpusId": "265315253"
                },
                {
                    "start": 369,
                    "end": 372,
                    "matchedPaperCorpusId": "267413187"
                },
                {
                    "start": 578,
                    "end": 581,
                    "matchedPaperCorpusId": "258688053"
                },
                {
                    "start": 820,
                    "end": 823,
                    "matchedPaperCorpusId": "237298625"
                },
                {
                    "start": 823,
                    "end": 825,
                    "matchedPaperCorpusId": "261530629"
                },
                {
                    "start": 929,
                    "end": 932,
                    "matchedPaperCorpusId": "265212726"
                },
                {
                    "start": 932,
                    "end": 935,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 1115,
                    "end": 1119,
                    "matchedPaperCorpusId": "271097745"
                },
                {
                    "start": 1763,
                    "end": 1767,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "265497827",
            "title": "Measurement and Mitigation of Bias in Artificial Intelligence: A Narrative Literature Review for Regulatory Science",
            "text": "n AI model's parameters also present a notable bias challenge.Guo, Yang, and Abbasi 21 proposed an automatic method of mitigating social biases in pretrained language models.This method, Auto-Debias, consists of two stages: (i) automatically crafting biased prompts by maximizing disagreement between the masked language model completions, and (ii) leveraging these prompts to fine-tune the masked language model by minimizing the disagreement between its completions. 21SEAT was used to compare Auto-Debias to other debiasing techniques, including Sent-Debias.This new debiasing technique obtained the lowest average effect size across all tested methods, demonstrating that it produces an elevated debiasing performance.Overall, emerging debiasing technologies, such as those described above, can be used to reduce bias in several different areas within AI systems, including data representations, fine-tuning parameters, and algorithms.\n\nDue to their size and complexity, LLMs, including ChatGPT and GPT-4, may pose unique challenges regarding bias mitigation.Mitigating bias in these AI models, especially those that are not open source, will require a collaborative effort among AI developers, users, and affected communities.As outlined by Ferrara, 11 some potential avenues for mitigating bias in ChatGPT and similar models include the following: (i) engaging with disadvantaged communities during model development, (ii) collaborating with experts from multiple disciplines, (iii) considering user feedback and evaluation of model outputs, (iv) being open and transparent about the methodologies, data sources, and potential biases of the model, and (v) establishing partnerships between researchers and outside parties for the sharing of knowledge and best practices.With these strategies, and those discussed earlier, more fair and inclusive AI systems can be developed, leading to a world with less bias toward disadvantaged individuals or groups.",
            "score": 0.5310223208721417,
            "section_title": "REVIEW",
            "char_start_offset": 35495,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 62,
                    "end": 174
                },
                {
                    "start": 174,
                    "end": 471
                },
                {
                    "start": 471,
                    "end": 561
                },
                {
                    "start": 561,
                    "end": 722
                },
                {
                    "start": 722,
                    "end": 939
                },
                {
                    "start": 941,
                    "end": 1063
                },
                {
                    "start": 1063,
                    "end": 1231
                },
                {
                    "start": 1231,
                    "end": 1776
                },
                {
                    "start": 1776,
                    "end": 1958
                }
            ],
            "ref_mentions": [
                {
                    "start": 469,
                    "end": 471,
                    "matchedPaperCorpusId": "248780440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7548828125
        },
        {
            "corpus_id": "268041168",
            "title": "Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information",
            "text": "With the advent of large language models (LLMs), the societal biases inherent in these models have been extensively researched (Kolisko and Anderson, 2023;Zhou et al., 2022;Feng et al., 2023;Santy et al., 2023). These models learn human-like biases associated with race, gender, ethnicity, and others from human-written data (Schramowski et al., 2023;Peters and Matz, 2023). Although many studies have attempted to mitigate societal biases in LLMs (Barocas and Selbst, 2016;Mayson, 2018;Panch, Mattie, and Atun, 2019), recent research has explored methods to retrieve and induce these biases for understanding human opinions (Santurkar et al., 2023;Chu et al., 2023;Mills et al., 2023). Opinions about or from the perspective of certain human groups can be generated by models by conditioning the inherent societal biases of the model (Jiang et al., 2022;Simmons, 2022;Caron and Srivastava 2022;Hartmann et al., 2023). \n\nThese attempts to leverage the biases of LLMs have attracted the attention of social science researchers who conduct public opinion polls and surveys. The surveys commonly used in social science research were constrained in terms of the high cost and reliability of responses (Shapiro, 2019;Keeter et al., 2017;Kennedy et al., 2018). Consequently, several studies attempted to overcome the limitations of traditional surveys that use LLMs (Jansen et al., 2023;Brand et al., 2023;Grossmann et al., 2023). For example, some studies focused on conditioning language models with demographic information of human subpopulations for investigating whether language models can generate responses similar to actual respondents (Hwang et al., 2023;Kalinin, 2023) and simulate group-level opinions (Santurkar et al., 2023;Dominguez-Olmedo et al., 2023;Park et al., 2023;Bisbee et al., 2023).",
            "score": 0.5310043336608973,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 918
                },
                {
                    "start": 921,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1801
                }
            ],
            "ref_mentions": [
                {
                    "start": 448,
                    "end": 474,
                    "matchedPaperCorpusId": "143133374"
                },
                {
                    "start": 487,
                    "end": 517,
                    "matchedPaperCorpusId": "208355156"
                },
                {
                    "start": 666,
                    "end": 685,
                    "matchedPaperCorpusId": "259662249"
                },
                {
                    "start": 835,
                    "end": 855,
                    "matchedPaperCorpusId": "252280427"
                },
                {
                    "start": 1232,
                    "end": 1253,
                    "matchedPaperCorpusId": "148734837"
                },
                {
                    "start": 1360,
                    "end": 1381,
                    "matchedPaperCorpusId": "259491872"
                },
                {
                    "start": 1400,
                    "end": 1423,
                    "matchedPaperCorpusId": "259166062"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94677734375
        },
        {
            "corpus_id": "267372092",
            "title": "Shortcut Learning Explanations for Deep Natural Language Processing: A Survey on Dataset Biases",
            "text": "Machine learning uses data to create models that can evaluate the categories and attributes of new data. However, training data often includes biases in areas that we would prefer not to use for decision-making. Machine learning creates models that are accurate to training data, which might lead to the perpetuation of these unwanted biases. While the ability to generate coherent text is becoming more and more useful, it also encourages models to internalize social biases found in training corpora. Thus, there has been a lot of research interest in examining the social impact and fairness of text produced using language models [74]. A range of cultural connections and unfavorable social biases can be detected by NLP algorithms. Numerous NLP tasks showed comprehensive imbalances, including gender biases in word embedding [58] and [67], biases in sentiment analysis [74], and linguistic generational biases due to demographics [65]. In the ideal scenario, we would be able to create a model that accurately captures the generalizations from the data that are required for completing a task but does not discriminate in a manner that the models deem unjust. Several useful measurements for fairness have been defined in the work of training machine learning systems to output fair decisions: Conditional Demographic Parity, Counterfactual Fairness, Equalized Odds, and more. The context of adversarial debiasing is used to investigate these fairness measures [73]. In another study, counterfactual data augmentation was suggested as a solution to occupation-specific gender biases, and it was discovered that it can significantly better maintain model performance than debiasing word embeddings, notably in language modeling [75]. There are some points for comparative analysis of mitigating strategies to deal with shortcut learning in large language models by different authors as shown in Table 1.",
            "score": 0.5301222785166505,
            "section_title": "VI. COMPARATIVE ANALYSIS OF BIAS MEASURES FOR SHORTCUT LEARNING IN LARGE LANGUAGE MODELS",
            "char_start_offset": 40983,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1908
                }
            ],
            "ref_mentions": [
                {
                    "start": 634,
                    "end": 638,
                    "matchedPaperCorpusId": "207847197"
                },
                {
                    "start": 840,
                    "end": 844,
                    "matchedPaperCorpusId": "195316733"
                },
                {
                    "start": 875,
                    "end": 879,
                    "matchedPaperCorpusId": "207847197"
                },
                {
                    "start": 936,
                    "end": 940,
                    "matchedPaperCorpusId": "196185011"
                },
                {
                    "start": 1733,
                    "end": 1737,
                    "matchedPaperCorpusId": "51888520"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8984375
        },
        {
            "corpus_id": "266374755",
            "title": "Quantifying Bias in Text-to-Image Generative Models",
            "text": "Bias in AI is consistently highlighted as a potential issue in regard to fairness, explainability and AI regulation [7], [10], [15], [16], [17]. Bias can stem from various development processes including the construction of a dataset, the training of embedded machine learning models and the development of inference tools [1]. Generative model biases have been discussed extensively in large language models (LLMs). Ferrara discusses the challenges and risks of bias in ChatGPT, defining categories of bias and identifying risks to fairness and accountability [18]. Liang et al. define sources of representational biases and propose LLM benchmarks, identifying local and global biases in LLMs using sensitive tokens [19]. Abid et al. report a case of persistent anti-Muslim bias, showing prompts containing 'Muslim' often led to violent outputs [20]. \n\nLuccioni et al. [4] discuss cultural and gender biases present in Stable Diffusion and Dall-E 2 models through their StableBias method. They use captioning and visual question answering to extract gender and ethnic information from generated images [4], finding biases toward Caucasian and male groups. Cho et al. propose the 'DALL-Eval' method to measure social biases and visual reasoning skills of T2I models [3]. They propose using gender, skin-tone and image attributes to measure social biases. Seshadri et al. and Naik et al. [5], [6] discuss social biases of T2I models, with [6] focusing on gender imbalances and [5] identifying gender, race, age and geographic biases. The above works identify social bias as a key concern but fail to capture and quantify general T2I model biases. \n\nWhile the removal of all biases is near impossible, mitigation and detection strategies do exist [1], [7]. Qraitem et al. propose a bias mimicking technique to improve representation within datasets [24]. Garcia   We evaluate four T2I models (stable diffusion v2.0 variant performs similarly), reporting the number of times 'n i ' an object 'w i ' is detected.",
            "score": 0.5298909347736914,
            "section_title": "RELATED WORK 2.1 Generative Model Biases",
            "char_start_offset": 3887,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1645
                },
                {
                    "start": 1648,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 119,
                    "matchedPaperCorpusId": "254221195"
                },
                {
                    "start": 121,
                    "end": 125,
                    "matchedPaperCorpusId": "198967636"
                },
                {
                    "start": 127,
                    "end": 131,
                    "matchedPaperCorpusId": "204824113"
                },
                {
                    "start": 133,
                    "end": 137,
                    "matchedPaperCorpusId": "246595433"
                },
                {
                    "start": 139,
                    "end": 143,
                    "matchedPaperCorpusId": "248987532"
                },
                {
                    "start": 717,
                    "end": 721,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 846,
                    "end": 850,
                    "matchedPaperCorpusId": "231603388"
                },
                {
                    "start": 870,
                    "end": 873,
                    "matchedPaperCorpusId": "268064656"
                },
                {
                    "start": 1103,
                    "end": 1106,
                    "matchedPaperCorpusId": "268064656"
                },
                {
                    "start": 1750,
                    "end": 1753,
                    "matchedPaperCorpusId": "254221195"
                },
                {
                    "start": 1847,
                    "end": 1851,
                    "matchedPaperCorpusId": "252668364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86083984375
        },
        {
            "corpus_id": "259095584",
            "title": "An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models",
            "text": "Pre-trained language models are able to encode rich linguistic and factual knowledge by learning the co-occurrence information of words in large realworld corpora (Devlin et al., 2019;Petroni et al., 2019;Raffel et al., 2020;Brown et al., 2020). Since most of these corpora are internet-based and not carefully curated, they are likely to contain unbalanced or stereotyped information for certain demographic groups. As a result, pre-trained language models are often demonstrated to inherit bias from human society and exhibit potential harms (Blodgett et al., 2020;Bender et al., 2021;May et al., 2019;Zhao et al., 2019;Sheng et al., 2019;Nangia et al., 2020;Nadeem et al., 2021). Hence, much re-search effort has been devoted to debias pre-trained language models (Meade et al., 2022).\n\nWith the size of language models becoming incredibly large (Brown et al., 2020;Hoffmann et al., 2022;Smith et al., 2022), they are not only at higher risk of exhibiting biased behaviors (Bender et al., 2021), but also hard to debias because of prohibitive computational cost. Therefore, recent parameter-efficient methods (He et al., 2022;Ding et al., 2022) have been applied to bias mitigation, where only a small portion of the parameters are updated (Lauscher et al., 2021;Gira et al., 2022). However, these works are limited in terms of evaluation dimensions, making it unclear how different parameter-efficient methods' performance compare to each other, whether one parameter-efficient method is effective across different types of language models, and whether they are also effective for mitigating religious and racial bias in addition to gender bias. Moreover, direct comparisons with strong post-hoc debiasing methods Schick et al., 2021), as well as evaluations of bias mitigation's impact on the language model's internal knowledge, are often insufficient.\n\nGiven these observations, we investigate three popular parameter-efficient methods, i.e., prefix tuning (Li and Liang, 2021), prompt tuning",
            "score": 0.5298245325887265,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 184,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 184,
                    "end": 205,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 544,
                    "end": 567,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 587,
                    "end": 604,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 604,
                    "end": 622,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 622,
                    "end": 641,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 641,
                    "end": 661,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 661,
                    "end": 681,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 767,
                    "end": 787,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 1112,
                    "end": 1129,
                    "matchedPaperCorpusId": "238583580"
                },
                {
                    "start": 1266,
                    "end": 1284,
                    "matchedPaperCorpusId": "248780268"
                },
                {
                    "start": 1718,
                    "end": 1738,
                    "matchedPaperCorpusId": "232075876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90380859375
        },
        {
            "corpus_id": "267740621",
            "title": "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models",
            "text": "This exploration extends to various prompt-based learning scenarios: in-context learning and prompt-based fine-tuning. Prior approaches to mitigate intrinsic bias primarily focus on achieving social fairness, and often require laborious corpora augmentation and costly re-training (Huang et al., 2020;Kaneko and Bollegala, 2021;Solaiman and Dennison, 2021;Li et al., 2023a). To improve efficiency in both data generation and model updates, we propose leveraging auto-generated null-meaning inputs to prompt pre-trained LMs for intrinsic bias probing, and subsequently updating only bias parameters B LM of LMs for bias calibration. Nullmeaning inputs are essentially normal text devoid of meaningful content or sentiment. Unlike Figure 1: We demonstrate our calibration method significantly improves classification performance of pre-trained LM. Upper: The pipeline of proposed null-input prompting method for intrinsic bias calibration targeting AGNews task (Zhang et al., 2015). Lower left: Performance comparison of zero-shot in-context learning using: original LM (Orig. RoBERTa); calibrated (Calib.) LM with full model updates (W LM + B LM ); calibrated LM with only B LM updates. Lower right: Case study illustrating that LM makes correct prediction after intrinsic bias calibration. numerical-zero inputs, they maintain the contextual framework of prompts, ensuring the proper functioning of contextual LMs. Our motivation stems from the expectation that bias-calibrated models should produce uniform probabilities across all categories if the input in a prompt delivers null information (Zhao et al., 2021). B LM functions as offsets in neural networks, and strategically updating only B LM could potentially counteract intrinsic bias of pre-trained models, achieving higher efficiency (updating \u223c 0.1% parameters of entire LM). The approach promotes an equitable starting point, and we expect that the light model updates preserve pre-trained models' language modeling abilities while maintaining the focus on bias calibration, ultimately making LMs better zero/few-shot learners. \n\nThe pipeline of our calibration method is illustrated in Figure 1.",
            "score": 0.5296128264188003,
            "section_title": "Introduction",
            "char_start_offset": 1709,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 2089
                },
                {
                    "start": 2092,
                    "end": 2158
                }
            ],
            "ref_mentions": [
                {
                    "start": 328,
                    "end": 356,
                    "matchedPaperCorpusId": "235489789"
                },
                {
                    "start": 356,
                    "end": 373,
                    "matchedPaperCorpusId": "250390474"
                },
                {
                    "start": 959,
                    "end": 979,
                    "matchedPaperCorpusId": "368182"
                },
                {
                    "start": 1595,
                    "end": 1614,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.712890625
        },
        {
            "corpus_id": "263333947",
            "title": "Unlocking Bias Detection: Leveraging Transformer-Based Models for Content Analysis",
            "text": "A S Natural Language Processing (NLP) rapidly evolves, its significance extends well beyond text analysis. NLP influences diverse sectors, ranging from social media analytics to advanced healthcare diagnostics. The pervasive reach of NLP showcases not only its achievements, but also highlights vital challenges. Among these challenges, linguistic biases [1], which are often embedded in both data and algorithms, are a significant concern. These biases do more than just perpetuate stereotypes; they risk distorting data interpretations, affecting decision-making processes. \n\nAs seen in Figure 1, the first statement displays a genderbased bias, while the second one reflects a bias for ableism. Here, \"bias\" refers to the predisposition or inclination towards a particular group (based on gender or race), often rooted in societal stereotypes, that can unduly influence the representation or treatment of particular groups [2]. These biases are often manifested in the tone, choice of words, or conveyed messages in textual conversations. A conventional NLP model primarily trained on a specific domain, such as political discourse, may detect bias within that domain but would be hesitant when presented with similar biases in a health-centric context. This inconsistency of traditional NLP Despite significant advancements in state-of-the-art models [3]- [8], consistent bias identification across diverse domains remains an ongoing challenge. Some recent research has mainly focused on evaluating and debiasing language models (LMs) [9]- [11] -which is indeed a critical step toward minimizing AI risks -the pressing need to detect biases inherent in the data itself, persists [12]. This highlights the urgency for a holistic NLP framework that not only stands up to academic observation, but also ensures ethical and accurate data interpretation. \n\nTo address this issue, our research introduces the novel Contextualized Bi-Directional Dual Transformer (CBDT) classifier. At its core, the CBDT integrates two specialized transformer-based LMs: the Context Transformer and the Entity Transformer. The former assesses the overall bias in an input text, while the latter focuses on particular lexicons in the text to identify potentially biased words and entities.",
            "score": 0.5295687693878965,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1853
                },
                {
                    "start": 1856,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2102
                },
                {
                    "start": 2103,
                    "end": 2268
                }
            ],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 358,
                    "matchedPaperCorpusId": "247050244"
                },
                {
                    "start": 926,
                    "end": 929,
                    "matchedPaperCorpusId": "334423"
                },
                {
                    "start": 1355,
                    "end": 1358,
                    "matchedPaperCorpusId": "120178733"
                },
                {
                    "start": 1360,
                    "end": 1363,
                    "matchedPaperCorpusId": "257079697"
                },
                {
                    "start": 1539,
                    "end": 1542,
                    "matchedPaperCorpusId": "253116931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90087890625
        },
        {
            "corpus_id": "272881257",
            "title": "A Comprehensive Survey of Bias in LLMs: Current Landscape and Future Directions",
            "text": "A novel bias testing framework tailored for code generation tasks show 20.29% to 44.93% of generated code functions exhibit bias in sensitive tasks (Huang et. al., 2023). The paper further evaluates five bias mitigation strategies, with one-shot and few-shot learning proving most effective, removing up to 90% of bias in GPT-4's code generation. Direct Preference Optimization (DPO) aims to mitigate gender, racial, and religious biases in LLM-generated English text by developing a loss function that favors less biased over biased completions. The framework shows significant bias reduction and outperforms baseline models on bias benchmarks. Social Contact Debiasing (SCD) technique (Raj et. al., 2024) shows the possibility of reducing bias effectively. The paper uses Contact Hypothesis from social psychology to mitigate bias in Large Language Models (LLMs) by simulating social interactions through prompts, creating a dataset of 108,000 entries that measure 13 types of social bias across three models: LLaMA 2, Tulu, and NousHermes. The novel Social Contact Debiasing (SCD) technique involves instruction-tuning these models with unbiased responses. The results show that biases in LLaMA 2 can be reduced by up to 40% after just one epoch of tuning, demonstrating the effectiveness of social contact methods in addressing bias in LLM-generated content. PoliTune approach introduces a fine-tuning framework that uses Parameter-Efficient Fine-Tuning (PEFT) to systematically align LLMs, such as Llama3-70B, with targeted political ideologies by modifying only a small subset of parameters. This methodology provides insights into how LLMs can be tailored to reflect specific ideological biases without requiring extensive retraining, thus raising critical questions about ethical AI deployment in politically sensitive applications (Agiza et. al., 2024) The widespread integration of Large Language Models (LLMs) into software applications has brought attention to the biases these models may propagate, often stemming from the vast datasets sourced from the internet. A notable contribution in this space is LangBiTe, a platform designed to systematically evaluate bias in LLMs.",
            "score": 0.5295654642430478,
            "section_title": "Bias Detection and Measurement",
            "char_start_offset": 12906,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2187
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.939453125
        },
        {
            "corpus_id": "271213154",
            "title": "BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs",
            "text": "Large Language Models (LLMs), characterized by their extensive parameter sets and substantial training datasets, have brought significant efficiency improvements across various fields (Achiam et al., 2023;Touvron et al., 2023).However, recent studies have shown that LLMs exhibit social bias stemming from their training data (Navigli et al., 2023;Sheng et al., 2021).Evaluating social bias in LLMs can not only enhance their fairness and reliability but also expedite their widespread deployment, which garners increasing attention from researchers, practitioners, and the broader public (Nadeem et al., 2020;Gallegos et al., 2023).\n\nMany efforts have been made to evaluate the fairness of LLMs, which mainly fall into two categories: embedding or probability-based methods  assess LLMs by computing distances in the embedding space or comparing token probability predictions from counterfactual inputs (Caliskan et al., 2017;Nadeem et al., 2020;May et al., 2019;Nangia et al., 2020).Generated-text-based methods evaluate LLMs by prompting them to complete texts or answer questions (Dhamala et al., 2021;Wan et al., 2023), and they measure bias by analyzing the co-occurrence distributions or frequencies of predefined words or choices (Bordia and Bowman, 2019;Nozza et al., 2021;Huang et al., 2019).However, all these approaches rely on fixed-form inputs and outputs, which show weak correlations with flexible and diverse practical open-text generation scenarios such as text completion and question answering (Delobelle et al., 2022;Cabello et al., 2023).Furthermore, the challenge of evaluating bias in open-text generation tasks is exacerbated by the lack of reliable and efficient methods to judge bias in the generated content.\n\nTo bridge this gap, we introduce BiasAlert, a plug-and-play tool for detecting social bias as shown in Figure 1.Specifically, BiasAlert takes",
            "score": 0.5295128961345099,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 227,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 633
                },
                {
                    "start": 635,
                    "end": 985
                },
                {
                    "start": 985,
                    "end": 1302
                },
                {
                    "start": 1302,
                    "end": 1560
                },
                {
                    "start": 1560,
                    "end": 1736
                },
                {
                    "start": 1738,
                    "end": 1850
                },
                {
                    "start": 1850,
                    "end": 1879
                }
            ],
            "ref_mentions": [
                {
                    "start": 326,
                    "end": 348,
                    "matchedPaperCorpusId": "258688053"
                },
                {
                    "start": 904,
                    "end": 927,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 947,
                    "end": 964,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 1084,
                    "end": 1106,
                    "matchedPaperCorpusId": "231719337"
                },
                {
                    "start": 1106,
                    "end": 1123,
                    "matchedPaperCorpusId": "258833296"
                },
                {
                    "start": 1263,
                    "end": 1282,
                    "matchedPaperCorpusId": "235097294"
                },
                {
                    "start": 1514,
                    "end": 1538,
                    "matchedPaperCorpusId": "250390561"
                },
                {
                    "start": 1538,
                    "end": 1559,
                    "matchedPaperCorpusId": "258236466"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9365234375
        },
        {
            "corpus_id": "253384669",
            "title": "No Word Embedding Model Is Perfect: Evaluating the Representation Accuracy for Social Bias in the Media",
            "text": "We suggest (1) training frequency agnostic embeddings to compensate for lower quality of rare tokens, (2) a fine-tuned language model to account for smaller datasets, and (3) decontextualized embeddings to alleviate the \"unnatural input\" problem with contextualized models. \n\nFor our experiments, we introduce a large-scale media bias corpus in Section 4, covering more than 500,000 news articles from 47 English-language US online news outlets over 12 years (2010)(2011)(2012)(2013)(2014)(2015)(2016)(2017)(2018)(2019)(2020)(2021). Given the corpus, we evaluate each potential improvement and compare their capability to encode and represent social bias a text corpus (Section 5). To this end, we systematically generate word embedding models from subsets of different political biases. In a second analysis, we explore the development of social bias in outlets over time in a respective manner. We can quantify the considered types of social bias for all models using WEAT. \n\nOur results in Section 5 provide evidence that the general embeddings quality improves notably over standard static embeddings. Additionally, the proposed algorithms better model the expected social bias, though still not fully align with the literature. \n\nThis work provides three contributions to computational research on bias in language: \n\n1. Findings on how to combine embedding models and bias measures to adequately quantify social bias in text corpora; \n\n2. a large-scale news resource annotated for political bias; and 3. empirical insights into the interaction of social and media bias in US online news, and its development over time.1",
            "score": 0.528903577851648,
            "section_title": "Introduction",
            "char_start_offset": 3600,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 276,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1232
                },
                {
                    "start": 1235,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1439
                },
                {
                    "start": 1442,
                    "end": 1625
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8349609375
        },
        {
            "corpus_id": "264306211",
            "title": "Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model",
            "text": "Modern neural language models exhibit social biases, such as biases based on gender, religion, ethnicity and other protected attributes. These biases may lead to real harms when used in downstream applications (e.g. Hovy and Spruit, 2016;Weidinger et al., 2021). Detecting and mitigating biases in language models has therefore become an important area of research. \n\nEarly detection methods relied on lists of words to measure associations with e.g., specific genders (e.g. Caliskan et al., 2017). Most current detection * Shared senior authorship. methods work with curated sets of sentence pairs or triplets, and measure differences in sentence probabilities or anaphora resolution probabilities (e.g. May et al., 2019;Nadeem et al., 2021;Nangia et al., 2020;Basta et al., 2019). Proposed mitigation strategies include targeted changes to the training data (e.g., CDA; Lu et al., 2020), training procedure (e.g., adversarial learning; Zhang et al., 2018), model parameters (e.g., INLP;Ravfogel et al., 2020), or language generation procedure (e.g., \"self-debiasing\"; Schick et al., 2021). \n\nDespite this work, we still lack a proper understanding of how to best measure biases (how do we guarantee the representativeness for real-world harm of a set of sentence pairs, or of a linguistic phenomenon such as anaphora resolution?), how biases are implemented in the language model internals (is there a unified locus, or is, e.g., gender bias the aggregate effect of many independent model decisions?), and what techniques are effective at reducing undesirable downstream behavior (e.g., is data curation more or less effective than filtering output? Is intervening in the model internals feasible?).",
            "score": 0.5288149877288744,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 365
                },
                {
                    "start": 368,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 1091
                },
                {
                    "start": 1094,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1701
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 238,
                    "matchedPaperCorpusId": "1083991"
                },
                {
                    "start": 475,
                    "end": 497,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 705,
                    "end": 722,
                    "matchedPaperCorpusId": "85518027"
                },
                {
                    "start": 722,
                    "end": 742,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 742,
                    "end": 762,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 762,
                    "end": 781,
                    "matchedPaperCorpusId": "121125604"
                },
                {
                    "start": 1070,
                    "end": 1090,
                    "matchedPaperCorpusId": "232075876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9208984375
        },
        {
            "corpus_id": "272880907",
            "title": "Do the Right Thing, Just Debias! Multi-Category Bias Mitigation Using LLMs",
            "text": "Section 3 provides a detailed discussion on the preparation strategy for ANUBIS. \n\nWhile existing research has made significant progress in mitigating bias in language models, there is a growing need for models that demonstrate robustness across different domains. The ability to generalize beyond the specific training data is crucial for real-world applications, where language-models are likely to encounter diverse contexts and perspectives. While domain adapta- tion techniques involving fine-tuning, have shown promise (Sun et al., 2020;Wang et al., 2020), they often require substantial amount of training data from the target domain. This limits their practicality and raises a crucial question: Can bias mitigation models trained in one category effectively generalize to multiple new categories without extensive training data? To answer this question, we conducted extensive experiments on two different training datasets: WIKIBIAS and ANUBIS, evaluating them on various metrics (as mentioned in Section 4). In the effort for sustainable and environmentally conscious practices, we also assessed the \"greenness\" of the models trained on these datasets, highlighting the importance of considering environmental impact alongside traditional evaluation metrics (See Section 6). Our research makes the following contributions: \n\n\u2022 We have evaluated the effectiveness of stateof-the-art models, including the T5 model and Large Language Models, in mitigating multi-class social bias in texts using Supervised Fine-Tuning (SFT). \n\n\u2022 We have set up a tri-step configuration to effectively reduce bias in texts using Supervised Fine-Tuning (SFT), coupled with Reinforcement Learning (RL) techniques including a) \n\nProximal Policy Optimization (PPO) and b) Direct Preference Optimization (DPO) (As illustrated in Figure 1), and as a third step, c) In-Context Learning (ICL) (See Section 6). \n\n\u2022 We introduce ANUBIS + : ANother UnBIased dataSet + , a new dataset consisting of 1507 perfectly debiased sentence pairs spanning 9 different bias classes (See Table 3) providing a valuable resource for fine-tuning and aligning language models to effectively mitigate bias in texts while preserving context and linguistic quality, and devised a simple yet strict grammar-based evaluation metric to classify a given sentence pair as biased or debiased (See Section 3).",
            "score": 0.5286653897363003,
            "section_title": "Introduction",
            "char_start_offset": 2114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 83,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1333
                },
                {
                    "start": 1336,
                    "end": 1533
                },
                {
                    "start": 1536,
                    "end": 1714
                },
                {
                    "start": 1717,
                    "end": 1892
                },
                {
                    "start": 1895,
                    "end": 2363
                }
            ],
            "ref_mentions": [
                {
                    "start": 525,
                    "end": 543,
                    "matchedPaperCorpusId": "220301705"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90478515625
        },
        {
            "corpus_id": "277066604",
            "title": "No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language models",
            "text": "Given the exceptional capabilities of large language models (LLMs) in performing a variety of tasks, bias detection is a critical factor in enhancing the reliability of these models' outputs (Navigli et al., 2023;Gallegos et al., 2024). Existing literature includes numerous studies that focus on detecting biases in different areas, such as gender and race bias (Nadeem et al., 2021;Li et al., 2020;Rudinger et al., 2018), social bias (Nozza et al., 2022;Nangia et al., 2020;Qu and Wang, 2024), cultural bias (Naous et al., 2024), entity bias (Wang et al., 2023), nationality bias (Zhu et al., 2024), and holistic bias (Smith et al., 2022). Additionally, some studies delve into bias detection and mitigation techniques (Gallegos et al., 2024). However, no work has yet provided an experimental study analyzing the various types of bias presence in LLMs. In this study, we aim to fill this gap by offering an experimental survey and designing approaches to address different aspects of bias in LLMs.",
            "score": 0.5280281781416264,
            "section_title": "Related Work",
            "char_start_offset": 19894,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1000
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 213,
                    "matchedPaperCorpusId": "258688053"
                },
                {
                    "start": 213,
                    "end": 235,
                    "matchedPaperCorpusId": "261530629"
                },
                {
                    "start": 363,
                    "end": 384,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 384,
                    "end": 400,
                    "matchedPaperCorpusId": "222141056"
                },
                {
                    "start": 400,
                    "end": 422,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 436,
                    "end": 456,
                    "matchedPaperCorpusId": "248780490"
                },
                {
                    "start": 456,
                    "end": 476,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 476,
                    "end": 494,
                    "matchedPaperCorpusId": "271146767"
                },
                {
                    "start": 510,
                    "end": 530,
                    "matchedPaperCorpusId": "258865272"
                },
                {
                    "start": 544,
                    "end": 563,
                    "matchedPaperCorpusId": "258865519"
                },
                {
                    "start": 582,
                    "end": 600,
                    "matchedPaperCorpusId": "269757411"
                },
                {
                    "start": 620,
                    "end": 640,
                    "matchedPaperCorpusId": "253224433"
                },
                {
                    "start": 721,
                    "end": 744,
                    "matchedPaperCorpusId": "261530629"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94091796875
        },
        {
            "corpus_id": "236477795",
            "title": "He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation",
            "text": "As shown on Figure 1, our method takes a pretrained language model (BERT) and further pretrains it on the given dataset, while mitigating the existing social biases using the demographic word pairs. The approach consists of two stages.",
            "score": 0.5273292039025883,
            "section_title": "DEBIASBERT",
            "char_start_offset": 8358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 235
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8623046875
        },
        {
            "corpus_id": "266174740",
            "title": "SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models",
            "text": "There is significant work on bias evaluation of language models, such as auditing for unwanted social bias through benchmarks (Baldini et al. 2022;Blodgett et al. 2020;Parrish et al. 2022a;Aky\u00fcrek et al. 2022;Smith et al. 2022;Selvam et al. 2023;Dhamala et al. 2021;Nangia et al. 2020;Nadeem, Bethke, and Reddy 2020;Wang, Wang, and Yang 2022). Recent efforts propose a holistic evaluation of LMs (Srivastava et al. 2022;Liang et al. 2022) across many datasets, tasks, and metrics. Raji et al. (2021) document the pitfalls of generalizing model ability through a set of benchmarks, while Bowman (2022) discusses the dangers of underclaiming LM abilities. Researchers scrutinized deficiencies of current datasets (Blodgett et al. 2021) and the lack of clarity on the definition of social bias in NLP models and its measures (Blodgett et al. 2020;Selvam et al. 2022). BBQ (Parrish et al. 2022b) is a bias benchmark for QA which utilizes nine social dimensions defined by the US Equal Employment Opportunities Commission (e.g., age, gender identity, physical appearance, etc.). UnQover (Li et al. 2020b) is also a QA dataset that focuses on ambiguous questions for assessing bias across dimensions such as religion, nationality and gender. Smith et al. (2022) introduces a holistic dataset, utilizing a dozen social demographic axes. Importantly, our benchmark offers both a wider variety of categorizations which pertain to stigmas (e.g. voluntarily childless, sex worker, etc.) as well as going deeper into existing segments of these dimensions (e.g. for physical appearancehaving limb scars or multiple tattoos, for perceived social status -living in a trailer park, being a gang member, etc.).",
            "score": 0.5268695996997427,
            "section_title": "Social Bias Evaluation in Language Models",
            "char_start_offset": 5242,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1693
                }
            ],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 147,
                    "matchedPaperCorpusId": "248177981"
                },
                {
                    "start": 147,
                    "end": 168,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 189,
                    "end": 209,
                    "matchedPaperCorpusId": "249017622"
                },
                {
                    "start": 209,
                    "end": 227,
                    "matchedPaperCorpusId": "253224433"
                },
                {
                    "start": 227,
                    "end": 246,
                    "matchedPaperCorpusId": "252968208"
                },
                {
                    "start": 246,
                    "end": 266,
                    "matchedPaperCorpusId": "231719337"
                },
                {
                    "start": 316,
                    "end": 342,
                    "matchedPaperCorpusId": "258556812"
                },
                {
                    "start": 396,
                    "end": 420,
                    "matchedPaperCorpusId": "263625818"
                },
                {
                    "start": 420,
                    "end": 437,
                    "matchedPaperCorpusId": "259129801"
                },
                {
                    "start": 481,
                    "end": 499,
                    "matchedPaperCorpusId": "244729397"
                },
                {
                    "start": 587,
                    "end": 600,
                    "matchedPaperCorpusId": "247315802"
                },
                {
                    "start": 711,
                    "end": 732,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 822,
                    "end": 844,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 869,
                    "end": 890,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 1082,
                    "end": 1098,
                    "matchedPaperCorpusId": "222141056"
                },
                {
                    "start": 1236,
                    "end": 1255,
                    "matchedPaperCorpusId": "253224433"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "paperId": "a153c0910140c098ded32ed313c4d216e24b867d",
            "corpusId": 252579643,
            "title": "Fine-Tuning Language Models to Mitigate Gender Bias in Sentence Encoders",
            "venue": "International Conference on Big Data Computing Service and Applications",
            "year": 2022,
            "referenceCount": 10,
            "citationCount": 6,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/BigDataService55688.2022.00036?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/BigDataService55688.2022.00036, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2182555493",
                    "name": "Tommaso Dolci"
                }
            ],
            "abstract": "Language models are used for a variety of downstream applications, such as improving web search results or parsing CVs to identify the best candidate for a job position. At the same time, concern is growing around word and sentence embeddings, popular language models that have been shown to exhibit large amount of social bias. In this work, by leveraging the possibility to further train state-of-the-art pre-trained embedding models, we propose to mitigate gender bias by fine-tuning sentence encoders on a semantic similarity task built around gender stereotype sentences and corresponding gender-swapped anti-stereotypes, in order to enforce similarity between the two categories. We test our intuition on two popular language models, BERT-Base and DistilBERT, and measure the amount of gender bias mitigation using the Sentence Encoder Association Test (SEAT). Our solution shows promising results despite using a small amount of training data, proving that post-processing bias mitigation techniques based on fine-tuning can effectively reduce gender bias in sentence encoders.",
            "corpus_id": "252579643",
            "text": "Language models are used for a variety of downstream applications, such as improving web search results or parsing CVs to identify the best candidate for a job position. At the same time, concern is growing around word and sentence embeddings, popular language models that have been shown to exhibit large amount of social bias. In this work, by leveraging the possibility to further train state-of-the-art pre-trained embedding models, we propose to mitigate gender bias by fine-tuning sentence encoders on a semantic similarity task built around gender stereotype sentences and corresponding gender-swapped anti-stereotypes, in order to enforce similarity between the two categories. We test our intuition on two popular language models, BERT-Base and DistilBERT, and measure the amount of gender bias mitigation using the Sentence Encoder Association Test (SEAT). Our solution shows promising results despite using a small amount of training data, proving that post-processing bias mitigation techniques based on fine-tuning can effectively reduce gender bias in sentence encoders.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.91064453125
        },
        {
            "paperId": "e0b711ef6bc72b3b9e4acb6d30f2a8d2036a187d",
            "corpusId": 271520803,
            "title": "TagDebias: Entity and Concept Tagging for Social Bias Mitigation in Pretrained Language Models",
            "venue": "NAACL-HLT",
            "year": 2024,
            "referenceCount": 24,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2024.findings-naacl.101?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2024.findings-naacl.101, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313533580",
                    "name": "Mehrnaz Moslemi"
                },
                {
                    "authorId": "2313539918",
                    "name": "Amal Zouaq"
                }
            ],
            "abstract": "Pre-trained language models (PLMs) play a crucial role in various applications, including sensitive domains such as the hiring process. However, extensive research has unveiled that these models tend to replicate social biases present in their pre-training data, raising ethical concerns. In this study, we propose the TagDebias method, which proposes debiasing a dataset using type tags. It then proceeds to fine-tune PLMs on this debiased dataset. Experiments show that our proposed TagDebias model, when applied to a ranking task, exhibits significant improvements in bias scores.",
            "corpus_id": "271520803",
            "text": "Pre-trained language models (PLMs) play a crucial role in various applications, including sensitive domains such as the hiring process. However, extensive research has unveiled that these models tend to replicate social biases present in their pre-training data, raising ethical concerns. In this study, we propose the TagDebias method, which proposes debiasing a dataset using type tags. It then proceeds to fine-tune PLMs on this debiased dataset. Experiments show that our proposed TagDebias model, when applied to a ranking task, exhibits significant improvements in bias scores.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.91162109375
        },
        {
            "paperId": "8d863cafea3493fb033fcdcf9f272a1a4912628b",
            "corpusId": 248780439,
            "title": "Upstream Mitigation Is \n Not\n All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2022,
            "referenceCount": 45,
            "citationCount": 46,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.acl-long.247.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2022.acl-long.247, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145882580",
                    "name": "Ryan Steed"
                },
                {
                    "authorId": "1721493",
                    "name": "Swetasudha Panda"
                },
                {
                    "authorId": "2441120",
                    "name": "Ari Kobren"
                },
                {
                    "authorId": "2987641",
                    "name": "Michael L. Wick"
                }
            ],
            "abstract": "A few large, homogenous, pre-trained models undergird many machine learning systems \u2014 and often, these models contain harmful stereotypes learned from the internet. We investigate the bias transfer hypothesis: the theory that social biases (such as stereotypes) internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning. For two classification tasks, we find that reducing intrinsic bias with controlled interventions before fine-tuning does little to mitigate the classifier\u2019s discriminatory behavior after fine-tuning. Regression analysis suggests that downstream disparities are better explained by biases in the fine-tuning dataset. Still, pre-training plays a role: simple alterations to co-occurrence rates in the fine-tuning dataset are ineffective when the model has been pre-trained. Our results encourage practitioners to focus more on dataset quality and context-specific harms.",
            "corpus_id": "248780439",
            "text": "A few large, homogenous, pre-trained models undergird many machine learning systems \u2014 and often, these models contain harmful stereotypes learned from the internet. We investigate the bias transfer hypothesis: the theory that social biases (such as stereotypes) internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning. For two classification tasks, we find that reducing intrinsic bias with controlled interventions before fine-tuning does little to mitigate the classifier\u2019s discriminatory behavior after fine-tuning. Regression analysis suggests that downstream disparities are better explained by biases in the fine-tuning dataset. Still, pre-training plays a role: simple alterations to co-occurrence rates in the fine-tuning dataset are ineffective when the model has been pre-trained. Our results encourage practitioners to focus more on dataset quality and context-specific harms.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9619140625
        },
        {
            "paperId": "de618ad236901e6dd59bba24856d104761b611b3",
            "corpusId": 274149960,
            "title": "Joint Vision-Language Social Bias Removal for CLIP",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 57,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.12785, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2331632350",
                    "name": "Haoyu Zhang"
                },
                {
                    "authorId": "2330864125",
                    "name": "Yangyang Guo"
                },
                {
                    "authorId": "1744045",
                    "name": "Mohan Kankanhalli"
                }
            ],
            "abstract": "Vision-Language (V-L) pre-trained models such as CLIP show prominent capabilities in various downstream tasks. Despite this promise, V-L models are notoriously limited by their inherent social biases. A typical demonstration is that V-L models often produce biased predictions against specific groups of people, significantly undermining their real-world applicability. Existing approaches endeavor to mitigate the social bias problem in V-L models by removing biased attribute information from model embeddings. However, after our revisiting of these methods, we find that their bias removal is frequently accompanied by greatly compromised V-L alignment capabilities. We then reveal that this performance degradation stems from the unbalanced debiasing in image and text embeddings. To address this issue, we propose a novel V-L debiasing framework to align image and text biases followed by removing them from both modalities. By doing so, our method achieves multi-modal bias mitigation while maintaining the V-L alignment in the debiased embeddings. Additionally, we advocate a new evaluation protocol that can 1) holistically quantify the model debiasing and V-L alignment ability, and 2) evaluate the generalization of social bias removal models. We believe this work will offer new insights and guidance for future studies addressing the social bias problem in CLIP.",
            "corpus_id": "274149960",
            "text": "Vision-Language (V-L) pre-trained models such as CLIP show prominent capabilities in various downstream tasks. Despite this promise, V-L models are notoriously limited by their inherent social biases. A typical demonstration is that V-L models often produce biased predictions against specific groups of people, significantly undermining their real-world applicability. Existing approaches endeavor to mitigate the social bias problem in V-L models by removing biased attribute information from model embeddings. However, after our revisiting of these methods, we find that their bias removal is frequently accompanied by greatly compromised V-L alignment capabilities. We then reveal that this performance degradation stems from the unbalanced debiasing in image and text embeddings. To address this issue, we propose a novel V-L debiasing framework to align image and text biases followed by removing them from both modalities. By doing so, our method achieves multi-modal bias mitigation while maintaining the V-L alignment in the debiased embeddings. Additionally, we advocate a new evaluation protocol that can 1) holistically quantify the model debiasing and V-L alignment ability, and 2) evaluate the generalization of social bias removal models. We believe this work will offer new insights and guidance for future studies addressing the social bias problem in CLIP.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.77197265625
        },
        {
            "paperId": "0ea10f2132233b4f5d7cf701300cd0055e93e2d8",
            "corpusId": 268385396,
            "title": "Ethos: Rectifying Language Models in Orthogonal Parameter Space",
            "venue": "NAACL-HLT",
            "year": 2024,
            "referenceCount": 77,
            "citationCount": 12,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.08994, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2291209211",
                    "name": "Lei Gao"
                },
                {
                    "authorId": "2260691804",
                    "name": "Yue Niu"
                },
                {
                    "authorId": "2291141893",
                    "name": "Tingting Tang"
                },
                {
                    "authorId": "5877233",
                    "name": "A. Avestimehr"
                },
                {
                    "authorId": "2171377377",
                    "name": "Murali Annavaram"
                }
            ],
            "abstract": "Language models (LMs) have greatly propelled the research on natural language processing. However, LMs also raise concerns regarding the generation of biased or toxic content and the potential disclosure of private information from the training dataset. In this work, we present a new efficient approach, Ethos, that rectifies LMs to mitigate toxicity and bias in outputs and avoid privacy leakage. Ethos is built on task arithmetic. However, unlike current task arithmetic algorithms, Ethos distinguishes general beneficial and undesired knowledge when reconstructing task vectors. Specifically, Ethos first obtains a set of principal components from the pre-trained models using singular value decomposition. Then, by projecting the task vector onto principal components, Ethos identifies the principal components that encode general or undesired knowledge. Ethos performs negating using the task vector with undesired knowledge only, thereby minimizing collateral damage on general model utility. We demonstrate the efficacy of our approach on three different tasks: debiasing, detoxification, and memorization unlearning. Evaluations show Ethos is more effective in removing undesired knowledge and maintaining the overall model performance compared to current task arithmetic methods.",
            "corpus_id": "268385396",
            "text": "Language models (LMs) have greatly propelled the research on natural language processing. However, LMs also raise concerns regarding the generation of biased or toxic content and the potential disclosure of private information from the training dataset. In this work, we present a new efficient approach, Ethos, that rectifies LMs to mitigate toxicity and bias in outputs and avoid privacy leakage. Ethos is built on task arithmetic. However, unlike current task arithmetic algorithms, Ethos distinguishes general beneficial and undesired knowledge when reconstructing task vectors. Specifically, Ethos first obtains a set of principal components from the pre-trained models using singular value decomposition. Then, by projecting the task vector onto principal components, Ethos identifies the principal components that encode general or undesired knowledge. Ethos performs negating using the task vector with undesired knowledge only, thereby minimizing collateral damage on general model utility. We demonstrate the efficacy of our approach on three different tasks: debiasing, detoxification, and memorization unlearning. Evaluations show Ethos is more effective in removing undesired knowledge and maintaining the overall model performance compared to current task arithmetic methods.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8515625
        },
        {
            "paperId": "402f26b67317018c53a2af368d2c29900fe1cfd5",
            "corpusId": 279071057,
            "title": "My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals",
            "venue": "",
            "year": 2025,
            "referenceCount": 39,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.23798, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2364687590",
                    "name": "Jian Lan"
                },
                {
                    "authorId": "2364643109",
                    "name": "Yifei Fu"
                },
                {
                    "authorId": "2364686142",
                    "name": "Udo Schlegel"
                },
                {
                    "authorId": "2364691434",
                    "name": "Gengyuan Zhang"
                },
                {
                    "authorId": "1723338634",
                    "name": "Tanveer Hannan"
                },
                {
                    "authorId": "2364709964",
                    "name": "Haokun Chen"
                },
                {
                    "authorId": "2273655728",
                    "name": "Thomas Seidl"
                }
            ],
            "abstract": "Social bias is a critical issue in large vision-language models (VLMs), where fairness- and ethics-related problems harm certain groups of people in society. It is unknown to what extent VLMs yield social bias in generative responses. In this study, we focus on evaluating and mitigating social bias on both the model's response and probability distribution. To do so, we first evaluate four state-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets with the multiple-choice selection task. Surprisingly, we find that models suffer from generating gender-biased or race-biased responses. We also observe that models are prone to stating their responses are fair, but indeed having mis-calibrated confidence levels towards particular social groups. While investigating why VLMs are unfair in this study, we observe that VLMs' hidden layers exhibit substantial fluctuations in fairness levels. Meanwhile, residuals in each layer show mixed effects on fairness, with some contributing positively while some lead to increased bias. Based on these findings, we propose a post-hoc method for the inference stage to mitigate social bias, which is training-free and model-agnostic. We achieve this by ablating bias-associated residuals while amplifying fairness-associated residuals on model hidden layers during inference. We demonstrate that our post-hoc method outperforms the competing training strategies, helping VLMs have fairer responses and more reliable confidence levels.",
            "corpus_id": "279071057",
            "text": "Social bias is a critical issue in large vision-language models (VLMs), where fairness- and ethics-related problems harm certain groups of people in society. It is unknown to what extent VLMs yield social bias in generative responses. In this study, we focus on evaluating and mitigating social bias on both the model's response and probability distribution. To do so, we first evaluate four state-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets with the multiple-choice selection task. Surprisingly, we find that models suffer from generating gender-biased or race-biased responses. We also observe that models are prone to stating their responses are fair, but indeed having mis-calibrated confidence levels towards particular social groups. While investigating why VLMs are unfair in this study, we observe that VLMs' hidden layers exhibit substantial fluctuations in fairness levels. Meanwhile, residuals in each layer show mixed effects on fairness, with some contributing positively while some lead to increased bias. Based on these findings, we propose a post-hoc method for the inference stage to mitigate social bias, which is training-free and model-agnostic. We achieve this by ablating bias-associated residuals while amplifying fairness-associated residuals on model hidden layers during inference. We demonstrate that our post-hoc method outperforms the competing training strategies, helping VLMs have fairer responses and more reliable confidence levels.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9169921875
        },
        {
            "paperId": "c58db176890fa0695bcc6dcf28ce8ea207189fc5",
            "corpusId": 264350636,
            "title": "Intersectional Bias Mitigation in Pre-trained Language Models: A Quantum-Inspired Approach",
            "venue": "International Conference on Information and Knowledge Management",
            "year": 2023,
            "referenceCount": 28,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3583780.3616003?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3583780.3616003, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2123318008",
                    "name": "Omid Shokrollahi"
                }
            ],
            "abstract": "The growing criticality of contextualized language models has raised concerns about the perpetuation of biases. Current fairness research often concentrates on single aspects of social groups. However, our study brings a multifaceted approach to the table, reflecting the intricate realities of intersectionality. We propose a bias mitigation algorithm inspired by quantum theory to fine-tune pre-trained language models. Our method applies Jensen-Shannon divergence alongside an entanglement entropy measure to quantify the complexity of entwined identities. Moreover, we utilize an innovative entanglement embedding neural network to address emergent features from different intersectional groups. An Actor-Critic setup facilitates effective fine-tuning. Our approach broadens the scope of intersectional fairness beyond just statistical parity, providing a strategic tool to tackle complex, interrelated biases. We anticipate that this fresh approach to bias mitigation will substantially enhance fairness in a wide range of language model applications.",
            "corpus_id": "264350636",
            "text": "The growing criticality of contextualized language models has raised concerns about the perpetuation of biases. Current fairness research often concentrates on single aspects of social groups. However, our study brings a multifaceted approach to the table, reflecting the intricate realities of intersectionality. We propose a bias mitigation algorithm inspired by quantum theory to fine-tune pre-trained language models. Our method applies Jensen-Shannon divergence alongside an entanglement entropy measure to quantify the complexity of entwined identities. Moreover, we utilize an innovative entanglement embedding neural network to address emergent features from different intersectional groups. An Actor-Critic setup facilitates effective fine-tuning. Our approach broadens the scope of intersectional fairness beyond just statistical parity, providing a strategic tool to tackle complex, interrelated biases. We anticipate that this fresh approach to bias mitigation will substantially enhance fairness in a wide range of language model applications.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.86328125
        }
    ],
    "quotes": {
        "cost": 0.18828599999999998,
        "quotes": [
            {
                "idx": 0,
                "key": "[235623756 | Liang et al. | 2021 | Citations: 394]",
                "snippets": "Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 237,
                        "end": 817,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[236477795 | Garimella et al. | 2021 | Citations: 59]",
                "snippets": "Social biases with respect to demographics (e.g., gender, age, race) in datasets are often encoded in the large pre-trained language models trained on them. Prior works have largely focused on mitigating biases in context-free representations, with recent shift to contextual ones. While this is useful for several word and sentence-level classi\ufb01cation tasks, mitigating biases in only the representations may not suf-\ufb01ce to use these models for language generation tasks, such as auto-completion, summarization, or dialogue generation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 536,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Social biases with respect to demographics (e.g., gender, age, race) in datasets are often encoded in the large pre-trained language models trained on them. Prior works have largely focused on mitigating biases in context-free representations, with recent shift to contextual ones. While this is useful for several word and sentence-level classi\ufb01cation tasks, mitigating biases in only the representations may not suf-\ufb01ce to use these models for language generation tasks, such as auto-completion, summarization, or dialogue generation."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[248780440 | Guo et al. | 2022 | Citations: 167]",
                "snippets": "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 227,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[253762006 | Garimella et al. | 2022 | Citations: 21]",
                "snippets": "BERT-like language models (LMs), when exposed to large unstructured datasets, are known to learn and sometimes even amplify the biases present in such data. These biases generally reflect social stereotypes with respect to gender, race, age, and others.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 253,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "BERT-like language models (LMs), when exposed to large unstructured datasets, are known to learn and sometimes even amplify the biases present in such data. These biases generally reflect social stereotypes with respect to gender, race, age, and others."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[258170403 | Sharma et al. | 2023 | Citations: 0]",
                "snippets": "Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[258865159 | Li et al. | 2022 | Citations: 4]",
                "snippets": "Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 221,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[259129801 | Mei et al. | 2023 | Citations: 51]",
                "snippets": "Social bias large language models is an endemic problem models inherit amplify stereotypical judgments undesirable statistical associations from training corpora...Caliskan et al. (Caliskan et al., 2016) demonstrate that word embeddings and language models (LMs) trained on a large amount of human-generated texts encode human-like social biases. Social biases encoded in these models are also reflected in their downstream tasks such as machine translation, sentiment classification, and natural language generation (Abid et al., 2021)(Jentzsch et al., 2023)[23](Kurita et al., 2019). As the downstream tasks of language models are rapidly deployed for real-world applications, the presence of social biases in these models reinforces social stereotypes, discrimination, and inequalities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[190000105 | Kurita et al. | 2019 | Citations: 451]": "Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.",
                    "[231603388 | Abid et al. | 2021 | Citations: 555]": "It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66% to 20%, but which is still higher than for other religious groups.",
                    "[23163324 | Caliskan et al. | 2016 | Citations: 2673]": "Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\u2014for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",
                    "[250391069 | Jentzsch et al. | 2023 | Citations: 33]": "Pretrained language models are publicly available and constantly finetuned for various real-life applications. As they become capable of grasping complex contextual information, harmful biases are likely increasingly intertwined with those models. This paper analyses gender bias in BERT models with two main contributions: First, a novel bias measure is introduced, defining biases as the difference in sentiment valuation of female and male sample versions. Second, we comprehensively analyse BERT?s biases on the example of a realistic IMDB movie classifier. By systematically varying elements of the training pipeline, we can conclude regarding their impact on the final model bias. Seven different public BERT models in nine training conditions, i.e. 63 models in total, are compared. Almost all conditions yield significant gender biases. Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage."
                },
                "metadata": [
                    {
                        "quote": "Social bias large language models is an endemic problem models inherit amplify stereotypical judgments undesirable statistical associations from training corpora",
                        "pdf_hash": ""
                    },
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 554,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 162
                            },
                            {
                                "start": 163,
                                "end": 349
                            },
                            {
                                "start": 350,
                                "end": 553
                            }
                        ],
                        "ref_mentions": [
                            "23163324",
                            "231603388",
                            "250391069",
                            "190000105"
                        ],
                        "quote": "Caliskan et al. (Caliskan et al., 2016) demonstrate that word embeddings and language models (LMs) trained on a large amount of human-generated texts encode human-like social biases. Social biases encoded in these models are also reflected in their downstream tasks such as machine translation, sentiment classification, and natural language generation (Abid et al., 2021)(Jentzsch et al., 2023)[23](Kurita et al., 2019). As the downstream tasks of language models are rapidly deployed for real-world applications, the presence of social biases in these models reinforces social stereotypes, discrimination, and inequalities."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[259716055 | Kolisko et al. | 2023 | Citations: 11]",
                "snippets": "Social bias in LLMs is concerning because it may result in harm to the targeted social groups. Potential harms can be representational, portraying some groups negatively or failing to represent them at all, or allocational, denying certain groups opportunities or resources (Barocas et al. 2017).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Social Biases in Large Language Models",
                        "pdf_hash": "",
                        "start": 478,
                        "end": 774,
                        "sentence_offsets": [
                            {
                                "start": 478,
                                "end": 572
                            },
                            {
                                "start": 573,
                                "end": 774
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Social bias in LLMs is concerning because it may result in harm to the targeted social groups. Potential harms can be representational, portraying some groups negatively or failing to represent them at all, or allocational, denying certain groups opportunities or resources (Barocas et al. 2017)."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[261530629 | Gallegos et al. | 2023 | Citations: 594]",
                "snippets": "Typically trained on an enormous scale of uncurated Internet-based data, LLMs inherit stereotypes, misrepresentations, derogatory and exclusionary language, and other denigrating behaviors that disproportionately affect already-vulnerable and marginalized communities Dodge et al., 2021;Sheng et al., 2021b). These harms are forms of \"social bias,\" a subjective and normative term we broadly use to refer to disparate treatment or outcomes between social groups that arise from historical and structural power asymmetries, which we define and discuss in Section 2. Though LLMs often reflect existing biases, they can amplify these biases too; in either case, the automated reproduction of injustice can reinforce systems of inequity (Benjamin, 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                    "[231979430 | Zhao et al. | 2021 | Citations: 1428]": "GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as \"N/A\". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.",
                    "[234337004 | Sheng et al. | 2021 | Citations: 220]": "Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.",
                    "[236493269 | Liu et al. | 2021 | Citations: 3992]": "This article surveys and organizes research works in a new paradigm in natural language processing, which we dub \u201cprompt-based learning.\u201d Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x\u2032 that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x\u0302, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia\u2013Pretrain including constantly updated survey and paperlist.",
                    "[237568724 | Dodge et al. | 2021 | Citations: 450]": "Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.",
                    "[249017743 | Kojima et al. | 2022 | Citations: 4513]": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 922,
                        "end": 1672,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "218971783",
                            "249017743",
                            "236493269",
                            "160025533",
                            "231979430",
                            "237568724",
                            "234337004"
                        ],
                        "quote": "Typically trained on an enormous scale of uncurated Internet-based data, LLMs inherit stereotypes, misrepresentations, derogatory and exclusionary language, and other denigrating behaviors that disproportionately affect already-vulnerable and marginalized communities Dodge et al., 2021;Sheng et al., 2021b). These harms are forms of \"social bias,\" a subjective and normative term we broadly use to refer to disparate treatment or outcomes between social groups that arise from historical and structural power asymmetries, which we define and discuss in Section 2. Though LLMs often reflect existing biases, they can amplify these biases too; in either case, the automated reproduction of injustice can reinforce systems of inequity (Benjamin, 2020)."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[265212726 | Manerba et al. | 2023 | Citations: 20]",
                "snippets": "The unparalleled ability of language models (LMs) to generalize from vast corpora is tinged by an inherent reinforcement of social biases. These biases are not merely encoded within LMs' representations but are also perpetuated to downstream tasks (Blodgett et al., 2021)Sta\u0144czak and Augenstein, 2021), where they can manifest in an uneven treatment of different demographic groups (Rudinger et al., 2018)(Stanovsky et al., 2019)(Kiritchenko et al., 2018)(Venkit et al., 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[13756572 | Rudinger et al. | 2018 | Citations: 645]": "We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender. With these \u201cWinogender schemas,\u201d we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.",
                    "[173991101 | Stanovsky et al. | 2019 | Citations: 406]": "We present the first challenge set and evaluation protocol for the analysis of gender bias in machine translation (MT). Our approach uses two recent coreference resolution datasets composed of English sentences which cast participants into non-stereotypical gender roles (e.g., \u201cThe doctor asked the nurse to help her in the operation\u201d). We devise an automatic gender bias evaluation method for eight target languages with grammatical gender, based on morphological analysis (e.g., the use of female inflection for the word \u201cdoctor\u201d). Our analyses show that four popular industrial MT systems and two recent state-of-the-art academic MT models are significantly prone to gender-biased translation errors for all tested target languages. Our data and code are publicly available at https://github.com/gabrielStanovsky/mt_gender.",
                    "[21670658 | Kiritchenko et al. | 2018 | Citations: 441]": "Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems. Further, there is no benchmark dataset for examining inappropriate biases in systems. Here for the first time, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders. We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 \u2018Affect in Tweets\u2019. We find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. We make the EEC freely available.",
                    "[236460302 | Blodgett et al. | 2021 | Citations: 306]": "Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system\u2019s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens\u2014originating from the social sciences\u2014to inventory a range of pitfalls that threaten these benchmarks\u2019 validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 480,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 138
                            },
                            {
                                "start": 139,
                                "end": 480
                            }
                        ],
                        "ref_mentions": [
                            "236460302",
                            "13756572",
                            "173991101",
                            "21670658",
                            "252819117"
                        ],
                        "quote": "The unparalleled ability of language models (LMs) to generalize from vast corpora is tinged by an inherent reinforcement of social biases. These biases are not merely encoded within LMs' representations but are also perpetuated to downstream tasks (Blodgett et al., 2021)Sta\u0144czak and Augenstein, 2021), where they can manifest in an uneven treatment of different demographic groups (Rudinger et al., 2018)(Stanovsky et al., 2019)(Kiritchenko et al., 2018)(Venkit et al., 2022)."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[265281188 | Ma et al. | 2023 | Citations: 1]",
                "snippets": "The training of large language models (LLMs) on extensive, unfiltered corpora sourced from the internet is a common and advantageous practice. Consequently, LLMs have learned and inadvertently reproduced various types of biases, including violent, offensive, and toxic language.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 278,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The training of large language models (LLMs) on extensive, unfiltered corpora sourced from the internet is a common and advantageous practice. Consequently, LLMs have learned and inadvertently reproduced various types of biases, including violent, offensive, and toxic language."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[266054040 | Prakash et al. | 2023 | Citations: 3]",
                "snippets": "In recent years, the NLP community has prioritized studying biases in LLMs. Early work by (Bolukbasi et al., 2016) revealed gender and ethnic biases in word embeddings like Word2Vec and GloVe. This trend of identifying biases continued with more complex models like BERT, where researchers examined how biases are encoded and propagated (Kurita et al., 2019)(May et al., 2019). Researchers have also developed datasets, such as StereoSet (Nadeem et al., 2020) and CrowS-Pairs (Nangia et al., 2020), specifically to measure and understand these biases. (Sap et al., 2019) delved into the effects of biased data, especially from human annotators, on the behavior of models. Alongside identification, efforts have been geared towards the mitigation of bias in LLMs. Techniques such as iterative nullspace projection (INLP) (Ravfogel et al., 2020) and Counterfactual Data Augmentation (CDA) (Zmigrod et al., 2019) have been proposed and implemented to mitigate biases in LLMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[1704893 | Bolukbasi et al. | 2016 | Citations: 3153]": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",
                    "[184486914 | Zmigrod et al. | 2019 | Citations: 283]": "Gender stereotypes are manifest in most of the world\u2019s languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages. We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages. For Spanish and Hebrew, our approach achieves F1 scores of 82% and 73% at the level of tags and accuracies of 90% and 87% at the level of forms. By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality.",
                    "[190000105 | Kurita et al. | 2019 | Citations: 451]": "Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.",
                    "[207853290 | Sap et al. | 2019 | Citations: 500]": "Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people\u2019s judgments about others. For example, given a statement that \u201cwe shouldn\u2019t lower our standards to hire more women,\u201d most listeners will infer the implicature intended by the speaker - that \u201cwomen (candidates) are less qualified.\u201d Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.",
                    "[215786522 | Ravfogel et al. | 2020 | Citations: 388]": "The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.",
                    "[215828184 | Nadeem et al. | 2020 | Citations: 1015]": "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.",
                    "[222090785 | Nangia et al. | 2020 | Citations: 685]": "Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.",
                    "[85518027 | May et al. | 2019 | Citations: 603]": "The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test\u2019s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 615,
                        "end": 1585,
                        "sentence_offsets": [
                            {
                                "start": 615,
                                "end": 690
                            },
                            {
                                "start": 691,
                                "end": 806
                            },
                            {
                                "start": 807,
                                "end": 990
                            },
                            {
                                "start": 991,
                                "end": 1164
                            },
                            {
                                "start": 1165,
                                "end": 1283
                            },
                            {
                                "start": 1284,
                                "end": 1374
                            },
                            {
                                "start": 1375,
                                "end": 1585
                            }
                        ],
                        "ref_mentions": [
                            "1704893",
                            "190000105",
                            "85518027",
                            "215828184",
                            "222090785",
                            "207853290",
                            "215786522",
                            "184486914"
                        ],
                        "quote": "In recent years, the NLP community has prioritized studying biases in LLMs. Early work by (Bolukbasi et al., 2016) revealed gender and ethnic biases in word embeddings like Word2Vec and GloVe. This trend of identifying biases continued with more complex models like BERT, where researchers examined how biases are encoded and propagated (Kurita et al., 2019)(May et al., 2019). Researchers have also developed datasets, such as StereoSet (Nadeem et al., 2020) and CrowS-Pairs (Nangia et al., 2020), specifically to measure and understand these biases. (Sap et al., 2019) delved into the effects of biased data, especially from human annotators, on the behavior of models. Alongside identification, efforts have been geared towards the mitigation of bias in LLMs. Techniques such as iterative nullspace projection (INLP) (Ravfogel et al., 2020) and Counterfactual Data Augmentation (CDA) (Zmigrod et al., 2019) have been proposed and implemented to mitigate biases in LLMs."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[266163873 | Goncalves et al. | 2023 | Citations: 11]",
                "snippets": "Social biases in LLMs are an ongoing problem that is propagated from pretraining to finetuning (Ladhak et al., 2023)(Gira et al., 2022). Biased pretrained models are hard to fix, as retraining is prohibitively expensive both financially and environmentally (Hessenthaler et al., 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[248780268 | Gira et al. | 2022 | Citations: 44]": "An explosion in the popularity of transformer-based language models (such as GPT-3, BERT, RoBERTa, and ALBERT) has opened the doors to new machine learning applications involving language modeling, text generation, and more. However, recent scrutiny reveals that these language models contain inherent biases towards certain demographics reflected in their training data. While research has tried mitigating this problem, existing approaches either fail to remove the bias completely, degrade performance (\u201ccatastrophic forgetting\u201d), or are costly to execute. This work examines how to reduce gender bias in a GPT-2 language model by fine-tuning less than 1% of its parameters. Through quantitative benchmarks, we show that this is a viable way to reduce prejudice in pre-trained language models while remaining cost-effective at scale.",
                    "[253397743 | Hessenthaler et al. | 2022 | Citations: 8]": "Fairness and environmental impact are important research directions for the sustainable development of artificial intelligence. However, while each topic is an active research area in natural language processing (NLP), there is a surprising lack of research on the interplay between the two fields. This lacuna is highly problematic, since there is increasing evidence that an exclusive focus on fairness can actually hinder environmental sustainability, and vice versa. In this work, we shed light on this crucial intersection in NLP by (1) investigating the efficiency of current fairness approaches through surveying example methods for reducing unfair stereotypical bias from the literature, and (2) evaluating a common technique to reduce energy consumption (and thus environmental impact) of English NLP models, knowledge distillation (KD), for its impact on fairness. In this case study, we evaluate the effect of important KD factors, including layer and dimensionality reduction, with respect to: (a) performance on the distillation task (natural language inference and semantic similarity prediction), and (b) multiple measures and dimensions of stereotypical bias (e.g., gender bias measured via the Word Embedding Association Test). Our results lead us to clarify current assumptions regarding the effect of KD on unfair bias: contrary to other findings, we show that KD can actually decrease model fairness.",
                    "[258378241 | Ladhak et al. | 2023 | Citations: 58]": "Large language models (LLMs) are subject to sociocultural and other biases previously identified using intrinsic evaluations. However, when and how these intrinsic biases in pre-trained LM representations propagate to downstream, fine-tuned NLP tasks like summarization is not well understood. In this work, we investigate one type of bias\u2014name-nationality bias\u2014and trace it from the pre-training stage to a downstream summarization task across multiple summarization modeling choices. We show that these biases manifest themselves as hallucinations in summarization, leading to factually incorrect summaries. We also find that this propagation of biases is algorithm-dependent: more abstractive models allow biases to propagate more directly to downstream tasks as hallucinated facts. Building on these observations, we further analyze how changes to the adaptation method and fine-tuning data set affect name nationality biases and show that while they can reduce the overall rate of hallucinations, they do not change the types of biases that do appear."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 504,
                        "end": 788,
                        "sentence_offsets": [
                            {
                                "start": 504,
                                "end": 639
                            },
                            {
                                "start": 640,
                                "end": 788
                            }
                        ],
                        "ref_mentions": [
                            "258378241",
                            "248780268",
                            "253397743"
                        ],
                        "quote": "Social biases in LLMs are an ongoing problem that is propagated from pretraining to finetuning (Ladhak et al., 2023)(Gira et al., 2022). Biased pretrained models are hard to fix, as retraining is prohibitively expensive both financially and environmentally (Hessenthaler et al., 2022)."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[267411833 | Gallegos et al. | 2024 | Citations: 23]",
                "snippets": "At the same time as this success, however, LLMs have been shown to learn, reproduce, and even amplify denigrating, stereotypical, and exclusionary social behaviors (e.g., (Bender et al., 2021)(Hutchinson et al., 2020)(Mei et al., 2023)(Sheng et al., 2021)Weidinger et al., 2022). We refer to this class of harms as \"social bias,\" a normative term that characterizes disparate representations, treatments, or outcomes between social groups due to historical and structural power imbalances.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259129801 | Mei et al. | 2023 | Citations: 51]": "Warning: The content of this paper may be upsetting or triggering. The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stigmatized groups. When prompts include stigmatized conditions, the probability of MLMs predicting negative words is, on average, 20 percent higher than when prompts have non-stigmatized conditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative. For example, the sentence \"They are people who have less than a high school education.\" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson\u2019s r =0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigmatized groups.",
                    "[218487466 | Hutchinson et al. | 2020 | Citations: 313]": "Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.",
                    "[234337004 | Sheng et al. | 2021 | Citations: 220]": "Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.",
                    "[262580630 | Bender et al. | 2021 | Citations: 4657]": "The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 628,
                        "end": 1114,
                        "sentence_offsets": [
                            {
                                "start": 628,
                                "end": 904
                            },
                            {
                                "start": 905,
                                "end": 1114
                            }
                        ],
                        "ref_mentions": [
                            "262580630",
                            "218487466",
                            "259129801",
                            "234337004"
                        ],
                        "quote": "At the same time as this success, however, LLMs have been shown to learn, reproduce, and even amplify denigrating, stereotypical, and exclusionary social behaviors (e.g., (Bender et al., 2021)(Hutchinson et al., 2020)(Mei et al., 2023)(Sheng et al., 2021)Weidinger et al., 2022). We refer to this class of harms as \"social bias,\" a normative term that characterizes disparate representations, treatments, or outcomes between social groups due to historical and structural power imbalances."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[267770177 | Yan et al. | 2024 | Citations: 9]",
                "snippets": "Large language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases. Mitigating these biases with fine-tuning could be both costly and data-hungry. Model editing methods, which focus on modifying LLMs in a post-hoc manner, are of great potential to address debiasing. However, it lacks a comprehensive study that facilitates both internal and external model editing methods, supports various bias types, as well as understands the pros and cons of applying editing methods to stereotypical debiasing.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Large language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases. Mitigating these biases with fine-tuning could be both costly and data-hungry. Model editing methods, which focus on modifying LLMs in a post-hoc manner, are of great potential to address debiasing. However, it lacks a comprehensive study that facilitates both internal and external model editing methods, supports various bias types, as well as understands the pros and cons of applying editing methods to stereotypical debiasing.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[268379141 | Li et al. | 2024 | Citations: 12]",
                "snippets": "Large language models (LLMs), despite their remarkable capabilities, are susceptible to generating biased and discriminatory responses. As LLMs increasingly influence high-stakes decision-making (e.g., hiring and healthcare), mitigating these biases becomes critical.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 267,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Large language models (LLMs), despite their remarkable capabilities, are susceptible to generating biased and discriminatory responses. As LLMs increasingly influence high-stakes decision-making (e.g., hiring and healthcare), mitigating these biases becomes critical."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[269449709 | Narayan et al. | 2024 | Citations: 1]",
                "snippets": "The burgeoning influence of Large Language Models (LLMs) in shaping public discourse and decision-making underscores the imperative to address inherent biases within these AI systems.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The burgeoning influence of Large Language Models (LLMs) in shaping public discourse and decision-making underscores the imperative to address inherent biases within these AI systems.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[269773271 | Chen et al. | 2024 | Citations: 14]",
                "snippets": "Pre-trained Large Language Models (LLMs) have demonstrated exceptional performance on many tasks (Devlin et al., 2018;Floridi & Chiriatti, 2020;(Brown et al., 2020).However, the encoded social stereotypes and human-like biases inevitably cause undesired behaviors when deploying LLMs in practice (Zhao et al., 2019;(Navigli et al., 2023)Sheng et al., 2021).Existing approaches to mitigate biases in LLMs are mainly categorized into: (1) Fine-tuning (Zmigrod et al., 2019;Webster et al., 2020;He et al., 2022;Liang et al., 2020;Lauscher et al., 2021), which includes techniques such as re-balanced corpus pre-training, contrastive learning, projection methods, and efficient parameter tuning.(2) Prompt-tuning (Guo et al., 2022)(Yang et al., 2022)Li et al., 2023b;Dong et al., 2023), which involves creating prompts to address social biases.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[248780440 | Guo et al. | 2022 | Citations: 167]": "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models\u2019 understanding abilities, as shown using the GLUE benchmark.",
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                    "[253446867 | Yang et al. | 2022 | Citations: 25]": "Several works have proven that finetuning is an applicable approach for debiasing contextualized word embeddings. Similarly, discrete prompts with semantic meanings have shown to be effective in debiasing tasks. With unfixed mathematical representation at the token level, continuous prompts usually surpass discrete ones at providing a pre-trained language model (PLM) with additional task-specific information. Despite this, relatively few efforts have been made to debias PLMs by prompt tuning with continuous prompts compared to its discrete counterpart. Furthermore, for most debiasing methods that alter a PLM's original parameters, a major problem is the need to not only decrease the bias in the PLM but also to ensure that the PLM does not lose its representation ability. Finetuning methods typically have a hard time maintaining this balance, as they tend to violently remove meanings of attribute words (like the words developing our concepts of \"male\" and \"female\" for gender), which also leads to an unstable and unpredictable training process. In this paper, we propose ADEPT, a method to debias PLMs using prompt tuning while maintaining the delicate balance between removing biases and ensuring representation ability. To achieve this, we propose a new training criterion inspired by manifold learning and equip it with an explicit debiasing term to optimize prompt tuning. In addition, we conduct several experiments with regard to the reliability, quality, and quantity of a previously proposed attribute training corpus in order to obtain a clearer prototype of a certain attribute, which indicates the attribute's position and relative distances to other words on the manifold. We evaluate ADEPT on several widely acknowledged debiasing benchmarks and downstream tasks, and find that it achieves competitive results while maintaining (and in some cases even improving) the PLM's representation ability. We further visualize words' correlation before and after debiasing a PLM, and give some possible explanations for the visible effects.",
                    "[258688053 | Navigli et al. | 2023 | Citations: 283]": "In this article, we introduce and discuss the pervasive issue of bias in the large language models that are currently at the core of mainstream approaches to Natural Language Processing (NLP). We first introduce data selection bias, that is, the bias caused by the choice of texts that make up a training corpus. Then, we survey the different types of social bias evidenced in the text generated by language models trained on such corpora, ranging from gender to age, from sexual orientation to ethnicity, and from religion to culture. We conclude with directions focused on measuring, reducing, and tackling the aforementioned types of bias."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 837,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 164
                            },
                            {
                                "start": 164,
                                "end": 355
                            },
                            {
                                "start": 355,
                                "end": 689
                            },
                            {
                                "start": 689,
                                "end": 837
                            }
                        ],
                        "ref_mentions": [
                            "218971783",
                            "258688053",
                            "248780440",
                            "253446867"
                        ],
                        "quote": "Pre-trained Large Language Models (LLMs) have demonstrated exceptional performance on many tasks (Devlin et al., 2018;Floridi & Chiriatti, 2020;(Brown et al., 2020).However, the encoded social stereotypes and human-like biases inevitably cause undesired behaviors when deploying LLMs in practice (Zhao et al., 2019;(Navigli et al., 2023)Sheng et al., 2021).Existing approaches to mitigate biases in LLMs are mainly categorized into: (1) Fine-tuning (Zmigrod et al., 2019;Webster et al., 2020;He et al., 2022;Liang et al., 2020;Lauscher et al., 2021), which includes techniques such as re-balanced corpus pre-training, contrastive learning, projection methods, and efficient parameter tuning.(2) Prompt-tuning (Guo et al., 2022)(Yang et al., 2022)Li et al., 2023b;Dong et al., 2023), which involves creating prompts to address social biases."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[270095018 | Sun et al. | 2024 | Citations: 1]",
                "snippets": "Large language models (LLMs) can elicit social bias during generations, especially when inference with toxic prompts....Recent studies have revealed that LLMs elicit the potential to exhibit unwelcome negative behaviors, such as the propagation of biases, particularly when provided with leading prompts or instructions [11,21,45]. These biases can contain toxic content with threats or profanity, social stereotypes, or prejudiced perceptions toward certain groups, further perpetuating societal inequities, reinforcing discriminations, and impacting LLMs' applicability [23].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[268379141 | Li et al. | 2024 | Citations: 12]": "Large language models (LLMs), despite their remarkable capabilities, are susceptible to generating biased and discriminatory responses. As LLMs increasingly influence high-stakes decision-making (e.g., hiring and healthcare), mitigating these biases becomes critical. In this work, we propose a causality-guided debiasing framework to tackle social biases, aiming to reduce the objectionable dependence between LLMs' decisions and the social information in the input. Our framework introduces a novel perspective to identify how social information can affect an LLM's decision through different causal pathways. Leveraging these causal insights, we outline principled prompting strategies that regulate these pathways through selection mechanisms. This framework not only unifies existing prompting-based debiasing techniques, but also opens up new directions for reducing bias by encouraging the model to prioritize fact-based reasoning over reliance on biased social cues. We validate our framework through extensive experiments on real-world datasets across multiple domains, demonstrating its effectiveness in debiasing LLM decisions, even with only black-box access to the model."
                },
                "metadata": [
                    {
                        "quote": "Large language models (LLMs) can elicit social bias during generations, especially when inference with toxic prompts",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 271,
                        "end": 729,
                        "sentence_offsets": [
                            {
                                "start": 236,
                                "end": 344
                            },
                            {
                                "start": 344,
                                "end": 555
                            },
                            {
                                "start": 555,
                                "end": 800
                            }
                        ],
                        "ref_mentions": [
                            "268379141"
                        ],
                        "quote": ".Recent studies have revealed that LLMs elicit the potential to exhibit unwelcome negative behaviors, such as the propagation of biases, particularly when provided with leading prompts or instructions [11,21,45]. These biases can contain toxic content with threats or profanity, social stereotypes, or prejudiced perceptions toward certain groups, further perpetuating societal inequities, reinforcing discriminations, and impacting LLMs' applicability [23]."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[270214849 | Liu et al. | 2024 | Citations: 1]",
                "snippets": "Notwithstanding, despite their remarkable performance, LMs suffer from the fairness issue, i.e., they may generate negative texts that are biased against underrepresented demographic groups (e.g., female) in our society (Sheng et al., 2019). For instance, GPT-2 (Radford et al., 2019) tends to generate more negative texts towards females (Huang et al., 2019). Such social biases, termed as global biases (Sheng et al., 2020), stem from the real world corpora due to historical reasons (Basta et al., 2019). Unsurprisingly, LMs reproduce or amplify the biases from the data whereon they are trained (Gehman et al., 2020; Schick et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[246426909 | Ouyang et al. | 2022 | Citations: 13203]": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
                    "[249017743 | Kojima et al. | 2022 | Citations: 4513]": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 738,
                        "end": 1380,
                        "sentence_offsets": [
                            {
                                "start": 611,
                                "end": 816
                            },
                            {
                                "start": 816,
                                "end": 989
                            },
                            {
                                "start": 991,
                                "end": 1232
                            },
                            {
                                "start": 1234,
                                "end": 1352
                            },
                            {
                                "start": 1352,
                                "end": 1498
                            }
                        ],
                        "ref_mentions": [
                            "246426909",
                            "249017743",
                            "160025533"
                        ],
                        "quote": "Notwithstanding, despite their remarkable performance, LMs suffer from the fairness issue, i.e., they may generate negative texts that are biased against underrepresented demographic groups (e.g., female) in our society (Sheng et al., 2019). For instance, GPT-2 (Radford et al., 2019) tends to generate more negative texts towards females (Huang et al., 2019). Such social biases, termed as global biases (Sheng et al., 2020), stem from the real world corpora due to historical reasons (Basta et al., 2019). Unsurprisingly, LMs reproduce or amplify the biases from the data whereon they are trained (Gehman et al., 2020; Schick et al., 2021)."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[270878706 | Raj et al. | 2024 | Citations: 14]",
                "snippets": "Large Language Models (LLMs) perpetuate social biases, reflecting prejudices in their training data and reinforcing societal stereotypes and inequalities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 154,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Large Language Models (LLMs) perpetuate social biases, reflecting prejudices in their training data and reinforcing societal stereotypes and inequalities."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[270924184 | Hida et al. | 2024 | Citations: 20]",
                "snippets": "Large Language Models (LLMs) exhibit considerable social biases, and various studies have tried to evaluate and mitigate these biases accurately.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 65,
                        "end": 210,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Large Language Models (LLMs) exhibit considerable social biases, and various studies have tried to evaluate and mitigate these biases accurately."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[271097745 | Cantini et al. | 2024 | Citations: 7]",
                "snippets": "Biases in data availability, selection, language, and social contexts may collectively reflect prejudices, disparities, and stereotypes that can inadvertently be learned and perpetuated by LLMs, leading to unfair and harmful responses.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 812,
                        "end": 1047,
                        "sentence_offsets": [
                            {
                                "start": 798,
                                "end": 1047
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Biases in data availability, selection, language, and social contexts may collectively reflect prejudices, disparities, and stereotypes that can inadvertently be learned and perpetuated by LLMs, leading to unfair and harmful responses."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[271310069 | Allam | 2024 | Citations: 10]",
                "snippets": "Large Language Models (LLMs) have become pivotal in advancing natural language processing, yet their potential to perpetuate biases poses significant concerns.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 159,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Large Language Models (LLMs) have become pivotal in advancing natural language processing, yet their potential to perpetuate biases poses significant concerns."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[271745282 | Kumar et al. | 2024 | Citations: 18]",
                "snippets": "Despite a fair amount of research happening on AI Safety and fairness, there is still a lack of understanding of measuring societal biases in language models systematically. Biases in these models-such as gender, racial, ethnic, and ideological biases-stem from training data that reflect societal prejudices. Consequently, language models can perpetuate and even amplify these biases, leading to outputs that reinforce stereotypes and disadvantage certain groups (Basta et al., 2019).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[121125604 | Basta et al. | 2019 | Citations: 193]": "Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 547,
                        "end": 1033,
                        "sentence_offsets": [
                            {
                                "start": 547,
                                "end": 720
                            },
                            {
                                "start": 721,
                                "end": 856
                            },
                            {
                                "start": 857,
                                "end": 1033
                            }
                        ],
                        "ref_mentions": [
                            "121125604"
                        ],
                        "quote": "Despite a fair amount of research happening on AI Safety and fairness, there is still a lack of understanding of measuring societal biases in language models systematically. Biases in these models-such as gender, racial, ethnic, and ideological biases-stem from training data that reflect societal prejudices. Consequently, language models can perpetuate and even amplify these biases, leading to outputs that reinforce stereotypes and disadvantage certain groups (Basta et al., 2019)."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[271859735 | Cheng et al. | 2024 | Citations: 2]",
                "snippets": "Multi-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities. However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 291,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Multi-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities. However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[271865498 | Lin et al. | 2024 | Citations: 5]",
                "snippets": "Warning: This paper may contain texts with uncomfortable content.Large Language Models (LLMs) have achieved remarkable performance in various tasks, including those involving multimodal data like speech. However, these models often exhibit biases due to the nature of their training data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 288,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Warning: This paper may contain texts with uncomfortable content.Large Language Models (LLMs) have achieved remarkable performance in various tasks, including those involving multimodal data like speech. However, these models often exhibit biases due to the nature of their training data."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[271902917 | Qureshi et al. | 2024 | Citations: 1]",
                "snippets": "With the introduction of (large) language models, there has been significant concern about the unintended bias such models may inherit from their training data. A number of studies have shown that such models propagate gender stereotypes, as well as geographical and racial bias, among other biases. While existing works tackle this issue by preprocessing data and debiasing embeddings, the proposed methods require a lot of computational resources and annotation effort while being limited to certain types of biases.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "With the introduction of (large) language models, there has been significant concern about the unintended bias such models may inherit from their training data. A number of studies have shown that such models propagate gender stereotypes, as well as geographical and racial bias, among other biases. While existing works tackle this issue by preprocessing data and debiasing embeddings, the proposed methods require a lot of computational resources and annotation effort while being limited to certain types of biases.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[271909713 | Deng et al. | 2024 | Citations: 3]",
                "snippets": "Large language models (LLMs) are trained on extensive text corpora, which inevitably include biased information. Although techniques such as Affective Alignment can mitigate some negative impacts of these biases, existing prompt-based attack methods can still extract these biases from the model's weights. Moreover, these biases frequently appear subtly when LLMs are prompted to perform identical tasks across different demographic groups, thereby camouflaging their presence.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 478,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Large language models (LLMs) are trained on extensive text corpora, which inevitably include biased information. Although techniques such as Affective Alignment can mitigate some negative impacts of these biases, existing prompt-based attack methods can still extract these biases from the model's weights. Moreover, these biases frequently appear subtly when LLMs are prompted to perform identical tasks across different demographic groups, thereby camouflaging their presence."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[271923841 | Chen et al. | 2024 | Citations: 7]",
                "snippets": "Issues related to fairness in LLMs can have dire outcomes, such as the amplification of bias, discrimination, and detrimental effects on marginalized groups. Consequently, substantial efforts are being made to assess and address biases in large language models (Gallegos et al., 2023;Li et al., 2023b;Fan et al., 2024a;Luo et al., 2024).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B Related Works",
                        "pdf_hash": "",
                        "start": 164,
                        "end": 501,
                        "sentence_offsets": [
                            {
                                "start": 164,
                                "end": 321
                            },
                            {
                                "start": 322,
                                "end": 501
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Issues related to fairness in LLMs can have dire outcomes, such as the amplification of bias, discrimination, and detrimental effects on marginalized groups. Consequently, substantial efforts are being made to assess and address biases in large language models (Gallegos et al., 2023;Li et al., 2023b;Fan et al., 2024a;Luo et al., 2024)."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[272826949 | Mirza et al. | 2024 | Citations: 2]",
                "snippets": "Recent advancements in Large Language Models(LLMs) have been notable, yet widespread enterprise adoption remains limited due to various constraints. This paper examines bias in LLMs-a crucial issue affecting their usability, reliability, and fairness. Researchers are developing strategies to mitigate bias, including debiasing layers, specialized reference datasets like Winogender and Winobias, and reinforcement learning with human feedback (RLHF). These techniques have been integrated into the latest LLMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Recent advancements in Large Language Models(LLMs) have been notable, yet widespread enterprise adoption remains limited due to various constraints. This paper examines bias in LLMs-a crucial issue affecting their usability, reliability, and fairness. Researchers are developing strategies to mitigate bias, including debiasing layers, specialized reference datasets like Winogender and Winobias, and reinforcement learning with human feedback (RLHF). These techniques have been integrated into the latest LLMs.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[274965310 | Xu et al. | 2024 | Citations: 1]",
                "snippets": "Natural language processing (NLP) has seen remarkable advancements with the development of large language models (LLMs). Despite these advancements, LLMs often produce socially biased outputs. Recent studies have mainly addressed this problem by prompting LLMs to behave ethically, but this approach results in unacceptable performance degradation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 348,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Natural language processing (NLP) has seen remarkable advancements with the development of large language models (LLMs). Despite these advancements, LLMs often produce socially biased outputs. Recent studies have mainly addressed this problem by prompting LLMs to behave ethically, but this approach results in unacceptable performance degradation."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[275336873 | Zhao et al. | 2025 | Citations: 4]",
                "snippets": "Large Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 114,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Large Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[276317810 | Narnaware et al. | 2025 | Citations: 3]",
                "snippets": "Bias in LMMs is particularly concerning in real-world applications, where fairness and inclusivity are essential for equi- table outcomes. Existing biases in training data often manifest in model responses, leading to unintended but impactful consequences. Addressing these biases is crucial to ensuring that LMMs contribute positively to society while minimizing potential harms.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Bias in LMMs is particularly concerning in real-world applications, where fairness and inclusivity are essential for equi- table outcomes. Existing biases in training data often manifest in model responses, leading to unintended but impactful consequences. Addressing these biases is crucial to ensuring that LMMs contribute positively to society while minimizing potential harms.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[276408214 | Xu et al. | 2025 | Citations: 0]",
                "snippets": "Pre-training large language models (LLMs) on vast text corpora enhances their performance in various natural language processing tasks (Touvron et al., 2023;(Zheng et al., 2023)Chiang et al., 2023) but risks encoding social biases, particularly gender bias, that are implicitly present in uncensored datasets (Liang et al., 2021)Luccioni and Viviano, 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[235623756 | Liang et al. | 2021 | Citations: 394]": "As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.",
                    "[259129398 | Zheng et al. | 2023 | Citations: 4439]": "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 355,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 355
                            }
                        ],
                        "ref_mentions": [
                            "259129398",
                            "235623756"
                        ],
                        "quote": "Pre-training large language models (LLMs) on vast text corpora enhances their performance in various natural language processing tasks (Touvron et al., 2023;(Zheng et al., 2023)Chiang et al., 2023) but risks encoding social biases, particularly gender bias, that are implicitly present in uncensored datasets (Liang et al., 2021)Luccioni and Viviano, 2021)."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[276647994 | Pan et al. | 2025 | Citations: 0]",
                "snippets": "The remarkable performance of Large Language Models (LLMs) is frequently accompanied by the propagation of social bias inherent in their training data (Gallegos et al., 2023)(Hofmann et al., 2024)(Navigli et al., 2023)Cui et al., 2024). These biases raise serious ethical concerns, as they perpetuate stereotypes, reinforce discrimination, and negatively impact real-world decision-making. In domains such as hiring, law enforcement, and content moderation, the use of these models in realworld applications may disproportionately harm marginalized individuals and communities (Parrish et al., 2021)(Nangia et al., 2020)(Nadeem et al., 2020)(Manerba et al., 2023)Bi et al., 2023;del Arco et al., 2024;(Kotek et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[261530629 | Gallegos et al. | 2023 | Citations: 594]": "Abstract Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.",
                    "[265212726 | Manerba et al. | 2023 | Citations: 20]": "While the impact of social biases in language models has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, limiting our understanding of bias complexities. This paper proposes a novel framework for probing language models for social biases by assessing disparate treatment, which involves treating individuals differently according to their affiliation with a sensitive demographic group. We curate SoFa, a large-scale benchmark designed to address the limitations of existing fairness collections. SoFa expands the analysis beyond the binary comparison of stereotypical versus anti-stereotypical identities to include a diverse range of identities and stereotypes. Comparing our methodology with existing benchmarks, we reveal that biases within language models are more nuanced than acknowledged, indicating a broader scope of encoded biases than previously recognized. Benchmarking LMs on SoFa, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models. Finally, our findings indicate that real-life adversities faced by various groups such as women and people with disabilities are mirrored in the behavior of these models.",
                    "[215828184 | Nadeem et al. | 2020 | Citations: 1015]": "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.",
                    "[222090785 | Nangia et al. | 2020 | Citations: 685]": "Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.",
                    "[239010011 | Parrish et al. | 2021 | Citations: 424]": "It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model\u2019s biases override a correct answer choice. We find that models often rely on stereotypes when the context is under-informative, meaning the model\u2019s outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested.",
                    "[258688053 | Navigli et al. | 2023 | Citations: 283]": "In this article, we introduce and discuss the pervasive issue of bias in the large language models that are currently at the core of mainstream approaches to Natural Language Processing (NLP). We first introduce data selection bias, that is, the bias caused by the choice of texts that make up a training corpus. Then, we survey the different types of social bias evidenced in the text generated by language models trained on such corpora, ranging from gender to age, from sexual orientation to ethnicity, and from religion to culture. We conclude with directions focused on measuring, reducing, and tackling the aforementioned types of bias.",
                    "[261276445 | Kotek et al. | 2023 | Citations: 236]": "Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs\u2019 behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women\u2019s occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person\u2019s gender; (b) these choices align with people\u2019s perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in sentence structure 95% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e) LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior. This highlights a key property of these models: LLMs are trained on imbalanced datasets; as such, even with the recent successes of reinforcement learning with human feedback, they tend to reflect those imbalances back at us. As with other types of societal biases, we suggest that LLMs must be carefully tested to ensure that they treat minoritized individuals and communities equitably.",
                    "[272214842 | Hofmann et al. | 2024 | Citations: 77]": "Hundreds of millions of people now interact with language models, with uses ranging from help with writing1,2 to informing hiring decisions3. However, these language models are known to perpetuate systematic racial prejudices, making their judgements biased in problematic ways about groups such as African Americans4\u20137. Although previous research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time, particularly in the United States after the civil rights movement8,9. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice, exhibiting raciolinguistic stereotypes about speakers of African American English (AAE) that are more negative than any human stereotypes about African Americans ever experimentally recorded. By contrast, the language models\u2019 overt stereotypes about African Americans are more positive. Dialect prejudice has the potential for harmful consequences: language models are more likely to suggest that speakers of AAE be assigned less-prestigious jobs, be convicted of crimes and be sentenced to death. Finally, we show that current practices of alleviating racial bias in language models, such as human preference alignment, exacerbate the discrepancy between covert and overt stereotypes, by superficially obscuring the racism that language models maintain on a deeper level. Our findings have far-reaching implications for the fair and safe use of language technology."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 727,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 235
                            },
                            {
                                "start": 236,
                                "end": 388
                            },
                            {
                                "start": 389,
                                "end": 727
                            }
                        ],
                        "ref_mentions": [
                            "261530629",
                            "272214842",
                            "258688053",
                            "239010011",
                            "222090785",
                            "215828184",
                            "265212726",
                            "261276445"
                        ],
                        "quote": "The remarkable performance of Large Language Models (LLMs) is frequently accompanied by the propagation of social bias inherent in their training data (Gallegos et al., 2023)(Hofmann et al., 2024)(Navigli et al., 2023)Cui et al., 2024). These biases raise serious ethical concerns, as they perpetuate stereotypes, reinforce discrimination, and negatively impact real-world decision-making. In domains such as hiring, law enforcement, and content moderation, the use of these models in realworld applications may disproportionately harm marginalized individuals and communities (Parrish et al., 2021)(Nangia et al., 2020)(Nadeem et al., 2020)(Manerba et al., 2023)Bi et al., 2023;del Arco et al., 2024;(Kotek et al., 2023)."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[277667520 | Cantini et al. | 2025 | Citations: 4]",
                "snippets": "Large Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised concerns about embedded biases, which can perpetuate stereotypes and compromise fairness. These biases stem from various sources, including historical inequalities in training data, linguistic imbalances, and adversarial manipulation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Large Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised concerns about embedded biases, which can perpetuate stereotypes and compromise fairness. These biases stem from various sources, including historical inequalities in training data, linguistic imbalances, and adversarial manipulation.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[277667666 | Xiao et al. | 2025 | Citations: 3]",
                "snippets": "LLMs often inherit social stereotypes and biases [96] from the training data (Hofmann et al., 2024)83,92], leading to biased behavior toward specific social groups, particularly in relation to protected attributes such as religion, race, and gender. For instance, GPT-3 (Brown et al., 2020) has been shown to frequently associate Muslims with violent contexts (Abid et al., 2021)[39], and Microsoft's AI chatbot Tay infamously produced racist and inappropriate content after interacting with users on social media [8].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                    "[231603388 | Abid et al. | 2021 | Citations: 555]": "It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66% to 20%, but which is still higher than for other religious groups.",
                    "[272214842 | Hofmann et al. | 2024 | Citations: 77]": "Hundreds of millions of people now interact with language models, with uses ranging from help with writing1,2 to informing hiring decisions3. However, these language models are known to perpetuate systematic racial prejudices, making their judgements biased in problematic ways about groups such as African Americans4\u20137. Although previous research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time, particularly in the United States after the civil rights movement8,9. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice, exhibiting raciolinguistic stereotypes about speakers of African American English (AAE) that are more negative than any human stereotypes about African Americans ever experimentally recorded. By contrast, the language models\u2019 overt stereotypes about African Americans are more positive. Dialect prejudice has the potential for harmful consequences: language models are more likely to suggest that speakers of AAE be assigned less-prestigious jobs, be convicted of crimes and be sentenced to death. Finally, we show that current practices of alleviating racial bias in language models, such as human preference alignment, exacerbate the discrepancy between covert and overt stereotypes, by superficially obscuring the racism that language models maintain on a deeper level. Our findings have far-reaching implications for the fair and safe use of language technology."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 602,
                        "end": 1069,
                        "sentence_offsets": [
                            {
                                "start": 602,
                                "end": 833
                            },
                            {
                                "start": 834,
                                "end": 1069
                            }
                        ],
                        "ref_mentions": [
                            "272214842",
                            "218971783",
                            "231603388"
                        ],
                        "quote": "LLMs often inherit social stereotypes and biases [96] from the training data (Hofmann et al., 2024)83,92], leading to biased behavior toward specific social groups, particularly in relation to protected attributes such as religion, race, and gender. For instance, GPT-3 (Brown et al., 2020) has been shown to frequently associate Muslims with violent contexts (Abid et al., 2021)[39], and Microsoft's AI chatbot Tay infamously produced racist and inappropriate content after interacting with users on social media [8]."
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[277758223 | Kamruzzaman | 2025 | Citations: 0]",
                "snippets": "The widespread integration of these models into commonplace technology has brought to light deep concerns about the biases they encompass, which could serve to perpetuate negative preconceptions and social injustices.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 149,
                        "end": 366,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The widespread integration of these models into commonplace technology has brought to light deep concerns about the biases they encompass, which could serve to perpetuate negative preconceptions and social injustices."
                    }
                ]
            },
            {
                "idx": 39,
                "key": "[277954809 | Udagawa et al. | 2025 | Citations: 0]",
                "snippets": "Large language models (LLMs) acquire general linguistic knowledge from massive-scale pretraining. However, pretraining data mainly comprised of web-crawled texts contain undesirable social biases which can be perpetuated or even amplified by LLMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 247,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Large language models (LLMs) acquire general linguistic knowledge from massive-scale pretraining. However, pretraining data mainly comprised of web-crawled texts contain undesirable social biases which can be perpetuated or even amplified by LLMs."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.026067,
        "cot": "Looking at the user query and the provided quotes, I need to organize the quotes into relevant dimensions that will help create a comprehensive summary about social biases in large language models (LLMs). The query is about how LLMs inherit and amplify social biases from their training data, and recent techniques to mitigate these biases.\n\nFor this summary, I'll organize the content into the following dimensions:\n\n1. \"Introduction to Social Bias in LLMs\": This section will provide background information about what social bias in LLMs is and why it's important to address. This should be a synthesis paragraph to give readers a foundational understanding.\n\n2. \"Sources and Manifestations of Social Bias\": This section will explain where these biases come from (training data) and how they manifest in LLMs. This should be a synthesis paragraph as it requires connecting and explaining relationships between concepts.\n\n3. \"Impacts and Harms of Social Bias\": This section will describe the negative consequences and potential harms of biased LLMs. This should be a synthesis paragraph to properly contextualize the various impacts.\n\n4. \"Bias Mitigation Techniques\": This section will outline the various approaches and techniques developed to mitigate social bias in LLMs. This works best as a list format since there are distinct approaches that can be enumerated.\n\n5. \"Challenges in Bias Mitigation\": This section will discuss the ongoing difficulties in effectively addressing bias in LLMs. This should be a synthesis paragraph to properly analyze the interconnected challenges.\n\nThese dimensions cover the key aspects of the query while ensuring all quotes can be appropriately assigned to relevant sections.",
        "plan": {
            "Introduction to Social Bias in LLMs (synthesis)": [
                0,
                2,
                6,
                20,
                21,
                23,
                32
            ],
            "Sources and Manifestations of Social Bias (synthesis)": [
                3,
                4,
                5,
                8,
                9,
                10,
                13,
                18,
                19,
                22,
                24,
                25,
                26,
                27,
                28,
                34,
                36,
                37,
                39
            ],
            "Impacts and Harms of Social Bias (synthesis)": [
                1,
                7,
                16,
                29,
                33,
                35,
                38
            ],
            "Bias Mitigation Techniques (list)": [
                11,
                14,
                15,
                17,
                30,
                31
            ],
            "Challenges in Bias Mitigation (synthesis)": [
                12
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Social Bias in LLMs",
                "tldr": "Large Language Models (LLMs) frequently exhibit social biases that reflect and sometimes amplify stereotypes present in their training data. These biases can perpetuate harmful generalizations related to gender, race, religion, and other social constructs, raising significant concerns as these models become increasingly deployed in real-world applications. (11 sources)",
                "text": "\nSocial bias in Large Language Models represents a persistent and endemic problem in natural language processing. These models inherently \"inherit and amplify stereotypical judgments and undesirable statistical associations from training corpora\" <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper>. The fundamental issue stems from how LLMs are designed to learn patterns from the vast amounts of human-generated text they are trained on, inevitably absorbing the biases present in those data sources. As demonstrated by Caliskan et al., language models trained on large quantities of human-generated texts encode human-like social biases that mirror implicit associations found in human cognition <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper> <Paper corpusId=\"23163324\" paperTitle=\"(Caliskan et al., 2016)\" isShortName></Paper>.\n\nThese biases manifest as \"harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs\" <Paper corpusId=\"235623756\" paperTitle=\"(Liang et al., 2021)\" isShortName></Paper>. For instance, research has shown that language models can exhibit persistent anti-Muslim bias, with GPT-3 associating \"Muslim\" with \"terrorist\" in 23% of test cases, a rate significantly higher than stereotypical associations for other religious groups <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper> <Paper corpusId=\"231603388\" paperTitle=\"(Abid et al., 2021)\" isShortName></Paper>.\n\nThe prevalence of these biases becomes increasingly concerning as LLMs are rapidly deployed in various real-world applications <Paper corpusId=\"270878706\" paperTitle=\"(Raj et al., 2024)\" isShortName></Paper> <Paper corpusId=\"271310069\" paperTitle=\"(Allam, 2024)\" isShortName></Paper>. Social biases encoded in these models are reflected in downstream tasks such as machine translation, sentiment classification, and natural language generation <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper> <Paper corpusId=\"250391069\" paperTitle=\"(Jentzsch et al., 2023)\" isShortName></Paper> <Paper corpusId=\"190000105\" paperTitle=\"(Kurita et al., 2019)\" isShortName></Paper>. For example, when analyzing gender bias in BERT models used for movie classification, researchers found significant gender biases across multiple training conditions, indicating that \"reflected biases stem from public BERT models rather than task-specific data\" <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper> <Paper corpusId=\"250391069\" paperTitle=\"(Jentzsch et al., 2023)\" isShortName></Paper>.\n\nAs LLMs continue to advance in their capabilities to grasp complex contextual information, harmful biases are \"likely increasingly intertwined with those models\" <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper> <Paper corpusId=\"250391069\" paperTitle=\"(Jentzsch et al., 2023)\" isShortName></Paper>. The widespread adoption of these models makes addressing such biases an \"emerging and important task\" <Paper corpusId=\"248780440\" paperTitle=\"(Guo et al., 2022)\" isShortName></Paper>, particularly as they have been consistently shown to \"exhibit various biases and stereotypes in their generated content\" <Paper corpusId=\"275336873\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270924184\" paperTitle=\"(Hida et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Mei et al., 2023)",
                        "snippets": [
                            "Social bias large language models is an endemic problem models inherit amplify stereotypical judgments undesirable statistical associations from training corpora",
                            "Caliskan et al. (Caliskan et al., 2016) demonstrate that word embeddings and language models (LMs) trained on a large amount of human-generated texts encode human-like social biases. Social biases encoded in these models are also reflected in their downstream tasks such as machine translation, sentiment classification, and natural language generation (Abid et al., 2021)(Jentzsch et al., 2023)[23](Kurita et al., 2019). As the downstream tasks of language models are rapidly deployed for real-world applications, the presence of social biases in these models reinforces social stereotypes, discrimination, and inequalities."
                        ],
                        "paper": {
                            "corpus_id": 259129801,
                            "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks",
                            "authors": [
                                {
                                    "authorId": "2189183000",
                                    "name": "Katelyn Mei"
                                },
                                {
                                    "authorId": "2196943720",
                                    "name": "Sonia Fereidooni"
                                },
                                {
                                    "authorId": "144537437",
                                    "name": "Aylin Caliskan"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Fairness, Accountability and Transparency",
                            "n_citations": 51
                        },
                        "score": 0.97216796875
                    },
                    {
                        "id": "(Caliskan et al., 2016)",
                        "snippets": [
                            "Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\u2014for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology."
                        ],
                        "paper": {
                            "corpus_id": 23163324,
                            "title": "Semantics derived automatically from language corpora contain human-like biases",
                            "authors": [
                                {
                                    "authorId": "144537437",
                                    "name": "Aylin Caliskan"
                                },
                                {
                                    "authorId": "145315445",
                                    "name": "J. Bryson"
                                },
                                {
                                    "authorId": "47735253",
                                    "name": "Arvind Narayanan"
                                }
                            ],
                            "year": 2016,
                            "venue": "Science",
                            "n_citations": 2673
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liang et al., 2021)",
                        "snippets": [
                            "Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation."
                        ],
                        "paper": {
                            "corpus_id": 235623756,
                            "title": "Towards Understanding and Mitigating Social Biases in Language Models",
                            "authors": [
                                {
                                    "authorId": "28130078",
                                    "name": "Paul Pu Liang"
                                },
                                {
                                    "authorId": "2115397918",
                                    "name": "Chiyu Wu"
                                },
                                {
                                    "authorId": "49933077",
                                    "name": "Louis-philippe Morency"
                                },
                                {
                                    "authorId": "145124475",
                                    "name": "R. Salakhutdinov"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 394
                        },
                        "score": 0.9677734375
                    },
                    {
                        "id": "(Abid et al., 2021)",
                        "snippets": [
                            "It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66% to 20%, but which is still higher than for other religious groups."
                        ],
                        "paper": {
                            "corpus_id": 231603388,
                            "title": "Persistent Anti-Muslim Bias in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "144948925",
                                    "name": "Abubakar Abid"
                                },
                                {
                                    "authorId": "77751476",
                                    "name": "Maheen Farooqi"
                                },
                                {
                                    "authorId": "145085305",
                                    "name": "James Y. Zou"
                                }
                            ],
                            "year": 2021,
                            "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
                            "n_citations": 555
                        },
                        "score": 0
                    },
                    {
                        "id": "(Raj et al., 2024)",
                        "snippets": [
                            "Large Language Models (LLMs) perpetuate social biases, reflecting prejudices in their training data and reinforcing societal stereotypes and inequalities."
                        ],
                        "paper": {
                            "corpus_id": 270878706,
                            "title": "Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis",
                            "authors": [
                                {
                                    "authorId": "2261742076",
                                    "name": "Chahat Raj"
                                },
                                {
                                    "authorId": "2125631153",
                                    "name": "A. Mukherjee"
                                },
                                {
                                    "authorId": "2306632484",
                                    "name": "Aylin Caliskan"
                                },
                                {
                                    "authorId": "2261741456",
                                    "name": "Antonios Anastasopoulos"
                                },
                                {
                                    "authorId": "2261887816",
                                    "name": "Ziwei Zhu"
                                }
                            ],
                            "year": 2024,
                            "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
                            "n_citations": 14
                        },
                        "score": 0.9765625
                    },
                    {
                        "id": "(Allam, 2024)",
                        "snippets": [
                            "Large Language Models (LLMs) have become pivotal in advancing natural language processing, yet their potential to perpetuate biases poses significant concerns."
                        ],
                        "paper": {
                            "corpus_id": 271310069,
                            "title": "BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization",
                            "authors": [
                                {
                                    "authorId": "2312204915",
                                    "name": "Ahmed Allam"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 10
                        },
                        "score": 0.96923828125
                    },
                    {
                        "id": "(Jentzsch et al., 2023)",
                        "snippets": [
                            "Pretrained language models are publicly available and constantly finetuned for various real-life applications. As they become capable of grasping complex contextual information, harmful biases are likely increasingly intertwined with those models. This paper analyses gender bias in BERT models with two main contributions: First, a novel bias measure is introduced, defining biases as the difference in sentiment valuation of female and male sample versions. Second, we comprehensively analyse BERT?s biases on the example of a realistic IMDB movie classifier. By systematically varying elements of the training pipeline, we can conclude regarding their impact on the final model bias. Seven different public BERT models in nine training conditions, i.e. 63 models in total, are compared. Almost all conditions yield significant gender biases. Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage."
                        ],
                        "paper": {
                            "corpus_id": 250391069,
                            "title": "Gender Bias in BERT - Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task",
                            "authors": [
                                {
                                    "authorId": "151209594",
                                    "name": "Sophie F. Jentzsch"
                                },
                                {
                                    "authorId": "13671251",
                                    "name": "Cigdem Turan"
                                }
                            ],
                            "year": 2023,
                            "venue": "GEBNLP",
                            "n_citations": 33
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kurita et al., 2019)",
                        "snippets": [
                            "Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases."
                        ],
                        "paper": {
                            "corpus_id": 190000105,
                            "title": "Measuring Bias in Contextualized Word Representations",
                            "authors": [
                                {
                                    "authorId": "147225682",
                                    "name": "Keita Kurita"
                                },
                                {
                                    "authorId": "47963068",
                                    "name": "Nidhi Vyas"
                                },
                                {
                                    "authorId": "18081101",
                                    "name": "Ayush Pareek"
                                },
                                {
                                    "authorId": "1690706",
                                    "name": "A. Black"
                                },
                                {
                                    "authorId": "145317727",
                                    "name": "Yulia Tsvetkov"
                                }
                            ],
                            "year": 2019,
                            "venue": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing",
                            "n_citations": 451
                        },
                        "score": 0
                    },
                    {
                        "id": "(Guo et al., 2022)",
                        "snippets": [
                            "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task."
                        ],
                        "paper": {
                            "corpus_id": 248780440,
                            "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Yue Guo"
                                },
                                {
                                    "authorId": "46285693",
                                    "name": "Yi Yang"
                                },
                                {
                                    "authorId": "144849629",
                                    "name": "A. Abbasi"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 167
                        },
                        "score": 0.97119140625
                    },
                    {
                        "id": "(Zhao et al., 2025)",
                        "snippets": [
                            "Large Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content."
                        ],
                        "paper": {
                            "corpus_id": 275336873,
                            "title": "Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection",
                            "authors": [
                                {
                                    "authorId": "2233316526",
                                    "name": "Yachao Zhao"
                                },
                                {
                                    "authorId": "2266189718",
                                    "name": "Bo Wang"
                                },
                                {
                                    "authorId": "2301797252",
                                    "name": "Yan Wang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.970703125
                    },
                    {
                        "id": "(Hida et al., 2024)",
                        "snippets": [
                            "Large Language Models (LLMs) exhibit considerable social biases, and various studies have tried to evaluate and mitigate these biases accurately."
                        ],
                        "paper": {
                            "corpus_id": 270924184,
                            "title": "Social Bias Evaluation for Large Language Models Requires Prompt Variations",
                            "authors": [
                                {
                                    "authorId": "46186371",
                                    "name": "Rem Hida"
                                },
                                {
                                    "authorId": "143655216",
                                    "name": "Masahiro Kaneko"
                                },
                                {
                                    "authorId": "2269460776",
                                    "name": "Naoaki Okazaki"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 20
                        },
                        "score": 0.9775390625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Sources and Manifestations of Social Bias",
                "tldr": "Large Language Models (LLMs) inherit social biases from their training data that primarily stem from unfiltered internet content, reflecting and sometimes amplifying societal stereotypes. These biases manifest across multiple dimensions including gender, race, religion, disability, and other demographic attributes, affecting various downstream applications. (14 sources)",
                "text": "\nSocial biases in Large Language Models originate primarily from their training methodology and data sources. LLMs are typically \"trained on enormous scale of uncurated Internet-based data\" <Paper corpusId=\"261530629\" paperTitle=\"(Gallegos et al., 2023)\" isShortName></Paper>, which inherently contains biased information reflecting historical and structural power imbalances in society. This uncurated approach means that \"biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models\" <Paper corpusId=\"258170403\" paperTitle=\"(Sharma et al., 2023)\" isShortName></Paper>.\n\nThe sources of these biases are multifaceted. They stem from \"historical inequalities in training data, linguistic imbalances, and adversarial manipulation\" <Paper corpusId=\"277667520\" paperTitle=\"(Cantini et al., 2025)\" isShortName></Paper>, as well as \"data availability, selection, language, and social contexts\" <Paper corpusId=\"271097745\" paperTitle=\"(Cantini et al., 2024)\" isShortName></Paper>. When exposed to such large unstructured datasets, LLMs \"learn and sometimes even amplify the biases present in such data\" <Paper corpusId=\"253762006\" paperTitle=\"(Garimella et al., 2022)\" isShortName></Paper>.\n\nThese biases manifest across multiple demographic dimensions. Studies have consistently demonstrated that LLMs exhibit:\n\n1. **Gender bias**: Models like GPT-2 \"tend to generate more negative texts towards females\" <Paper corpusId=\"270214849\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\n2. **Religious bias**: GPT-3 has been shown to \"frequently associate Muslims with violent contexts\" <Paper corpusId=\"277667666\" paperTitle=\"(Xiao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"231603388\" paperTitle=\"(Abid et al., 2021)\" isShortName></Paper>, with Muslims being \"analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases\" <Paper corpusId=\"277667666\" paperTitle=\"(Xiao et al., 2025)\" isShortName></Paper>.\n\n3. **Racial bias**: Language models embody \"covert racism in the form of dialect prejudice, exhibiting raciolinguistic stereotypes about speakers of African American English (AAE) that are more negative than any human stereotypes about African Americans ever experimentally recorded\" <Paper corpusId=\"277667666\" paperTitle=\"(Xiao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"272214842\" paperTitle=\"(Hofmann et al., 2024)\" isShortName></Paper>.\n\n4. **Disability bias**: Models show \"undesirable biases towards mentions of disability\" <Paper corpusId=\"265212726\" paperTitle=\"(Manerba et al., 2023)\" isShortName></Paper>. For instance, \"gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness\" <Paper corpusId=\"265212726\" paperTitle=\"(Manerba et al., 2023)\" isShortName></Paper> <Paper corpusId=\"218487466\" paperTitle=\"(Hutchinson et al., 2020)\" isShortName></Paper>.\n\n5. **Stigma-related bias**: Studies examining bias against 93 stigmatized groups found that \"when prompts include stigmatized conditions, the probability of MLMs predicting negative words is, on average, 20 percent higher than when prompts have non-stigmatized conditions\" <Paper corpusId=\"267411833\" paperTitle=\"(Gallegos et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper>.\n\nThese biases are not merely contained within the models themselves but are \"perpetuated to downstream tasks\" <Paper corpusId=\"265212726\" paperTitle=\"(Manerba et al., 2023)\" isShortName></Paper>. For example, \"bias against stigmatized groups is also reflected in four downstream sentiment classifiers\" where \"sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative\" <Paper corpusId=\"267411833\" paperTitle=\"(Gallegos et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper>.\n\nEven with advancements in alignment techniques such as Reinforcement Learning from Human Feedback (RLHF), the problem persists. Current practices intended to alleviate racial bias \"exacerbate the discrepancy between covert and overt stereotypes, by superficially obscuring the racism that language models maintain on a deeper level\" <Paper corpusId=\"277667666\" paperTitle=\"(Xiao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"272214842\" paperTitle=\"(Hofmann et al., 2024)\" isShortName></Paper>.\n\nThe manifestation of these biases becomes particularly concerning as LLMs are increasingly \"deployed in applications that affect millions of people\" <Paper corpusId=\"258170403\" paperTitle=\"(Sharma et al., 2023)\" isShortName></Paper>, where their \"inherent biases are harmful to the targeted social groups\" <Paper corpusId=\"258170403\" paperTitle=\"(Sharma et al., 2023)\" isShortName></Paper>. This harmful impact extends to various practical applications, with biases potentially leading to \"unfair predictions based on attributes like race and gender\" <Paper corpusId=\"271859735\" paperTitle=\"(Cheng et al., 2024)\" isShortName></Paper> across multimodal applications and other domains.",
                "citations": [
                    {
                        "id": "(Gallegos et al., 2023)",
                        "snippets": [
                            "Typically trained on an enormous scale of uncurated Internet-based data, LLMs inherit stereotypes, misrepresentations, derogatory and exclusionary language, and other denigrating behaviors that disproportionately affect already-vulnerable and marginalized communities Dodge et al., 2021;Sheng et al., 2021b). These harms are forms of \"social bias,\" a subjective and normative term we broadly use to refer to disparate treatment or outcomes between social groups that arise from historical and structural power asymmetries, which we define and discuss in Section 2. Though LLMs often reflect existing biases, they can amplify these biases too; in either case, the automated reproduction of injustice can reinforce systems of inequity (Benjamin, 2020)."
                        ],
                        "paper": {
                            "corpus_id": 261530629,
                            "title": "Bias and Fairness in Large Language Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2237806749",
                                    "name": "Isabel O. Gallegos"
                                },
                                {
                                    "authorId": "2066337266",
                                    "name": "Ryan A. Rossi"
                                },
                                {
                                    "authorId": "40080808",
                                    "name": "Joe Barrow"
                                },
                                {
                                    "authorId": "35631602",
                                    "name": "Md. Mehrab Tanjim"
                                },
                                {
                                    "authorId": "2109571021",
                                    "name": "Sungchul Kim"
                                },
                                {
                                    "authorId": "2462276",
                                    "name": "Franck Dernoncourt"
                                },
                                {
                                    "authorId": "1500399016",
                                    "name": "Tong Yu"
                                },
                                {
                                    "authorId": "1940556",
                                    "name": "Ruiyi Zhang"
                                },
                                {
                                    "authorId": "47699955",
                                    "name": "Nesreen Ahmed"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computational Linguistics",
                            "n_citations": 594
                        },
                        "score": 0.9892578125
                    },
                    {
                        "id": "(Sharma et al., 2023)",
                        "snippets": [
                            "Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups."
                        ],
                        "paper": {
                            "corpus_id": 258170403,
                            "title": "Evaluation of Social Biases in Recent Large Pre-Trained Models",
                            "authors": [
                                {
                                    "authorId": "2214583125",
                                    "name": "Swapnil Sharma"
                                },
                                {
                                    "authorId": "2214521713",
                                    "name": "Nikita Anand"
                                },
                                {
                                    "authorId": "1415341297",
                                    "name": "V. KranthiKiranG."
                                },
                                {
                                    "authorId": "2214565507",
                                    "name": "Alind Jain"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.96923828125
                    },
                    {
                        "id": "(Cantini et al., 2025)",
                        "snippets": [
                            "Large Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised concerns about embedded biases, which can perpetuate stereotypes and compromise fairness. These biases stem from various sources, including historical inequalities in training data, linguistic imbalances, and adversarial manipulation."
                        ],
                        "paper": {
                            "corpus_id": 277667520,
                            "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
                            "authors": [
                                {
                                    "authorId": "1585232914",
                                    "name": "Riccardo Cantini"
                                },
                                {
                                    "authorId": "96934840",
                                    "name": "A. Orsino"
                                },
                                {
                                    "authorId": "2354558125",
                                    "name": "Massimo Ruggiero"
                                },
                                {
                                    "authorId": "2299780920",
                                    "name": "Domenico Talia"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.970703125
                    },
                    {
                        "id": "(Cantini et al., 2024)",
                        "snippets": [
                            "Biases in data availability, selection, language, and social contexts may collectively reflect prejudices, disparities, and stereotypes that can inadvertently be learned and perpetuated by LLMs, leading to unfair and harmful responses."
                        ],
                        "paper": {
                            "corpus_id": 271097745,
                            "title": "Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation",
                            "authors": [
                                {
                                    "authorId": "1585232914",
                                    "name": "Riccardo Cantini"
                                },
                                {
                                    "authorId": "2310699340",
                                    "name": "Giada Cosenza"
                                },
                                {
                                    "authorId": "96934840",
                                    "name": "A. Orsino"
                                },
                                {
                                    "authorId": "2299780920",
                                    "name": "Domenico Talia"
                                }
                            ],
                            "year": 2024,
                            "venue": "IFIP Working Conference on Database Semantics",
                            "n_citations": 7
                        },
                        "score": 0.9736328125
                    },
                    {
                        "id": "(Garimella et al., 2022)",
                        "snippets": [
                            "BERT-like language models (LMs), when exposed to large unstructured datasets, are known to learn and sometimes even amplify the biases present in such data. These biases generally reflect social stereotypes with respect to gender, race, age, and others."
                        ],
                        "paper": {
                            "corpus_id": 253762006,
                            "title": "Demographic-Aware Language Model Fine-tuning as a Bias Mitigation Technique",
                            "authors": [
                                {
                                    "authorId": "31099365",
                                    "name": "Aparna Garimella"
                                },
                                {
                                    "authorId": "2105984203",
                                    "name": "Rada Mihalcea"
                                },
                                {
                                    "authorId": "2121347719",
                                    "name": "Akhash Amarnath"
                                }
                            ],
                            "year": 2022,
                            "venue": "AACL",
                            "n_citations": 21
                        },
                        "score": 0.98046875
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "Notwithstanding, despite their remarkable performance, LMs suffer from the fairness issue, i.e., they may generate negative texts that are biased against underrepresented demographic groups (e.g., female) in our society (Sheng et al., 2019). For instance, GPT-2 (Radford et al., 2019) tends to generate more negative texts towards females (Huang et al., 2019). Such social biases, termed as global biases (Sheng et al., 2020), stem from the real world corpora due to historical reasons (Basta et al., 2019). Unsurprisingly, LMs reproduce or amplify the biases from the data whereon they are trained (Gehman et al., 2020; Schick et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 270214849,
                            "title": "LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models",
                            "authors": [
                                {
                                    "authorId": "1803285",
                                    "name": "Tianci Liu"
                                },
                                {
                                    "authorId": "51225422",
                                    "name": "Haoyu Wang"
                                },
                                {
                                    "authorId": "1486407811",
                                    "name": "Shiyang Wang"
                                },
                                {
                                    "authorId": "2304607213",
                                    "name": "Yu Cheng"
                                },
                                {
                                    "authorId": "2284861474",
                                    "name": "Jing Gao"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1
                        },
                        "score": 0.97021484375
                    },
                    {
                        "id": "(Xiao et al., 2025)",
                        "snippets": [
                            "LLMs often inherit social stereotypes and biases [96] from the training data (Hofmann et al., 2024)83,92], leading to biased behavior toward specific social groups, particularly in relation to protected attributes such as religion, race, and gender. For instance, GPT-3 (Brown et al., 2020) has been shown to frequently associate Muslims with violent contexts (Abid et al., 2021)[39], and Microsoft's AI chatbot Tay infamously produced racist and inappropriate content after interacting with users on social media [8]."
                        ],
                        "paper": {
                            "corpus_id": 277667666,
                            "title": "Fairness Mediator: Neutralize Stereotype Associations to Mitigate Bias in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2276489683",
                                    "name": "Yisong Xiao"
                                },
                                {
                                    "authorId": "2257572247",
                                    "name": "Aishan Liu"
                                },
                                {
                                    "authorId": "2325884825",
                                    "name": "Siyuan Liang"
                                },
                                {
                                    "authorId": "2237942988",
                                    "name": "Xianglong Liu"
                                },
                                {
                                    "authorId": "2237906923",
                                    "name": "Dacheng Tao"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.9853515625
                    },
                    {
                        "id": "(Abid et al., 2021)",
                        "snippets": [
                            "It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66% to 20%, but which is still higher than for other religious groups."
                        ],
                        "paper": {
                            "corpus_id": 231603388,
                            "title": "Persistent Anti-Muslim Bias in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "144948925",
                                    "name": "Abubakar Abid"
                                },
                                {
                                    "authorId": "77751476",
                                    "name": "Maheen Farooqi"
                                },
                                {
                                    "authorId": "145085305",
                                    "name": "James Y. Zou"
                                }
                            ],
                            "year": 2021,
                            "venue": "AAAI/ACM Conference on AI, Ethics, and Society",
                            "n_citations": 555
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hofmann et al., 2024)",
                        "snippets": [
                            "Hundreds of millions of people now interact with language models, with uses ranging from help with writing1,2 to informing hiring decisions3. However, these language models are known to perpetuate systematic racial prejudices, making their judgements biased in problematic ways about groups such as African Americans4\u20137. Although previous research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time, particularly in the United States after the civil rights movement8,9. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice, exhibiting raciolinguistic stereotypes about speakers of African American English (AAE) that are more negative than any human stereotypes about African Americans ever experimentally recorded. By contrast, the language models\u2019 overt stereotypes about African Americans are more positive. Dialect prejudice has the potential for harmful consequences: language models are more likely to suggest that speakers of AAE be assigned less-prestigious jobs, be convicted of crimes and be sentenced to death. Finally, we show that current practices of alleviating racial bias in language models, such as human preference alignment, exacerbate the discrepancy between covert and overt stereotypes, by superficially obscuring the racism that language models maintain on a deeper level. Our findings have far-reaching implications for the fair and safe use of language technology."
                        ],
                        "paper": {
                            "corpus_id": 272214842,
                            "title": "AI generates covertly racist decisions about people based on their dialect",
                            "authors": [
                                {
                                    "authorId": "2289842227",
                                    "name": "Valentin Hofmann"
                                },
                                {
                                    "authorId": "13014201",
                                    "name": "Pratyusha Kalluri"
                                },
                                {
                                    "authorId": "2256674786",
                                    "name": "Dan Jurafsky"
                                },
                                {
                                    "authorId": "2289843030",
                                    "name": "Sharese King"
                                }
                            ],
                            "year": 2024,
                            "venue": "The Naturalist",
                            "n_citations": 77
                        },
                        "score": 0
                    },
                    {
                        "id": "(Manerba et al., 2023)",
                        "snippets": [
                            "The unparalleled ability of language models (LMs) to generalize from vast corpora is tinged by an inherent reinforcement of social biases. These biases are not merely encoded within LMs' representations but are also perpetuated to downstream tasks (Blodgett et al., 2021)Sta\u0144czak and Augenstein, 2021), where they can manifest in an uneven treatment of different demographic groups (Rudinger et al., 2018)(Stanovsky et al., 2019)(Kiritchenko et al., 2018)(Venkit et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 265212726,
                            "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
                            "authors": [
                                {
                                    "authorId": "2121386115",
                                    "name": "Marta Marchiori Manerba"
                                },
                                {
                                    "authorId": "82563120",
                                    "name": "Karolina Sta\u0144czak"
                                },
                                {
                                    "authorId": "2257013371",
                                    "name": "Riccardo Guidotti"
                                },
                                {
                                    "authorId": "1736067",
                                    "name": "Isabelle Augenstein"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 20
                        },
                        "score": 0.966796875
                    },
                    {
                        "id": "(Hutchinson et al., 2020)",
                        "snippets": [
                            "Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness."
                        ],
                        "paper": {
                            "corpus_id": 218487466,
                            "title": "Social Biases in NLP Models as Barriers for Persons with Disabilities",
                            "authors": [
                                {
                                    "authorId": "2044655623",
                                    "name": "Ben Hutchinson"
                                },
                                {
                                    "authorId": "3331141",
                                    "name": "Vinodkumar Prabhakaran"
                                },
                                {
                                    "authorId": "40081727",
                                    "name": "Emily L. Denton"
                                },
                                {
                                    "authorId": "20825661",
                                    "name": "Kellie Webster"
                                },
                                {
                                    "authorId": "2112887022",
                                    "name": "Yu Zhong"
                                },
                                {
                                    "authorId": "1667883461",
                                    "name": "Stephen Denuyl"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 313
                        },
                        "score": 0
                    },
                    {
                        "id": "(Gallegos et al., 2024)",
                        "snippets": [
                            "At the same time as this success, however, LLMs have been shown to learn, reproduce, and even amplify denigrating, stereotypical, and exclusionary social behaviors (e.g., (Bender et al., 2021)(Hutchinson et al., 2020)(Mei et al., 2023)(Sheng et al., 2021)Weidinger et al., 2022). We refer to this class of harms as \"social bias,\" a normative term that characterizes disparate representations, treatments, or outcomes between social groups due to historical and structural power imbalances."
                        ],
                        "paper": {
                            "corpus_id": 267411833,
                            "title": "Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes",
                            "authors": [
                                {
                                    "authorId": "2237806749",
                                    "name": "Isabel O. Gallegos"
                                },
                                {
                                    "authorId": "2066337266",
                                    "name": "Ryan A. Rossi"
                                },
                                {
                                    "authorId": "40080808",
                                    "name": "Joe Barrow"
                                },
                                {
                                    "authorId": "35631602",
                                    "name": "Md. Mehrab Tanjim"
                                },
                                {
                                    "authorId": "1500399016",
                                    "name": "Tong Yu"
                                },
                                {
                                    "authorId": "1787977",
                                    "name": "Hanieh Deilamsalehy"
                                },
                                {
                                    "authorId": "2283147661",
                                    "name": "Ruiyi Zhang"
                                },
                                {
                                    "authorId": "2261424174",
                                    "name": "Sungchul Kim"
                                },
                                {
                                    "authorId": "2462276",
                                    "name": "Franck Dernoncourt"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 23
                        },
                        "score": 0.9892578125
                    },
                    {
                        "id": "(Mei et al., 2023)",
                        "snippets": [
                            "Social bias large language models is an endemic problem models inherit amplify stereotypical judgments undesirable statistical associations from training corpora",
                            "Caliskan et al. (Caliskan et al., 2016) demonstrate that word embeddings and language models (LMs) trained on a large amount of human-generated texts encode human-like social biases. Social biases encoded in these models are also reflected in their downstream tasks such as machine translation, sentiment classification, and natural language generation (Abid et al., 2021)(Jentzsch et al., 2023)[23](Kurita et al., 2019). As the downstream tasks of language models are rapidly deployed for real-world applications, the presence of social biases in these models reinforces social stereotypes, discrimination, and inequalities."
                        ],
                        "paper": {
                            "corpus_id": 259129801,
                            "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks",
                            "authors": [
                                {
                                    "authorId": "2189183000",
                                    "name": "Katelyn Mei"
                                },
                                {
                                    "authorId": "2196943720",
                                    "name": "Sonia Fereidooni"
                                },
                                {
                                    "authorId": "144537437",
                                    "name": "Aylin Caliskan"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Fairness, Accountability and Transparency",
                            "n_citations": 51
                        },
                        "score": 0.97216796875
                    },
                    {
                        "id": "(Cheng et al., 2024)",
                        "snippets": [
                            "Multi-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities. However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender."
                        ],
                        "paper": {
                            "corpus_id": 271859735,
                            "title": "Social Debiasing for Fair Multi-modal LLMs",
                            "authors": [
                                {
                                    "authorId": "2149241557",
                                    "name": "Harry Cheng"
                                },
                                {
                                    "authorId": "1390575046",
                                    "name": "Yangyang Guo"
                                },
                                {
                                    "authorId": "2273322768",
                                    "name": "Qingpei Guo"
                                },
                                {
                                    "authorId": "2249834712",
                                    "name": "Ming Yang"
                                },
                                {
                                    "authorId": "2247906706",
                                    "name": "Tian Gan"
                                },
                                {
                                    "authorId": "2284688853",
                                    "name": "Liqiang Nie"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.982421875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Impacts and Harms of Social Bias",
                "tldr": "Social biases in Large Language Models (LLMs) result in significant real-world harms that disproportionately affect marginalized communities. These harms manifest as both representational damage\u2014reinforcing negative stereotypes about certain groups\u2014and allocational damage\u2014potentially denying opportunities or resources to members of disadvantaged communities. (11 sources)",
                "text": "\nSocial biases embedded in Large Language Models (LLMs) pose serious concerns as these systems increasingly influence public discourse and decision-making processes <Paper corpusId=\"269449709\" paperTitle=\"(Narayan et al., 2024)\" isShortName></Paper>. The impact of these biases extends beyond theoretical concerns into tangible, real-world harms that disproportionately affect marginalized communities. These harms generally fall into two primary categories: representational and allocational <Paper corpusId=\"259716055\" paperTitle=\"(Kolisko et al., 2023)\" isShortName></Paper>.\n\nRepresentational harms occur when LLMs \"portray some groups negatively or fail to represent them at all\" <Paper corpusId=\"259716055\" paperTitle=\"(Kolisko et al., 2023)\" isShortName></Paper>. This negative portrayal perpetuates stereotypes and reinforces discriminatory attitudes toward specific groups. For instance, studies have shown that biases in LLMs can \"perpetuate stereotypes, reinforce discrimination, and negatively impact real-world decision-making\" <Paper corpusId=\"276647994\" paperTitle=\"(Pan et al., 2025)\" isShortName></Paper>.\n\nAllocational harms, on the other hand, involve \"denying certain groups opportunities or resources\" <Paper corpusId=\"259716055\" paperTitle=\"(Kolisko et al., 2023)\" isShortName></Paper>. These harms have significant implications in critical domains such as \"hiring, law enforcement, and content moderation,\" where biased LLM outputs \"may disproportionately harm marginalized individuals and communities\" <Paper corpusId=\"276647994\" paperTitle=\"(Pan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"222090785\" paperTitle=\"(Nangia et al., 2020)\" isShortName></Paper> <Paper corpusId=\"239010011\" paperTitle=\"(Parrish et al., 2021)\" isShortName></Paper>.\n\nThe impacts of these biases become particularly concerning as LLMs are deployed in applications affecting millions of people <Paper corpusId=\"276317810\" paperTitle=\"(Narnaware et al., 2025)\" isShortName></Paper>. In practical applications, fairness and inclusivity are essential for equitable outcomes, yet \"existing biases in training data often manifest in model responses, leading to unintended but impactful consequences\" <Paper corpusId=\"276317810\" paperTitle=\"(Narnaware et al., 2025)\" isShortName></Paper>. For example, when used in question-answering systems with under-informative contexts, models often rely on stereotypes, consistently reproducing harmful biases <Paper corpusId=\"239010011\" paperTitle=\"(Parrish et al., 2021)\" isShortName></Paper>.\n\nThe consequences of these biases can be particularly dire, resulting in \"amplification of bias, discrimination, and detrimental effects on marginalized groups\" <Paper corpusId=\"271923841\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>. Even more concerning is evidence that LLMs not only reflect existing biases but may actually amplify them beyond what is reflected in human perceptions or ground truth statistics <Paper corpusId=\"261276445\" paperTitle=\"(Kotek et al., 2023)\" isShortName></Paper>. This amplification effect poses a significant risk as these models are \"integrated into commonplace technology,\" potentially perpetuating \"negative preconceptions and social injustices\" on a broader scale <Paper corpusId=\"277758223\" paperTitle=\"(Kamruzzaman, 2025)\" isShortName></Paper>.\n\nFurthermore, the impact of biases extends beyond static representations to generation tasks. As Garimella et al. note, \"mitigating biases in only the representations may not suffice to use these models for language generation tasks, such as auto-completion, summarization, or dialogue generation\" <Paper corpusId=\"236477795\" paperTitle=\"(Garimella et al., 2021)\" isShortName></Paper>. This indicates that even when models perform well on traditional bias benchmarks, they may still produce biased outputs in practical applications.\n\nPerhaps most troubling is the evidence that current bias mitigation techniques, such as human preference alignment, may actually \"exacerbate the discrepancy between covert and overt stereotypes, by superficially obscuring the racism that language models maintain on a deeper level\" <Paper corpusId=\"272214842\" paperTitle=\"(Hofmann et al., 2024)\" isShortName></Paper>. This suggests that addressing the harms of social bias in LLMs requires more fundamental approaches than simply filtering out overtly biased responses.",
                "citations": [
                    {
                        "id": "(Narayan et al., 2024)",
                        "snippets": [
                            "The burgeoning influence of Large Language Models (LLMs) in shaping public discourse and decision-making underscores the imperative to address inherent biases within these AI systems."
                        ],
                        "paper": {
                            "corpus_id": 269449709,
                            "title": "Bias Neutralization Framework: Measuring Fairness in Large Language Models with Bias Intelligence Quotient (BiQ)",
                            "authors": [
                                {
                                    "authorId": "2232497",
                                    "name": "M. Narayan"
                                },
                                {
                                    "authorId": "2298907347",
                                    "name": "John Pasmore"
                                },
                                {
                                    "authorId": "2298908887",
                                    "name": "Elton Sampaio"
                                },
                                {
                                    "authorId": "2298907296",
                                    "name": "Vijay Raghavan"
                                },
                                {
                                    "authorId": "2298907100",
                                    "name": "Gabriella Waters"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.96728515625
                    },
                    {
                        "id": "(Kolisko et al., 2023)",
                        "snippets": [
                            "Social bias in LLMs is concerning because it may result in harm to the targeted social groups. Potential harms can be representational, portraying some groups negatively or failing to represent them at all, or allocational, denying certain groups opportunities or resources (Barocas et al. 2017)."
                        ],
                        "paper": {
                            "corpus_id": 259716055,
                            "title": "Exploring Social Biases of Large Language Models in a College Artificial Intelligence Course",
                            "authors": [
                                {
                                    "authorId": "2222666109",
                                    "name": "Skylar Kolisko"
                                },
                                {
                                    "authorId": "144901955",
                                    "name": "Carolyn Jane Anderson"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 11
                        },
                        "score": 0.9833984375
                    },
                    {
                        "id": "(Pan et al., 2025)",
                        "snippets": [
                            "The remarkable performance of Large Language Models (LLMs) is frequently accompanied by the propagation of social bias inherent in their training data (Gallegos et al., 2023)(Hofmann et al., 2024)(Navigli et al., 2023)Cui et al., 2024). These biases raise serious ethical concerns, as they perpetuate stereotypes, reinforce discrimination, and negatively impact real-world decision-making. In domains such as hiring, law enforcement, and content moderation, the use of these models in realworld applications may disproportionately harm marginalized individuals and communities (Parrish et al., 2021)(Nangia et al., 2020)(Nadeem et al., 2020)(Manerba et al., 2023)Bi et al., 2023;del Arco et al., 2024;(Kotek et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 276647994,
                            "title": "Beneath the Surface: How Large Language Models Reflect Hidden Bias",
                            "authors": [
                                {
                                    "authorId": "2294143417",
                                    "name": "Jinhao Pan"
                                },
                                {
                                    "authorId": "2261742076",
                                    "name": "Chahat Raj"
                                },
                                {
                                    "authorId": "2323112615",
                                    "name": "Ziyu Yao"
                                },
                                {
                                    "authorId": "2220183419",
                                    "name": "Ziwei Zhu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9697265625
                    },
                    {
                        "id": "(Nangia et al., 2020)",
                        "snippets": [
                            "Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress."
                        ],
                        "paper": {
                            "corpus_id": 222090785,
                            "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
                            "authors": [
                                {
                                    "authorId": "10666396",
                                    "name": "Nikita Nangia"
                                },
                                {
                                    "authorId": "3054462",
                                    "name": "Clara Vania"
                                },
                                {
                                    "authorId": "49550275",
                                    "name": "Rasika Bhalerao"
                                },
                                {
                                    "authorId": "3644767",
                                    "name": "Samuel R. Bowman"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 685
                        },
                        "score": 0
                    },
                    {
                        "id": "(Parrish et al., 2021)",
                        "snippets": [
                            "It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model\u2019s biases override a correct answer choice. We find that models often rely on stereotypes when the context is under-informative, meaning the model\u2019s outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested."
                        ],
                        "paper": {
                            "corpus_id": 239010011,
                            "title": "BBQ: A hand-built bias benchmark for question answering",
                            "authors": [
                                {
                                    "authorId": "119389860",
                                    "name": "Alicia Parrish"
                                },
                                {
                                    "authorId": "13336152",
                                    "name": "Angelica Chen"
                                },
                                {
                                    "authorId": "10666396",
                                    "name": "Nikita Nangia"
                                },
                                {
                                    "authorId": "2044959912",
                                    "name": "Vishakh Padmakumar"
                                },
                                {
                                    "authorId": "80842917",
                                    "name": "Jason Phang"
                                },
                                {
                                    "authorId": "2148444557",
                                    "name": "Jana Thompson"
                                },
                                {
                                    "authorId": "41022736",
                                    "name": "Phu Mon Htut"
                                },
                                {
                                    "authorId": "1799822",
                                    "name": "Sam Bowman"
                                }
                            ],
                            "year": 2021,
                            "venue": "Findings",
                            "n_citations": 424
                        },
                        "score": 0
                    },
                    {
                        "id": "(Narnaware et al., 2025)",
                        "snippets": [
                            "Bias in LMMs is particularly concerning in real-world applications, where fairness and inclusivity are essential for equi- table outcomes. Existing biases in training data often manifest in model responses, leading to unintended but impactful consequences. Addressing these biases is crucial to ensuring that LMMs contribute positively to society while minimizing potential harms."
                        ],
                        "paper": {
                            "corpus_id": 276317810,
                            "title": "SB-Bench: Stereotype Bias Benchmark for Large Multimodal Models",
                            "authors": [
                                {
                                    "authorId": "2345186336",
                                    "name": "Vishal Narnaware"
                                },
                                {
                                    "authorId": "2287846115",
                                    "name": "Ashmal Vayani"
                                },
                                {
                                    "authorId": "2110003398",
                                    "name": "Rohit Gupta"
                                },
                                {
                                    "authorId": "143951905",
                                    "name": "S. Swetha"
                                },
                                {
                                    "authorId": "2287971163",
                                    "name": "Mubarak Shah"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.96240234375
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "Issues related to fairness in LLMs can have dire outcomes, such as the amplification of bias, discrimination, and detrimental effects on marginalized groups. Consequently, substantial efforts are being made to assess and address biases in large language models (Gallegos et al., 2023;Li et al., 2023b;Fan et al., 2024a;Luo et al., 2024)."
                        ],
                        "paper": {
                            "corpus_id": 271923841,
                            "title": "Identifying and Mitigating Social Bias Knowledge in Language Models",
                            "authors": [
                                {
                                    "authorId": "2255346632",
                                    "name": "Ruizhe Chen"
                                },
                                {
                                    "authorId": "2301405716",
                                    "name": "Yichen Li"
                                },
                                {
                                    "authorId": "2260614480",
                                    "name": "Jianfei Yang"
                                },
                                {
                                    "authorId": "2253854049",
                                    "name": "Yang Feng"
                                },
                                {
                                    "authorId": "2253900280",
                                    "name": "J. Zhou"
                                },
                                {
                                    "authorId": "2253868642",
                                    "name": "Jian Wu"
                                },
                                {
                                    "authorId": "2311458018",
                                    "name": "Zuozhu Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 7
                        },
                        "score": 0.978515625
                    },
                    {
                        "id": "(Kotek et al., 2023)",
                        "snippets": [
                            "Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs\u2019 behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women\u2019s occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person\u2019s gender; (b) these choices align with people\u2019s perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in sentence structure 95% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e) LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior. This highlights a key property of these models: LLMs are trained on imbalanced datasets; as such, even with the recent successes of reinforcement learning with human feedback, they tend to reflect those imbalances back at us. As with other types of societal biases, we suggest that LLMs must be carefully tested to ensure that they treat minoritized individuals and communities equitably."
                        ],
                        "paper": {
                            "corpus_id": 261276445,
                            "title": "Gender bias and stereotypes in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "3365389",
                                    "name": "Hadas Kotek"
                                },
                                {
                                    "authorId": "90166394",
                                    "name": "Rikker Dockum"
                                },
                                {
                                    "authorId": "32100412",
                                    "name": "David Q. Sun"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Climate Informatics",
                            "n_citations": 236
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kamruzzaman, 2025)",
                        "snippets": [
                            "The widespread integration of these models into commonplace technology has brought to light deep concerns about the biases they encompass, which could serve to perpetuate negative preconceptions and social injustices."
                        ],
                        "paper": {
                            "corpus_id": 277758223,
                            "title": "Investigating and Mitigating Undesirable Biases in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2077526744",
                                    "name": "M. Kamruzzaman"
                                }
                            ],
                            "year": 2025,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 0
                        },
                        "score": 0.9833984375
                    },
                    {
                        "id": "(Garimella et al., 2021)",
                        "snippets": [
                            "Social biases with respect to demographics (e.g., gender, age, race) in datasets are often encoded in the large pre-trained language models trained on them. Prior works have largely focused on mitigating biases in context-free representations, with recent shift to contextual ones. While this is useful for several word and sentence-level classi\ufb01cation tasks, mitigating biases in only the representations may not suf-\ufb01ce to use these models for language generation tasks, such as auto-completion, summarization, or dialogue generation."
                        ],
                        "paper": {
                            "corpus_id": 236477795,
                            "title": "He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation",
                            "authors": [
                                {
                                    "authorId": "31099365",
                                    "name": "Aparna Garimella"
                                },
                                {
                                    "authorId": "2121347719",
                                    "name": "Akhash Amarnath"
                                },
                                {
                                    "authorId": "2110632520",
                                    "name": "K. Kumar"
                                },
                                {
                                    "authorId": "2121368400",
                                    "name": "Akash Pramod Yalla"
                                },
                                {
                                    "authorId": "3365985",
                                    "name": "Anandhavelu Natarajan"
                                },
                                {
                                    "authorId": "2954043",
                                    "name": "Niyati Chhaya"
                                },
                                {
                                    "authorId": "2881425",
                                    "name": "Balaji Vasan Srinivasan"
                                }
                            ],
                            "year": 2021,
                            "venue": "Findings",
                            "n_citations": 59
                        },
                        "score": 0.974609375
                    },
                    {
                        "id": "(Hofmann et al., 2024)",
                        "snippets": [
                            "Hundreds of millions of people now interact with language models, with uses ranging from help with writing1,2 to informing hiring decisions3. However, these language models are known to perpetuate systematic racial prejudices, making their judgements biased in problematic ways about groups such as African Americans4\u20137. Although previous research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time, particularly in the United States after the civil rights movement8,9. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice, exhibiting raciolinguistic stereotypes about speakers of African American English (AAE) that are more negative than any human stereotypes about African Americans ever experimentally recorded. By contrast, the language models\u2019 overt stereotypes about African Americans are more positive. Dialect prejudice has the potential for harmful consequences: language models are more likely to suggest that speakers of AAE be assigned less-prestigious jobs, be convicted of crimes and be sentenced to death. Finally, we show that current practices of alleviating racial bias in language models, such as human preference alignment, exacerbate the discrepancy between covert and overt stereotypes, by superficially obscuring the racism that language models maintain on a deeper level. Our findings have far-reaching implications for the fair and safe use of language technology."
                        ],
                        "paper": {
                            "corpus_id": 272214842,
                            "title": "AI generates covertly racist decisions about people based on their dialect",
                            "authors": [
                                {
                                    "authorId": "2289842227",
                                    "name": "Valentin Hofmann"
                                },
                                {
                                    "authorId": "13014201",
                                    "name": "Pratyusha Kalluri"
                                },
                                {
                                    "authorId": "2256674786",
                                    "name": "Dan Jurafsky"
                                },
                                {
                                    "authorId": "2289843030",
                                    "name": "Sharese King"
                                }
                            ],
                            "year": 2024,
                            "venue": "The Naturalist",
                            "n_citations": 77
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Bias Mitigation Techniques",
                "tldr": "Researchers have developed various techniques to mitigate social biases in Large Language Models, including fine-tuning approaches, model editing methods, prompt-based debiasing, and projection-based approaches that modify representations. Each approach offers different trade-offs between effectiveness, computational cost, and preservation of model performance on downstream tasks. (12 sources)",
                "text": "\nA wide range of techniques have been developed to address social biases in Large Language Models. These approaches generally fall into several categories:\n\n1. **Fine-tuning-based approaches**:\n - **Counterfactual Data Augmentation (CDA)**: This technique involves creating balanced training datasets by generating counterfactual examples that swap demographic attributes, helping models learn more equitable representations <Paper corpusId=\"184486914\" paperTitle=\"(Zmigrod et al., 2019)\" isShortName></Paper> <Paper corpusId=\"269773271\" paperTitle=\"(Chen et al._1, 2024)\" isShortName></Paper>.\n - **Re-balanced corpus pre-training**: Using demographically balanced datasets during pre-training or fine-tuning to reduce the impact of imbalanced representation in original training data <Paper corpusId=\"269773271\" paperTitle=\"(Chen et al._1, 2024)\" isShortName></Paper>.\n - **Contrastive learning**: Teaching models to distinguish between biased and unbiased content through contrastive objectives <Paper corpusId=\"269773271\" paperTitle=\"(Chen et al._1, 2024)\" isShortName></Paper>.\n\n2. **Representation modification approaches**:\n - **Iterative Null-space Projection (INLP)**: This method involves repeatedly training linear classifiers to predict protected attributes, then projecting representations onto their null-space to remove information related to those attributes <Paper corpusId=\"215786522\" paperTitle=\"(Ravfogel et al., 2020)\" isShortName></Paper> <Paper corpusId=\"266054040\" paperTitle=\"(Prakash et al., 2023)\" isShortName></Paper>.\n - **Debiasing layers**: Adding specialized layers to model architectures specifically designed to reduce bias in representations <Paper corpusId=\"272826949\" paperTitle=\"(Mirza et al., 2024)\" isShortName></Paper>.\n - **Model editing methods**: Post-hoc modification techniques that alter specific model weights or activations to reduce stereotypical associations without requiring complete retraining <Paper corpusId=\"267770177\" paperTitle=\"(Yan et al., 2024)\" isShortName></Paper>.\n\n3. **Prompt-based approaches**:\n - **Prompt tuning**: Using specially designed prompts that guide models away from producing biased outputs <Paper corpusId=\"248780440\" paperTitle=\"(Guo et al., 2022)\" isShortName></Paper> <Paper corpusId=\"269773271\" paperTitle=\"(Chen et al._1, 2024)\" isShortName></Paper>.\n - **Auto-Debias**: Automatically searching for biased prompts and then applying distribution alignment loss to mitigate differences in completions across demographic groups <Paper corpusId=\"248780440\" paperTitle=\"(Guo et al., 2022)\" isShortName></Paper>.\n - **Continuous prompt tuning**: Unlike discrete prompts with fixed semantic meanings, continuous prompts use unfixed mathematical representations at the token level to debias models more effectively <Paper corpusId=\"253446867\" paperTitle=\"(Yang et al., 2022)\" isShortName></Paper>.\n\n4. **Feedback-based approaches**:\n - **Reinforcement Learning from Human Feedback (RLHF)**: Using human evaluations to guide model outputs away from biased responses <Paper corpusId=\"272826949\" paperTitle=\"(Mirza et al., 2024)\" isShortName></Paper>.\n - **Ethical prompting**: Instructing models to behave ethically, though this approach often results in performance degradation <Paper corpusId=\"274965310\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>.\n\n5. **Evaluation and benchmarking tools**:\n - **Specialized reference datasets**: Resources like Winogender and Winobias provide standardized benchmarks for measuring and addressing gender bias <Paper corpusId=\"272826949\" paperTitle=\"(Mirza et al., 2024)\" isShortName></Paper>.\n - **Stereotype measurement datasets**: Tools like StereoSet <Paper corpusId=\"215828184\" paperTitle=\"(Nadeem et al., 2020)\" isShortName></Paper> and CrowS-Pairs <Paper corpusId=\"222090785\" paperTitle=\"(Nangia et al., 2020)\" isShortName></Paper> enable systematic evaluation of stereotypical biases across different domains and protected attributes.\n\nEach approach involves trade-offs between debiasing effectiveness, computational cost, and preservation of model performance. As bias mitigation becomes increasingly critical for responsible AI deployment in high-stakes contexts like healthcare and hiring <Paper corpusId=\"268379141\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>, developing techniques that effectively reduce bias without compromising model utility remains an active area of research.",
                "citations": [
                    {
                        "id": "(Zmigrod et al., 2019)",
                        "snippets": [
                            "Gender stereotypes are manifest in most of the world\u2019s languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages. We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages. For Spanish and Hebrew, our approach achieves F1 scores of 82% and 73% at the level of tags and accuracies of 90% and 87% at the level of forms. By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality."
                        ],
                        "paper": {
                            "corpus_id": 184486914,
                            "title": "Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology",
                            "authors": [
                                {
                                    "authorId": "51044403",
                                    "name": "Ran Zmigrod"
                                },
                                {
                                    "authorId": "27689253",
                                    "name": "Sabrina J. Mielke"
                                },
                                {
                                    "authorId": "1831395",
                                    "name": "Hanna M. Wallach"
                                },
                                {
                                    "authorId": "1750769",
                                    "name": "Ryan Cotterell"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 283
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al._1, 2024)",
                        "snippets": [
                            "Pre-trained Large Language Models (LLMs) have demonstrated exceptional performance on many tasks (Devlin et al., 2018;Floridi & Chiriatti, 2020;(Brown et al., 2020).However, the encoded social stereotypes and human-like biases inevitably cause undesired behaviors when deploying LLMs in practice (Zhao et al., 2019;(Navigli et al., 2023)Sheng et al., 2021).Existing approaches to mitigate biases in LLMs are mainly categorized into: (1) Fine-tuning (Zmigrod et al., 2019;Webster et al., 2020;He et al., 2022;Liang et al., 2020;Lauscher et al., 2021), which includes techniques such as re-balanced corpus pre-training, contrastive learning, projection methods, and efficient parameter tuning.(2) Prompt-tuning (Guo et al., 2022)(Yang et al., 2022)Li et al., 2023b;Dong et al., 2023), which involves creating prompts to address social biases."
                        ],
                        "paper": {
                            "corpus_id": 269773271,
                            "title": "Large Language Model Bias Mitigation from the Perspective of Knowledge Editing",
                            "authors": [
                                {
                                    "authorId": "2255346632",
                                    "name": "Ruizhe Chen"
                                },
                                {
                                    "authorId": "2301405716",
                                    "name": "Yichen Li"
                                },
                                {
                                    "authorId": "2257215912",
                                    "name": "Zikai Xiao"
                                },
                                {
                                    "authorId": "2155333146",
                                    "name": "Zuo-Qiang Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 14
                        },
                        "score": 0.98046875
                    },
                    {
                        "id": "(Ravfogel et al., 2020)",
                        "snippets": [
                            "The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification."
                        ],
                        "paper": {
                            "corpus_id": 215786522,
                            "title": "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection",
                            "authors": [
                                {
                                    "authorId": "51432464",
                                    "name": "Shauli Ravfogel"
                                },
                                {
                                    "authorId": "51131518",
                                    "name": "Yanai Elazar"
                                },
                                {
                                    "authorId": "1821892",
                                    "name": "Hila Gonen"
                                },
                                {
                                    "authorId": "102707804",
                                    "name": "Michael Twiton"
                                },
                                {
                                    "authorId": "79775260",
                                    "name": "Yoav Goldberg"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 388
                        },
                        "score": 0
                    },
                    {
                        "id": "(Prakash et al., 2023)",
                        "snippets": [
                            "In recent years, the NLP community has prioritized studying biases in LLMs. Early work by (Bolukbasi et al., 2016) revealed gender and ethnic biases in word embeddings like Word2Vec and GloVe. This trend of identifying biases continued with more complex models like BERT, where researchers examined how biases are encoded and propagated (Kurita et al., 2019)(May et al., 2019). Researchers have also developed datasets, such as StereoSet (Nadeem et al., 2020) and CrowS-Pairs (Nangia et al., 2020), specifically to measure and understand these biases. (Sap et al., 2019) delved into the effects of biased data, especially from human annotators, on the behavior of models. Alongside identification, efforts have been geared towards the mitigation of bias in LLMs. Techniques such as iterative nullspace projection (INLP) (Ravfogel et al., 2020) and Counterfactual Data Augmentation (CDA) (Zmigrod et al., 2019) have been proposed and implemented to mitigate biases in LLMs."
                        ],
                        "paper": {
                            "corpus_id": 266054040,
                            "title": "Layered Bias: Interpreting Bias in Pretrained Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2218633063",
                                    "name": "Nirmalendu Prakash"
                                },
                                {
                                    "authorId": "2261922609",
                                    "name": "Roy Ka-Wei Lee"
                                }
                            ],
                            "year": 2023,
                            "venue": "BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
                            "n_citations": 3
                        },
                        "score": 0.97509765625
                    },
                    {
                        "id": "(Mirza et al., 2024)",
                        "snippets": [
                            "Recent advancements in Large Language Models(LLMs) have been notable, yet widespread enterprise adoption remains limited due to various constraints. This paper examines bias in LLMs-a crucial issue affecting their usability, reliability, and fairness. Researchers are developing strategies to mitigate bias, including debiasing layers, specialized reference datasets like Winogender and Winobias, and reinforcement learning with human feedback (RLHF). These techniques have been integrated into the latest LLMs."
                        ],
                        "paper": {
                            "corpus_id": 272826949,
                            "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios",
                            "authors": [
                                {
                                    "authorId": "2322445184",
                                    "name": "Vishal Mirza"
                                },
                                {
                                    "authorId": "2322445481",
                                    "name": "Rahul Kulkarni"
                                },
                                {
                                    "authorId": "2322445728",
                                    "name": "Aakanksha Jadhav"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.98095703125
                    },
                    {
                        "id": "(Yan et al., 2024)",
                        "snippets": [
                            "Large language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases. Mitigating these biases with fine-tuning could be both costly and data-hungry. Model editing methods, which focus on modifying LLMs in a post-hoc manner, are of great potential to address debiasing. However, it lacks a comprehensive study that facilitates both internal and external model editing methods, supports various bias types, as well as understands the pros and cons of applying editing methods to stereotypical debiasing."
                        ],
                        "paper": {
                            "corpus_id": 267770177,
                            "title": "Potential and Challenges of Model Editing for Social Debiasing",
                            "authors": [
                                {
                                    "authorId": "134233854",
                                    "name": "Jianhao Yan"
                                },
                                {
                                    "authorId": "2285582246",
                                    "name": "Futing Wang"
                                },
                                {
                                    "authorId": "2110450452",
                                    "name": "Yafu Li"
                                },
                                {
                                    "authorId": "2249762135",
                                    "name": "Yue Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.97119140625
                    },
                    {
                        "id": "(Guo et al., 2022)",
                        "snippets": [
                            "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task."
                        ],
                        "paper": {
                            "corpus_id": 248780440,
                            "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Yue Guo"
                                },
                                {
                                    "authorId": "46285693",
                                    "name": "Yi Yang"
                                },
                                {
                                    "authorId": "144849629",
                                    "name": "A. Abbasi"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 167
                        },
                        "score": 0.97119140625
                    },
                    {
                        "id": "(Yang et al., 2022)",
                        "snippets": [
                            "Several works have proven that finetuning is an applicable approach for debiasing contextualized word embeddings. Similarly, discrete prompts with semantic meanings have shown to be effective in debiasing tasks. With unfixed mathematical representation at the token level, continuous prompts usually surpass discrete ones at providing a pre-trained language model (PLM) with additional task-specific information. Despite this, relatively few efforts have been made to debias PLMs by prompt tuning with continuous prompts compared to its discrete counterpart. Furthermore, for most debiasing methods that alter a PLM's original parameters, a major problem is the need to not only decrease the bias in the PLM but also to ensure that the PLM does not lose its representation ability. Finetuning methods typically have a hard time maintaining this balance, as they tend to violently remove meanings of attribute words (like the words developing our concepts of \"male\" and \"female\" for gender), which also leads to an unstable and unpredictable training process. In this paper, we propose ADEPT, a method to debias PLMs using prompt tuning while maintaining the delicate balance between removing biases and ensuring representation ability. To achieve this, we propose a new training criterion inspired by manifold learning and equip it with an explicit debiasing term to optimize prompt tuning. In addition, we conduct several experiments with regard to the reliability, quality, and quantity of a previously proposed attribute training corpus in order to obtain a clearer prototype of a certain attribute, which indicates the attribute's position and relative distances to other words on the manifold. We evaluate ADEPT on several widely acknowledged debiasing benchmarks and downstream tasks, and find that it achieves competitive results while maintaining (and in some cases even improving) the PLM's representation ability. We further visualize words' correlation before and after debiasing a PLM, and give some possible explanations for the visible effects."
                        ],
                        "paper": {
                            "corpus_id": 253446867,
                            "title": "ADEPT: A DEbiasing PrompT Framework",
                            "authors": [
                                {
                                    "authorId": "2277527247",
                                    "name": "Ke Yang"
                                },
                                {
                                    "authorId": "2110963190",
                                    "name": "Charles Yu"
                                },
                                {
                                    "authorId": "51135899",
                                    "name": "Y. Fung"
                                },
                                {
                                    "authorId": "2118482058",
                                    "name": "Manling Li"
                                },
                                {
                                    "authorId": "2113323573",
                                    "name": "Heng Ji"
                                }
                            ],
                            "year": 2022,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 25
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "Natural language processing (NLP) has seen remarkable advancements with the development of large language models (LLMs). Despite these advancements, LLMs often produce socially biased outputs. Recent studies have mainly addressed this problem by prompting LLMs to behave ethically, but this approach results in unacceptable performance degradation."
                        ],
                        "paper": {
                            "corpus_id": 274965310,
                            "title": "Mitigating Social Bias in Large Language Models: A Multi-Objective Approach within a Multi-Agent Framework",
                            "authors": [
                                {
                                    "authorId": "2336859072",
                                    "name": "Zhenjie Xu"
                                },
                                {
                                    "authorId": "2279760195",
                                    "name": "Wenqing Chen"
                                },
                                {
                                    "authorId": "2336730514",
                                    "name": "Yi Tang"
                                },
                                {
                                    "authorId": "2336828833",
                                    "name": "Xuanying Li"
                                },
                                {
                                    "authorId": "2336830483",
                                    "name": "Cheng Hu"
                                },
                                {
                                    "authorId": "2303254578",
                                    "name": "Zhixuan Chu"
                                },
                                {
                                    "authorId": "2302800539",
                                    "name": "Kui Ren"
                                },
                                {
                                    "authorId": "2316508020",
                                    "name": "Zibin Zheng"
                                },
                                {
                                    "authorId": "2336738374",
                                    "name": "Zhichao Lu School of Software Engineering"
                                },
                                {
                                    "authorId": "89574632",
                                    "name": "Sun Yat-sen University"
                                },
                                {
                                    "authorId": "89909107",
                                    "name": "S. O. Physics"
                                },
                                {
                                    "authorId": "2063003285",
                                    "name": "Astronomy"
                                },
                                {
                                    "authorId": "102700312",
                                    "name": "School of Materials Science"
                                },
                                {
                                    "authorId": "2321564437",
                                    "name": "Technology"
                                },
                                {
                                    "authorId": "103231213",
                                    "name": "Zhejiang University"
                                },
                                {
                                    "authorId": "2212824158",
                                    "name": "Department of Computer Science"
                                },
                                {
                                    "authorId": "120391927",
                                    "name": "City University of Hong Kong"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.97900390625
                    },
                    {
                        "id": "(Nadeem et al., 2020)",
                        "snippets": [
                            "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu."
                        ],
                        "paper": {
                            "corpus_id": 215828184,
                            "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
                            "authors": [
                                {
                                    "authorId": "50411022",
                                    "name": "Moin Nadeem"
                                },
                                {
                                    "authorId": "78850252",
                                    "name": "Anna Bethke"
                                },
                                {
                                    "authorId": "145732771",
                                    "name": "Siva Reddy"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 1015
                        },
                        "score": 0
                    },
                    {
                        "id": "(Nangia et al., 2020)",
                        "snippets": [
                            "Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress."
                        ],
                        "paper": {
                            "corpus_id": 222090785,
                            "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
                            "authors": [
                                {
                                    "authorId": "10666396",
                                    "name": "Nikita Nangia"
                                },
                                {
                                    "authorId": "3054462",
                                    "name": "Clara Vania"
                                },
                                {
                                    "authorId": "49550275",
                                    "name": "Rasika Bhalerao"
                                },
                                {
                                    "authorId": "3644767",
                                    "name": "Samuel R. Bowman"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 685
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Large language models (LLMs), despite their remarkable capabilities, are susceptible to generating biased and discriminatory responses. As LLMs increasingly influence high-stakes decision-making (e.g., hiring and healthcare), mitigating these biases becomes critical."
                        ],
                        "paper": {
                            "corpus_id": 268379141,
                            "title": "Prompting Fairness: Integrating Causality to Debias Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2291078963",
                                    "name": "Jingling Li"
                                },
                                {
                                    "authorId": "2125563094",
                                    "name": "Zeyu Tang"
                                },
                                {
                                    "authorId": "2181841542",
                                    "name": "Xiaoyu Liu"
                                },
                                {
                                    "authorId": "143648560",
                                    "name": "P. Spirtes"
                                },
                                {
                                    "authorId": "2268848576",
                                    "name": "Kun Zhang"
                                },
                                {
                                    "authorId": "51435222",
                                    "name": "Liu Leqi"
                                },
                                {
                                    "authorId": "2268439518",
                                    "name": "Yang Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 12
                        },
                        "score": 0.98779296875
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Challenges in Bias Mitigation",
                "tldr": "Despite significant progress in developing bias mitigation techniques for LLMs, researchers face substantial challenges including computational costs, environmental impact, and the persistence of biases through fine-tuning. Complete bias elimination remains elusive as improvements in one dimension can lead to new biases or performance degradation in other areas. (4 sources)",
                "text": "\nMitigating social bias in Large Language Models faces several significant challenges that complicate implementation of effective solutions. One of the most substantial barriers is the prohibitive cost associated with retraining large models. As Goncalves et al. note, \"biased pretrained models are hard to fix, as retraining is prohibitively expensive both financially and environmentally\" <Paper corpusId=\"266163873\" paperTitle=\"(Goncalves et al., 2023)\" isShortName></Paper> <Paper corpusId=\"253397743\" paperTitle=\"(Hessenthaler et al., 2022)\" isShortName></Paper>. This creates a practical limitation for researchers and organizations without access to extensive computational resources.\n\nThe challenge of bias persistence across model development stages presents another significant obstacle. Research has shown that \"social biases in LLMs are an ongoing problem that is propagated from pretraining to finetuning\" <Paper corpusId=\"266163873\" paperTitle=\"(Goncalves et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258378241\" paperTitle=\"(Ladhak et al., 2023)\" isShortName></Paper> <Paper corpusId=\"248780268\" paperTitle=\"(Gira et al., 2022)\" isShortName></Paper>. This means that biases embedded during pre-training can persist even after fine-tuning for specific downstream tasks, making comprehensive bias mitigation particularly difficult.\n\nCurrent approaches to bias mitigation often face trade-offs between effectiveness and performance. Many existing methods \"either fail to remove the bias completely, degrade performance ('catastrophic forgetting'), or are costly to execute\" <Paper corpusId=\"266163873\" paperTitle=\"(Goncalves et al., 2023)\" isShortName></Paper> <Paper corpusId=\"248780268\" paperTitle=\"(Gira et al., 2022)\" isShortName></Paper>. This creates a challenging balance between reducing bias and maintaining model utility for intended applications.\n\nThe interplay between fairness and environmental sustainability presents another complex challenge. There is \"increasing evidence that an exclusive focus on fairness can actually hinder environmental sustainability, and vice versa\" <Paper corpusId=\"266163873\" paperTitle=\"(Goncalves et al., 2023)\" isShortName></Paper> <Paper corpusId=\"253397743\" paperTitle=\"(Hessenthaler et al., 2022)\" isShortName></Paper>. This tension between different ethical considerations complicates the development of holistic approaches to bias mitigation.\n\nAdditionally, biases in models can manifest in unexpected ways across different applications. For example, in summarization tasks, name-nationality biases from pre-training can lead to factual hallucinations in generated summaries, with \"more abstractive models allow biases to propagate more directly to downstream tasks as hallucinated facts\" <Paper corpusId=\"266163873\" paperTitle=\"(Goncalves et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258378241\" paperTitle=\"(Ladhak et al., 2023)\" isShortName></Paper>. This highlights the challenge of comprehensively addressing biases across diverse applications and model architectures.\n\nEven well-intentioned mitigation approaches can have unexpected consequences. For instance, knowledge distillation\u2014a technique often used to reduce the environmental impact of large models\u2014can \"actually decrease model fairness\" in some contexts <Paper corpusId=\"266163873\" paperTitle=\"(Goncalves et al., 2023)\" isShortName></Paper> <Paper corpusId=\"253397743\" paperTitle=\"(Hessenthaler et al., 2022)\" isShortName></Paper>. This demonstrates how solutions targeting one aspect of AI ethics might inadvertently exacerbate problems in another dimension.\n\nThese challenges underscore the need for more holistic, efficient, and accessible approaches to bias mitigation that can address the multifaceted nature of social biases in large language models while considering broader ethical and practical constraints.",
                "citations": [
                    {
                        "id": "(Goncalves et al., 2023)",
                        "snippets": [
                            "Social biases in LLMs are an ongoing problem that is propagated from pretraining to finetuning (Ladhak et al., 2023)(Gira et al., 2022). Biased pretrained models are hard to fix, as retraining is prohibitively expensive both financially and environmentally (Hessenthaler et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 266163873,
                            "title": "Understanding the Effect of Model Compression on Social Bias in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2273536347",
                                    "name": "Gustavo Gon\u00e7alves"
                                },
                                {
                                    "authorId": "2268272",
                                    "name": "Emma Strubell"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 11
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(Hessenthaler et al., 2022)",
                        "snippets": [
                            "Fairness and environmental impact are important research directions for the sustainable development of artificial intelligence. However, while each topic is an active research area in natural language processing (NLP), there is a surprising lack of research on the interplay between the two fields. This lacuna is highly problematic, since there is increasing evidence that an exclusive focus on fairness can actually hinder environmental sustainability, and vice versa. In this work, we shed light on this crucial intersection in NLP by (1) investigating the efficiency of current fairness approaches through surveying example methods for reducing unfair stereotypical bias from the literature, and (2) evaluating a common technique to reduce energy consumption (and thus environmental impact) of English NLP models, knowledge distillation (KD), for its impact on fairness. In this case study, we evaluate the effect of important KD factors, including layer and dimensionality reduction, with respect to: (a) performance on the distillation task (natural language inference and semantic similarity prediction), and (b) multiple measures and dimensions of stereotypical bias (e.g., gender bias measured via the Word Embedding Association Test). Our results lead us to clarify current assumptions regarding the effect of KD on unfair bias: contrary to other findings, we show that KD can actually decrease model fairness."
                        ],
                        "paper": {
                            "corpus_id": 253397743,
                            "title": "Bridging Fairness and Environmental Sustainability in Natural Language Processing",
                            "authors": [
                                {
                                    "authorId": "2190173964",
                                    "name": "Marius Hessenthaler"
                                },
                                {
                                    "authorId": "2268272",
                                    "name": "Emma Strubell"
                                },
                                {
                                    "authorId": "2022288",
                                    "name": "Dirk Hovy"
                                },
                                {
                                    "authorId": "29891652",
                                    "name": "Anne Lauscher"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 8
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ladhak et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) are subject to sociocultural and other biases previously identified using intrinsic evaluations. However, when and how these intrinsic biases in pre-trained LM representations propagate to downstream, fine-tuned NLP tasks like summarization is not well understood. In this work, we investigate one type of bias\u2014name-nationality bias\u2014and trace it from the pre-training stage to a downstream summarization task across multiple summarization modeling choices. We show that these biases manifest themselves as hallucinations in summarization, leading to factually incorrect summaries. We also find that this propagation of biases is algorithm-dependent: more abstractive models allow biases to propagate more directly to downstream tasks as hallucinated facts. Building on these observations, we further analyze how changes to the adaptation method and fine-tuning data set affect name nationality biases and show that while they can reduce the overall rate of hallucinations, they do not change the types of biases that do appear."
                        ],
                        "paper": {
                            "corpus_id": 258378241,
                            "title": "When Do Pre-Training Biases Propagate to Downstream Tasks? A Case Study in Text Summarization",
                            "authors": [
                                {
                                    "authorId": "8759332",
                                    "name": "Faisal Ladhak"
                                },
                                {
                                    "authorId": "41152329",
                                    "name": "Esin Durmus"
                                },
                                {
                                    "authorId": "51903517",
                                    "name": "Mirac Suzgun"
                                },
                                {
                                    "authorId": "2146332311",
                                    "name": "Tianyi Zhang"
                                },
                                {
                                    "authorId": "1746807",
                                    "name": "Dan Jurafsky"
                                },
                                {
                                    "authorId": "145590324",
                                    "name": "K. McKeown"
                                },
                                {
                                    "authorId": "2117567142",
                                    "name": "Tatsunori Hashimoto"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 58
                        },
                        "score": 0
                    },
                    {
                        "id": "(Gira et al., 2022)",
                        "snippets": [
                            "An explosion in the popularity of transformer-based language models (such as GPT-3, BERT, RoBERTa, and ALBERT) has opened the doors to new machine learning applications involving language modeling, text generation, and more. However, recent scrutiny reveals that these language models contain inherent biases towards certain demographics reflected in their training data. While research has tried mitigating this problem, existing approaches either fail to remove the bias completely, degrade performance (\"catastrophic forgetting\"), or are costly to execute. This work examines how to reduce gender bias in a GPT-2 language model by fine-tuning less than 1% of its parameters. Through quantitative benchmarks, we show that this is a viable way to reduce prejudice in pre-trained language models while remaining cost-effective at scale."
                        ],
                        "paper": {
                            "corpus_id": 248780268,
                            "title": "Debiasing Pre-Trained Language Models via Efficient Fine-Tuning",
                            "authors": [
                                {
                                    "authorId": "2165226185",
                                    "name": "Michael Gira"
                                },
                                {
                                    "authorId": "2165321339",
                                    "name": "Ruisu Zhang"
                                },
                                {
                                    "authorId": "2115495251",
                                    "name": "Kangwook Lee"
                                }
                            ],
                            "year": 2022,
                            "venue": "LTEDI",
                            "n_citations": 44
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.173823
    }
}