{
    "query": "What are the most recent methods and benchmarks for detecting whether a given text was generated by a large language model, including classifier-based approaches, zero-shot detection, and watermarking techniques?",
    "user_id": "lib_user",
    "task_id": "a2196856-e16a-42eb-9c6b-3619dc645db7",
    "timestamp": "2025-06-23T23:20:13.880482",
    "n_retrieval": 256,
    "n_retrieved": 271,
    "n_candidates": 41,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.707844,
    "decomposed_query": {
        "rewritten_query": "Methods and benchmarks for detecting whether text was generated by a large language model, including classifier-based approaches, zero-shot detection, and watermarking techniques.",
        "keyword_query": "text detection large language model classifier-based zero-shot watermarking techniques benchmarks",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010101,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.02787, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2210267013",
                    "name": "Amirhossein Dabiriaghdam"
                },
                {
                    "authorId": "2294899071",
                    "name": "Lele Wang"
                }
            ],
            "abstract": "The rapid proliferation of large language models (LLMs) has created an urgent need for reliable methods to detect whether a text is generated by such models. In this paper, we propose SimMark, a posthoc watermarking algorithm that makes LLMs' outputs traceable without requiring access to the model's internal logits, enabling compatibility with a wide range of LLMs, including API-only models. By leveraging the similarity of semantic sentence embeddings and rejection sampling to impose detectable statistical patterns imperceptible to humans, and employing a soft counting mechanism, SimMark achieves robustness against paraphrasing attacks. Experimental results demonstrate that SimMark sets a new benchmark for robust watermarking of LLM-generated content, surpassing prior sentence-level watermarking techniques in robustness, sampling efficiency, and applicability across diverse domains, all while preserving the text quality.",
            "corpus_id": 276116307,
            "sentences": [
                {
                    "corpus_id": "276116307",
                    "title": "SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models",
                    "text": "The rapid proliferation of large language models (LLMs) has created an urgent need for reliable methods to detect whether a text is generated by such models. In this paper, we propose SimMark, a posthoc watermarking algorithm that makes LLMs' outputs traceable without requiring access to the model's internal logits, enabling compatibility with a wide range of LLMs, including API-only models. By leveraging the similarity of semantic sentence embeddings and rejection sampling to impose detectable statistical patterns imperceptible to humans, and employing a soft counting mechanism, SimMark achieves robustness against paraphrasing attacks. Experimental results demonstrate that SimMark sets a new benchmark for robust watermarking of LLM-generated content, surpassing prior sentence-level watermarking techniques in robustness, sampling efficiency, and applicability across diverse domains, all while preserving the text quality.",
                    "score": 0.5738127812110826,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98388671875
                }
            ],
            "relevance_judgement": 0.98388671875,
            "relevance_judgment_input_expanded": "# Title: SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models\n# Venue: arXiv.org\n# Authors: Amirhossein Dabiriaghdam, Lele Wang\n## Abstract\nThe rapid proliferation of large language models (LLMs) has created an urgent need for reliable methods to detect whether a text is generated by such models. In this paper, we propose SimMark, a posthoc watermarking algorithm that makes LLMs' outputs traceable without requiring access to the model's internal logits, enabling compatibility with a wide range of LLMs, including API-only models. By leveraging the similarity of semantic sentence embeddings and rejection sampling to impose detectable statistical patterns imperceptible to humans, and employing a soft counting mechanism, SimMark achieves robustness against paraphrasing attacks. Experimental results demonstrate that SimMark sets a new benchmark for robust watermarking of LLM-generated content, surpassing prior sentence-level watermarking techniques in robustness, sampling efficiency, and applicability across diverse domains, all while preserving the text quality.\n",
            "reference_string": "[276116307 | Dabiriaghdam et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Unveiling Large Language Models Generated Texts: A Multi-Level Fine-Grained Detection Framework",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 49,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.14231, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2299329349",
                    "name": "Zhen Tao"
                },
                {
                    "authorId": "2279173209",
                    "name": "Zhiyu Li"
                },
                {
                    "authorId": "2506761",
                    "name": "Runyu Chen"
                },
                {
                    "authorId": "2128365150",
                    "name": "Dinghao Xi"
                },
                {
                    "authorId": "2301200175",
                    "name": "Wei Xu"
                }
            ],
            "abstract": "Large language models (LLMs) have transformed human writing by enhancing grammar correction, content expansion, and stylistic refinement. However, their widespread use raises concerns about authorship, originality, and ethics, even potentially threatening scholarly integrity. Existing detection methods, which mainly rely on single-feature analysis and binary classification, often fail to effectively identify LLM-generated text in academic contexts. To address these challenges, we propose a novel Multi-level Fine-grained Detection (MFD) framework that detects LLM-generated text by integrating low-level structural, high-level semantic, and deep-level linguistic features, while conducting sentence-level evaluations of lexicon, grammar, and syntax for comprehensive analysis. To improve detection of subtle differences in LLM-generated text and enhance robustness against paraphrasing, we apply two mainstream evasion techniques to rewrite the text. These variations, along with original texts, are used to train a text encoder via contrastive learning, extracting high-level semantic features of sentence to boost detection generalization. Furthermore, we leverage advanced LLM to analyze the entire text and extract deep-level linguistic features, enhancing the model's ability to capture complex patterns and nuances while effectively incorporating contextual information. Extensive experiments on public datasets show that the MFD model outperforms existing methods, achieving an MAE of 0.1346 and an accuracy of 88.56%. Our research provides institutions and publishers with an effective mechanism to detect LLM-generated text, mitigating risks of compromised authorship. Educators and editors can use the model's predictions to refine verification and plagiarism prevention protocols, ensuring adherence to standards.",
            "corpus_id": 273482129,
            "sentences": [
                {
                    "corpus_id": "273482129",
                    "title": "Unveiling Large Language Models Generated Texts: A Multi-Level Fine-Grained Detection Framework",
                    "text": "Large language models has revolutionized numerous industries with their exceptional text generation capabilities [18]. However, these advancements also pose risks of misuse. Overreliance on LLMs can facilitate the spread of misinformation [19] and lead to unfair competition, threatening integrity in academic and societal contexts [20,21]. Therefore, timely detection of AI-generated text is crucial to mitigate these adverse impacts. \n\nExisting detection methods are categorized into metricbased and model-based approaches. Metric-based methods quantify linguistic features, such as stylometric analysis and perplexity scoring, to determine text origin. Gehrmann et al. [22] introduced GLTR, which uses metrics like word probability and entropy for detection, while Solaiman et al. [23] proposed a zero-shot detection method leveraging pre-trained models like GPT-2 or GROVER. However, as LLMs advance, these methods have become less effective. To address these limitations, more advanced and sophisticated detection techniques have emerged. Mitchell et al. [24] presented DetectGPT, a zero-shot method utilizing probability curvature analysis, and Tian et al. [25] developed the Multiscale Positive-Unlabeled (MPU) framework to enhance detection across varying text lengths. While metricbased approaches rely on linguistic analysis, model-based methods use advanced machine learning to classify text as human or AI-generated. These often involve fine-tuning language models or developing specialized architectures. For instance, Guo et al. [26] fine-tuned a RoBERTa model to detect ChatGPT-generated text, while Wang et al. [27] introduced SeqXGPT, a sentence-level method using logprobability lists and self-attention networks for enhanced detection. Liu et al. [28] proposed COCO, which improves detection in low-resource settings by combining an entity coherence graph with contrastive learning. However, most methods focus on binary classification, which limits their application. As LLMs are frequently used for text expansion and refinement, fine-grained detection is increasingly necessary for practical use. \n\nTo address this gap, we propose a novel LLM involvement prediction algorithm.",
                    "score": 0.6403543821670494,
                    "section_title": "AI-generated Content Detection",
                    "char_start_offset": 7092,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 118
                        },
                        {
                            "start": 119,
                            "end": 173
                        },
                        {
                            "start": 174,
                            "end": 340
                        },
                        {
                            "start": 341,
                            "end": 435
                        },
                        {
                            "start": 438,
                            "end": 525
                        },
                        {
                            "start": 526,
                            "end": 655
                        },
                        {
                            "start": 656,
                            "end": 878
                        },
                        {
                            "start": 879,
                            "end": 946
                        },
                        {
                            "start": 947,
                            "end": 1043
                        },
                        {
                            "start": 1044,
                            "end": 1277
                        },
                        {
                            "start": 1278,
                            "end": 1428
                        },
                        {
                            "start": 1429,
                            "end": 1517
                        },
                        {
                            "start": 1518,
                            "end": 1754
                        },
                        {
                            "start": 1755,
                            "end": 1901
                        },
                        {
                            "start": 1902,
                            "end": 1987
                        },
                        {
                            "start": 1988,
                            "end": 2118
                        },
                        {
                            "start": 2121,
                            "end": 2198
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 113,
                            "end": 117,
                            "matchedPaperCorpusId": "259360395"
                        },
                        {
                            "start": 332,
                            "end": 336,
                            "matchedPaperCorpusId": "257166266"
                        },
                        {
                            "start": 336,
                            "end": 339,
                            "matchedPaperCorpusId": "232099112"
                        },
                        {
                            "start": 1060,
                            "end": 1064,
                            "matchedPaperCorpusId": "256274849"
                        },
                        {
                            "start": 1766,
                            "end": 1770,
                            "matchedPaperCorpusId": "264406273"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.978515625
                }
            ],
            "relevance_judgement": 0.978515625,
            "relevance_judgment_input_expanded": "# Title: Unveiling Large Language Models Generated Texts: A Multi-Level Fine-Grained Detection Framework\n# Venue: arXiv.org\n# Authors: Zhen Tao, Zhiyu Li, Runyu Chen, Dinghao Xi, Wei Xu\n## Abstract\nLarge language models (LLMs) have transformed human writing by enhancing grammar correction, content expansion, and stylistic refinement. However, their widespread use raises concerns about authorship, originality, and ethics, even potentially threatening scholarly integrity. Existing detection methods, which mainly rely on single-feature analysis and binary classification, often fail to effectively identify LLM-generated text in academic contexts. To address these challenges, we propose a novel Multi-level Fine-grained Detection (MFD) framework that detects LLM-generated text by integrating low-level structural, high-level semantic, and deep-level linguistic features, while conducting sentence-level evaluations of lexicon, grammar, and syntax for comprehensive analysis. To improve detection of subtle differences in LLM-generated text and enhance robustness against paraphrasing, we apply two mainstream evasion techniques to rewrite the text. These variations, along with original texts, are used to train a text encoder via contrastive learning, extracting high-level semantic features of sentence to boost detection generalization. Furthermore, we leverage advanced LLM to analyze the entire text and extract deep-level linguistic features, enhancing the model's ability to capture complex patterns and nuances while effectively incorporating contextual information. Extensive experiments on public datasets show that the MFD model outperforms existing methods, achieving an MAE of 0.1346 and an accuracy of 88.56%. Our research provides institutions and publishers with an effective mechanism to detect LLM-generated text, mitigating risks of compromised authorship. Educators and editors can use the model's predictions to refine verification and plagiarism prevention protocols, ensuring adherence to standards.\n## AI-generated Content Detection\nLarge language models has revolutionized numerous industries with their exceptional text generation capabilities [18]. However, these advancements also pose risks of misuse. Overreliance on LLMs can facilitate the spread of misinformation [19] and lead to unfair competition, threatening integrity in academic and societal contexts [20,21]. Therefore, timely detection of AI-generated text is crucial to mitigate these adverse impacts. \n\nExisting detection methods are categorized into metricbased and model-based approaches. Metric-based methods quantify linguistic features, such as stylometric analysis and perplexity scoring, to determine text origin. Gehrmann et al. [22] introduced GLTR, which uses metrics like word probability and entropy for detection, while Solaiman et al. [23] proposed a zero-shot detection method leveraging pre-trained models like GPT-2 or GROVER. However, as LLMs advance, these methods have become less effective. To address these limitations, more advanced and sophisticated detection techniques have emerged. Mitchell et al. [24] presented DetectGPT, a zero-shot method utilizing probability curvature analysis, and Tian et al. [25] developed the Multiscale Positive-Unlabeled (MPU) framework to enhance detection across varying text lengths. While metricbased approaches rely on linguistic analysis, model-based methods use advanced machine learning to classify text as human or AI-generated. These often involve fine-tuning language models or developing specialized architectures. For instance, Guo et al. [26] fine-tuned a RoBERTa model to detect ChatGPT-generated text, while Wang et al. [27] introduced SeqXGPT, a sentence-level method using logprobability lists and self-attention networks for enhanced detection. Liu et al. [28] proposed COCO, which improves detection in low-resource settings by combining an entity coherence graph with contrastive learning. However, most methods focus on binary classification, which limits their application. As LLMs are frequently used for text expansion and refinement, fine-grained detection is increasingly necessary for practical use. \n\nTo address this gap, we propose a novel LLM involvement prediction algorithm.",
            "reference_string": "[273482129 | Tao et al. | 2024 | Citations: 2]"
        },
        {
            "title": "From Text to Source: Results in Detecting Large Language Model-Generated Content",
            "venue": "International Conference on Language Resources and Evaluation",
            "year": 2023,
            "reference_count": 54,
            "citation_count": 13,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.13322",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.13322, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51040671",
                    "name": "Wissam Antoun"
                },
                {
                    "authorId": "68990982",
                    "name": "Beno\u00eet Sagot"
                },
                {
                    "authorId": "1679170",
                    "name": "Djam\u00e9 Seddah"
                }
            ],
            "abstract": "The widespread use of Large Language Models (LLMs), celebrated for their ability to generate human-like text, has raised concerns about misinformation and ethical implications. Addressing these concerns necessitates the development of robust methods to detect and attribute text generated by LLMs. This paper investigates \u201cCross-Model Detection,\u201d by evaluating whether a classifier trained to distinguish between source LLM-generated and human-written text can also detect text from a target LLM without further training. The study comprehensively explores various LLM sizes and families and assesses the impact of conversational fine-tuning techniques, quantization, and watermarking on classifier generalization. The research also explores Model Attribution, encompassing source model identification, model family, and model size classification, in addition to quantization and watermarking detection. Our results reveal several key findings: a clear inverse relationship between classifier effectiveness and model size, with larger LLMs being more challenging to detect, especially when the classifier is trained on data from smaller models. Training on data from similarly sized LLMs can improve detection performance from larger models but may lead to decreased performance when dealing with smaller models. Additionally, model attribution experiments show promising results in identifying source models and model families, highlighting detectable signatures in LLM-generated text, with particularly remarkable outcomes in watermarking detection, while no detectable signatures of quantization were observed. Overall, our study contributes valuable insights into the interplay of model size, family, and training data in LLM detection and attribution.",
            "corpus_id": 262465111,
            "sentences": [
                {
                    "corpus_id": "262465111",
                    "title": "From Text to Source: Results in Detecting Large Language Model-Generated Content",
                    "text": "The widespread use of Large Language Models (LLMs), celebrated for their ability to generate human-like text, has raised concerns about misinformation and ethical implications. Addressing these concerns necessitates the development of robust methods to detect and attribute text generated by LLMs. This paper investigates \u201cCross-Model Detection,\u201d by evaluating whether a classifier trained to distinguish between source LLM-generated and human-written text can also detect text from a target LLM without further training. The study comprehensively explores various LLM sizes and families and assesses the impact of conversational fine-tuning techniques, quantization, and watermarking on classifier generalization. The research also explores Model Attribution, encompassing source model identification, model family, and model size classification, in addition to quantization and watermarking detection. Our results reveal several key findings: a clear inverse relationship between classifier effectiveness and model size, with larger LLMs being more challenging to detect, especially when the classifier is trained on data from smaller models. Training on data from similarly sized LLMs can improve detection performance from larger models but may lead to decreased performance when dealing with smaller models. Additionally, model attribution experiments show promising results in identifying source models and model families, highlighting detectable signatures in LLM-generated text, with particularly remarkable outcomes in watermarking detection, while no detectable signatures of quantization were observed. Overall, our study contributes valuable insights into the interplay of model size, family, and training data in LLM detection and attribution.",
                    "score": 0.6158215601340693,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.978515625
                }
            ],
            "relevance_judgement": 0.978515625,
            "relevance_judgment_input_expanded": "# Title: From Text to Source: Results in Detecting Large Language Model-Generated Content\n# Venue: International Conference on Language Resources and Evaluation\n# Authors: Wissam Antoun, Beno\u00eet Sagot, Djam\u00e9 Seddah\n## Abstract\nThe widespread use of Large Language Models (LLMs), celebrated for their ability to generate human-like text, has raised concerns about misinformation and ethical implications. Addressing these concerns necessitates the development of robust methods to detect and attribute text generated by LLMs. This paper investigates \u201cCross-Model Detection,\u201d by evaluating whether a classifier trained to distinguish between source LLM-generated and human-written text can also detect text from a target LLM without further training. The study comprehensively explores various LLM sizes and families and assesses the impact of conversational fine-tuning techniques, quantization, and watermarking on classifier generalization. The research also explores Model Attribution, encompassing source model identification, model family, and model size classification, in addition to quantization and watermarking detection. Our results reveal several key findings: a clear inverse relationship between classifier effectiveness and model size, with larger LLMs being more challenging to detect, especially when the classifier is trained on data from smaller models. Training on data from similarly sized LLMs can improve detection performance from larger models but may lead to decreased performance when dealing with smaller models. Additionally, model attribution experiments show promising results in identifying source models and model families, highlighting detectable signatures in LLM-generated text, with particularly remarkable outcomes in watermarking detection, while no detectable signatures of quantization were observed. Overall, our study contributes valuable insights into the interplay of model size, family, and training data in LLM detection and attribution.\n",
            "reference_string": "[262465111 | Antoun et al. | 2023 | Citations: 13]"
        },
        {
            "title": "Fine-tuning Large Language Models for Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 22,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.12326, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280331364",
                    "name": "Feng Xiong"
                },
                {
                    "authorId": "52196310",
                    "name": "Thanet Markchom"
                },
                {
                    "authorId": "2280391458",
                    "name": "Ziwei Zheng"
                },
                {
                    "authorId": "2280666724",
                    "name": "Subin Jung"
                },
                {
                    "authorId": "2293393169",
                    "name": "Varun Ojha"
                },
                {
                    "authorId": "2281373610",
                    "name": "Huizhi Liang"
                }
            ],
            "abstract": "SemEval-2024 Task 8 introduces the challenge of identifying machine-generated texts from diverse Large Language Models (LLMs) in various languages and domains. The task comprises three subtasks: binary classification in monolingual and multilingual (Subtask A), multi-class classification (Subtask B), and mixed text detection (Subtask C). This paper focuses on Subtask A&B. Each subtask is supported by three datasets for training, development, and testing. To tackle this task, two methods: 1) using traditional machine learning (ML) with natural language preprocessing (NLP) for feature extraction, and 2) fine-tuning LLMs for text classification. The results show that transformer models, particularly LoRA-RoBERTa, exceed traditional ML methods in effectiveness, with majority voting being particularly effective in multilingual contexts for identifying machine-generated texts.",
            "corpus_id": 267094724,
            "sentences": [
                {
                    "corpus_id": "267094724",
                    "title": "Fine-tuning Large Language Models for Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection",
                    "text": "In the realm of distinguishing texts generated by large language models (LLMs) from those authored by humans, researchers have employed a variety of methods and tools. Broadly, these approaches can be categorized into two main types: black-box and white-box detection methods (Tang et al., 2023). Black-box detection relies on API-level access to LLMs, utilizing textual samples from both human and machine sources to train classification models (Dugan et al., 2020). The study by Guo et al. (2023) integrated existing question-and-answer datasets and leveraged fine-tuning of pre-trained models to investigate the characteristics and similarities between human-generated and AI-generated texts (Guo et al., 2023). \n\nIn the context of white-box detection, Kirchenbauer et al. ( 2023) introduced a novel approach involving the embedding of watermarks in the outputs of large language models to facilitate the detection of AI-generated text (Kirchenbauer et al., 2023). Additionally, a variety of tools and methodologies, including XGBoost, decision trees, and transformer-based models, have been evaluated for their efficacy in detecting texts produced by AI (Zaitsu and Jin, 2023). These techniques incorporate multiple stylistic measurement features to differentiate between AI-generated and human-generated texts (Shijaku and Canhasi, 2023). \n\nSpecific tools and techniques in this domain include the GLTR tool developed by Gehrmann et al. (2019), which analyzes the usage of rare words in texts to distinguish between those generated by the GPT-2 model and human writers (Gehrmann et al., 2019). The DetectGPT method posits that minor rewrites of LLM-generated texts tend to reduce the log probability under the model, a hypothesis that has been explored in depth (Mitchell et al., 2023). Furthermore, intrinsic dimension analysis, including methods like the Persistent Homology Dimension estimator (PHD), has been applied to distinguish between authentic texts and those generated artificially (Tulchinskii et al., 2023).",
                    "score": 0.7248232384918813,
                    "section_title": "Related Work",
                    "char_start_offset": 2483,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 168,
                            "end": 296
                        },
                        {
                            "start": 297,
                            "end": 467
                        },
                        {
                            "start": 468,
                            "end": 714
                        },
                        {
                            "start": 717,
                            "end": 967
                        },
                        {
                            "start": 968,
                            "end": 1181
                        },
                        {
                            "start": 1182,
                            "end": 1343
                        },
                        {
                            "start": 1346,
                            "end": 1598
                        },
                        {
                            "start": 1599,
                            "end": 1791
                        },
                        {
                            "start": 1792,
                            "end": 2025
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97705078125
                }
            ],
            "relevance_judgement": 0.97705078125,
            "relevance_judgment_input_expanded": "# Title: Fine-tuning Large Language Models for Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection\n# Venue: arXiv.org\n# Authors: Feng Xiong, Thanet Markchom, Ziwei Zheng, Subin Jung, Varun Ojha, Huizhi Liang\n## Abstract\nSemEval-2024 Task 8 introduces the challenge of identifying machine-generated texts from diverse Large Language Models (LLMs) in various languages and domains. The task comprises three subtasks: binary classification in monolingual and multilingual (Subtask A), multi-class classification (Subtask B), and mixed text detection (Subtask C). This paper focuses on Subtask A&B. Each subtask is supported by three datasets for training, development, and testing. To tackle this task, two methods: 1) using traditional machine learning (ML) with natural language preprocessing (NLP) for feature extraction, and 2) fine-tuning LLMs for text classification. The results show that transformer models, particularly LoRA-RoBERTa, exceed traditional ML methods in effectiveness, with majority voting being particularly effective in multilingual contexts for identifying machine-generated texts.\n## Related Work\nIn the realm of distinguishing texts generated by large language models (LLMs) from those authored by humans, researchers have employed a variety of methods and tools. Broadly, these approaches can be categorized into two main types: black-box and white-box detection methods (Tang et al., 2023). Black-box detection relies on API-level access to LLMs, utilizing textual samples from both human and machine sources to train classification models (Dugan et al., 2020). The study by Guo et al. (2023) integrated existing question-and-answer datasets and leveraged fine-tuning of pre-trained models to investigate the characteristics and similarities between human-generated and AI-generated texts (Guo et al., 2023). \n\nIn the context of white-box detection, Kirchenbauer et al. ( 2023) introduced a novel approach involving the embedding of watermarks in the outputs of large language models to facilitate the detection of AI-generated text (Kirchenbauer et al., 2023). Additionally, a variety of tools and methodologies, including XGBoost, decision trees, and transformer-based models, have been evaluated for their efficacy in detecting texts produced by AI (Zaitsu and Jin, 2023). These techniques incorporate multiple stylistic measurement features to differentiate between AI-generated and human-generated texts (Shijaku and Canhasi, 2023). \n\nSpecific tools and techniques in this domain include the GLTR tool developed by Gehrmann et al. (2019), which analyzes the usage of rare words in texts to distinguish between those generated by the GPT-2 model and human writers (Gehrmann et al., 2019). The DetectGPT method posits that minor rewrites of LLM-generated texts tend to reduce the log probability under the model, a hypothesis that has been explored in depth (Mitchell et al., 2023). Furthermore, intrinsic dimension analysis, including methods like the Persistent Homology Dimension estimator (PHD), has been applied to distinguish between authentic texts and those generated artificially (Tulchinskii et al., 2023).",
            "reference_string": "[267094724 | Xiong et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Signal Watermark on Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 31,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.06545, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2041278503",
                    "name": "Zhenyu Xu"
                },
                {
                    "authorId": "2278314006",
                    "name": "Victor S. Sheng"
                }
            ],
            "abstract": "As Large Language Models (LLMs) become increasingly sophisticated, they raise significant security concerns, including the creation of fake news and academic misuse. Most detectors for identifying model-generated text are limited by their reliance on variance in perplexity and burstiness, and they require substantial computational resources. In this paper, we proposed a watermarking method embedding a specific watermark into the text during its generation by LLMs, based on a pre-defined signal pattern. This technique not only ensures the watermark's invisibility to humans but also maintains the quality and grammatical integrity of model-generated text. We utilize LLMs and Fast Fourier Transform (FFT) for token probability computation and detection of the signal watermark. The unique application of signal processing principles within the realm of text generation by LLMs allows for subtle yet effective embedding of watermarks, which do not compromise the quality or coherence of the generated text. Our method has been empirically validated across multiple LLMs, consistently maintaining high detection accuracy, even with variations in temperature settings during text generation. In the experiment of distinguishing between human-written and watermarked text, our method achieved an AUROC score of 0.97, significantly outperforming existing methods like GPTZero, which scored 0.64. The watermark's resilience to various attacking scenarios further confirms its robustness, addressing significant challenges in model-generated text authentication.",
            "corpus_id": 273228395,
            "sentences": [
                {
                    "corpus_id": "273228395",
                    "title": "Signal Watermark on Large Language Models",
                    "text": "1) Watermark Methods: Recent advancements in watermarking techniques for language models have been crucial in ensuring text authenticity and model protection. Abdelnabi and Fritz [20] pioneered the Adversarial Watermarking Transformer (AWT), marking a significant step in embedding binary messages into text and demonstrating resilience against multiple attacks. He et al. [21] developed CATER, a conditional watermarking framework that secures text generation APIs against imitation attacks while preserving word distribution integrity. Zhao, Li, and Wang [22] introduced Distillation-Resistant Watermarking (DRW), a method effective in protecting NLP models from distillation theft and maintaining accuracy. Kirchenbauer et al. [23] proposed a framework for embedding watermarks in language models that are detectable with minimal impact on text quality. In a similar vein, Zhao, Wang, and Li's GINSEW technique [24] innovatively embeds secret signals in the probability vector of decoding steps, enhancing IP infringement detection. Peng et al. [25] developed EmbMarker for EaaS, embedding watermarks in trigger words for effective copyright verification. Yoo et al. [26] explored robust multi-bit watermarking, focusing on error resistance and enhanced payload. Lastly, Li et al. [27] introduced a novel strategy for embedding watermarks in the weight quantization process of large language models, ensuring model security without compromising operational modes. Together, these contributions represent a diverse and evolving approach to watermarking in the rapidly advancing field of language models. \n\n2) Detectors: The advancement in language models has prompted the development of various detection tools for identifying model-generated text. Gehrmann [28] developed GLTR, a tool to assist non-experts in detecting texts generated by models. It uses statistical methods to identify generation artifacts and has been shown to improve human detection rates of fake text significantly. GPT2-Detector developed by OpenAI [29] is a RoBERTa base fine-tuned with outputs from the 1.5Bparameter GPT-2 model. Its primary function is to predict if text was generated by a GPT-2 model.",
                    "score": 0.6412364792038093,
                    "section_title": "B. Detection for Model-generated Text",
                    "char_start_offset": 11169,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 159,
                            "end": 362
                        },
                        {
                            "start": 363,
                            "end": 537
                        },
                        {
                            "start": 538,
                            "end": 709
                        },
                        {
                            "start": 710,
                            "end": 856
                        },
                        {
                            "start": 857,
                            "end": 1035
                        },
                        {
                            "start": 1036,
                            "end": 1158
                        },
                        {
                            "start": 1159,
                            "end": 1265
                        },
                        {
                            "start": 1266,
                            "end": 1466
                        },
                        {
                            "start": 1467,
                            "end": 1605
                        },
                        {
                            "start": 1608,
                            "end": 1750
                        },
                        {
                            "start": 1751,
                            "end": 1849
                        },
                        {
                            "start": 1850,
                            "end": 1990
                        },
                        {
                            "start": 1991,
                            "end": 2107
                        },
                        {
                            "start": 2108,
                            "end": 2182
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 179,
                            "end": 183,
                            "matchedPaperCorpusId": "221516138"
                        },
                        {
                            "start": 373,
                            "end": 377,
                            "matchedPaperCorpusId": "252367820"
                        },
                        {
                            "start": 1170,
                            "end": 1174,
                            "matchedPaperCorpusId": "259129912"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97705078125
                },
                {
                    "corpus_id": "273228395",
                    "title": "Signal Watermark on Large Language Models",
                    "text": "As Large Language Models (LLMs) become increasingly sophisticated, they raise significant security concerns, including the creation of fake news and academic misuse. Most detectors for identifying model-generated text are limited by their reliance on variance in perplexity and burstiness, and they require substantial computational resources. In this paper, we proposed a watermarking method embedding a specific watermark into the text during its generation by LLMs, based on a pre-defined signal pattern. This technique not only ensures the watermark's invisibility to humans but also maintains the quality and grammatical integrity of model-generated text. We utilize LLMs and Fast Fourier Transform (FFT) for token probability computation and detection of the signal watermark. The unique application of signal processing principles within the realm of text generation by LLMs allows for subtle yet effective embedding of watermarks, which do not compromise the quality or coherence of the generated text. Our method has been empirically validated across multiple LLMs, consistently maintaining high detection accuracy, even with variations in temperature settings during text generation. In the experiment of distinguishing between human-written and watermarked text, our method achieved an AUROC score of 0.97, significantly outperforming existing methods like GPTZero, which scored 0.64. The watermark's resilience to various attacking scenarios further confirms its robustness, addressing significant challenges in model-generated text authentication.",
                    "score": 0.6013451598477597,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9560546875
                },
                {
                    "corpus_id": "273228395",
                    "title": "Signal Watermark on Large Language Models",
                    "text": "With the advancement of Large Language Models (LLMs) such as ChatGPT [1], their capabilities have become increasingly sophisticated, enabling them to perform a wide range of tasks with remarkable efficiency and accuracy. Demonstrating formidable abilities, these models can answer scientific and mathematical queries, understand historical events and biographies, and offer assistance with daily life. Even though language models may occasionally produce illusions or incorrect answers, the potential and impact of LLMs remain significant. However, this enhanced functionality also presents new challenges in terms of security and potential misuse. For instance, there's a risk of these models being used to create fake news or media rumors, and students might leverage them to generate assignments, thereby impacting academic fairness. Consequently, there is a growing need for reliable methods to identify model-generated text to effectively address and mitigate these emerging concerns. \n\nNumerous open-source and commercial tools have been developed to detect model-generated text, such as GPT2-Detector [29], RoBERTa-QA [31], DetectGPT [30], GPTZero [32], Writer [33], and others. However, there are three significant limitations to these methods. First, their probability predictions cannot provide conclusive certainty and should only be regarded as a reference. Second, most current approaches rely on methods that differentiate human text from modelgenerated text based on variations in perplexity and burstiness. Third, methods like DetectGPT and RoBERTa-QA, which require multiple text-generation process and model retraining, consume substantial computational resources and time when detecting model-generated text. As LLMs advance, the generated text increasingly resembles human writing, making these methods based on variance in perplexity between human and model-generated text less applicable. Therefore, embedding watermarks in text generated by LLMs presents a faster, more stable, and reliable method of verification. \n\nWatermarks in Large Language Models need to meet three arXiv:2410.06545v1 [cs.CR] 9 Oct 2024 Fig. 2: This figure illustrates an example of embedding a signal watermark while generating text by LLMs.",
                    "score": 0.631835975267425,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 18,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 220
                        },
                        {
                            "start": 221,
                            "end": 401
                        },
                        {
                            "start": 402,
                            "end": 539
                        },
                        {
                            "start": 540,
                            "end": 648
                        },
                        {
                            "start": 649,
                            "end": 836
                        },
                        {
                            "start": 837,
                            "end": 989
                        },
                        {
                            "start": 992,
                            "end": 1185
                        },
                        {
                            "start": 1186,
                            "end": 1252
                        },
                        {
                            "start": 1253,
                            "end": 1369
                        },
                        {
                            "start": 1370,
                            "end": 1522
                        },
                        {
                            "start": 1523,
                            "end": 1727
                        },
                        {
                            "start": 1728,
                            "end": 1910
                        },
                        {
                            "start": 1911,
                            "end": 2037
                        },
                        {
                            "start": 2040,
                            "end": 2113
                        },
                        {
                            "start": 2114,
                            "end": 2238
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.947265625
                }
            ],
            "relevance_judgement": 0.97705078125,
            "relevance_judgment_input_expanded": "# Title: Signal Watermark on Large Language Models\n# Venue: arXiv.org\n# Authors: Zhenyu Xu, Victor S. Sheng\n## Abstract\nAs Large Language Models (LLMs) become increasingly sophisticated, they raise significant security concerns, including the creation of fake news and academic misuse. Most detectors for identifying model-generated text are limited by their reliance on variance in perplexity and burstiness, and they require substantial computational resources. In this paper, we proposed a watermarking method embedding a specific watermark into the text during its generation by LLMs, based on a pre-defined signal pattern. This technique not only ensures the watermark's invisibility to humans but also maintains the quality and grammatical integrity of model-generated text. We utilize LLMs and Fast Fourier Transform (FFT) for token probability computation and detection of the signal watermark. The unique application of signal processing principles within the realm of text generation by LLMs allows for subtle yet effective embedding of watermarks, which do not compromise the quality or coherence of the generated text. Our method has been empirically validated across multiple LLMs, consistently maintaining high detection accuracy, even with variations in temperature settings during text generation. In the experiment of distinguishing between human-written and watermarked text, our method achieved an AUROC score of 0.97, significantly outperforming existing methods like GPTZero, which scored 0.64. The watermark's resilience to various attacking scenarios further confirms its robustness, addressing significant challenges in model-generated text authentication.\n## I. INTRODUCTION\nWith the advancement of Large Language Models (LLMs) such as ChatGPT [1], their capabilities have become increasingly sophisticated, enabling them to perform a wide range of tasks with remarkable efficiency and accuracy. Demonstrating formidable abilities, these models can answer scientific and mathematical queries, understand historical events and biographies, and offer assistance with daily life. Even though language models may occasionally produce illusions or incorrect answers, the potential and impact of LLMs remain significant. However, this enhanced functionality also presents new challenges in terms of security and potential misuse. For instance, there's a risk of these models being used to create fake news or media rumors, and students might leverage them to generate assignments, thereby impacting academic fairness. Consequently, there is a growing need for reliable methods to identify model-generated text to effectively address and mitigate these emerging concerns. \n\nNumerous open-source and commercial tools have been developed to detect model-generated text, such as GPT2-Detector [29], RoBERTa-QA [31], DetectGPT [30], GPTZero [32], Writer [33], and others. However, there are three significant limitations to these methods. First, their probability predictions cannot provide conclusive certainty and should only be regarded as a reference. Second, most current approaches rely on methods that differentiate human text from modelgenerated text based on variations in perplexity and burstiness. Third, methods like DetectGPT and RoBERTa-QA, which require multiple text-generation process and model retraining, consume substantial computational resources and time when detecting model-generated text. As LLMs advance, the generated text increasingly resembles human writing, making these methods based on variance in perplexity between human and model-generated text less applicable. Therefore, embedding watermarks in text generated by LLMs presents a faster, more stable, and reliable method of verification. \n\nWatermarks in Large Language Models need to meet three arXiv:2410.06545v1 [cs.CR] 9 Oct 2024 Fig. 2: This figure illustrates an example of embedding a signal watermark while generating text by LLMs.\n\n## B. Detection for Model-generated Text\n1) Watermark Methods: Recent advancements in watermarking techniques for language models have been crucial in ensuring text authenticity and model protection. Abdelnabi and Fritz [20] pioneered the Adversarial Watermarking Transformer (AWT), marking a significant step in embedding binary messages into text and demonstrating resilience against multiple attacks. He et al. [21] developed CATER, a conditional watermarking framework that secures text generation APIs against imitation attacks while preserving word distribution integrity. Zhao, Li, and Wang [22] introduced Distillation-Resistant Watermarking (DRW), a method effective in protecting NLP models from distillation theft and maintaining accuracy. Kirchenbauer et al. [23] proposed a framework for embedding watermarks in language models that are detectable with minimal impact on text quality. In a similar vein, Zhao, Wang, and Li's GINSEW technique [24] innovatively embeds secret signals in the probability vector of decoding steps, enhancing IP infringement detection. Peng et al. [25] developed EmbMarker for EaaS, embedding watermarks in trigger words for effective copyright verification. Yoo et al. [26] explored robust multi-bit watermarking, focusing on error resistance and enhanced payload. Lastly, Li et al. [27] introduced a novel strategy for embedding watermarks in the weight quantization process of large language models, ensuring model security without compromising operational modes. Together, these contributions represent a diverse and evolving approach to watermarking in the rapidly advancing field of language models. \n\n2) Detectors: The advancement in language models has prompted the development of various detection tools for identifying model-generated text. Gehrmann [28] developed GLTR, a tool to assist non-experts in detecting texts generated by models. It uses statistical methods to identify generation artifacts and has been shown to improve human detection rates of fake text significantly. GPT2-Detector developed by OpenAI [29] is a RoBERTa base fine-tuned with outputs from the 1.5Bparameter GPT-2 model. Its primary function is to predict if text was generated by a GPT-2 model.",
            "reference_string": "[273228395 | Xu et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Three Bricks to Consolidate Watermarks for Large Language Models",
            "venue": "International Workshop on Information Forensics and Security",
            "year": 2023,
            "reference_count": 36,
            "citation_count": 55,
            "influential_citation_count": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.00113",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.00113, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2147013351",
                    "name": "Pierre Fernandez"
                },
                {
                    "authorId": "2129106958",
                    "name": "Antoine Chaffin"
                },
                {
                    "authorId": "2144093662",
                    "name": "Karim Tit"
                },
                {
                    "authorId": "2686030",
                    "name": "Vivien Chappelier"
                },
                {
                    "authorId": "1775704",
                    "name": "T. Furon"
                }
            ],
            "abstract": "Discerning between generated and natural texts is increasingly challenging. In this context, watermarking emerges as a promising technique for ascribing text to a specific generative model. It alters the sampling generation process to leave an invisible trace in the output, facilitating later detection. This research consolidates watermarks for large language models based on three theoretical and empirical considerations. First, we introduce new statistical tests that offer robust theoretical guarantees which remain valid even at low false-positive rates (less than 10\u22126). Second, we compare the effectiveness of watermarks using classical benchmarks in the field of natural language processing, gaining insights into their real-world applicability. Third, we develop advanced detection schemes for scenarios where access to the LLM is available, as well as multi-bit watermarking.",
            "corpus_id": 260351507,
            "sentences": [
                {
                    "corpus_id": "260351507",
                    "title": "Three Bricks to Consolidate Watermarks for Large Language Models",
                    "text": "Discerning between generated and natural texts is increasingly challenging. In this context, watermarking emerges as a promising technique for ascribing text to a specific generative model. It alters the sampling generation process to leave an invisible trace in the output, facilitating later detection. This research consolidates watermarks for large language models based on three theoretical and empirical considerations. First, we introduce new statistical tests that offer robust theoretical guarantees which remain valid even at low false-positive rates (less than 10\u22126). Second, we compare the effectiveness of watermarks using classical benchmarks in the field of natural language processing, gaining insights into their real-world applicability. Third, we develop advanced detection schemes for scenarios where access to the LLM is available, as well as multi-bit watermarking.",
                    "score": 0.6966463737624835,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9736328125
                }
            ],
            "relevance_judgement": 0.9736328125,
            "relevance_judgment_input_expanded": "# Title: Three Bricks to Consolidate Watermarks for Large Language Models\n# Venue: International Workshop on Information Forensics and Security\n# Authors: Pierre Fernandez, Antoine Chaffin, Karim Tit, Vivien Chappelier, T. Furon\n## Abstract\nDiscerning between generated and natural texts is increasingly challenging. In this context, watermarking emerges as a promising technique for ascribing text to a specific generative model. It alters the sampling generation process to leave an invisible trace in the output, facilitating later detection. This research consolidates watermarks for large language models based on three theoretical and empirical considerations. First, we introduce new statistical tests that offer robust theoretical guarantees which remain valid even at low false-positive rates (less than 10\u22126). Second, we compare the effectiveness of watermarks using classical benchmarks in the field of natural language processing, gaining insights into their real-world applicability. Third, we develop advanced detection schemes for scenarios where access to the LLM is available, as well as multi-bit watermarking.\n",
            "reference_string": "[260351507 | Fernandez et al. | 2023 | Citations: 55]"
        },
        {
            "title": "DetectGPT-SC: Improving Detection of Text Generated by Large Language Models through Self-Consistency with Masked Predictions",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 0,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.14479, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261313297",
                    "name": "Rongsheng Wang"
                },
                {
                    "authorId": "2261359496",
                    "name": "Qi Li"
                },
                {
                    "authorId": "2261347599",
                    "name": "Sihong Xie"
                }
            ],
            "abstract": "General large language models (LLMs) such as ChatGPT have shown remarkable success, but it has also raised concerns among people about the misuse of AI-generated texts. Therefore, an important question is how to detect whether the texts are generated by ChatGPT or by humans. Existing detectors are built on the assumption that there is a distribution gap between human-generated and AI-generated texts. These gaps are typically identified using statistical information or classifiers. In contrast to prior research methods, we find that large language models such as ChatGPT exhibit strong self-consistency in text generation and continuation. Self-consistency capitalizes on the intuition that AI-generated texts can still be reasoned with by large language models using the same logical reasoning when portions of the texts are masked, which differs from human-generated texts. Using this observation, we subsequently proposed a new method for AI-generated texts detection based on self-consistency with masked predictions to determine whether a text is generated by LLMs. This method, which we call DetectGPT-SC. We conducted a series of experiments to evaluate the performance of DetectGPT-SC. In these experiments, we employed various mask scheme, zero-shot, and simple prompt for completing masked texts and self-consistency predictions. The results indicate that DetectGPT-SC outperforms the current state-of-the-art across different tasks.",
            "corpus_id": 264426339,
            "sentences": [
                {
                    "corpus_id": "264426339",
                    "title": "DetectGPT-SC: Improving Detection of Text Generated by Large Language Models through Self-Consistency with Masked Predictions",
                    "text": "Recent research has shown promising results in the development of detection methods. The existing detectors are built on the assumption that there is a distributional difference between human-generated texts and AI-generated texts. These differences are typically identified by training classifiers or using statistical information. \n\nClassifier-based detectors. Classifier-based detectors are commonly used in natural language processing detection paradigms, especially in fake news and misinformation detection [3]. Guo et al. [4] proposed the ChatGPT Detector, where they initially constructed a dataset consisting of ChatGPT conversations with human questions and answers, and trained a text detection classifier based on this dataset. The use of these methods requires substantial data collection and incurs the cost of training these classifier models. \n\nStatistical-based detectors. Statistical-based detectors utilize statistical metrics such as entropy, perplexity, and n-gram frequency to differentiate between human-generated and AI-generated texts [1]. \n\nSome other work is based on watermark-based detectors. In previous research, watermarks have been applied in the field of image processing and computer vision to protect copyrighted content and prevent intellectual property theft [6]. \n\nRecently, with the emergence of ChatGPT, the work by Kirchenbauer et al. [7] demonstrated how to incorporate a watermark using only the logarithmic credentials of each step to mark AI-generated texts. While watermark-based detectors are an intriguing area of research, adding watermarks may affect the readability of the texts, and the removal of watermarks is also a challenge we need to address. \n\nHowever, with the emergence of ChatGPT, an innovative statistical detection method called DetectGPT [5] has been developed. Its principle is that text generated by the model typically resides in the negative curvature region of the model's log probability. DetectGPT [5] generates and compares multiple variants of model-generated texts to determine whether the texts are machine-generated based on the log probabilities of the original texts and these variants. \n\nDetectGPT [5] outperforms the vast majority of existing zero-shot methods in terms of model sample detection, achieving very high AUC. It is based on this concept that DetectGPT-SC was proposed.",
                    "score": 0.6642230949070014,
                    "section_title": "Related Work",
                    "char_start_offset": 3773,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 84
                        },
                        {
                            "start": 85,
                            "end": 231
                        },
                        {
                            "start": 232,
                            "end": 332
                        },
                        {
                            "start": 335,
                            "end": 362
                        },
                        {
                            "start": 363,
                            "end": 517
                        },
                        {
                            "start": 518,
                            "end": 739
                        },
                        {
                            "start": 740,
                            "end": 858
                        },
                        {
                            "start": 861,
                            "end": 889
                        },
                        {
                            "start": 890,
                            "end": 1064
                        },
                        {
                            "start": 1067,
                            "end": 1121
                        },
                        {
                            "start": 1122,
                            "end": 1301
                        },
                        {
                            "start": 1304,
                            "end": 1504
                        },
                        {
                            "start": 1505,
                            "end": 1701
                        },
                        {
                            "start": 1704,
                            "end": 1827
                        },
                        {
                            "start": 1828,
                            "end": 1960
                        },
                        {
                            "start": 1961,
                            "end": 2166
                        },
                        {
                            "start": 2169,
                            "end": 2303
                        },
                        {
                            "start": 2304,
                            "end": 2363
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1297,
                            "end": 1300,
                            "matchedPaperCorpusId": "62101493"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.970703125
                }
            ],
            "relevance_judgement": 0.970703125,
            "relevance_judgment_input_expanded": "# Title: DetectGPT-SC: Improving Detection of Text Generated by Large Language Models through Self-Consistency with Masked Predictions\n# Venue: arXiv.org\n# Authors: Rongsheng Wang, Qi Li, Sihong Xie\n## Abstract\nGeneral large language models (LLMs) such as ChatGPT have shown remarkable success, but it has also raised concerns among people about the misuse of AI-generated texts. Therefore, an important question is how to detect whether the texts are generated by ChatGPT or by humans. Existing detectors are built on the assumption that there is a distribution gap between human-generated and AI-generated texts. These gaps are typically identified using statistical information or classifiers. In contrast to prior research methods, we find that large language models such as ChatGPT exhibit strong self-consistency in text generation and continuation. Self-consistency capitalizes on the intuition that AI-generated texts can still be reasoned with by large language models using the same logical reasoning when portions of the texts are masked, which differs from human-generated texts. Using this observation, we subsequently proposed a new method for AI-generated texts detection based on self-consistency with masked predictions to determine whether a text is generated by LLMs. This method, which we call DetectGPT-SC. We conducted a series of experiments to evaluate the performance of DetectGPT-SC. In these experiments, we employed various mask scheme, zero-shot, and simple prompt for completing masked texts and self-consistency predictions. The results indicate that DetectGPT-SC outperforms the current state-of-the-art across different tasks.\n## Related Work\nRecent research has shown promising results in the development of detection methods. The existing detectors are built on the assumption that there is a distributional difference between human-generated texts and AI-generated texts. These differences are typically identified by training classifiers or using statistical information. \n\nClassifier-based detectors. Classifier-based detectors are commonly used in natural language processing detection paradigms, especially in fake news and misinformation detection [3]. Guo et al. [4] proposed the ChatGPT Detector, where they initially constructed a dataset consisting of ChatGPT conversations with human questions and answers, and trained a text detection classifier based on this dataset. The use of these methods requires substantial data collection and incurs the cost of training these classifier models. \n\nStatistical-based detectors. Statistical-based detectors utilize statistical metrics such as entropy, perplexity, and n-gram frequency to differentiate between human-generated and AI-generated texts [1]. \n\nSome other work is based on watermark-based detectors. In previous research, watermarks have been applied in the field of image processing and computer vision to protect copyrighted content and prevent intellectual property theft [6]. \n\nRecently, with the emergence of ChatGPT, the work by Kirchenbauer et al. [7] demonstrated how to incorporate a watermark using only the logarithmic credentials of each step to mark AI-generated texts. While watermark-based detectors are an intriguing area of research, adding watermarks may affect the readability of the texts, and the removal of watermarks is also a challenge we need to address. \n\nHowever, with the emergence of ChatGPT, an innovative statistical detection method called DetectGPT [5] has been developed. Its principle is that text generated by the model typically resides in the negative curvature region of the model's log probability. DetectGPT [5] generates and compares multiple variants of model-generated texts to determine whether the texts are machine-generated based on the log probabilities of the original texts and these variants. \n\nDetectGPT [5] outperforms the vast majority of existing zero-shot methods in terms of model sample detection, achieving very high AUC. It is based on this concept that DetectGPT-SC was proposed.",
            "reference_string": "[264426339 | Wang et al. | 2023 | Citations: 7]"
        },
        {
            "title": "A Survey on the Detection and Impacts of Deepfakes in Visual, Audio, and Textual Formats",
            "venue": "IEEE Access",
            "year": 2023,
            "reference_count": 250,
            "citation_count": 36,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10365143.pdf",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2023.3344653?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2023.3344653, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275319803",
                    "name": "Rami Mubarak"
                },
                {
                    "authorId": "3277359",
                    "name": "Tariq A. A. Alsboui"
                },
                {
                    "authorId": "2207298991",
                    "name": "Omar Alshaikh"
                },
                {
                    "authorId": "2276533468",
                    "name": "Isa Inuwa-Dutse"
                },
                {
                    "authorId": "143955457",
                    "name": "Saad Khan"
                },
                {
                    "authorId": "100581778",
                    "name": "Si\u00f4n Parkinson"
                }
            ],
            "abstract": "In the rapidly evolving digital landscape, the generation of fake visual, audio, and textual content poses a significant threat to the trust of society, political stability, and integrity of information. The generation process has been enhanced and simplified using Artificial Intelligence techniques, which have been termed deepfake. Although significant attention has been paid to visual and audio deepfakes, there is also a burgeoning need to consider text-based deepfakes. Due to advancements in natural language processing and large language models, the potential of manipulating textual content to reshape online discourse and misinformation has increased. This study comprehensively examines the multifaceted nature and impacts of deep-fake-generated media. This work explains the broad implications of deepfakes in social, political, economic, and technological domains. State-of-the-art detection methodologies for all types of deepfake are critically reviewed, highlighting the need for unified, real-time, adaptable, and generalised solutions. As the challenges posed by deepfakes intensify, this study underscores the importance of a holistic approach that integrates technical solutions with public awareness and legislative action. By providing a comprehensive overview and establishing a framework for future exploration, this study seeks to assist researchers, policymakers, and practitioners navigate the complexities of deepfake phenomena.",
            "corpus_id": 266369103,
            "sentences": [
                {
                    "corpus_id": "266369103",
                    "title": "A Survey on the Detection and Impacts of Deepfakes in Visual, Audio, and Textual Formats",
                    "text": "In the context of the rapid advancement of Large Language Models (LLMs), the ability to produce highly accurate and contextually relevant text is becoming an increasing concern in online security. The sophisticated text generation capabilities of these models can potentially facilitate harmful activities on online platforms, emphasising the importance of detecting AI-generated texts as a mitigation strategy. Several detection methodologies have been developed, each with varying degrees of effectiveness influenced by factors such as the architecture of the language model, the decoding strategy, the length of the text and the availability of raw log probabilities from the LLMs. \n\nSimple classifiers offer a basic, yet sometimes limited, approach, particularly with shorter texts and complex generation strategies. Zero-shot models provide a robust detection strategy by exploiting unique statistical patterns in machine-generated text; however, they are susceptible to adversarial actions or different generation strategies. Finetuned language models represent another approach that provides a more promising route for enhancing the detection accuracy. However, their effectiveness can be influenced by the sampling method and the length of the generated sequences. \n\nWatermarking, a more recent development, has emerged as a unique method for detection. This involves embedding watermarks in the logits of an LLM during text generation, forming a pattern that can potentially identify synthetic text. Despite the promise of watermarking, its vulnerability to spoofing/paraphrasing attacks and evasion techniques presents significant challenges that warrant further research. Interestingly, despite being distinct research domains, bot detection and AI-generated text detection overlap. Both fields aim to distinguish between human-and machine-driven activities in their respective domains. Bot detection primarily identifies automated accounts or actions on online platforms, whereas AI-generated text detection seeks to discern between machine-generated and human-generated content. \n\nThe detection methodologies developed for bot detection, such as identifying unique behavioural patterns, could provide valuable insights and potential strategies for AI-generated text detection. Furthermore, techniques such as frequency analysis, sentiment analysis, and behavioural pattern detection from bot detection can be adapted for AI-based text detection. However, recognising that synthetic text and bot activities represent different aspects of online manipulation is also crucial. Although the detection strategies share similarities, the specifics of each domain require customised methodologies designed to address distinct challenges and characteristics.",
                    "score": 0.6464026632967861,
                    "section_title": "D. DETECTING SYNTHETIC TEXT AND THEIR OVERLAP WITH BOT DETECTION",
                    "char_start_offset": 114719,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 196
                        },
                        {
                            "start": 197,
                            "end": 411
                        },
                        {
                            "start": 412,
                            "end": 684
                        },
                        {
                            "start": 687,
                            "end": 820
                        },
                        {
                            "start": 821,
                            "end": 1031
                        },
                        {
                            "start": 1032,
                            "end": 1159
                        },
                        {
                            "start": 1160,
                            "end": 1272
                        },
                        {
                            "start": 1275,
                            "end": 1361
                        },
                        {
                            "start": 1362,
                            "end": 1508
                        },
                        {
                            "start": 1509,
                            "end": 1682
                        },
                        {
                            "start": 1683,
                            "end": 1793
                        },
                        {
                            "start": 1794,
                            "end": 1897
                        },
                        {
                            "start": 1898,
                            "end": 2091
                        },
                        {
                            "start": 2094,
                            "end": 2289
                        },
                        {
                            "start": 2290,
                            "end": 2458
                        },
                        {
                            "start": 2459,
                            "end": 2586
                        },
                        {
                            "start": 2587,
                            "end": 2763
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.970703125
                },
                {
                    "corpus_id": "266369103",
                    "title": "A Survey on the Detection and Impacts of Deepfakes in Visual, Audio, and Textual Formats",
                    "text": "One way to detect whether a text is generated by a machine is to use the generative models themselves, such as GPT-2 or Grover, without any additional fine-tuning. These models can recognise the output of similar generative models [239]. Autoregressive models, such as GPT-2, GPT-3, and Grover, predict the next word in a sequence based on prior words, creating a unique statistical pattern in their generated text. \n\nThese patterns, which may include specific phrases, repetitions, or syntax, can be used to differentiate machine-generated text from human-written text. Thus, even without specific fine-tuning, detection algorithms can take advantage of these patterns to identify the content that is likely to be generated by these models [48], [237], [240]. \n\nIn their 2019 study, Solaiman et al. [237] proposed a baseline zero-shot approach that leveraged the total log probability produced by a transformer-based language generative model to discern machine-generated text from human-authored content. Their system operates by determining a threshold value: if a text's total log probability, as determined by the GPT-2 model, is nearer to the average probability of machine-generated texts than human-authored texts, it classifies the text as machine-generated. However, their previously mentioned simple classifier approach proved to be more accurate [237]. Zellers et al. [240] examined the challenges posed by artificially generated text, particularly in the context of fake news. They introduced Grover, a language model constructed similarly to GPT-2 but specifically designed for both generating and detecting fabricated news articles. Grover's approach is based on the idea that knowing how a text is generated is the key to detecting it. To achieve this, Grover leveraged its ability to produce synthetic text, which helps it spot patterns that are common in machine-generated content. This allows Grover to identify materials that are likely to be produced using similar models. Grover was specifically trained to identify fake news and distinguish it from other zero-shot models. It uses statistical patterns learnt from its training data to detect anomalies that may indicate machine-generated text.",
                    "score": 0.6881543730266563,
                    "section_title": "3) METHODS BASED ON ZERO SHOT MODELS",
                    "char_start_offset": 87987,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 415
                        },
                        {
                            "start": 418,
                            "end": 570
                        },
                        {
                            "start": 571,
                            "end": 760
                        },
                        {
                            "start": 763,
                            "end": 1006
                        },
                        {
                            "start": 1007,
                            "end": 1267
                        },
                        {
                            "start": 1268,
                            "end": 1364
                        },
                        {
                            "start": 1365,
                            "end": 1489
                        },
                        {
                            "start": 1490,
                            "end": 1647
                        },
                        {
                            "start": 1648,
                            "end": 1751
                        },
                        {
                            "start": 1752,
                            "end": 1899
                        },
                        {
                            "start": 1900,
                            "end": 1993
                        },
                        {
                            "start": 1994,
                            "end": 2095
                        },
                        {
                            "start": 2096,
                            "end": 2216
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 231,
                            "end": 236,
                            "matchedPaperCorpusId": "252907813"
                        },
                        {
                            "start": 741,
                            "end": 745,
                            "matchedPaperCorpusId": "160025533"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.951171875
                }
            ],
            "relevance_judgement": 0.970703125,
            "relevance_judgment_input_expanded": "# Title: A Survey on the Detection and Impacts of Deepfakes in Visual, Audio, and Textual Formats\n# Venue: IEEE Access\n# Authors: Rami Mubarak, Tariq A. A. Alsboui, Omar Alshaikh, Isa Inuwa-Dutse, Saad Khan, Si\u00f4n Parkinson\n## Abstract\nIn the rapidly evolving digital landscape, the generation of fake visual, audio, and textual content poses a significant threat to the trust of society, political stability, and integrity of information. The generation process has been enhanced and simplified using Artificial Intelligence techniques, which have been termed deepfake. Although significant attention has been paid to visual and audio deepfakes, there is also a burgeoning need to consider text-based deepfakes. Due to advancements in natural language processing and large language models, the potential of manipulating textual content to reshape online discourse and misinformation has increased. This study comprehensively examines the multifaceted nature and impacts of deep-fake-generated media. This work explains the broad implications of deepfakes in social, political, economic, and technological domains. State-of-the-art detection methodologies for all types of deepfake are critically reviewed, highlighting the need for unified, real-time, adaptable, and generalised solutions. As the challenges posed by deepfakes intensify, this study underscores the importance of a holistic approach that integrates technical solutions with public awareness and legislative action. By providing a comprehensive overview and establishing a framework for future exploration, this study seeks to assist researchers, policymakers, and practitioners navigate the complexities of deepfake phenomena.\n## 3) METHODS BASED ON ZERO SHOT MODELS\nOne way to detect whether a text is generated by a machine is to use the generative models themselves, such as GPT-2 or Grover, without any additional fine-tuning. These models can recognise the output of similar generative models [239]. Autoregressive models, such as GPT-2, GPT-3, and Grover, predict the next word in a sequence based on prior words, creating a unique statistical pattern in their generated text. \n\nThese patterns, which may include specific phrases, repetitions, or syntax, can be used to differentiate machine-generated text from human-written text. Thus, even without specific fine-tuning, detection algorithms can take advantage of these patterns to identify the content that is likely to be generated by these models [48], [237], [240]. \n\nIn their 2019 study, Solaiman et al. [237] proposed a baseline zero-shot approach that leveraged the total log probability produced by a transformer-based language generative model to discern machine-generated text from human-authored content. Their system operates by determining a threshold value: if a text's total log probability, as determined by the GPT-2 model, is nearer to the average probability of machine-generated texts than human-authored texts, it classifies the text as machine-generated. However, their previously mentioned simple classifier approach proved to be more accurate [237]. Zellers et al. [240] examined the challenges posed by artificially generated text, particularly in the context of fake news. They introduced Grover, a language model constructed similarly to GPT-2 but specifically designed for both generating and detecting fabricated news articles. Grover's approach is based on the idea that knowing how a text is generated is the key to detecting it. To achieve this, Grover leveraged its ability to produce synthetic text, which helps it spot patterns that are common in machine-generated content. This allows Grover to identify materials that are likely to be produced using similar models. Grover was specifically trained to identify fake news and distinguish it from other zero-shot models. It uses statistical patterns learnt from its training data to detect anomalies that may indicate machine-generated text.\n\n## D. DETECTING SYNTHETIC TEXT AND THEIR OVERLAP WITH BOT DETECTION\nIn the context of the rapid advancement of Large Language Models (LLMs), the ability to produce highly accurate and contextually relevant text is becoming an increasing concern in online security. The sophisticated text generation capabilities of these models can potentially facilitate harmful activities on online platforms, emphasising the importance of detecting AI-generated texts as a mitigation strategy. Several detection methodologies have been developed, each with varying degrees of effectiveness influenced by factors such as the architecture of the language model, the decoding strategy, the length of the text and the availability of raw log probabilities from the LLMs. \n\nSimple classifiers offer a basic, yet sometimes limited, approach, particularly with shorter texts and complex generation strategies. Zero-shot models provide a robust detection strategy by exploiting unique statistical patterns in machine-generated text; however, they are susceptible to adversarial actions or different generation strategies. Finetuned language models represent another approach that provides a more promising route for enhancing the detection accuracy. However, their effectiveness can be influenced by the sampling method and the length of the generated sequences. \n\nWatermarking, a more recent development, has emerged as a unique method for detection. This involves embedding watermarks in the logits of an LLM during text generation, forming a pattern that can potentially identify synthetic text. Despite the promise of watermarking, its vulnerability to spoofing/paraphrasing attacks and evasion techniques presents significant challenges that warrant further research. Interestingly, despite being distinct research domains, bot detection and AI-generated text detection overlap. Both fields aim to distinguish between human-and machine-driven activities in their respective domains. Bot detection primarily identifies automated accounts or actions on online platforms, whereas AI-generated text detection seeks to discern between machine-generated and human-generated content. \n\nThe detection methodologies developed for bot detection, such as identifying unique behavioural patterns, could provide valuable insights and potential strategies for AI-generated text detection. Furthermore, techniques such as frequency analysis, sentiment analysis, and behavioural pattern detection from bot detection can be adapted for AI-based text detection. However, recognising that synthetic text and bot activities represent different aspects of online manipulation is also crucial. Although the detection strategies share similarities, the specifics of each domain require customised methodologies designed to address distinct challenges and characteristics.",
            "reference_string": "[266369103 | Mubarak et al. | 2023 | Citations: 36]"
        },
        {
            "title": "Baselines for Identifying Watermarked Large Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 10,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.18456",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.18456, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2144484107",
                    "name": "Leonard Tang"
                },
                {
                    "authorId": "2143997407",
                    "name": "Gavin Uberti"
                },
                {
                    "authorId": "2211332429",
                    "name": "Tom Shlomi"
                }
            ],
            "abstract": "We consider the emerging problem of identifying the presence and use of watermarking schemes in widely used, publicly hosted, closed source large language models (LLMs). We introduce a suite of baseline algorithms for identifying watermarks in LLMs that rely on analyzing distributions of output tokens and logits generated by watermarked and unmarked LLMs. Notably, watermarked LLMs tend to produce distributions that diverge qualitatively and identifiably from standard models. Furthermore, we investigate the identifiability of watermarks at varying strengths and consider the tradeoffs of each of our identification mechanisms with respect to watermarking scenario. Along the way, we formalize the specific problem of identifying watermarks in LLMs, as well as LLM watermarks and watermark detection in general, providing a framework and foundations for studying them.",
            "corpus_id": 258967971,
            "sentences": [
                {
                    "corpus_id": "258967971",
                    "title": "Baselines for Identifying Watermarked Large Language Models",
                    "text": "Recent methods such as DetectGPT and GPTZero distinguish between machine-generated and human-written text by analyzing their statistical discrepancies (Tian, 2023;Mitchell et al., 2023). DetectGPT compares the log probability computed by a model on unperturbed text and perturbed variations, leveraging the observation that text sampled from a LLM generally occupy negative curvature regions of the model's log probability function. GPTZero instead uses perplexity and burstiness to distinguish human from machine text, with lower perplexity and burstiness indicating a greater likelihood of machine-generated text. However, these heuristics do not generalize and are often fallible. \n\nDetection by Learning Classifiers Several papers have proposed to train classifiers to distinguish between AI and human generated text. During the initial GPT-2 release, OpenAI trained a RoBERTa classifier to detect GPT-2 generated text with 95% accuracy (Solaiman et al., 2019). More recently, OpenAI fine-tuned a GPT model on a dataset of machine-generated and human texts focusing on the same topic, with a true positive identification rate of 26% (Ope-nAI, 2023). Similarly, Guo et al. (2023) collected the Human ChatGPT Comparison Corpuse (HC3) and fine-tuned RoBERTa for the detection task. \n\nNotably, the capabilities of such classifiers decrease as machine-generated text becomes increasingly human-like. Sadasivan et al. (2023) show theoretically that for sufficiently advanced language models, machine-generated text detectors offer only a marginal improvement over random classifiers. Moreover, such methods are prone to adversarial attacks and are not robust to out-of-distribution text. \n\nWatermarking Large Language Models An alternative to detecting of machine-generated text using statistical discrepancies and learned classifiers is the concept of watermarks. Watermarks are hidden patterns in machinegenerated text that are imperceptible to humans, but algorithmically identifiable as synthetic. Natural language watermarks long predate the development of LLMs, relying on methods such as synonym substitution, as well as syntactic and semantic transformations (Topkara et al., 2005).",
                    "score": 0.6440418152462469,
                    "section_title": "Related Work Generated Text Detection Via Statistical Discrepancies",
                    "char_start_offset": 1073,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 186
                        },
                        {
                            "start": 187,
                            "end": 432
                        },
                        {
                            "start": 433,
                            "end": 615
                        },
                        {
                            "start": 616,
                            "end": 683
                        },
                        {
                            "start": 686,
                            "end": 821
                        },
                        {
                            "start": 822,
                            "end": 965
                        },
                        {
                            "start": 966,
                            "end": 1153
                        },
                        {
                            "start": 1154,
                            "end": 1282
                        },
                        {
                            "start": 1285,
                            "end": 1398
                        },
                        {
                            "start": 1399,
                            "end": 1581
                        },
                        {
                            "start": 1582,
                            "end": 1685
                        },
                        {
                            "start": 1688,
                            "end": 1862
                        },
                        {
                            "start": 1863,
                            "end": 1999
                        },
                        {
                            "start": 2000,
                            "end": 2188
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.970703125
                },
                {
                    "corpus_id": "258967971",
                    "title": "Baselines for Identifying Watermarked Large Language Models",
                    "text": "Recent progress in large language models (LLMs) has resulted in a rapid increase in the ability of models to produce convincingly human-like text, sparking worries that LLMs could be used to spread disinformation, enable plagiarism, and maliciously impersonate people. As such, researchers have begun to develop methods to detect AI generated text. These include watermarking algorithms, which subtly modify the outputted text to allow for better detection, given that the detector has sufficient access to watermarking parameters. Differing from previous work, where the focus is on determining if text has been produced by a watermarked model, here we study the problem of if a language model has been watermarked. Critically, our black-box algorithms only require querying the model and do not necessitate any knowledge of underlying watermarking parameters. \n\n1 Department of Computer Science, Harvard University. \n\nCorrespondence to: Leonard Tang <leonardtang@college.harvard.edu>.",
                    "score": 0.6257769473984341,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 268
                        },
                        {
                            "start": 269,
                            "end": 348
                        },
                        {
                            "start": 349,
                            "end": 531
                        },
                        {
                            "start": 532,
                            "end": 716
                        },
                        {
                            "start": 717,
                            "end": 861
                        },
                        {
                            "start": 864,
                            "end": 917
                        },
                        {
                            "start": 920,
                            "end": 986
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9609375
                }
            ],
            "relevance_judgement": 0.970703125,
            "relevance_judgment_input_expanded": "# Title: Baselines for Identifying Watermarked Large Language Models\n# Venue: arXiv.org\n# Authors: Leonard Tang, Gavin Uberti, Tom Shlomi\n## Abstract\nWe consider the emerging problem of identifying the presence and use of watermarking schemes in widely used, publicly hosted, closed source large language models (LLMs). We introduce a suite of baseline algorithms for identifying watermarks in LLMs that rely on analyzing distributions of output tokens and logits generated by watermarked and unmarked LLMs. Notably, watermarked LLMs tend to produce distributions that diverge qualitatively and identifiably from standard models. Furthermore, we investigate the identifiability of watermarks at varying strengths and consider the tradeoffs of each of our identification mechanisms with respect to watermarking scenario. Along the way, we formalize the specific problem of identifying watermarks in LLMs, as well as LLM watermarks and watermark detection in general, providing a framework and foundations for studying them.\n## Introduction\nRecent progress in large language models (LLMs) has resulted in a rapid increase in the ability of models to produce convincingly human-like text, sparking worries that LLMs could be used to spread disinformation, enable plagiarism, and maliciously impersonate people. As such, researchers have begun to develop methods to detect AI generated text. These include watermarking algorithms, which subtly modify the outputted text to allow for better detection, given that the detector has sufficient access to watermarking parameters. Differing from previous work, where the focus is on determining if text has been produced by a watermarked model, here we study the problem of if a language model has been watermarked. Critically, our black-box algorithms only require querying the model and do not necessitate any knowledge of underlying watermarking parameters. \n\n1 Department of Computer Science, Harvard University. \n\nCorrespondence to: Leonard Tang <leonardtang@college.harvard.edu>.\n\n## Related Work Generated Text Detection Via Statistical Discrepancies\nRecent methods such as DetectGPT and GPTZero distinguish between machine-generated and human-written text by analyzing their statistical discrepancies (Tian, 2023;Mitchell et al., 2023). DetectGPT compares the log probability computed by a model on unperturbed text and perturbed variations, leveraging the observation that text sampled from a LLM generally occupy negative curvature regions of the model's log probability function. GPTZero instead uses perplexity and burstiness to distinguish human from machine text, with lower perplexity and burstiness indicating a greater likelihood of machine-generated text. However, these heuristics do not generalize and are often fallible. \n\nDetection by Learning Classifiers Several papers have proposed to train classifiers to distinguish between AI and human generated text. During the initial GPT-2 release, OpenAI trained a RoBERTa classifier to detect GPT-2 generated text with 95% accuracy (Solaiman et al., 2019). More recently, OpenAI fine-tuned a GPT model on a dataset of machine-generated and human texts focusing on the same topic, with a true positive identification rate of 26% (Ope-nAI, 2023). Similarly, Guo et al. (2023) collected the Human ChatGPT Comparison Corpuse (HC3) and fine-tuned RoBERTa for the detection task. \n\nNotably, the capabilities of such classifiers decrease as machine-generated text becomes increasingly human-like. Sadasivan et al. (2023) show theoretically that for sufficiently advanced language models, machine-generated text detectors offer only a marginal improvement over random classifiers. Moreover, such methods are prone to adversarial attacks and are not robust to out-of-distribution text. \n\nWatermarking Large Language Models An alternative to detecting of machine-generated text using statistical discrepancies and learned classifiers is the concept of watermarks. Watermarks are hidden patterns in machinegenerated text that are imperceptible to humans, but algorithmically identifiable as synthetic. Natural language watermarks long predate the development of LLMs, relying on methods such as synonym substitution, as well as syntactic and semantic transformations (Topkara et al., 2005).",
            "reference_string": "[258967971 | Tang et al. | 2023 | Citations: 3]"
        },
        {
            "title": "Watermarking Text Generated by Black-Box Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 41,
            "citation_count": 62,
            "influential_citation_count": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.08883",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.08883, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2143365293",
                    "name": "Xi Yang"
                },
                {
                    "authorId": "8780109",
                    "name": "Kejiang Chen"
                },
                {
                    "authorId": "51027868",
                    "name": "Weiming Zhang"
                },
                {
                    "authorId": "40997227",
                    "name": "Chang-rui Liu"
                },
                {
                    "authorId": "2217971466",
                    "name": "Yuang Qi"
                },
                {
                    "authorId": "2155863060",
                    "name": "Jie Zhang"
                },
                {
                    "authorId": "12464207",
                    "name": "Han Fang"
                },
                {
                    "authorId": "2052212945",
                    "name": "Neng H. Yu"
                }
            ],
            "abstract": "LLMs now exhibit human-like skills in various fields, leading to worries about misuse. Thus, detecting generated text is crucial. However, passive detection methods are stuck in domain specificity and limited adversarial robustness. To achieve reliable detection, a watermark-based method was proposed for white-box LLMs, allowing them to embed watermarks during text generation. The method involves randomly dividing the model vocabulary to obtain a special list and adjusting the probability distribution to promote the selection of words in the list. A detection algorithm aware of the list can identify the watermarked text. However, this method is not applicable in many real-world scenarios where only black-box language models are available. For instance, third-parties that develop API-based vertical applications cannot watermark text themselves because API providers only supply generated text and withhold probability distributions to shield their commercial interests. To allow third-parties to autonomously inject watermarks into generated text, we develop a watermarking framework for black-box language model usage scenarios. Specifically, we first define a binary encoding function to compute a random binary encoding corresponding to a word. The encodings computed for non-watermarked text conform to a Bernoulli distribution, wherein the probability of a word representing bit-1 being approximately 0.5. To inject a watermark, we alter the distribution by selectively replacing words representing bit-0 with context-based synonyms that represent bit-1. A statistical test is then used to identify the watermark. Experiments demonstrate the effectiveness of our method on both Chinese and English datasets. Furthermore, results under re-translation, polishing, word deletion, and synonym substitution attacks reveal that it is arduous to remove the watermark without compromising the original semantics.",
            "corpus_id": 258714683,
            "sentences": [
                {
                    "corpus_id": "258714683",
                    "title": "Watermarking Text Generated by Black-Box Language Models",
                    "text": "Statistical Discrepancy Detection. Several methods distinguish between generated and human-written text by identifying statistical discrepancies between them, as exemplified by two recent tools: GPTZero [40] and DetectGPT [18]. GPTZero uses perplexity and burstiness to tell apart human-written and generated text, as language models tend to produce more predictable and consistent text based on the patterns they learned from training data, resulting in lower perplexity scores for generated text. DetectGPT exploits the negative curvature regions of a model's log probability function to identify generated text by comparing the log probability of unperturbed and perturbed text variations. However, as language models are constantly improving and becoming more sophisticated, these heuristic features struggle to achieve robustness and generalization. Deep Learning-based Detection. Deep learning-based methods rely on gathering human-written and generated samples to train classifiers. Recently, OpenAI fine-tuned a GPT model for this discrimination task using a dataset comprising paired human and AI-generated texts on identical topics [25]. Similarly, Guo et al. [10] fine-tuned a text classifier based on pre-trained autoencoding LLMs (e.g., RoBERTa) by collecting the Human ChatGPT Comparison Corpus (HC3). Deep learning-based methods exhibit strong performance under the training data distribution, but they are susceptible to adversarial attacks, lack interpretability, and struggle to provide reliable judgments in human-AI collaboration scenarios. Watermark-based Detection. Kirchenbauer et al. [12] proposed the watermarking framework for white-box language models. The watermarking operates by randomly selecting a random set of \"greenlist\" words from the model's vocabulary and softly encouraging the use of these \"greenlist\" words by interfering with the sampling process at each generation step. The watermark can be detected by testing the following null hypothesis,  0 : The text sequence is generated with no knowledge of the selection rule of \"greenlist\" words. \n\nIf the null hypothesis is rejected, it can be concluded that the text was generated by the given model. This method is suitable for model owners who have access to the model's output probability distribution and can interfere with the sampling process.",
                    "score": 0.6669843102203198,
                    "section_title": "Recent Generated Text Detection Methods",
                    "char_start_offset": 9898,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 34
                        },
                        {
                            "start": 35,
                            "end": 227
                        },
                        {
                            "start": 228,
                            "end": 498
                        },
                        {
                            "start": 499,
                            "end": 692
                        },
                        {
                            "start": 693,
                            "end": 854
                        },
                        {
                            "start": 855,
                            "end": 885
                        },
                        {
                            "start": 886,
                            "end": 989
                        },
                        {
                            "start": 990,
                            "end": 1147
                        },
                        {
                            "start": 1148,
                            "end": 1315
                        },
                        {
                            "start": 1316,
                            "end": 1560
                        },
                        {
                            "start": 1561,
                            "end": 1587
                        },
                        {
                            "start": 1588,
                            "end": 1679
                        },
                        {
                            "start": 1680,
                            "end": 1913
                        },
                        {
                            "start": 1914,
                            "end": 2083
                        },
                        {
                            "start": 2086,
                            "end": 2189
                        },
                        {
                            "start": 2190,
                            "end": 2338
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97021484375
                },
                {
                    "corpus_id": "258714683",
                    "title": "Watermarking Text Generated by Black-Box Language Models",
                    "text": "Recent advances in large language models (LLMs) have enabled them to reach human-level proficiency across numerous professional and academic tasks [24,26,33]. One of the most impressive examples is OpenAI's ChatGPT [23], which has demonstrated remarkable prowess in answering questions, composing emails, essays, and even generating code. However, this impressive ability to create human-like text with remarkable efficiency has ignited apprehension regarding the potential abuse of LLMs for malicious purposes [6,7,15,31], such as phishing, disinformation campaigns, and academic dishonesty. Several countries and institutions have imposed bans on ChatGPT, citing concerns about privacy breaches, ideological influences, and academic dishonesty [19,20]. Additionally, media outlets have cautioned the public regarding the possibility of misleading information generated by LLMs [9]. These growing concerns have cast a shadow on the positive applications of LLMs. Therefore, detecting and authenticating generated text becomes crucial to ensure the responsible and secure use of LLMs. \n\nA prevalent solution is passive detection [1, 10,18,25,28,40], where a text classifier, usually fine-tuned on a pretrained language model like RoBERTa [16] and GPT-2 [30], is adopted to distinguish between generated and human-written text. However, these learning-based methods perform well only when the input data share a similar distribution with the training data, thereby limiting their applicability to specific domains. Moreover, as LLMs advance rapidly and human reliance on generated content grows, the line between human-written and generated text will gradually become more indistinct. For example, in the evaluations on a \"challenge set\" of English texts, OpenAI's text classifier only identifies 26% of generated text [25]. Besides, these classifiers are vulnerable to adversarial attacks [11,29,38] and are biased against non-native language writers [14], causing more false positives and negatives. \n\nTo achieve more reliable detection, Kirchenbauer et al. [12] proposed a watermark-based detection method for white-box language model usage scenarios.",
                    "score": 0.6375514295840754,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 159,
                            "end": 338
                        },
                        {
                            "start": 339,
                            "end": 592
                        },
                        {
                            "start": 593,
                            "end": 754
                        },
                        {
                            "start": 755,
                            "end": 883
                        },
                        {
                            "start": 884,
                            "end": 963
                        },
                        {
                            "start": 964,
                            "end": 1084
                        },
                        {
                            "start": 1087,
                            "end": 1326
                        },
                        {
                            "start": 1327,
                            "end": 1513
                        },
                        {
                            "start": 1514,
                            "end": 1683
                        },
                        {
                            "start": 1684,
                            "end": 1823
                        },
                        {
                            "start": 1824,
                            "end": 2000
                        },
                        {
                            "start": 2003,
                            "end": 2153
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 151,
                            "end": 154,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 514,
                            "end": 516,
                            "matchedPaperCorpusId": "256229858"
                        },
                        {
                            "start": 516,
                            "end": 519,
                            "matchedPaperCorpusId": "256655912"
                        },
                        {
                            "start": 519,
                            "end": 522,
                            "matchedPaperCorpusId": "254530623"
                        },
                        {
                            "start": 1253,
                            "end": 1257,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 1889,
                            "end": 1893,
                            "matchedPaperCorpusId": "202539059"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94287109375
                }
            ],
            "relevance_judgement": 0.97021484375,
            "relevance_judgment_input_expanded": "# Title: Watermarking Text Generated by Black-Box Language Models\n# Venue: arXiv.org\n# Authors: Xi Yang, Kejiang Chen, Weiming Zhang, Chang-rui Liu, Yuang Qi, Jie Zhang, Han Fang, Neng H. Yu\n## Abstract\nLLMs now exhibit human-like skills in various fields, leading to worries about misuse. Thus, detecting generated text is crucial. However, passive detection methods are stuck in domain specificity and limited adversarial robustness. To achieve reliable detection, a watermark-based method was proposed for white-box LLMs, allowing them to embed watermarks during text generation. The method involves randomly dividing the model vocabulary to obtain a special list and adjusting the probability distribution to promote the selection of words in the list. A detection algorithm aware of the list can identify the watermarked text. However, this method is not applicable in many real-world scenarios where only black-box language models are available. For instance, third-parties that develop API-based vertical applications cannot watermark text themselves because API providers only supply generated text and withhold probability distributions to shield their commercial interests. To allow third-parties to autonomously inject watermarks into generated text, we develop a watermarking framework for black-box language model usage scenarios. Specifically, we first define a binary encoding function to compute a random binary encoding corresponding to a word. The encodings computed for non-watermarked text conform to a Bernoulli distribution, wherein the probability of a word representing bit-1 being approximately 0.5. To inject a watermark, we alter the distribution by selectively replacing words representing bit-0 with context-based synonyms that represent bit-1. A statistical test is then used to identify the watermark. Experiments demonstrate the effectiveness of our method on both Chinese and English datasets. Furthermore, results under re-translation, polishing, word deletion, and synonym substitution attacks reveal that it is arduous to remove the watermark without compromising the original semantics.\n## INTRODUCTION\nRecent advances in large language models (LLMs) have enabled them to reach human-level proficiency across numerous professional and academic tasks [24,26,33]. One of the most impressive examples is OpenAI's ChatGPT [23], which has demonstrated remarkable prowess in answering questions, composing emails, essays, and even generating code. However, this impressive ability to create human-like text with remarkable efficiency has ignited apprehension regarding the potential abuse of LLMs for malicious purposes [6,7,15,31], such as phishing, disinformation campaigns, and academic dishonesty. Several countries and institutions have imposed bans on ChatGPT, citing concerns about privacy breaches, ideological influences, and academic dishonesty [19,20]. Additionally, media outlets have cautioned the public regarding the possibility of misleading information generated by LLMs [9]. These growing concerns have cast a shadow on the positive applications of LLMs. Therefore, detecting and authenticating generated text becomes crucial to ensure the responsible and secure use of LLMs. \n\nA prevalent solution is passive detection [1, 10,18,25,28,40], where a text classifier, usually fine-tuned on a pretrained language model like RoBERTa [16] and GPT-2 [30], is adopted to distinguish between generated and human-written text. However, these learning-based methods perform well only when the input data share a similar distribution with the training data, thereby limiting their applicability to specific domains. Moreover, as LLMs advance rapidly and human reliance on generated content grows, the line between human-written and generated text will gradually become more indistinct. For example, in the evaluations on a \"challenge set\" of English texts, OpenAI's text classifier only identifies 26% of generated text [25]. Besides, these classifiers are vulnerable to adversarial attacks [11,29,38] and are biased against non-native language writers [14], causing more false positives and negatives. \n\nTo achieve more reliable detection, Kirchenbauer et al. [12] proposed a watermark-based detection method for white-box language model usage scenarios.\n\n## Recent Generated Text Detection Methods\nStatistical Discrepancy Detection. Several methods distinguish between generated and human-written text by identifying statistical discrepancies between them, as exemplified by two recent tools: GPTZero [40] and DetectGPT [18]. GPTZero uses perplexity and burstiness to tell apart human-written and generated text, as language models tend to produce more predictable and consistent text based on the patterns they learned from training data, resulting in lower perplexity scores for generated text. DetectGPT exploits the negative curvature regions of a model's log probability function to identify generated text by comparing the log probability of unperturbed and perturbed text variations. However, as language models are constantly improving and becoming more sophisticated, these heuristic features struggle to achieve robustness and generalization. Deep Learning-based Detection. Deep learning-based methods rely on gathering human-written and generated samples to train classifiers. Recently, OpenAI fine-tuned a GPT model for this discrimination task using a dataset comprising paired human and AI-generated texts on identical topics [25]. Similarly, Guo et al. [10] fine-tuned a text classifier based on pre-trained autoencoding LLMs (e.g., RoBERTa) by collecting the Human ChatGPT Comparison Corpus (HC3). Deep learning-based methods exhibit strong performance under the training data distribution, but they are susceptible to adversarial attacks, lack interpretability, and struggle to provide reliable judgments in human-AI collaboration scenarios. Watermark-based Detection. Kirchenbauer et al. [12] proposed the watermarking framework for white-box language models. The watermarking operates by randomly selecting a random set of \"greenlist\" words from the model's vocabulary and softly encouraging the use of these \"greenlist\" words by interfering with the sampling process at each generation step. The watermark can be detected by testing the following null hypothesis,  0 : The text sequence is generated with no knowledge of the selection rule of \"greenlist\" words. \n\nIf the null hypothesis is rejected, it can be concluded that the text was generated by the given model. This method is suitable for model owners who have access to the model's output probability distribution and can interfere with the sampling process.",
            "reference_string": "[258714683 | Yang et al. | 2023 | Citations: 62]"
        },
        {
            "title": "Detecting AI-Generated Text: Factors Influencing Detectability with Current Methods",
            "venue": "Journal of Artificial Intelligence Research",
            "year": 2024,
            "reference_count": 118,
            "citation_count": 13,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15583, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2243977370",
                    "name": "Kathleen C. Fraser"
                },
                {
                    "authorId": "49044849",
                    "name": "Hillary Dawkins"
                },
                {
                    "authorId": "2886725",
                    "name": "S. Kiritchenko"
                }
            ],
            "abstract": "Large language models (LLMs) have advanced to a point that even humans have difficulty discerning whether a text was generated by another human, or by a computer. However, knowing whether a text was produced by human or artificial intelligence (AI) is important to determining its trustworthiness, and has applications in many domains including detecting fraud and academic dishonesty, as well as combating the spread of misinformation and political propaganda. The task of AI-generated text (AIGT) detection is therefore both very challenging, and highly critical. In this survey, we summarize stateof-the art approaches to AIGT detection, including watermarking, statistical and stylistic analysis, and machine learning classification. We also provide information about existing datasets for this task. Synthesizing the research findings, we aim to provide insight into the salient factors that combine to determine how \u201cdetectable\u201d AIGT text is under different scenarios, and to make practical recommendations for future work towards this significant technical and societal challenge.",
            "corpus_id": 270703139,
            "sentences": [
                {
                    "corpus_id": "270703139",
                    "title": "Detecting AI-Generated Text: Factors Influencing Detectability with Current Methods",
                    "text": "Large language models (LLMs) have advanced to a point that even humans have difficulty discerning whether a text was generated by another human, or by a computer. However, knowing whether a text was produced by human or artificial intelligence (AI) is important to determining its trustworthiness, and has applications in many domains including detecting fraud and academic dishonesty, as well as combating the spread of misinformation and political propaganda. The task of AI-generated text (AIGT) detection is therefore both very challenging, and highly critical. In this survey, we summarize stateof-the art approaches to AIGT detection, including watermarking, statistical and stylistic analysis, and machine learning classification. We also provide information about existing datasets for this task. Synthesizing the research findings, we aim to provide insight into the salient factors that combine to determine how \u201cdetectable\u201d AIGT text is under different scenarios, and to make practical recommendations for future work towards this significant technical and societal challenge.",
                    "score": 0.6734783001349617,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9677734375
                },
                {
                    "corpus_id": "270703139",
                    "title": "Detecting AI-Generated Text: Factors Influencing Detectability with Current Methods",
                    "text": "As described in the previous section, there are three main methods for the detection of AI-generated text (AIGT): watermarking, statistical and stylistic analysis, and the use of pre-trained language models (LMs). The detection of a watermark requires knowledge of the watermark extraction algorithm; beyond that, no extra data are needed. However, the other two broad detection methods require data in order to learn the patterns that distinguish AIGT from human-written text -ideally, a dataset where the positive examples were generated by the AI model we wish to detect, and in a setting as close as possible to how we expect to encounter these texts in the real world. Previous work has indicated that the most effective detectors are trained on data from the same domain (news articles, social media posts, academic essays, etc.), language (English, Chinese, French, etc.) and model settings (decoding algorithm, prompt, length of output, etc.) as the test data. At the same time, the research also shows that for maximal generalizability and robustness, it is essential to train a detector on a wide variety of data so that it is not overfit to one very narrow range of data samples. Therefore, for any given application, it is important to select appropriate data to first train the detector, and also to test the detector for its accuracy: \"The effectiveness of black-box detection models is heavily dependent on the quality and diversity of the acquired data\" (Tang et al., 2024). \n\nTable 3 lists some of the most frequently used datasets that include human-written and AI-generated texts. We observe that the vast majority of available datasets are in English, with another sizeable chunk being multilingual (including English as well as other languages). There is a well-known bias in the field of NLP that much of the research focuses on English, to the detriment of other languages. This bias is no doubt exacerbated by the fact that many of the large language models (LLMs) used to generate these datasets were, at least initially, only available in English.",
                    "score": 0.5766219431200634,
                    "section_title": "AIGT Datasets",
                    "char_start_offset": 65586,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 213
                        },
                        {
                            "start": 214,
                            "end": 339
                        },
                        {
                            "start": 340,
                            "end": 673
                        },
                        {
                            "start": 674,
                            "end": 968
                        },
                        {
                            "start": 969,
                            "end": 1190
                        },
                        {
                            "start": 1191,
                            "end": 1490
                        },
                        {
                            "start": 1493,
                            "end": 1599
                        },
                        {
                            "start": 1600,
                            "end": 1766
                        },
                        {
                            "start": 1767,
                            "end": 1896
                        },
                        {
                            "start": 1897,
                            "end": 2073
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1470,
                            "end": 1489,
                            "matchedPaperCorpusId": "257496757"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94384765625
                }
            ],
            "relevance_judgement": 0.9677734375,
            "relevance_judgment_input_expanded": "# Title: Detecting AI-Generated Text: Factors Influencing Detectability with Current Methods\n# Venue: Journal of Artificial Intelligence Research\n# Authors: Kathleen C. Fraser, Hillary Dawkins, S. Kiritchenko\n## Abstract\nLarge language models (LLMs) have advanced to a point that even humans have difficulty discerning whether a text was generated by another human, or by a computer. However, knowing whether a text was produced by human or artificial intelligence (AI) is important to determining its trustworthiness, and has applications in many domains including detecting fraud and academic dishonesty, as well as combating the spread of misinformation and political propaganda. The task of AI-generated text (AIGT) detection is therefore both very challenging, and highly critical. In this survey, we summarize stateof-the art approaches to AIGT detection, including watermarking, statistical and stylistic analysis, and machine learning classification. We also provide information about existing datasets for this task. Synthesizing the research findings, we aim to provide insight into the salient factors that combine to determine how \u201cdetectable\u201d AIGT text is under different scenarios, and to make practical recommendations for future work towards this significant technical and societal challenge.\n## AIGT Datasets\nAs described in the previous section, there are three main methods for the detection of AI-generated text (AIGT): watermarking, statistical and stylistic analysis, and the use of pre-trained language models (LMs). The detection of a watermark requires knowledge of the watermark extraction algorithm; beyond that, no extra data are needed. However, the other two broad detection methods require data in order to learn the patterns that distinguish AIGT from human-written text -ideally, a dataset where the positive examples were generated by the AI model we wish to detect, and in a setting as close as possible to how we expect to encounter these texts in the real world. Previous work has indicated that the most effective detectors are trained on data from the same domain (news articles, social media posts, academic essays, etc.), language (English, Chinese, French, etc.) and model settings (decoding algorithm, prompt, length of output, etc.) as the test data. At the same time, the research also shows that for maximal generalizability and robustness, it is essential to train a detector on a wide variety of data so that it is not overfit to one very narrow range of data samples. Therefore, for any given application, it is important to select appropriate data to first train the detector, and also to test the detector for its accuracy: \"The effectiveness of black-box detection models is heavily dependent on the quality and diversity of the acquired data\" (Tang et al., 2024). \n\nTable 3 lists some of the most frequently used datasets that include human-written and AI-generated texts. We observe that the vast majority of available datasets are in English, with another sizeable chunk being multilingual (including English as well as other languages). There is a well-known bias in the field of NLP that much of the research focuses on English, to the detriment of other languages. This bias is no doubt exacerbated by the fact that many of the large language models (LLMs) used to generate these datasets were, at least initially, only available in English.",
            "reference_string": "[270703139 | Fraser et al. | 2024 | Citations: 13]"
        },
        {
            "title": "On the Possibilities of AI-Generated Text Detection",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 53,
            "citation_count": 127,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2304.04736",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.04736, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "49081354",
                    "name": "Souradip Chakraborty"
                },
                {
                    "authorId": "3387859",
                    "name": "A. S. Bedi"
                },
                {
                    "authorId": "2110018762",
                    "name": "Sicheng Zhu"
                },
                {
                    "authorId": "49640821",
                    "name": "Bang An"
                },
                {
                    "authorId": "2172597446",
                    "name": "Dinesh Manocha"
                },
                {
                    "authorId": "2117426487",
                    "name": "Furong Huang"
                }
            ],
            "abstract": "Our work addresses the critical issue of distinguishing text generated by Large Language Models (LLMs) from human-produced text, a task essential for numerous applications. Despite ongoing debate about the feasibility of such differentiation, we present evidence supporting its consistent achievability, except when human and machine text distributions are indistinguishable across their entire support. Drawing from information theory, we argue that as machine-generated text approximates human-like quality, the sample size needed for detection increases. We establish precise sample complexity bounds for detecting AI-generated text, laying groundwork for future research aimed at developing advanced, multi-sample detectors. Our empirical evaluations across multiple datasets (Xsum, Squad, IMDb, and Kaggle FakeNews) confirm the viability of enhanced detection methods. We test various state-of-the-art text generators, including GPT-2, GPT-3.5-Turbo, Llama, Llama-2-13B-Chat-HF, and Llama-2-70B-Chat-HF, against detectors, including oBERTa-Large/Base-Detector, GPTZero. Our findings align with OpenAI's empirical data related to sequence length, marking the first theoretical substantiation for these observations.",
            "corpus_id": 258048481,
            "sentences": [
                {
                    "corpus_id": "258048481",
                    "title": "On the Possibilities of AI-Generated Text Detection",
                    "text": "Recent research has shown promising results in developing detection methods. Some of these methods use statistical approaches to identify differences in the linguistic patterns of human and machine-generated text. We survey the existing approaches here. \n\nTraditional approaches. They involve statistical outlier detection methods, which employ statistical metrics such as entropy, perplexity, and n-gram frequency to differentiate between human and machine-generated texts (Lavergne et al., 2008;Gehrmann et al., 2019). However, with the advent of ChatGPT (OpenAI) (OpenAI, 2023), a new innovative statistical detection methodology, DetectGPT (Mitchell et al., 2023), has been developed. It operates on the principle that text generated by the model tends to lie in the negative curvature areas of the model's log probability. DetectGPT (Mitchell et al., 2023) generates and compares multiple perturbations of model-generated text to determine whether the text is machine-generated or not based on the log probability of the original text and the perturbed versions. DetectGPT significantly outperforms the majority of the existing zero-shot methods for model sample detection with very high AUC scores (note that we use the terms AUROC and AUC interchangeably for presentation convenience). \n\nClassifier-based detectors. In contrast to statistical methods, classifier-based detectors are common in natural language detection paradigms, particularly in fake news and misinformation detection (Schildhauer, 2022;Zou & Ling, 2021). OpenAI has recently fine-tuned a GPT model (OpenAI, 2023) using data from Wikipedia, WebText, and internal human demonstration data to create a web interface for a discrimination task using text generated by 34 language models. This approach combines a classifier-based approach with a human evaluation component to determine whether a given text was machine-generated or not. These recent advancements in the field of detecting AI-generated text have significant implications for detecting and preventing the spread of misinformation and fake news, thereby contributing to the betterment of society (Schildhauer, 2022;Zou & Ling, 2021;Kshetri & Voas, 2022). \n\nWatermark-based identification.",
                    "score": 0.7676900531016942,
                    "section_title": "Background on AI-Generated Text Detectors and Related Works",
                    "char_start_offset": 4175,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 76
                        },
                        {
                            "start": 77,
                            "end": 213
                        },
                        {
                            "start": 214,
                            "end": 253
                        },
                        {
                            "start": 256,
                            "end": 279
                        },
                        {
                            "start": 280,
                            "end": 520
                        },
                        {
                            "start": 521,
                            "end": 688
                        },
                        {
                            "start": 689,
                            "end": 827
                        },
                        {
                            "start": 828,
                            "end": 1067
                        },
                        {
                            "start": 1068,
                            "end": 1292
                        },
                        {
                            "start": 1295,
                            "end": 1322
                        },
                        {
                            "start": 1323,
                            "end": 1530
                        },
                        {
                            "start": 1531,
                            "end": 1758
                        },
                        {
                            "start": 1759,
                            "end": 1907
                        },
                        {
                            "start": 1908,
                            "end": 2189
                        },
                        {
                            "start": 2192,
                            "end": 2223
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 474,
                            "end": 497,
                            "matchedPaperCorpusId": "12098535"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9658203125
                }
            ],
            "relevance_judgement": 0.9658203125,
            "relevance_judgment_input_expanded": "# Title: On the Possibilities of AI-Generated Text Detection\n# Venue: arXiv.org\n# Authors: Souradip Chakraborty, A. S. Bedi, Sicheng Zhu, Bang An, Dinesh Manocha, Furong Huang\n## Abstract\nOur work addresses the critical issue of distinguishing text generated by Large Language Models (LLMs) from human-produced text, a task essential for numerous applications. Despite ongoing debate about the feasibility of such differentiation, we present evidence supporting its consistent achievability, except when human and machine text distributions are indistinguishable across their entire support. Drawing from information theory, we argue that as machine-generated text approximates human-like quality, the sample size needed for detection increases. We establish precise sample complexity bounds for detecting AI-generated text, laying groundwork for future research aimed at developing advanced, multi-sample detectors. Our empirical evaluations across multiple datasets (Xsum, Squad, IMDb, and Kaggle FakeNews) confirm the viability of enhanced detection methods. We test various state-of-the-art text generators, including GPT-2, GPT-3.5-Turbo, Llama, Llama-2-13B-Chat-HF, and Llama-2-70B-Chat-HF, against detectors, including oBERTa-Large/Base-Detector, GPTZero. Our findings align with OpenAI's empirical data related to sequence length, marking the first theoretical substantiation for these observations.\n## Background on AI-Generated Text Detectors and Related Works\nRecent research has shown promising results in developing detection methods. Some of these methods use statistical approaches to identify differences in the linguistic patterns of human and machine-generated text. We survey the existing approaches here. \n\nTraditional approaches. They involve statistical outlier detection methods, which employ statistical metrics such as entropy, perplexity, and n-gram frequency to differentiate between human and machine-generated texts (Lavergne et al., 2008;Gehrmann et al., 2019). However, with the advent of ChatGPT (OpenAI) (OpenAI, 2023), a new innovative statistical detection methodology, DetectGPT (Mitchell et al., 2023), has been developed. It operates on the principle that text generated by the model tends to lie in the negative curvature areas of the model's log probability. DetectGPT (Mitchell et al., 2023) generates and compares multiple perturbations of model-generated text to determine whether the text is machine-generated or not based on the log probability of the original text and the perturbed versions. DetectGPT significantly outperforms the majority of the existing zero-shot methods for model sample detection with very high AUC scores (note that we use the terms AUROC and AUC interchangeably for presentation convenience). \n\nClassifier-based detectors. In contrast to statistical methods, classifier-based detectors are common in natural language detection paradigms, particularly in fake news and misinformation detection (Schildhauer, 2022;Zou & Ling, 2021). OpenAI has recently fine-tuned a GPT model (OpenAI, 2023) using data from Wikipedia, WebText, and internal human demonstration data to create a web interface for a discrimination task using text generated by 34 language models. This approach combines a classifier-based approach with a human evaluation component to determine whether a given text was machine-generated or not. These recent advancements in the field of detecting AI-generated text have significant implications for detecting and preventing the spread of misinformation and fake news, thereby contributing to the betterment of society (Schildhauer, 2022;Zou & Ling, 2021;Kshetri & Voas, 2022). \n\nWatermark-based identification.",
            "reference_string": "[258048481 | Chakraborty et al. | 2023 | Citations: 127]"
        },
        {
            "title": "Context-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 31,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.1260, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2330513747",
                    "name": "Yuxuan Guo"
                },
                {
                    "authorId": "2260832680",
                    "name": "Zhiliang Tian"
                },
                {
                    "authorId": "2261069334",
                    "name": "Yiping Song"
                },
                {
                    "authorId": "2329747280",
                    "name": "Tianlun Liu"
                },
                {
                    "authorId": "46573238",
                    "name": "Liang Ding"
                },
                {
                    "authorId": "2163335292",
                    "name": "Dongsheng Li"
                }
            ],
            "abstract": "Watermarking enables people to determine whether the text is generated by a specific model. It injects a unique signature based on the \u201cgreen-red\u201d list that can be tracked during detection, where the words in green lists are encouraged to be generated. Recent researchers propose to fix the green/red lists or increase the proportion of green tokens to defend against paraphrasing attacks. However, these methods cause degradation of text quality due to semantic disparities between the watermarked text and the unwatermarked text. In this paper, we propose a semantic-aware watermark method that considers contexts to generate a semantic-aware key to split a semantically balanced green/red list for watermark injection. The semantic balanced list reduces the performance drop due to adding bias on green lists. To defend against paraphrasing attacks, we generate the watermark key considering the semantics of contexts via locally sensitive hashing. To improve the text quality, we propose to split green/red lists considering semantics to enable the green list to cover almost all semantics. We also dynamically adapt the bias to balance text quality and robustness. The experiments show our advantages in both robustness and text quality comparable to existing baselines.",
            "corpus_id": 273901404,
            "sentences": [
                {
                    "corpus_id": "273901404",
                    "title": "Context-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models",
                    "text": "Large Language Models (LLMs) show their power on text generations but their formidable power may be used for unethical purposes such as plagiarism (Augenstein et al., 2023). Current research injects watermarks into LLMs' generated texts, thereby enabling people to distinguish between LLM-generated text and human-written text. Recent watermark methods inject a unique signature into LLM-generated text, which can only be perceived by watermarking methods, facilitating the detection of whether a text was generated by LLMs. \n\nCurrent watermark methods mainly inject the signature according to \"green-red list\" (Ren et al., 2023): they divide vocabulary into green/red lists, regard words in green lists as the unique signature, and encourage to generate green tokens, which is easy to be recognized. The methods can be divided into two categories: (1) Token sampling biasing-based watermark forces LLMs to select only green tokens during generation. EXP-Edit (Kuditipudi et al., 2023) intervenes in the sampling process of each token. However, forcing LLMs to sample green tokens restricts the semantic richness of LLM-generated text, thus undermining its text quality. (2) To improve the generation quality, researchers further propose token probability biasing-based watermark, which enriches the semantics of watermarked texts by introducing a bias to the probability distribution to softly encourage generating green tokens instead of restricting to select green tokens. Takezawa et al. (2023) proposed NS-Mark to constrain the frequency of biasing. Wu et al. (2023) introduced DiPMark to approximate the biased probability distribution to the original one. These methods mitigate the impact of biasing on text quality and ensure superior text quality. \n\nThe above methods narrow the gap in text quality between the watermarked text and the unwatermarked text but lack robustness against paraphrasing attacks. Paraphrasing attacks (Krishna et al., 2024) use language models to modify the watermarked text to evade the unique signature of the watermarked text.",
                    "score": 0.6543769534179502,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 173
                        },
                        {
                            "start": 174,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 524
                        },
                        {
                            "start": 527,
                            "end": 800
                        },
                        {
                            "start": 801,
                            "end": 950
                        },
                        {
                            "start": 951,
                            "end": 1035
                        },
                        {
                            "start": 1036,
                            "end": 1170
                        },
                        {
                            "start": 1171,
                            "end": 1475
                        },
                        {
                            "start": 1476,
                            "end": 1554
                        },
                        {
                            "start": 1555,
                            "end": 1662
                        },
                        {
                            "start": 1663,
                            "end": 1757
                        },
                        {
                            "start": 1760,
                            "end": 1914
                        },
                        {
                            "start": 1915,
                            "end": 2064
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9638671875
                }
            ],
            "relevance_judgement": 0.9638671875,
            "relevance_judgment_input_expanded": "# Title: Context-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Yuxuan Guo, Zhiliang Tian, Yiping Song, Tianlun Liu, Liang Ding, Dongsheng Li\n## Abstract\nWatermarking enables people to determine whether the text is generated by a specific model. It injects a unique signature based on the \u201cgreen-red\u201d list that can be tracked during detection, where the words in green lists are encouraged to be generated. Recent researchers propose to fix the green/red lists or increase the proportion of green tokens to defend against paraphrasing attacks. However, these methods cause degradation of text quality due to semantic disparities between the watermarked text and the unwatermarked text. In this paper, we propose a semantic-aware watermark method that considers contexts to generate a semantic-aware key to split a semantically balanced green/red list for watermark injection. The semantic balanced list reduces the performance drop due to adding bias on green lists. To defend against paraphrasing attacks, we generate the watermark key considering the semantics of contexts via locally sensitive hashing. To improve the text quality, we propose to split green/red lists considering semantics to enable the green list to cover almost all semantics. We also dynamically adapt the bias to balance text quality and robustness. The experiments show our advantages in both robustness and text quality comparable to existing baselines.\n## Introduction\nLarge Language Models (LLMs) show their power on text generations but their formidable power may be used for unethical purposes such as plagiarism (Augenstein et al., 2023). Current research injects watermarks into LLMs' generated texts, thereby enabling people to distinguish between LLM-generated text and human-written text. Recent watermark methods inject a unique signature into LLM-generated text, which can only be perceived by watermarking methods, facilitating the detection of whether a text was generated by LLMs. \n\nCurrent watermark methods mainly inject the signature according to \"green-red list\" (Ren et al., 2023): they divide vocabulary into green/red lists, regard words in green lists as the unique signature, and encourage to generate green tokens, which is easy to be recognized. The methods can be divided into two categories: (1) Token sampling biasing-based watermark forces LLMs to select only green tokens during generation. EXP-Edit (Kuditipudi et al., 2023) intervenes in the sampling process of each token. However, forcing LLMs to sample green tokens restricts the semantic richness of LLM-generated text, thus undermining its text quality. (2) To improve the generation quality, researchers further propose token probability biasing-based watermark, which enriches the semantics of watermarked texts by introducing a bias to the probability distribution to softly encourage generating green tokens instead of restricting to select green tokens. Takezawa et al. (2023) proposed NS-Mark to constrain the frequency of biasing. Wu et al. (2023) introduced DiPMark to approximate the biased probability distribution to the original one. These methods mitigate the impact of biasing on text quality and ensure superior text quality. \n\nThe above methods narrow the gap in text quality between the watermarked text and the unwatermarked text but lack robustness against paraphrasing attacks. Paraphrasing attacks (Krishna et al., 2024) use language models to modify the watermarked text to evade the unique signature of the watermarked text.",
            "reference_string": "[273901404 | Guo et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Understanding the Effects of Human-written Paraphrases in LLM-generated Text Detection",
            "venue": "Natural Language Processing Journal",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.03806, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2329369337",
                    "name": "Hiu Ting Lau"
                },
                {
                    "authorId": "2805349",
                    "name": "A. Zubiaga"
                }
            ],
            "abstract": "Natural Language Generation has been rapidly developing with the advent of large language models (LLMs). While their usage has sparked significant attention from the general public, it is important for readers to be aware when a piece of text is LLM-generated. This has brought about the need for building models that enable automated LLM-generated text detection, with the aim of mitigating potential negative outcomes of such content. Existing LLM-generated detectors show competitive performances in telling apart LLM-generated and human-written text, but this performance is likely to deteriorate when paraphrased texts are considered. In this study, we devise a new data collection strategy to collect Human&LLM Paraphrase Collection (HLPC), a first-of-its-kind dataset that incorporates human-written texts and paraphrases, as well as LLM-generated texts and paraphrases. With the aim of understanding the effects of human-written paraphrases on the performance of state-of-the-art LLM-generated text detectors OpenAI RoBERTa and watermark detectors, we perform classification experiments that incorporate human-written paraphrases, watermarked and non-watermarked LLM-generated documents from GPT and OPT, and LLM-generated paraphrases from DIPPER and BART. The results show that the inclusion of human-written paraphrases has a significant impact of LLM-generated detector performance, promoting TPR@1%FPR with a possible trade-off of AUROC and accuracy.",
            "corpus_id": 273849981,
            "sentences": [
                {
                    "corpus_id": "273849981",
                    "title": "Understanding the Effects of Human-written Paraphrases in LLM-generated Text Detection",
                    "text": "Large language models (LLMs) have become essential in Natural Language Processing (NLP) thanks to their advanced capabilities for text processing and generation, which is achieved through analysis of patterns and relationships between words and sentences using transformer models [42]. Consequently, LLMs have had a significant impact on Natural Language Generation (NLG), as they have provided improved capacity for automatically generating high quality text [4]. \n\nWhile the advancement of LLMs in the context of NLG has aided tasks such as machine translation [38] and text summarization [40], it has also given rise to undesired social problems, including intentional malicious usage, ethical concerns and information inaccuracy. This has brought about the need for researching the development of methods for automated LLM-generated text detection which distinguishes if a text is human-or LLM-generated [12]. Currently, there are 2 major streams of LLM-generated text detectors: (i) zero-shot classifiers [25,33], which identify LLM-generated text based on the pattern and characteristics of the input, and (ii) watermark detectors [17], which rely on detecting the presence of watermarks which are imprinted into the text during the generation process [16], and are effective in the cases where the watermarks have been added by the LLM. The detectors then examine the input, classifying it as LLM-generated if the level of watermarking exceeds a set threshold, or as human-generated otherwise. \n\nBoth kinds of detectors have demonstrated excellent performance in LLM-generated text detection. However, research testing these detectors has primarily focused on datasets involving texts which are exclusively generated by humans or by LLMs. There can be, however, more complicated cases, such as paraphrased texts, which have been seldom considered in previous research. Paraphrasing is defined as the rewriting of context in a simpler and shorter form [8]; an LLM-generated text which is then paraphrased by humans leads to modified texts where the statistical properties of watermarks in the LLM-generated text is no longer identifiable.",
                    "score": 0.6393146762033723,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 285
                        },
                        {
                            "start": 286,
                            "end": 464
                        },
                        {
                            "start": 467,
                            "end": 733
                        },
                        {
                            "start": 734,
                            "end": 913
                        },
                        {
                            "start": 914,
                            "end": 1343
                        },
                        {
                            "start": 1344,
                            "end": 1500
                        },
                        {
                            "start": 1503,
                            "end": 1599
                        },
                        {
                            "start": 1600,
                            "end": 1745
                        },
                        {
                            "start": 1746,
                            "end": 1875
                        },
                        {
                            "start": 1876,
                            "end": 2144
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 280,
                            "end": 284,
                            "matchedPaperCorpusId": "267152841"
                        },
                        {
                            "start": 563,
                            "end": 567,
                            "matchedPaperCorpusId": "255942578"
                        },
                        {
                            "start": 591,
                            "end": 595,
                            "matchedPaperCorpusId": "256416014"
                        },
                        {
                            "start": 908,
                            "end": 912,
                            "matchedPaperCorpusId": "233563022"
                        },
                        {
                            "start": 1258,
                            "end": 1262,
                            "matchedPaperCorpusId": "256194179"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9638671875
                }
            ],
            "relevance_judgement": 0.9638671875,
            "relevance_judgment_input_expanded": "# Title: Understanding the Effects of Human-written Paraphrases in LLM-generated Text Detection\n# Venue: Natural Language Processing Journal\n# Authors: Hiu Ting Lau, A. Zubiaga\n## Abstract\nNatural Language Generation has been rapidly developing with the advent of large language models (LLMs). While their usage has sparked significant attention from the general public, it is important for readers to be aware when a piece of text is LLM-generated. This has brought about the need for building models that enable automated LLM-generated text detection, with the aim of mitigating potential negative outcomes of such content. Existing LLM-generated detectors show competitive performances in telling apart LLM-generated and human-written text, but this performance is likely to deteriorate when paraphrased texts are considered. In this study, we devise a new data collection strategy to collect Human&LLM Paraphrase Collection (HLPC), a first-of-its-kind dataset that incorporates human-written texts and paraphrases, as well as LLM-generated texts and paraphrases. With the aim of understanding the effects of human-written paraphrases on the performance of state-of-the-art LLM-generated text detectors OpenAI RoBERTa and watermark detectors, we perform classification experiments that incorporate human-written paraphrases, watermarked and non-watermarked LLM-generated documents from GPT and OPT, and LLM-generated paraphrases from DIPPER and BART. The results show that the inclusion of human-written paraphrases has a significant impact of LLM-generated detector performance, promoting TPR@1%FPR with a possible trade-off of AUROC and accuracy.\n## Introduction\nLarge language models (LLMs) have become essential in Natural Language Processing (NLP) thanks to their advanced capabilities for text processing and generation, which is achieved through analysis of patterns and relationships between words and sentences using transformer models [42]. Consequently, LLMs have had a significant impact on Natural Language Generation (NLG), as they have provided improved capacity for automatically generating high quality text [4]. \n\nWhile the advancement of LLMs in the context of NLG has aided tasks such as machine translation [38] and text summarization [40], it has also given rise to undesired social problems, including intentional malicious usage, ethical concerns and information inaccuracy. This has brought about the need for researching the development of methods for automated LLM-generated text detection which distinguishes if a text is human-or LLM-generated [12]. Currently, there are 2 major streams of LLM-generated text detectors: (i) zero-shot classifiers [25,33], which identify LLM-generated text based on the pattern and characteristics of the input, and (ii) watermark detectors [17], which rely on detecting the presence of watermarks which are imprinted into the text during the generation process [16], and are effective in the cases where the watermarks have been added by the LLM. The detectors then examine the input, classifying it as LLM-generated if the level of watermarking exceeds a set threshold, or as human-generated otherwise. \n\nBoth kinds of detectors have demonstrated excellent performance in LLM-generated text detection. However, research testing these detectors has primarily focused on datasets involving texts which are exclusively generated by humans or by LLMs. There can be, however, more complicated cases, such as paraphrased texts, which have been seldom considered in previous research. Paraphrasing is defined as the rewriting of context in a simpler and shorter form [8]; an LLM-generated text which is then paraphrased by humans leads to modified texts where the statistical properties of watermarks in the LLM-generated text is no longer identifiable.",
            "reference_string": "[273849981 | Lau et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text",
            "venue": "European Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 55,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA240862",
                "status": "HYBRID",
                "license": "CCBYNC",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.09364, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115944163",
                    "name": "Lucio La Cava"
                },
                {
                    "authorId": "2070008514",
                    "name": "Davide Costa"
                },
                {
                    "authorId": "2261507500",
                    "name": "Andrea Tagarelli"
                }
            ],
            "abstract": "The significant progress in the development of Large Language Models has contributed to blurring the distinction between human and AI-generated text. The increasing pervasiveness of AI-generated text and the difficulty in detecting it poses new challenges for our society. In this paper, we tackle the problem of detecting and attributing AI-generated text by proposing WhosAI, a triplet-network contrastive learning framework designed to predict whether a given input text has been generated by humans or AI and to unveil the authorship of the text. Unlike most existing approaches, our proposed framework is conceived to learn semantic similarity representations from multiple generators at once, thus equally handling both detection and attribution tasks. Furthermore, WhosAI is model-agnostic and scalable to the release of new AI text-generation models by incorporating their generated instances into the embedding space learned by our framework. Experimental results on the TuringBench benchmark of 200K news articles show that our proposed framework achieves outstanding results in both the Turing Test and Authorship Attribution tasks, outperforming all the methods listed in the TuringBench benchmark leaderboards.",
            "corpus_id": 271161969,
            "sentences": [
                {
                    "corpus_id": "271161969",
                    "title": "Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text",
                    "text": "The remarkable boost in human-like text generation performances achieved by Large Language Models (LLMs) in recent years has determined a rising challenge in detecting whether and to what extent texts have been generated by humans or machines [21,49,39]. In this context, the \"watermarking\" paradigm rapidly gained attention [25,53,28,50], as it allows embedding specific signals into generated texts that remain invisible to humans but are algorithmically detectable. Statistical learning methods also offer advanced solutions for detecting the authorship of texts. These include probabilistic models [31,1,47,14], log rank information [38], perplexity [44], discourse motifs [24], and other statistical approaches [16,40,45] More recently, we have witnessed the emergence of deep learning to detect or attribute AI-generated content, which stands as a promising body of research. Researchers have been exploiting LLMs to detect generated text [18,46], using ChatGPT itself as a detector [2], or combining LLMs with topological aspects [43]. \n\nA very recent trend involves leveraging contrastive learning to handle textual information. Indeed, despite its origins in the computer vision domain, contrastive representation learning has been proven particularly effective in NLP contexts to improve research on semantic similarity related problems, such as text classification [33,10], spotting hate-speech [23], unveiling intents [55], and eventually detecting AI-generated text through domain adaptation [3] or domain adversarial training [4]. \n\nDespite the advancements in research on detection of AI text generation, each of the above mentioned approaches faces significant challenges. Watermarking approaches are conditioned by the con-crete possibility of watermarking a given text, leaving the detection of non-watermarked texts an open issue. Statistical learning methods typically require access to the models' internals or to information that might be unavailable, limiting their applicability. Yet, more importantly for the sake of comparison with our proposed approach, existing contrastive-learning-based methods require or are better developed when learning a separate model for each AI generator. \n\nContributions.",
                    "score": 0.6007203267280924,
                    "section_title": "Introduction",
                    "char_start_offset": 2188,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 254
                        },
                        {
                            "start": 255,
                            "end": 468
                        },
                        {
                            "start": 469,
                            "end": 566
                        },
                        {
                            "start": 567,
                            "end": 881
                        },
                        {
                            "start": 882,
                            "end": 1042
                        },
                        {
                            "start": 1045,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1544
                        },
                        {
                            "start": 1547,
                            "end": 1688
                        },
                        {
                            "start": 1689,
                            "end": 1849
                        },
                        {
                            "start": 1850,
                            "end": 2003
                        },
                        {
                            "start": 2004,
                            "end": 2210
                        },
                        {
                            "start": 2213,
                            "end": 2227
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 325,
                            "end": 329,
                            "matchedPaperCorpusId": "256194179"
                        },
                        {
                            "start": 329,
                            "end": 332,
                            "matchedPaperCorpusId": "259129912"
                        },
                        {
                            "start": 602,
                            "end": 606,
                            "matchedPaperCorpusId": "256274849"
                        },
                        {
                            "start": 716,
                            "end": 720,
                            "matchedPaperCorpusId": "182952848"
                        },
                        {
                            "start": 720,
                            "end": 723,
                            "matchedPaperCorpusId": "259108779"
                        },
                        {
                            "start": 989,
                            "end": 992,
                            "matchedPaperCorpusId": "260379022"
                        },
                        {
                            "start": 1376,
                            "end": 1380,
                            "matchedPaperCorpusId": "236155217"
                        },
                        {
                            "start": 1380,
                            "end": 1383,
                            "matchedPaperCorpusId": "249840771"
                        },
                        {
                            "start": 1406,
                            "end": 1410,
                            "matchedPaperCorpusId": "252818980"
                        },
                        {
                            "start": 1505,
                            "end": 1508,
                            "matchedPaperCorpusId": "261660497"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96337890625
                }
            ],
            "relevance_judgement": 0.96337890625,
            "relevance_judgment_input_expanded": "# Title: Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text\n# Venue: European Conference on Artificial Intelligence\n# Authors: Lucio La Cava, Davide Costa, Andrea Tagarelli\n## Abstract\nThe significant progress in the development of Large Language Models has contributed to blurring the distinction between human and AI-generated text. The increasing pervasiveness of AI-generated text and the difficulty in detecting it poses new challenges for our society. In this paper, we tackle the problem of detecting and attributing AI-generated text by proposing WhosAI, a triplet-network contrastive learning framework designed to predict whether a given input text has been generated by humans or AI and to unveil the authorship of the text. Unlike most existing approaches, our proposed framework is conceived to learn semantic similarity representations from multiple generators at once, thus equally handling both detection and attribution tasks. Furthermore, WhosAI is model-agnostic and scalable to the release of new AI text-generation models by incorporating their generated instances into the embedding space learned by our framework. Experimental results on the TuringBench benchmark of 200K news articles show that our proposed framework achieves outstanding results in both the Turing Test and Authorship Attribution tasks, outperforming all the methods listed in the TuringBench benchmark leaderboards.\n## Introduction\nThe remarkable boost in human-like text generation performances achieved by Large Language Models (LLMs) in recent years has determined a rising challenge in detecting whether and to what extent texts have been generated by humans or machines [21,49,39]. In this context, the \"watermarking\" paradigm rapidly gained attention [25,53,28,50], as it allows embedding specific signals into generated texts that remain invisible to humans but are algorithmically detectable. Statistical learning methods also offer advanced solutions for detecting the authorship of texts. These include probabilistic models [31,1,47,14], log rank information [38], perplexity [44], discourse motifs [24], and other statistical approaches [16,40,45] More recently, we have witnessed the emergence of deep learning to detect or attribute AI-generated content, which stands as a promising body of research. Researchers have been exploiting LLMs to detect generated text [18,46], using ChatGPT itself as a detector [2], or combining LLMs with topological aspects [43]. \n\nA very recent trend involves leveraging contrastive learning to handle textual information. Indeed, despite its origins in the computer vision domain, contrastive representation learning has been proven particularly effective in NLP contexts to improve research on semantic similarity related problems, such as text classification [33,10], spotting hate-speech [23], unveiling intents [55], and eventually detecting AI-generated text through domain adaptation [3] or domain adversarial training [4]. \n\nDespite the advancements in research on detection of AI text generation, each of the above mentioned approaches faces significant challenges. Watermarking approaches are conditioned by the con-crete possibility of watermarking a given text, leaving the detection of non-watermarked texts an open issue. Statistical learning methods typically require access to the models' internals or to information that might be unavailable, limiting their applicability. Yet, more importantly for the sake of comparison with our proposed approach, existing contrastive-learning-based methods require or are better developed when learning a separate model for each AI generator. \n\nContributions.",
            "reference_string": "[271161969 | Cava et al. | 2024 | Citations: 3]"
        },
        {
            "title": "DeepTextMark: Deep Learning based Text Watermarking for Detection of Large Language Model Generated Text",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 57,
            "citation_count": 32,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.05773",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2305.05773?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2305.05773, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2106504469",
                    "name": "Travis J. E. Munyer"
                },
                {
                    "authorId": "2113310369",
                    "name": "Xin Zhong"
                }
            ],
            "abstract": "The capabilities of text generators have grown with the rapid development of Large Language Models (LLM). To prevent potential misuse, the ability to detect whether texts are produced by LLM has become increasingly important. Several related works have attempted to solve this problem using binary classifiers that categorize input text as human-written or LLM-generated. However, these classifiers have been shown to be unreliable. As impactful decisions could be made based on the result of the classification, the text source detection needs to be high-quality. To this end, this paper presents DeepTextMark, a deep learning-based text watermarking method for text source detection. Applying Word2Vec and Sentence Encoding for watermark insertion and a transformer-based classifier for watermark detection, DeepTextMark achieves blindness, robustness, imperceptibility, and reliability simultaneously. As discussed further in the paper, these traits are indispensable for generic text source detection, and the application focus of this paper is on the text generated by LLM. DeepTextMark can be implemented as an \"add-on\" to existing text generation systems. That is, the method does not require access or modification to the text generation technique. Experiments have shown high imperceptibility, high detection accuracy, enhanced robustness, reliability, and fast running speed of DeepTextMark.",
            "corpus_id": 258588289,
            "sentences": [],
            "relevance_judgement": 0.96337890625,
            "relevance_judgment_input_expanded": "# Title: DeepTextMark: Deep Learning based Text Watermarking for Detection of Large Language Model Generated Text\n# Venue: arXiv.org\n# Authors: Travis J. E. Munyer, Xin Zhong\n## Abstract\nThe capabilities of text generators have grown with the rapid development of Large Language Models (LLM). To prevent potential misuse, the ability to detect whether texts are produced by LLM has become increasingly important. Several related works have attempted to solve this problem using binary classifiers that categorize input text as human-written or LLM-generated. However, these classifiers have been shown to be unreliable. As impactful decisions could be made based on the result of the classification, the text source detection needs to be high-quality. To this end, this paper presents DeepTextMark, a deep learning-based text watermarking method for text source detection. Applying Word2Vec and Sentence Encoding for watermark insertion and a transformer-based classifier for watermark detection, DeepTextMark achieves blindness, robustness, imperceptibility, and reliability simultaneously. As discussed further in the paper, these traits are indispensable for generic text source detection, and the application focus of this paper is on the text generated by LLM. DeepTextMark can be implemented as an \"add-on\" to existing text generation systems. That is, the method does not require access or modification to the text generation technique. Experiments have shown high imperceptibility, high detection accuracy, enhanced robustness, reliability, and fast running speed of DeepTextMark.\n",
            "reference_string": "[258588289 | Munyer et al. | 2023 | Citations: 32]"
        },
        {
            "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks for AI generated text detection",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 76,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.03940, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2338890136",
                    "name": "Pablo Miralles-Gonz'alez"
                },
                {
                    "authorId": "1411252016",
                    "name": "Javier Huertas-Tato"
                },
                {
                    "authorId": "2332692726",
                    "name": "Alejandro Mart'in"
                },
                {
                    "authorId": "2320307287",
                    "name": "David Camacho"
                }
            ],
            "abstract": "The rapid advancement in large language models (LLMs) has significantly enhanced their ability to generate coherent and contextually relevant text, raising concerns about the misuse of AI-generated content and making it critical to detect it. However, the task remains challenging, particularly in unseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution outputs offers a theoretically appealing approach for detection, as they encapsulate insights from the models' extensive pre-training on diverse corpora. Despite its promise, zero-shot methods that attempt to operationalize these outputs have met with limited success. We hypothesize that one of the problems is that they use the mean to aggregate next-token distribution metrics across tokens, when some tokens are naturally easier or harder to predict and should be weighted differently. Based on this idea, we propose the Perplexity Attention Weighted Network (PAWN), which uses the last hidden states of the LLM and positions to weight the sum of a series of features based on metrics from the next-token distribution across the sequence length. Although not zero-shot, our method allows us to cache the last hidden states and next-token distribution metrics on disk, greatly reducing the training resource requirements. PAWN shows competitive and even better performance in-distribution than the strongest baselines (fine-tuned LMs) with a fraction of their trainable parameters. Our model also generalizes better to unseen domains and source models, with smaller variability in the decision boundary across distribution shifts. It is also more robust to adversarial attacks, and if the backbone has multilingual capabilities, it presents decent generalization to languages not seen during supervised training, with LLaMA3-1B reaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine languages.",
            "corpus_id": 275342724,
            "sentences": [
                {
                    "corpus_id": "275342724",
                    "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks for AI generated text detection",
                    "text": "There is a large and growing body of work focused on detection methods for AI-generated text. We review some of the main methods that appear in the literature, breaking them into four categories. The first category includes watermarking methods, which add hidden patterns to AI-generated text to make it identifiable. Zero-shot detectors rely on large language models (LLMs) pre-trained for next-token prediction to identify AI-generated content without additional training, using the next-token distributions of the text to generate statistics that separate human-and machine-generated examples. Other approaches involve fine-tuning language models and training them specifically for the detection classification task. Finally, LLMs have also been prompted to detect whether a text is AI-generated, asking them to complete the given prompt with the answer.",
                    "score": 0.8109808600680237,
                    "section_title": "Detection Methods",
                    "char_start_offset": 4896,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 93
                        },
                        {
                            "start": 94,
                            "end": 195
                        },
                        {
                            "start": 196,
                            "end": 317
                        },
                        {
                            "start": 318,
                            "end": 596
                        },
                        {
                            "start": 597,
                            "end": 719
                        },
                        {
                            "start": 720,
                            "end": 857
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9619140625
                }
            ],
            "relevance_judgement": 0.9619140625,
            "relevance_judgment_input_expanded": "# Title: Not all tokens are created equal: Perplexity Attention Weighted Networks for AI generated text detection\n# Venue: arXiv.org\n# Authors: Pablo Miralles-Gonz'alez, Javier Huertas-Tato, Alejandro Mart'in, David Camacho\n## Abstract\nThe rapid advancement in large language models (LLMs) has significantly enhanced their ability to generate coherent and contextually relevant text, raising concerns about the misuse of AI-generated content and making it critical to detect it. However, the task remains challenging, particularly in unseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution outputs offers a theoretically appealing approach for detection, as they encapsulate insights from the models' extensive pre-training on diverse corpora. Despite its promise, zero-shot methods that attempt to operationalize these outputs have met with limited success. We hypothesize that one of the problems is that they use the mean to aggregate next-token distribution metrics across tokens, when some tokens are naturally easier or harder to predict and should be weighted differently. Based on this idea, we propose the Perplexity Attention Weighted Network (PAWN), which uses the last hidden states of the LLM and positions to weight the sum of a series of features based on metrics from the next-token distribution across the sequence length. Although not zero-shot, our method allows us to cache the last hidden states and next-token distribution metrics on disk, greatly reducing the training resource requirements. PAWN shows competitive and even better performance in-distribution than the strongest baselines (fine-tuned LMs) with a fraction of their trainable parameters. Our model also generalizes better to unseen domains and source models, with smaller variability in the decision boundary across distribution shifts. It is also more robust to adversarial attacks, and if the backbone has multilingual capabilities, it presents decent generalization to languages not seen during supervised training, with LLaMA3-1B reaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine languages.\n## Detection Methods\nThere is a large and growing body of work focused on detection methods for AI-generated text. We review some of the main methods that appear in the literature, breaking them into four categories. The first category includes watermarking methods, which add hidden patterns to AI-generated text to make it identifiable. Zero-shot detectors rely on large language models (LLMs) pre-trained for next-token prediction to identify AI-generated content without additional training, using the next-token distributions of the text to generate statistics that separate human-and machine-generated examples. Other approaches involve fine-tuning language models and training them specifically for the detection classification task. Finally, LLMs have also been prompted to detect whether a text is AI-generated, asking them to complete the given prompt with the answer.",
            "reference_string": "[275342724 | Miralles-Gonz'alez et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Improving Logits-based Detector without Logits from Black-box LLMs",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.05232, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305616199",
                    "name": "Cong Zeng"
                },
                {
                    "authorId": "2249891007",
                    "name": "Shengkun Tang"
                },
                {
                    "authorId": "2145170944",
                    "name": "Xianjun Yang"
                },
                {
                    "authorId": "2305646169",
                    "name": "Yuanzhou Chen"
                },
                {
                    "authorId": "2283302224",
                    "name": "Yiyou Sun"
                },
                {
                    "authorId": "2306829217",
                    "name": "zhiqiang xu"
                },
                {
                    "authorId": "2307300932",
                    "name": "Yao Li"
                },
                {
                    "authorId": "2249915042",
                    "name": "Haifeng Chen"
                },
                {
                    "authorId": "2249879747",
                    "name": "Wei Cheng"
                },
                {
                    "authorId": "2249602967",
                    "name": "Dongkuan Xu"
                }
            ],
            "abstract": "The advent of Large Language Models (LLMs) has revolutionized text generation, producing outputs that closely mimic human writing. This blurring of lines between machine- and human-written text presents new challenges in distinguishing one from the other a task further complicated by the frequent updates and closed nature of leading proprietary LLMs. Traditional logits-based detection methods leverage surrogate models for identifying LLM-generated content when the exact logits are unavailable from black-box LLMs. However, these methods grapple with the misalignment between the distributions of the surrogate and the often undisclosed target models, leading to performance degradation, particularly with the introduction of new, closed-source models. Furthermore, while current methodologies are generally effective when the source model is identified, they falter in scenarios where the model version remains unknown, or the test set comprises outputs from various source models. To address these limitations, we present Distribution-Aligned LLMs Detection (DALD), an innovative framework that redefines the state-of-the-art performance in black-box text detection even without logits from source LLMs. DALD is designed to align the surrogate model's distribution with that of unknown target LLMs, ensuring enhanced detection capability and resilience against rapid model iterations with minimal training investment. By leveraging corpus samples from publicly accessible outputs of advanced models such as ChatGPT, GPT-4 and Claude-3, DALD fine-tunes surrogate models to synchronize with unknown source model distributions effectively.",
            "corpus_id": 270370917,
            "sentences": [
                {
                    "corpus_id": "270370917",
                    "title": "Improving Logits-based Detector without Logits from Black-box LLMs",
                    "text": "Large language models (LLMs) such as ChatGPT [1], GPT-4 [2], Llama [3][4][5] and Claude-3 [6] have profoundly impacted both industrial and academic domains, reshaping productivity across various  sectors including news reporting, story writing, and academic research [7]. Nevertheless, their misuse also raises concerns, particularly regarding the dissemination of fake news [8], the proliferation of malicious product reviews [9], and instances of plagiarism [10]. Instances of AI-synthesized scientific abstracts deluding scientists [11,12] have raised doubts about the reliability of scientific discourse. Accurate and reliable machine-generated text detection methods are necessary in order to address these issues [13][14][15][16][17]. \n\nMethods for detecting text generated by Large Language Models are broadly categorized into watermarking [18][19][20][21], training-based classifiers [22][23][24][25][26], and zero-shot detectors. Watermarking methods discreetly embed identifiable markers within the text output, striving to retain the model's linguistic integrity. However, this tactic is implementable solely by the model provider. Trainingbased classifiers, while effective, are costly and often lack the agility to adapt to new domains or model updates. Our emphasis is on zero-shot detectors that exploit the intrinsic differences between text written by machines and humans, offering the advantage of being generally training-free. \n\nMost zero-shot detectors primarily depend on analyzing model output logits for detection. Notably, DetectGPT [27] operates on probability divergence based upon principles of perturbation theory, while DNA-GPT [28] harnesses reprompting-based probability divergence, and Fast-DetectGPT [29] builds on variations in conditional probability distributions. In scenarios requiring the scrutiny of black-box models, these strategies commonly leverage a surrogate model to approximate the behavior of the target model.",
                    "score": 0.709588683910685,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 271
                        },
                        {
                            "start": 272,
                            "end": 465
                        },
                        {
                            "start": 466,
                            "end": 608
                        },
                        {
                            "start": 609,
                            "end": 740
                        },
                        {
                            "start": 743,
                            "end": 938
                        },
                        {
                            "start": 939,
                            "end": 1074
                        },
                        {
                            "start": 1075,
                            "end": 1142
                        },
                        {
                            "start": 1143,
                            "end": 1266
                        },
                        {
                            "start": 1267,
                            "end": 1446
                        },
                        {
                            "start": 1449,
                            "end": 1538
                        },
                        {
                            "start": 1539,
                            "end": 1801
                        },
                        {
                            "start": 1802,
                            "end": 1960
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 267,
                            "end": 270,
                            "matchedPaperCorpusId": "255624599"
                        },
                        {
                            "start": 427,
                            "end": 430,
                            "matchedPaperCorpusId": "198147840"
                        },
                        {
                            "start": 460,
                            "end": 464,
                            "matchedPaperCorpusId": "247450984"
                        },
                        {
                            "start": 535,
                            "end": 539,
                            "matchedPaperCorpusId": "255187118"
                        },
                        {
                            "start": 539,
                            "end": 542,
                            "matchedPaperCorpusId": "255773668"
                        },
                        {
                            "start": 847,
                            "end": 851,
                            "matchedPaperCorpusId": "221516138"
                        },
                        {
                            "start": 859,
                            "end": 863,
                            "matchedPaperCorpusId": "259092330"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9619140625
                }
            ],
            "relevance_judgement": 0.9619140625,
            "relevance_judgment_input_expanded": "# Title: Improving Logits-based Detector without Logits from Black-box LLMs\n# Venue: Neural Information Processing Systems\n# Authors: Cong Zeng, Shengkun Tang, Xianjun Yang, Yuanzhou Chen, Yiyou Sun, zhiqiang xu, Yao Li, Haifeng Chen, Wei Cheng, Dongkuan Xu\n## Abstract\nThe advent of Large Language Models (LLMs) has revolutionized text generation, producing outputs that closely mimic human writing. This blurring of lines between machine- and human-written text presents new challenges in distinguishing one from the other a task further complicated by the frequent updates and closed nature of leading proprietary LLMs. Traditional logits-based detection methods leverage surrogate models for identifying LLM-generated content when the exact logits are unavailable from black-box LLMs. However, these methods grapple with the misalignment between the distributions of the surrogate and the often undisclosed target models, leading to performance degradation, particularly with the introduction of new, closed-source models. Furthermore, while current methodologies are generally effective when the source model is identified, they falter in scenarios where the model version remains unknown, or the test set comprises outputs from various source models. To address these limitations, we present Distribution-Aligned LLMs Detection (DALD), an innovative framework that redefines the state-of-the-art performance in black-box text detection even without logits from source LLMs. DALD is designed to align the surrogate model's distribution with that of unknown target LLMs, ensuring enhanced detection capability and resilience against rapid model iterations with minimal training investment. By leveraging corpus samples from publicly accessible outputs of advanced models such as ChatGPT, GPT-4 and Claude-3, DALD fine-tunes surrogate models to synchronize with unknown source model distributions effectively.\n## Introduction\nLarge language models (LLMs) such as ChatGPT [1], GPT-4 [2], Llama [3][4][5] and Claude-3 [6] have profoundly impacted both industrial and academic domains, reshaping productivity across various  sectors including news reporting, story writing, and academic research [7]. Nevertheless, their misuse also raises concerns, particularly regarding the dissemination of fake news [8], the proliferation of malicious product reviews [9], and instances of plagiarism [10]. Instances of AI-synthesized scientific abstracts deluding scientists [11,12] have raised doubts about the reliability of scientific discourse. Accurate and reliable machine-generated text detection methods are necessary in order to address these issues [13][14][15][16][17]. \n\nMethods for detecting text generated by Large Language Models are broadly categorized into watermarking [18][19][20][21], training-based classifiers [22][23][24][25][26], and zero-shot detectors. Watermarking methods discreetly embed identifiable markers within the text output, striving to retain the model's linguistic integrity. However, this tactic is implementable solely by the model provider. Trainingbased classifiers, while effective, are costly and often lack the agility to adapt to new domains or model updates. Our emphasis is on zero-shot detectors that exploit the intrinsic differences between text written by machines and humans, offering the advantage of being generally training-free. \n\nMost zero-shot detectors primarily depend on analyzing model output logits for detection. Notably, DetectGPT [27] operates on probability divergence based upon principles of perturbation theory, while DNA-GPT [28] harnesses reprompting-based probability divergence, and Fast-DetectGPT [29] builds on variations in conditional probability distributions. In scenarios requiring the scrutiny of black-box models, these strategies commonly leverage a surrogate model to approximate the behavior of the target model.",
            "reference_string": "[270370917 | Zeng et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Large Language Models can be Guided to Evade AI-Generated Text Detection",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2023,
            "reference_count": 51,
            "citation_count": 53,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.10847",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.10847, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2147128045",
                    "name": "Ning Lu"
                },
                {
                    "authorId": "2152939552",
                    "name": "Shengcai Liu"
                },
                {
                    "authorId": "22272507",
                    "name": "Ruidan He"
                },
                {
                    "authorId": "2078692567",
                    "name": "Ke Tang"
                }
            ],
            "abstract": "Large language models (LLMs) have shown remarkable performance in various tasks and have been extensively utilized by the public. However, the increasing concerns regarding the misuse of LLMs, such as plagiarism and spamming, have led to the development of multiple detectors, including fine-tuned classifiers and statistical methods. In this study, we equip LLMs with prompts, rather than relying on an external paraphraser, to evaluate the vulnerability of these detectors. We propose a novel Substitution-based In-Context example Optimization method (SICO) to automatically construct prompts for evading the detectors. SICO is cost-efficient as it requires only 40 human-written examples and a limited number of LLM inferences to generate a prompt. Moreover, once a task-specific prompt has been constructed, it can be universally used against a wide range of detectors. Extensive experiments across three real-world tasks demonstrate that SICO significantly outperforms the paraphraser baselines and enables GPT-3.5 to successfully evade six detectors, decreasing their AUC by 0.5 on average. Furthermore, a comprehensive human evaluation show that the SICO-generated text achieves human-level readability and task completion rates, while preserving high imperceptibility. Finally, we propose an ensemble approach to enhance the robustness of detectors against SICO attack. The code is publicly available at https://github.com/ColinLu50/Evade-GPT-Detector.",
            "corpus_id": 258762215,
            "sentences": [
                {
                    "corpus_id": "258762215",
                    "title": "Large Language Models can be Guided to Evade AI-Generated Text Detection",
                    "text": "In recent years, the research community has developed a wide range of detectors for AI-generated contents. In general, these detectors can be classified into three categories: training-based, statistical, and watermarking methods. Training-based methods treat the detection problem as a binary classification task, where neural networks are trained using AI-generated text and human-written text. Early studies utilized classifiers to identify fake reviews (Hovy, 2016) and fake news (Zellers et al., 2019). More recently, researchers have trained classifiers using text generated by LLMs, such as the GPT-3.5 detector (Guo et al., 2023) and GPT-2 detector (Solaiman et al., 2019). Statistical methods, on the other hand, focus on zero-shot detection without any additional training overhead. These methods seek to distinguish between human-written text and AI-generated text based on the statistical characteristics of text, such as the statistical irregularities in measures like entropy (Lavergne et al., 2008), perplexity (Beresneva, 2016) and token rank (Gehrmann et al., 2019). A recent method, DetectGPT (Mitchell et al., 2023), exploits the phenomenon that AI-generated text tends to lie in the negative curvature regions of log probability of text. The watermarking methods involve modifying the LLM's text generation process to imprint specific patterns on the generated text, such that it can be detected (Abdelnabi & Fritz, 2021;Grinbaum & Adomaitis, 2022;Kirchenbauer et al., 2023). Although the proposed method SICO primarily focuses on the first two types of detection methods, it can also help evade watermarking when acted as an external paraphraser, as shown in Appendix F. \n\nRecent studies have found that paraphrasing can evade these detectors, which trains an additional neural network to rewrite the original AI-generated text (Krishna et al., 2023;Sadasivan et al., 2023). In contrast, SICO eliminates the need for extra models or training steps.",
                    "score": 0.6986523235479467,
                    "section_title": "Related works 2.1 AI-generated text detection",
                    "char_start_offset": 2098,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 106
                        },
                        {
                            "start": 107,
                            "end": 230
                        },
                        {
                            "start": 231,
                            "end": 396
                        },
                        {
                            "start": 397,
                            "end": 507
                        },
                        {
                            "start": 508,
                            "end": 681
                        },
                        {
                            "start": 682,
                            "end": 792
                        },
                        {
                            "start": 793,
                            "end": 1083
                        },
                        {
                            "start": 1084,
                            "end": 1257
                        },
                        {
                            "start": 1258,
                            "end": 1495
                        },
                        {
                            "start": 1496,
                            "end": 1691
                        },
                        {
                            "start": 1694,
                            "end": 1895
                        },
                        {
                            "start": 1896,
                            "end": 1969
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 484,
                            "end": 506,
                            "matchedPaperCorpusId": "168169824"
                        },
                        {
                            "start": 990,
                            "end": 1013,
                            "matchedPaperCorpusId": "12098535"
                        },
                        {
                            "start": 1026,
                            "end": 1043,
                            "matchedPaperCorpusId": "1175726"
                        },
                        {
                            "start": 1059,
                            "end": 1082,
                            "matchedPaperCorpusId": "182952848"
                        },
                        {
                            "start": 1416,
                            "end": 1441,
                            "matchedPaperCorpusId": "221516138"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9609375
                }
            ],
            "relevance_judgement": 0.9609375,
            "relevance_judgment_input_expanded": "# Title: Large Language Models can be Guided to Evade AI-Generated Text Detection\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Ning Lu, Shengcai Liu, Ruidan He, Ke Tang\n## Abstract\nLarge language models (LLMs) have shown remarkable performance in various tasks and have been extensively utilized by the public. However, the increasing concerns regarding the misuse of LLMs, such as plagiarism and spamming, have led to the development of multiple detectors, including fine-tuned classifiers and statistical methods. In this study, we equip LLMs with prompts, rather than relying on an external paraphraser, to evaluate the vulnerability of these detectors. We propose a novel Substitution-based In-Context example Optimization method (SICO) to automatically construct prompts for evading the detectors. SICO is cost-efficient as it requires only 40 human-written examples and a limited number of LLM inferences to generate a prompt. Moreover, once a task-specific prompt has been constructed, it can be universally used against a wide range of detectors. Extensive experiments across three real-world tasks demonstrate that SICO significantly outperforms the paraphraser baselines and enables GPT-3.5 to successfully evade six detectors, decreasing their AUC by 0.5 on average. Furthermore, a comprehensive human evaluation show that the SICO-generated text achieves human-level readability and task completion rates, while preserving high imperceptibility. Finally, we propose an ensemble approach to enhance the robustness of detectors against SICO attack. The code is publicly available at https://github.com/ColinLu50/Evade-GPT-Detector.\n## Related works 2.1 AI-generated text detection\nIn recent years, the research community has developed a wide range of detectors for AI-generated contents. In general, these detectors can be classified into three categories: training-based, statistical, and watermarking methods. Training-based methods treat the detection problem as a binary classification task, where neural networks are trained using AI-generated text and human-written text. Early studies utilized classifiers to identify fake reviews (Hovy, 2016) and fake news (Zellers et al., 2019). More recently, researchers have trained classifiers using text generated by LLMs, such as the GPT-3.5 detector (Guo et al., 2023) and GPT-2 detector (Solaiman et al., 2019). Statistical methods, on the other hand, focus on zero-shot detection without any additional training overhead. These methods seek to distinguish between human-written text and AI-generated text based on the statistical characteristics of text, such as the statistical irregularities in measures like entropy (Lavergne et al., 2008), perplexity (Beresneva, 2016) and token rank (Gehrmann et al., 2019). A recent method, DetectGPT (Mitchell et al., 2023), exploits the phenomenon that AI-generated text tends to lie in the negative curvature regions of log probability of text. The watermarking methods involve modifying the LLM's text generation process to imprint specific patterns on the generated text, such that it can be detected (Abdelnabi & Fritz, 2021;Grinbaum & Adomaitis, 2022;Kirchenbauer et al., 2023). Although the proposed method SICO primarily focuses on the first two types of detection methods, it can also help evade watermarking when acted as an external paraphraser, as shown in Appendix F. \n\nRecent studies have found that paraphrasing can evade these detectors, which trains an additional neural network to rewrite the original AI-generated text (Krishna et al., 2023;Sadasivan et al., 2023). In contrast, SICO eliminates the need for extra models or training steps.",
            "reference_string": "[258762215 | Lu et al. | 2023 | Citations: 53]"
        },
        {
            "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
            "venue": "International Conference on Machine Learning",
            "year": 2023,
            "reference_count": 46,
            "citation_count": 627,
            "influential_citation_count": 138,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2301.11305",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.11305, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "49688913",
                    "name": "E. Mitchell"
                },
                {
                    "authorId": "2110392124",
                    "name": "Yoonho Lee"
                },
                {
                    "authorId": "121873407",
                    "name": "Alexander Khazatsky"
                },
                {
                    "authorId": "144783904",
                    "name": "Christopher D. Manning"
                },
                {
                    "authorId": "46881670",
                    "name": "Chelsea Finn"
                }
            ],
            "abstract": "The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.",
            "corpus_id": 256274849,
            "sentences": [
                {
                    "corpus_id": "256274849",
                    "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
                    "text": "The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.",
                    "score": 0.6903242279002464,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9609375
                },
                {
                    "corpus_id": "256274849",
                    "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
                    "text": "As large language models continue to improve, they will become increasingly attractive tools for replacing human writers in a variety of contexts, such as education, journalism, and art. While legitimate uses of language model technologies exist in all of these settings, teachers, readers, and consumers are likely to demand tools for verifying the human origin of certain content with high educational, societal, or artistic significance, particularly when factuality (and not just fluency) is crucial. \n\nIn light of these elevated stakes and the regular emergence of new large language models, we study the zero-shot machinegenerated text detection problem, in which we use only the raw log probabilities computed by a generative model to determine if a candidate passage was sampled from it. We identify a property of the log probability function computed by a wide variety of large language models, showing that a tractable approximation to the trace of the Hessian of the model's log probability function provides a useful signal for detecting model samples. Our experiments find that this signal is more discriminative than existing zero-shot detection methods and is competitive with bespoke detection models trained with millions of model samples. \n\nDetectGPT and Watermarking. One interpretation of the perturbation function is producing semantically similar rephrasings of the original passage. If these rephrasings are systematically lower-probability than the original passage, the model is exposing its bias toward the specific (and roughly arbitrary, by human standards) phrasing used. In other words, LLMs that do not perfectly imitate human writing essentially watermark themselves implicitly. Under this interpretation, efforts to manually add watermarking biases to model outputs (Aaronson, 2022;Kirchenbauer et al., 2023) may further improve the effectiveness of methods such as DetectGPT, even as LLMs continue to improve. \n\nLimitations. One limitation of probability-based methods for zero-shot machine-generated text detection (like Detect-GPT) is the white-box assumption that we can evaluate log probabilities of the model(s) in question. For models behind APIs that do provide probabilities (such as GPT-3), evaluating probabilities nonetheless costs money. Another assumption of DetectGPT is access to a reasonable perturbation function.",
                    "score": 0.749295300916941,
                    "section_title": "Discussion",
                    "char_start_offset": 29476,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 186
                        },
                        {
                            "start": 187,
                            "end": 504
                        },
                        {
                            "start": 507,
                            "end": 795
                        },
                        {
                            "start": 796,
                            "end": 1064
                        },
                        {
                            "start": 1065,
                            "end": 1256
                        },
                        {
                            "start": 1259,
                            "end": 1286
                        },
                        {
                            "start": 1287,
                            "end": 1405
                        },
                        {
                            "start": 1406,
                            "end": 1600
                        },
                        {
                            "start": 1601,
                            "end": 1710
                        },
                        {
                            "start": 1711,
                            "end": 1943
                        },
                        {
                            "start": 1946,
                            "end": 1958
                        },
                        {
                            "start": 1959,
                            "end": 2163
                        },
                        {
                            "start": 2164,
                            "end": 2283
                        },
                        {
                            "start": 2284,
                            "end": 2364
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9580078125
                }
            ],
            "relevance_judgement": 0.9609375,
            "relevance_judgment_input_expanded": "# Title: DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature\n# Venue: International Conference on Machine Learning\n# Authors: E. Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, Chelsea Finn\n## Abstract\nThe increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.\n## Discussion\nAs large language models continue to improve, they will become increasingly attractive tools for replacing human writers in a variety of contexts, such as education, journalism, and art. While legitimate uses of language model technologies exist in all of these settings, teachers, readers, and consumers are likely to demand tools for verifying the human origin of certain content with high educational, societal, or artistic significance, particularly when factuality (and not just fluency) is crucial. \n\nIn light of these elevated stakes and the regular emergence of new large language models, we study the zero-shot machinegenerated text detection problem, in which we use only the raw log probabilities computed by a generative model to determine if a candidate passage was sampled from it. We identify a property of the log probability function computed by a wide variety of large language models, showing that a tractable approximation to the trace of the Hessian of the model's log probability function provides a useful signal for detecting model samples. Our experiments find that this signal is more discriminative than existing zero-shot detection methods and is competitive with bespoke detection models trained with millions of model samples. \n\nDetectGPT and Watermarking. One interpretation of the perturbation function is producing semantically similar rephrasings of the original passage. If these rephrasings are systematically lower-probability than the original passage, the model is exposing its bias toward the specific (and roughly arbitrary, by human standards) phrasing used. In other words, LLMs that do not perfectly imitate human writing essentially watermark themselves implicitly. Under this interpretation, efforts to manually add watermarking biases to model outputs (Aaronson, 2022;Kirchenbauer et al., 2023) may further improve the effectiveness of methods such as DetectGPT, even as LLMs continue to improve. \n\nLimitations. One limitation of probability-based methods for zero-shot machine-generated text detection (like Detect-GPT) is the white-box assumption that we can evaluate log probabilities of the model(s) in question. For models behind APIs that do provide probabilities (such as GPT-3), evaluating probabilities nonetheless costs money. Another assumption of DetectGPT is access to a reasonable perturbation function.",
            "reference_string": "[256274849 | Mitchell et al. | 2023 | Citations: 627]"
        },
        {
            "title": "A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 22,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.01245, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294777336",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "2294359739",
                    "name": "Feng Ruan"
                },
                {
                    "authorId": "2294380671",
                    "name": "Huiyuan Wang"
                },
                {
                    "authorId": "2055685346",
                    "name": "Qi Long"
                },
                {
                    "authorId": "2288099064",
                    "name": "Weijie J. Su"
                }
            ],
            "abstract": "Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of incorrectly classifying LLM-generated text as human-written). Our framework further reduces the problem of determining the optimal detection rule to solving a minimax optimization program. We apply this framework to two representative watermarks -- one of which has been internally implemented at OpenAI -- and obtain several findings that can be instrumental in guiding the practice of implementing watermarks. In particular, we derive optimal detection rules for these watermarks under our framework. These theoretically derived detection rules are demonstrated to be competitive and sometimes enjoy a higher power than existing detection approaches through numerical experiments.",
            "corpus_id": 268819799,
            "sentences": [
                {
                    "corpus_id": "268819799",
                    "title": "A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules",
                    "text": "Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of incorrectly classifying LLM-generated text as human-written). Our framework further reduces the problem of determining the optimal detection rule to solving a minimax optimization program. We apply this framework to two representative watermarks -- one of which has been internally implemented at OpenAI -- and obtain several findings that can be instrumental in guiding the practice of implementing watermarks. In particular, we derive optimal detection rules for these watermarks under our framework. These theoretically derived detection rules are demonstrated to be competitive and sometimes enjoy a higher power than existing detection approaches through numerical experiments.",
                    "score": 0.5735401811517837,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96044921875
                }
            ],
            "relevance_judgement": 0.96044921875,
            "relevance_judgment_input_expanded": "# Title: A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules\n# Venue: arXiv.org\n# Authors: Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su\n## Abstract\nSince ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of incorrectly classifying LLM-generated text as human-written). Our framework further reduces the problem of determining the optimal detection rule to solving a minimax optimization program. We apply this framework to two representative watermarks -- one of which has been internally implemented at OpenAI -- and obtain several findings that can be instrumental in guiding the practice of implementing watermarks. In particular, we derive optimal detection rules for these watermarks under our framework. These theoretically derived detection rules are demonstrated to be competitive and sometimes enjoy a higher power than existing detection approaches through numerical experiments.\n",
            "reference_string": "[268819799 | Li et al. | 2024 | Citations: 22]"
        },
        {
            "title": "The Impact of Prompts on Zero-Shot Detection of AI-Generated Text",
            "venue": "AISafety@IJCAI",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.20127, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2204866986",
                    "name": "Kaito Taguchi"
                },
                {
                    "authorId": "2294337288",
                    "name": "Yujie Gu"
                },
                {
                    "authorId": "2294175154",
                    "name": "Kouichi Sakurai"
                }
            ],
            "abstract": "In recent years, there have been significant advancements in the development of Large Language Models (LLMs). While their practical applications are now widespread, their potential for misuse, such as generating fake news and committing plagiarism, has posed significant concerns. To address this issue, detectors have been developed to evaluate whether a given text is human-generated or AI-generated. Among others, zero-shot detectors stand out as effective approaches that do not require additional training data and are often likelihood-based. In chat-based applications, users commonly input prompts and utilize the AI-generated texts. However, zero-shot detectors typically analyze these texts in isolation, neglecting the impact of the original prompts. It is conceivable that this approach may lead to a discrepancy in likelihood assessments between the text generation phase and the detection phase. So far, there remains an unverified gap concerning how the presence or absence of prompts impacts detection accuracy for zero-shot detectors. In this paper, we introduce an evaluative framework to empirically analyze the impact of prompts on the detection accuracy of AI-generated text. We assess various zero-shot detectors using both white-box detection, which leverages the prompt, and black-box detection, which operates without prompt information. Our experiments reveal the significant influence of prompts on detection accuracy. Remarkably, compared with black-box detection without prompts, the white-box methods using prompts demonstrate an increase in AUC of at least $0.1$ across all zero-shot detectors tested. Code is available: \\url{https://github.com/kaito25atugich/Detector}.",
            "corpus_id": 268793677,
            "sentences": [
                {
                    "corpus_id": "268793677",
                    "title": "The Impact of Prompts on Zero-Shot Detection of AI-Generated Text",
                    "text": "Recent years have seen significant advancements in the development of Large Language Models (LLMs) [1,2,3], and their practical applications have become widespread.Meanwhile, their potential misuse have raised significant concerns.For example, the generation of fake news and plagiarism using LLMs is a notable issue.Detectors that evaluate whether a given text is human-generated or AI-generated serve as a defense mechanism against such misuse.\n\nDetectors for AI-generated text can be broadly classified into three categories: a zero-shot detec-tor leveraging statistical properties [4,5,6,7,8,9,10,11], a detector employing supervised learning [12,13,14,15], and a detector utilizing watermarking [16,17].\n\nZero-shot detectors, such as DetectGPT [5], which do not require additional training, are designed in many methods using likelihood-based scores.A summary of zero-shot detectors is illustrated in Table 1.In other words, the zero-shot detection is carried out by replicating the likelihood at the generation phase.When using LLMs, we usually input prompts and utilize the generated output.However, at the detection phase, it is anticipated that reproducing likelihood Log-likelihood Detect using the log likelihood of the given text.",
                    "score": 0.791175854730479,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 164
                        },
                        {
                            "start": 164,
                            "end": 231
                        },
                        {
                            "start": 231,
                            "end": 317
                        },
                        {
                            "start": 317,
                            "end": 446
                        },
                        {
                            "start": 448,
                            "end": 708
                        },
                        {
                            "start": 710,
                            "end": 855
                        },
                        {
                            "start": 855,
                            "end": 914
                        },
                        {
                            "start": 914,
                            "end": 1023
                        },
                        {
                            "start": 1023,
                            "end": 1098
                        },
                        {
                            "start": 1098,
                            "end": 1242
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 585,
                            "end": 588,
                            "matchedPaperCorpusId": "182952848"
                        },
                        {
                            "start": 588,
                            "end": 590,
                            "matchedPaperCorpusId": "256274849"
                        },
                        {
                            "start": 700,
                            "end": 704,
                            "matchedPaperCorpusId": "256194179"
                        },
                        {
                            "start": 749,
                            "end": 752,
                            "matchedPaperCorpusId": "256274849"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.958984375
                },
                {
                    "corpus_id": "268793677",
                    "title": "The Impact of Prompts on Zero-Shot Detection of AI-Generated Text",
                    "text": "In recent years, there have been significant advancements in the development of Large Language Models (LLMs). While their practical applications are now widespread, their potential for misuse, such as generating fake news and committing plagiarism, has posed significant concerns. To address this issue, detectors have been developed to evaluate whether a given text is human-generated or AI-generated. Among others, zero-shot detectors stand out as effective approaches that do not require additional training data and are often likelihood-based. In chat-based applications, users commonly input prompts and utilize the AI-generated texts. However, zero-shot detectors typically analyze these texts in isolation, neglecting the impact of the original prompts. It is conceivable that this approach may lead to a discrepancy in likelihood assessments between the text generation phase and the detection phase. So far, there remains an unverified gap concerning how the presence or absence of prompts impacts detection accuracy for zero-shot detectors. In this paper, we introduce an evaluative framework to empirically analyze the impact of prompts on the detection accuracy of AI-generated text. We assess various zero-shot detectors using both white-box detection, which leverages the prompt, and black-box detection, which operates without prompt information. Our experiments reveal the significant influence of prompts on detection accuracy. Remarkably, compared with black-box detection without prompts, the white-box methods using prompts demonstrate an increase in AUC of at least $0.1$ across all zero-shot detectors tested. Code is available: \\url{https://github.com/kaito25atugich/Detector}.",
                    "score": 0.5746105308051994,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94775390625
                }
            ],
            "relevance_judgement": 0.958984375,
            "relevance_judgment_input_expanded": "# Title: The Impact of Prompts on Zero-Shot Detection of AI-Generated Text\n# Venue: AISafety@IJCAI\n# Authors: Kaito Taguchi, Yujie Gu, Kouichi Sakurai\n## Abstract\nIn recent years, there have been significant advancements in the development of Large Language Models (LLMs). While their practical applications are now widespread, their potential for misuse, such as generating fake news and committing plagiarism, has posed significant concerns. To address this issue, detectors have been developed to evaluate whether a given text is human-generated or AI-generated. Among others, zero-shot detectors stand out as effective approaches that do not require additional training data and are often likelihood-based. In chat-based applications, users commonly input prompts and utilize the AI-generated texts. However, zero-shot detectors typically analyze these texts in isolation, neglecting the impact of the original prompts. It is conceivable that this approach may lead to a discrepancy in likelihood assessments between the text generation phase and the detection phase. So far, there remains an unverified gap concerning how the presence or absence of prompts impacts detection accuracy for zero-shot detectors. In this paper, we introduce an evaluative framework to empirically analyze the impact of prompts on the detection accuracy of AI-generated text. We assess various zero-shot detectors using both white-box detection, which leverages the prompt, and black-box detection, which operates without prompt information. Our experiments reveal the significant influence of prompts on detection accuracy. Remarkably, compared with black-box detection without prompts, the white-box methods using prompts demonstrate an increase in AUC of at least $0.1$ across all zero-shot detectors tested. Code is available: \\url{https://github.com/kaito25atugich/Detector}.\n## Introduction\nRecent years have seen significant advancements in the development of Large Language Models (LLMs) [1,2,3], and their practical applications have become widespread.Meanwhile, their potential misuse have raised significant concerns.For example, the generation of fake news and plagiarism using LLMs is a notable issue.Detectors that evaluate whether a given text is human-generated or AI-generated serve as a defense mechanism against such misuse.\n\nDetectors for AI-generated text can be broadly classified into three categories: a zero-shot detec-tor leveraging statistical properties [4,5,6,7,8,9,10,11], a detector employing supervised learning [12,13,14,15], and a detector utilizing watermarking [16,17].\n\nZero-shot detectors, such as DetectGPT [5], which do not require additional training, are designed in many methods using likelihood-based scores.A summary of zero-shot detectors is illustrated in Table 1.In other words, the zero-shot detection is carried out by replicating the likelihood at the generation phase.When using LLMs, we usually input prompts and utilize the generated output.However, at the detection phase, it is anticipated that reproducing likelihood Log-likelihood Detect using the log likelihood of the given text.",
            "reference_string": "[268793677 | Taguchi et al. | 2024 | Citations: 7]"
        },
        {
            "title": "DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for Identifying Large Language Model Generated Text",
            "venue": "IEEE Access",
            "year": 2023,
            "reference_count": 24,
            "citation_count": 15,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/10380310/10471537.pdf",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.05773, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2106504469",
                    "name": "Travis J. E. Munyer"
                },
                {
                    "authorId": "2215112081",
                    "name": "A. Tanvir"
                },
                {
                    "authorId": "2107200061",
                    "name": "A. Das"
                },
                {
                    "authorId": "2257984128",
                    "name": "Xin Zhong"
                }
            ],
            "abstract": "The rapid advancement of Large Language Models (LLMs) has significantly enhanced the capabilities of text generators. With the potential for misuse escalating, the importance of discerning whether texts are human-authored or generated by LLMs has become paramount. Several preceding studies have ventured to address this challenge by employing binary classifiers to differentiate between human-written and LLM-generated text. Nevertheless, the reliability of these classifiers has been subject to question. Given that consequential decisions may hinge on the outcome of such classification, it is imperative that text source detection is of high caliber. In light of this, the present paper introduces DeepTextMark, a deep learning-driven text watermarking methodology devised for text source identification. By leveraging Word2Vec and Sentence Encoding for watermark insertion, alongside a transformer-based classifier for watermark detection, DeepTextMark epitomizes a blend of blindness, robustness, imperceptibility, and reliability. As elaborated within the paper, these attributes are crucial for universal text source detection, with a particular emphasis in this paper on text produced by LLMs. DeepTextMark offers a viable \u201cadd-on\u201d solution to prevailing text generation frameworks, requiring no direct access or alterations to the underlying text generation mechanism. Experimental evaluations underscore the high imperceptibility, elevated detection accuracy, augmented robustness, reliability, and swift execution of DeepTextMark.",
            "corpus_id": 268357168,
            "sentences": [
                {
                    "corpus_id": "268357168",
                    "title": "DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for Identifying Large Language Model Generated Text",
                    "text": "Recent endeavors have been directed towards developing classifiers aimed at differentiating between LLM-generated text and human-written text.The prevailing approach entails the collection and labeling of LLM-generated and humanwritten texts, followed by the training of a binary classifier through supervised learning.Although the efficacy of these classifiers has yet to be fully established, some preliminary analyses have been reported [8,9].One study [9] elucidated three distinct methods, substantiated with examples, to circumvent the GPTZero [7] classifier detection.Another investigation [8] conducted a direct assessment of GPTZero's accuracy, uncovering inconsistencies in its ability to detect human-written text.Moreover, classifier-based LLM-generated text detectors commonly necessitate a substantial character count to perform detection accurately.For instance, GPTZero [7] required a minimum of 250 characters to initiate detection.Looking ahead, OpenAI is planning a cryptography-based watermarking system for ChatGPTgenerated text detection [15], although no definitive work has been disclosed as of yet.Zero-shot learning-based methods have also demonstrated some advancement.For example, Cer et al. [16] reported an increment in AUROC from 1% to 14% compared to other zero-shot detection strategies across various datasets; however, the accuracy might still fall short in real-world applications concerning text generated by models.\n\nA method has been proposed for detecting LLM-generated texts based on text watermarking [11], which involves watermarking the text by modifying the LLMs (sensitive tokens are defined and excluded from the output of the LLMs).In contrast, our proposed DeepTextMark does not necessitate access to or modifications of the LLM.Distinct from model-dependent methods, DeepTextMark exhibits a model-independent feature, enabling its application to any text.Moreover, DeepTextMark employs a substantially more compact architecture with about 50 million parameters, whereas the method in [11] necessitates billions of parameters to implement the watermarking process.",
                    "score": 0.6939171684531236,
                    "section_title": "Text Source Detection for Large Language Models",
                    "char_start_offset": 6349,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 142
                        },
                        {
                            "start": 142,
                            "end": 319
                        },
                        {
                            "start": 319,
                            "end": 446
                        },
                        {
                            "start": 446,
                            "end": 575
                        },
                        {
                            "start": 575,
                            "end": 725
                        },
                        {
                            "start": 725,
                            "end": 864
                        },
                        {
                            "start": 864,
                            "end": 949
                        },
                        {
                            "start": 949,
                            "end": 1123
                        },
                        {
                            "start": 1123,
                            "end": 1196
                        },
                        {
                            "start": 1196,
                            "end": 1453
                        },
                        {
                            "start": 1455,
                            "end": 1680
                        },
                        {
                            "start": 1680,
                            "end": 1778
                        },
                        {
                            "start": 1778,
                            "end": 1905
                        },
                        {
                            "start": 1905,
                            "end": 2113
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1543,
                            "end": 1547,
                            "matchedPaperCorpusId": "256194179"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.958984375
                }
            ],
            "relevance_judgement": 0.958984375,
            "relevance_judgment_input_expanded": "# Title: DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for Identifying Large Language Model Generated Text\n# Venue: IEEE Access\n# Authors: Travis J. E. Munyer, A. Tanvir, A. Das, Xin Zhong\n## Abstract\nThe rapid advancement of Large Language Models (LLMs) has significantly enhanced the capabilities of text generators. With the potential for misuse escalating, the importance of discerning whether texts are human-authored or generated by LLMs has become paramount. Several preceding studies have ventured to address this challenge by employing binary classifiers to differentiate between human-written and LLM-generated text. Nevertheless, the reliability of these classifiers has been subject to question. Given that consequential decisions may hinge on the outcome of such classification, it is imperative that text source detection is of high caliber. In light of this, the present paper introduces DeepTextMark, a deep learning-driven text watermarking methodology devised for text source identification. By leveraging Word2Vec and Sentence Encoding for watermark insertion, alongside a transformer-based classifier for watermark detection, DeepTextMark epitomizes a blend of blindness, robustness, imperceptibility, and reliability. As elaborated within the paper, these attributes are crucial for universal text source detection, with a particular emphasis in this paper on text produced by LLMs. DeepTextMark offers a viable \u201cadd-on\u201d solution to prevailing text generation frameworks, requiring no direct access or alterations to the underlying text generation mechanism. Experimental evaluations underscore the high imperceptibility, elevated detection accuracy, augmented robustness, reliability, and swift execution of DeepTextMark.\n## Text Source Detection for Large Language Models\nRecent endeavors have been directed towards developing classifiers aimed at differentiating between LLM-generated text and human-written text.The prevailing approach entails the collection and labeling of LLM-generated and humanwritten texts, followed by the training of a binary classifier through supervised learning.Although the efficacy of these classifiers has yet to be fully established, some preliminary analyses have been reported [8,9].One study [9] elucidated three distinct methods, substantiated with examples, to circumvent the GPTZero [7] classifier detection.Another investigation [8] conducted a direct assessment of GPTZero's accuracy, uncovering inconsistencies in its ability to detect human-written text.Moreover, classifier-based LLM-generated text detectors commonly necessitate a substantial character count to perform detection accurately.For instance, GPTZero [7] required a minimum of 250 characters to initiate detection.Looking ahead, OpenAI is planning a cryptography-based watermarking system for ChatGPTgenerated text detection [15], although no definitive work has been disclosed as of yet.Zero-shot learning-based methods have also demonstrated some advancement.For example, Cer et al. [16] reported an increment in AUROC from 1% to 14% compared to other zero-shot detection strategies across various datasets; however, the accuracy might still fall short in real-world applications concerning text generated by models.\n\nA method has been proposed for detecting LLM-generated texts based on text watermarking [11], which involves watermarking the text by modifying the LLMs (sensitive tokens are defined and excluded from the output of the LLMs).In contrast, our proposed DeepTextMark does not necessitate access to or modifications of the LLM.Distinct from model-dependent methods, DeepTextMark exhibits a model-independent feature, enabling its application to any text.Moreover, DeepTextMark employs a substantially more compact architecture with about 50 million parameters, whereas the method in [11] necessitates billions of parameters to implement the watermarking process.",
            "reference_string": "[268357168 | Munyer et al. | 2023 | Citations: 15]"
        },
        {
            "title": "DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 43,
            "citation_count": 141,
            "influential_citation_count": 29,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.05540",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.05540, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116966710",
                    "name": "Jinyan Su"
                },
                {
                    "authorId": "2080123731",
                    "name": "Terry Yue Zhuo"
                },
                {
                    "authorId": "2119265743",
                    "name": "Di Wang"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ],
            "abstract": "With the rapid progress of large language models (LLMs) and the huge amount of text they generated, it becomes more and more impractical to manually distinguish whether a text is machine-generated. Given the growing use of LLMs in social media and education, it prompts us to develop methods to detect machine-generated text, preventing malicious usage such as plagiarism, misinformation, and propaganda. Previous work has studied several zero-shot methods, which require no training data. These methods achieve good performance, but there is still a lot of room for improvement. In this paper, we introduce two novel zero-shot methods for detecting machine-generated text by leveraging the log rank information. One is called DetectLLM-LRR, which is fast and efficient, and the other is called DetectLLM-NPR, which is more accurate, but slower due to the need for perturbations. Our experiments on three datasets and seven language models show that our proposed methods improve over the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover, DetectLLM-NPR needs fewer perturbations than previous work to achieve the same level of performance, which makes it more practical for real-world use. We also investigate the efficiency--performance trade-off based on users preference on these two measures and we provide intuition for using them in practice effectively. We release the data and the code of both methods in https://github.com/mbzuai-nlp/DetectLLM",
            "corpus_id": 259129463,
            "sentences": [
                {
                    "corpus_id": "259129463",
                    "title": "DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text",
                    "text": "With the rapid progress of large language models (LLMs) and the huge amount of text they generated, it becomes more and more impractical to manually distinguish whether a text is machine-generated. Given the growing use of LLMs in social media and education, it prompts us to develop methods to detect machine-generated text, preventing malicious usage such as plagiarism, misinformation, and propaganda. Previous work has studied several zero-shot methods, which require no training data. These methods achieve good performance, but there is still a lot of room for improvement. In this paper, we introduce two novel zero-shot methods for detecting machine-generated text by leveraging the log rank information. One is called DetectLLM-LRR, which is fast and efficient, and the other is called DetectLLM-NPR, which is more accurate, but slower due to the need for perturbations. Our experiments on three datasets and seven language models show that our proposed methods improve over the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover, DetectLLM-NPR needs fewer perturbations than previous work to achieve the same level of performance, which makes it more practical for real-world use. We also investigate the efficiency--performance trade-off based on users preference on these two measures and we provide intuition for using them in practice effectively. We release the data and the code of both methods in https://github.com/mbzuai-nlp/DetectLLM",
                    "score": 0.6385516203623102,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95849609375
                }
            ],
            "relevance_judgement": 0.95849609375,
            "relevance_judgment_input_expanded": "# Title: DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Jinyan Su, Terry Yue Zhuo, Di Wang, Preslav Nakov\n## Abstract\nWith the rapid progress of large language models (LLMs) and the huge amount of text they generated, it becomes more and more impractical to manually distinguish whether a text is machine-generated. Given the growing use of LLMs in social media and education, it prompts us to develop methods to detect machine-generated text, preventing malicious usage such as plagiarism, misinformation, and propaganda. Previous work has studied several zero-shot methods, which require no training data. These methods achieve good performance, but there is still a lot of room for improvement. In this paper, we introduce two novel zero-shot methods for detecting machine-generated text by leveraging the log rank information. One is called DetectLLM-LRR, which is fast and efficient, and the other is called DetectLLM-NPR, which is more accurate, but slower due to the need for perturbations. Our experiments on three datasets and seven language models show that our proposed methods improve over the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover, DetectLLM-NPR needs fewer perturbations than previous work to achieve the same level of performance, which makes it more practical for real-world use. We also investigate the efficiency--performance trade-off based on users preference on these two measures and we provide intuition for using them in practice effectively. We release the data and the code of both methods in https://github.com/mbzuai-nlp/DetectLLM\n",
            "reference_string": "[259129463 | Su et al. | 2023 | Citations: 141]"
        },
        {
            "title": "Scalable watermarking for identifying large language model outputs",
            "venue": "The Naturalist",
            "year": 2024,
            "reference_count": 34,
            "citation_count": 74,
            "influential_citation_count": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1038/s41586-024-08025-4",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11499265, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3491117",
                    "name": "Sumanth Dathathri"
                },
                {
                    "authorId": "2268761249",
                    "name": "Abigail See"
                },
                {
                    "authorId": "145053127",
                    "name": "Sumedh Ghaisas"
                },
                {
                    "authorId": "2268826600",
                    "name": "Po-Sen Huang"
                },
                {
                    "authorId": "2327318512",
                    "name": "Rob McAdam"
                },
                {
                    "authorId": "1851564",
                    "name": "Johannes Welbl"
                },
                {
                    "authorId": "2327316530",
                    "name": "Vandana Bachani"
                },
                {
                    "authorId": "2275186627",
                    "name": "Alex Kaskasoli"
                },
                {
                    "authorId": "49860489",
                    "name": "Robert Stanforth"
                },
                {
                    "authorId": "2327319709",
                    "name": "Tatiana Matejovicova"
                },
                {
                    "authorId": "2270174566",
                    "name": "Jamie Hayes"
                },
                {
                    "authorId": "2275153245",
                    "name": "Nidhi Vyas"
                },
                {
                    "authorId": "2089890458",
                    "name": "Majd Al Merey"
                },
                {
                    "authorId": "1400348545",
                    "name": "Jonah Brown-Cohen"
                },
                {
                    "authorId": "3407947",
                    "name": "Rudy Bunel"
                },
                {
                    "authorId": "2324784942",
                    "name": "Borja Balle"
                },
                {
                    "authorId": "122347685",
                    "name": "taylan. cemgil"
                },
                {
                    "authorId": "2268758347",
                    "name": "Zahra Ahmed"
                },
                {
                    "authorId": "1396786528",
                    "name": "Kitty Stacpoole"
                },
                {
                    "authorId": "47473421",
                    "name": "Ilia Shumailov"
                },
                {
                    "authorId": "2324799480",
                    "name": "Cip Baetu"
                },
                {
                    "authorId": "2071666",
                    "name": "Sven Gowal"
                },
                {
                    "authorId": "48987704",
                    "name": "D. Hassabis"
                },
                {
                    "authorId": "143967473",
                    "name": "Pushmeet Kohli"
                }
            ],
            "abstract": "Large language models (LLMs) have enabled the generation of high-quality synthetic text, often indistinguishable from human-written content, at a scale that can markedly affect the nature of the information ecosystem1\u20133. Watermarking can help identify synthetic text and limit accidental or deliberate misuse4, but has not been adopted in production systems owing to stringent quality, detectability and computational efficiency requirements. Here we describe SynthID-Text, a production-ready text watermarking scheme that preserves text quality and enables high detection accuracy, with minimal latency overhead. SynthID-Text does not affect LLM training and modifies only the sampling procedure; watermark detection is computationally efficient, without using the underlying LLM. To enable watermarking at scale, we develop an algorithm integrating watermarking with speculative sampling, an efficiency technique frequently used in production systems5. Evaluations across multiple LLMs empirically show that SynthID-Text provides improved detectability over comparable methods, and standard benchmarks and human side-by-side ratings indicate no change in LLM capabilities. To demonstrate the feasibility of watermarking in large-scale-production systems, we conducted a live experiment that assessed feedback from nearly 20\u2009million Gemini6 responses, again confirming the preservation of text quality. We hope that the availability of SynthID-Text7 will facilitate further development of watermarking and responsible use of LLM systems.",
            "corpus_id": 273547059,
            "sentences": [
                {
                    "corpus_id": "273547059",
                    "title": "Scalable watermarking for identifying large language model outputs",
                    "text": "Large language models (LLMs) have enabled the generation of high-quality synthetic text, often indistinguishable from human-written content, at a scale that can markedly affect the nature of the information ecosystem [1][2][3] . Watermarking can help identify synthetic text and limit accidental or deliberate misuse 4 , but has not been adopted in production systems owing to stringent quality, detectability and computational efficiency requirements. Here we describe SynthID-Text, a production-ready text watermarking scheme that preserves text quality and enables high detection accuracy, with minimal latency overhead. SynthID-Text does not affect LLM training and modifies only the sampling procedure; watermark detection is computationally efficient, without using the underlying LLM. To enable watermarking at scale, we develop an algorithm integrating watermarking with speculative sampling, an efficiency technique frequently used in production systems 5 . Evaluations across multiple LLMs empirically show that SynthID-Text provides improved detectability over comparable methods, and standard benchmarks and human side-by-side ratings indicate no change in LLM capabilities. To demonstrate the feasibility of watermarking in large-scale-production systems, we conducted a live experiment that assessed feedback from nearly 20 million Gemini 6 responses, again confirming the preservation of text quality. We hope that the availability of SynthID-Text 7 will facilitate further development of watermarking and responsible use of LLM systems. \n\nLarge language models (LLMs) are widely adopted tools for synthetic text generation, finding applications in language-based assistants, code generation, writing support and various other domains. As LLMs advance in quality, coherence, coverage and expertise, it can become difficult to distinguish synthetically generated text from human-written text [1][2][3] . Given the widespread use of LLMs in education, software development and web content generation, identification and attribution of LLM text is critical to ensure safe and responsible use of the technology [8][9][10][11] . \n\nMultiple strategies have emerged to address this problem. One is a retrieval-based approach, which involves keeping a growing record of all generated texts and checking against it for matches 12 . This requires scale and coordination, and raises privacy concerns as it requires accessing and storing all LLM interactions.",
                    "score": 0.6566738081117697,
                    "section_title": "body",
                    "char_start_offset": 1,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 228
                        },
                        {
                            "start": 229,
                            "end": 452
                        },
                        {
                            "start": 453,
                            "end": 623
                        },
                        {
                            "start": 624,
                            "end": 791
                        },
                        {
                            "start": 792,
                            "end": 966
                        },
                        {
                            "start": 967,
                            "end": 1186
                        },
                        {
                            "start": 1187,
                            "end": 1416
                        },
                        {
                            "start": 1417,
                            "end": 1552
                        },
                        {
                            "start": 1555,
                            "end": 1750
                        },
                        {
                            "start": 1751,
                            "end": 1917
                        },
                        {
                            "start": 1918,
                            "end": 2138
                        },
                        {
                            "start": 2141,
                            "end": 2198
                        },
                        {
                            "start": 2199,
                            "end": 2337
                        },
                        {
                            "start": 2338,
                            "end": 2462
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 217,
                            "end": 220,
                            "matchedPaperCorpusId": "221562332"
                        },
                        {
                            "start": 220,
                            "end": 223,
                            "matchedPaperCorpusId": "235694265"
                        },
                        {
                            "start": 223,
                            "end": 226,
                            "matchedPaperCorpusId": "249674779"
                        },
                        {
                            "start": 1906,
                            "end": 1909,
                            "matchedPaperCorpusId": "221562332"
                        },
                        {
                            "start": 1909,
                            "end": 1912,
                            "matchedPaperCorpusId": "235694265"
                        },
                        {
                            "start": 1912,
                            "end": 1915,
                            "matchedPaperCorpusId": "249674779"
                        },
                        {
                            "start": 2122,
                            "end": 2125,
                            "matchedPaperCorpusId": "271448069"
                        },
                        {
                            "start": 2125,
                            "end": 2128,
                            "matchedPaperCorpusId": "259341801"
                        },
                        {
                            "start": 2128,
                            "end": 2132,
                            "matchedPaperCorpusId": "252118610"
                        },
                        {
                            "start": 2132,
                            "end": 2136,
                            "matchedPaperCorpusId": "268363462"
                        },
                        {
                            "start": 2333,
                            "end": 2335,
                            "matchedPaperCorpusId": "257687440"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9580078125
                }
            ],
            "relevance_judgement": 0.9580078125,
            "relevance_judgment_input_expanded": "# Title: Scalable watermarking for identifying large language model outputs\n# Venue: The Naturalist\n# Authors: Sumanth Dathathri, Abigail See, Sumedh Ghaisas, Po-Sen Huang, Rob McAdam, Johannes Welbl, Vandana Bachani, Alex Kaskasoli, Robert Stanforth, Tatiana Matejovicova, Jamie Hayes, Nidhi Vyas, Majd Al Merey, Jonah Brown-Cohen, Rudy Bunel, Borja Balle, taylan. cemgil, Zahra Ahmed, Kitty Stacpoole, Ilia Shumailov, Cip Baetu, Sven Gowal, D. Hassabis, Pushmeet Kohli\n## Abstract\nLarge language models (LLMs) have enabled the generation of high-quality synthetic text, often indistinguishable from human-written content, at a scale that can markedly affect the nature of the information ecosystem1\u20133. Watermarking can help identify synthetic text and limit accidental or deliberate misuse4, but has not been adopted in production systems owing to stringent quality, detectability and computational efficiency requirements. Here we describe SynthID-Text, a production-ready text watermarking scheme that preserves text quality and enables high detection accuracy, with minimal latency overhead. SynthID-Text does not affect LLM training and modifies only the sampling procedure; watermark detection is computationally efficient, without using the underlying LLM. To enable watermarking at scale, we develop an algorithm integrating watermarking with speculative sampling, an efficiency technique frequently used in production systems5. Evaluations across multiple LLMs empirically show that SynthID-Text provides improved detectability over comparable methods, and standard benchmarks and human side-by-side ratings indicate no change in LLM capabilities. To demonstrate the feasibility of watermarking in large-scale-production systems, we conducted a live experiment that assessed feedback from nearly 20\u2009million Gemini6 responses, again confirming the preservation of text quality. We hope that the availability of SynthID-Text7 will facilitate further development of watermarking and responsible use of LLM systems.\n## body\nLarge language models (LLMs) have enabled the generation of high-quality synthetic text, often indistinguishable from human-written content, at a scale that can markedly affect the nature of the information ecosystem [1][2][3] . Watermarking can help identify synthetic text and limit accidental or deliberate misuse 4 , but has not been adopted in production systems owing to stringent quality, detectability and computational efficiency requirements. Here we describe SynthID-Text, a production-ready text watermarking scheme that preserves text quality and enables high detection accuracy, with minimal latency overhead. SynthID-Text does not affect LLM training and modifies only the sampling procedure; watermark detection is computationally efficient, without using the underlying LLM. To enable watermarking at scale, we develop an algorithm integrating watermarking with speculative sampling, an efficiency technique frequently used in production systems 5 . Evaluations across multiple LLMs empirically show that SynthID-Text provides improved detectability over comparable methods, and standard benchmarks and human side-by-side ratings indicate no change in LLM capabilities. To demonstrate the feasibility of watermarking in large-scale-production systems, we conducted a live experiment that assessed feedback from nearly 20 million Gemini 6 responses, again confirming the preservation of text quality. We hope that the availability of SynthID-Text 7 will facilitate further development of watermarking and responsible use of LLM systems. \n\nLarge language models (LLMs) are widely adopted tools for synthetic text generation, finding applications in language-based assistants, code generation, writing support and various other domains. As LLMs advance in quality, coherence, coverage and expertise, it can become difficult to distinguish synthetically generated text from human-written text [1][2][3] . Given the widespread use of LLMs in education, software development and web content generation, identification and attribution of LLM text is critical to ensure safe and responsible use of the technology [8][9][10][11] . \n\nMultiple strategies have emerged to address this problem. One is a retrieval-based approach, which involves keeping a growing record of all generated texts and checking against it for matches 12 . This requires scale and coordination, and raises privacy concerns as it requires accessing and storing all LLM interactions.",
            "reference_string": "[273547059 | Dathathri et al. | 2024 | Citations: 74]"
        },
        {
            "title": "SimLLM: Detecting Sentences Generated by Large Language Models Using Similarity between the Generation and its Re-generation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 25,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.1246, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1403588130",
                    "name": "Hoang-Quoc Nguyen-Son"
                },
                {
                    "authorId": "2260782421",
                    "name": "Minh-Son Dao"
                },
                {
                    "authorId": "2238657258",
                    "name": "Koji Zettsu"
                }
            ],
            "abstract": "Large language models have emerged as a significant phenomenon due to their ability to produce natural text across various applications. However, the proliferation of generated text raises concerns regarding its potential misuse in fraudulent activities such as academic dishonesty, spam dissemination, and misinformation propagation. Prior studies have detected the generation of non-analogous text, which manifests numerous differences between original and generated text. We have observed that the similarity between the original text and its generation is notably higher than that between the generated text and its subsequent regeneration. To address this, we propose a novel approach named SimLLM, aimed at estimating the similarity between an input sentence and its generated counterpart to detect analogous machine-generated sentences that closely mimic human-written ones. Our empirical analysis demonstrates SimLLM\u2019s superior performance compared to existing methods.",
            "corpus_id": 273901392,
            "sentences": [
                {
                    "corpus_id": "273901392",
                    "title": "SimLLM: Detecting Sentences Generated by Large Language Models Using Similarity between the Generation and its Re-generation",
                    "text": "The rise of generative AI, especially large language models, has had a substantial impact across various applications. However, it also presents challenges, such as academic dishonesty and the spread of disinformation, stemming from the misuse of generated text. Therefore, our goal is to create a strategy to detect and mitigate the negative effects associated with the improper use of generated text. \n\nDetection of text generated by large language models uses three main techniques. Firstly, supervised learning methods (Solaiman et al., 2019;Wang et al., 2023;Hu et al., 2023;Wu et al., 2023) train classifiers on datasets of original and generated text, though this requires large volumes of training data. The zero-shot approach (Bhattacharjee and Liu, 2023;Mitchell et al., 2023) eliminates the need for training but is sensitive to out-of-distribution text. Recent research explores watermarking methodologies (Kirchenbauer et al., 2023) to force models to produce predefined words, aiding detection, but this requires modifying the models, which is impractical for proprietary models like ChatGPT. Previous studies mainly address non-analogous text with substantial differences between original and generated content. In contrast, we focus on analogous generated text, where changes to the original text are minimal. \n\nMotivation An AI model aims to extensively optimize original data to generate new data. This process often results in a significant disparity between the original and the generated data. When the model optimizes the generated data to create re-generated data, the already optimized nature of the generated data limits further optimization. As a result, the gap between the generated and re-generated data diminishes. To illustrate, we randomly selected a human sentence (h) from the Extreme Summarization (XSum) dataset (Narayan et al., 2018) (Figure 1). Then, a large language model, specifically ChatGPT (GPT 3.5-turbo), was tasked with generating a machine sentence (m ChatGP T ) conveying an opposite meaning to the original text.",
                    "score": 0.7678299970107244,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 118
                        },
                        {
                            "start": 119,
                            "end": 262
                        },
                        {
                            "start": 263,
                            "end": 402
                        },
                        {
                            "start": 405,
                            "end": 485
                        },
                        {
                            "start": 486,
                            "end": 711
                        },
                        {
                            "start": 712,
                            "end": 865
                        },
                        {
                            "start": 866,
                            "end": 1106
                        },
                        {
                            "start": 1107,
                            "end": 1226
                        },
                        {
                            "start": 1227,
                            "end": 1325
                        },
                        {
                            "start": 1328,
                            "end": 1415
                        },
                        {
                            "start": 1416,
                            "end": 1514
                        },
                        {
                            "start": 1515,
                            "end": 1667
                        },
                        {
                            "start": 1668,
                            "end": 1744
                        },
                        {
                            "start": 1745,
                            "end": 1882
                        },
                        {
                            "start": 1883,
                            "end": 2062
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 546,
                            "end": 564,
                            "matchedPaperCorpusId": "264128397"
                        },
                        {
                            "start": 564,
                            "end": 580,
                            "matchedPaperCorpusId": "259501842"
                        },
                        {
                            "start": 580,
                            "end": 596,
                            "matchedPaperCorpusId": "258865367"
                        },
                        {
                            "start": 764,
                            "end": 786,
                            "matchedPaperCorpusId": "256274849"
                        },
                        {
                            "start": 918,
                            "end": 945,
                            "matchedPaperCorpusId": "256194179"
                        },
                        {
                            "start": 1848,
                            "end": 1870,
                            "matchedPaperCorpusId": "215768182"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95654296875
                },
                {
                    "corpus_id": "273901392",
                    "title": "SimLLM: Detecting Sentences Generated by Large Language Models Using Similarity between the Generation and its Re-generation",
                    "text": "The methods previously used to detect text generated by large language models can be classified into three approaches. \n\nThe first strategy involves training models on large datasets to identify generated text characteristics, such as OpenAI's fine-tuning of the RoBERTa model (Solaiman et al., 2019). Some researchers have analyzed probability distributions in large language models' hidden layers (Wang et al., 2023), while others have used a paraphraser in a GAN to train the detector component (Hu et al., 2023). The intrinsic dimension of the embedding space from long texts has been estimated to understand the workings of these models better (Tulchinskii et al., 2023). Other approaches include building a proxy model to estimate generated text's perplexity (Wu et al., 2023), using positive-unlabeled learning (Tian et al., 2024) to improve performance on short text, and highlighting human text's coherence to spot machinegenerated text discrepancies (Liu et al., 2023). Some researchers have also incorporated top similarity texts from the training set into prompts and used in-context learning to boost detector and attacker capabilities (Koike et al., 2024). However, this approach is sensitive to out-of-distribution texts. \n\nWatermarking is another method where a language model is guided to generate text that meets specific criteria, acting as a watermark to identify generated content. For example, Kirchenbauer et al. (2023) instructed the model to use only a certain set of \"green\" words, avoiding the \"red\" ones. However, this method's downside is that it requires modifying the original models, which is impractical for real-world use, especially considering the proprietary nature of many large language models. \n\nThe third strategy involves zero-shot detection, where research identifies generated text without training. Bhattacharjee and Liu (2023) employed this method by prompting ChatGPT to detect generated texts from various large language models. Gehrmann et al. (2019) noted that large language models often predict the next word in a text sequence with high probability, which can be assessed through ranking, logarithms, and entropy.",
                    "score": 0.8704169204463568,
                    "section_title": "Related Work",
                    "char_start_offset": 4732,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 118
                        },
                        {
                            "start": 121,
                            "end": 301
                        },
                        {
                            "start": 302,
                            "end": 516
                        },
                        {
                            "start": 517,
                            "end": 676
                        },
                        {
                            "start": 677,
                            "end": 979
                        },
                        {
                            "start": 980,
                            "end": 1170
                        },
                        {
                            "start": 1171,
                            "end": 1236
                        },
                        {
                            "start": 1239,
                            "end": 1402
                        },
                        {
                            "start": 1403,
                            "end": 1532
                        },
                        {
                            "start": 1533,
                            "end": 1733
                        },
                        {
                            "start": 1736,
                            "end": 1843
                        },
                        {
                            "start": 1844,
                            "end": 1976
                        },
                        {
                            "start": 1977,
                            "end": 2166
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 399,
                            "end": 418,
                            "matchedPaperCorpusId": "264128397"
                        },
                        {
                            "start": 498,
                            "end": 515,
                            "matchedPaperCorpusId": "259501842"
                        },
                        {
                            "start": 649,
                            "end": 675,
                            "matchedPaperCorpusId": "259108779"
                        },
                        {
                            "start": 765,
                            "end": 782,
                            "matchedPaperCorpusId": "258865367"
                        },
                        {
                            "start": 818,
                            "end": 837,
                            "matchedPaperCorpusId": "258960584"
                        },
                        {
                            "start": 960,
                            "end": 978,
                            "matchedPaperCorpusId": "254877728"
                        },
                        {
                            "start": 1149,
                            "end": 1169,
                            "matchedPaperCorpusId": "260091573"
                        },
                        {
                            "start": 1416,
                            "end": 1442,
                            "matchedPaperCorpusId": "256194179"
                        },
                        {
                            "start": 1844,
                            "end": 1872,
                            "matchedPaperCorpusId": "260379022"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9462890625
                }
            ],
            "relevance_judgement": 0.95654296875,
            "relevance_judgment_input_expanded": "# Title: SimLLM: Detecting Sentences Generated by Large Language Models Using Similarity between the Generation and its Re-generation\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Hoang-Quoc Nguyen-Son, Minh-Son Dao, Koji Zettsu\n## Abstract\nLarge language models have emerged as a significant phenomenon due to their ability to produce natural text across various applications. However, the proliferation of generated text raises concerns regarding its potential misuse in fraudulent activities such as academic dishonesty, spam dissemination, and misinformation propagation. Prior studies have detected the generation of non-analogous text, which manifests numerous differences between original and generated text. We have observed that the similarity between the original text and its generation is notably higher than that between the generated text and its subsequent regeneration. To address this, we propose a novel approach named SimLLM, aimed at estimating the similarity between an input sentence and its generated counterpart to detect analogous machine-generated sentences that closely mimic human-written ones. Our empirical analysis demonstrates SimLLM\u2019s superior performance compared to existing methods.\n## Introduction\nThe rise of generative AI, especially large language models, has had a substantial impact across various applications. However, it also presents challenges, such as academic dishonesty and the spread of disinformation, stemming from the misuse of generated text. Therefore, our goal is to create a strategy to detect and mitigate the negative effects associated with the improper use of generated text. \n\nDetection of text generated by large language models uses three main techniques. Firstly, supervised learning methods (Solaiman et al., 2019;Wang et al., 2023;Hu et al., 2023;Wu et al., 2023) train classifiers on datasets of original and generated text, though this requires large volumes of training data. The zero-shot approach (Bhattacharjee and Liu, 2023;Mitchell et al., 2023) eliminates the need for training but is sensitive to out-of-distribution text. Recent research explores watermarking methodologies (Kirchenbauer et al., 2023) to force models to produce predefined words, aiding detection, but this requires modifying the models, which is impractical for proprietary models like ChatGPT. Previous studies mainly address non-analogous text with substantial differences between original and generated content. In contrast, we focus on analogous generated text, where changes to the original text are minimal. \n\nMotivation An AI model aims to extensively optimize original data to generate new data. This process often results in a significant disparity between the original and the generated data. When the model optimizes the generated data to create re-generated data, the already optimized nature of the generated data limits further optimization. As a result, the gap between the generated and re-generated data diminishes. To illustrate, we randomly selected a human sentence (h) from the Extreme Summarization (XSum) dataset (Narayan et al., 2018) (Figure 1). Then, a large language model, specifically ChatGPT (GPT 3.5-turbo), was tasked with generating a machine sentence (m ChatGP T ) conveying an opposite meaning to the original text.\n\n## Related Work\nThe methods previously used to detect text generated by large language models can be classified into three approaches. \n\nThe first strategy involves training models on large datasets to identify generated text characteristics, such as OpenAI's fine-tuning of the RoBERTa model (Solaiman et al., 2019). Some researchers have analyzed probability distributions in large language models' hidden layers (Wang et al., 2023), while others have used a paraphraser in a GAN to train the detector component (Hu et al., 2023). The intrinsic dimension of the embedding space from long texts has been estimated to understand the workings of these models better (Tulchinskii et al., 2023). Other approaches include building a proxy model to estimate generated text's perplexity (Wu et al., 2023), using positive-unlabeled learning (Tian et al., 2024) to improve performance on short text, and highlighting human text's coherence to spot machinegenerated text discrepancies (Liu et al., 2023). Some researchers have also incorporated top similarity texts from the training set into prompts and used in-context learning to boost detector and attacker capabilities (Koike et al., 2024). However, this approach is sensitive to out-of-distribution texts. \n\nWatermarking is another method where a language model is guided to generate text that meets specific criteria, acting as a watermark to identify generated content. For example, Kirchenbauer et al. (2023) instructed the model to use only a certain set of \"green\" words, avoiding the \"red\" ones. However, this method's downside is that it requires modifying the original models, which is impractical for real-world use, especially considering the proprietary nature of many large language models. \n\nThe third strategy involves zero-shot detection, where research identifies generated text without training. Bhattacharjee and Liu (2023) employed this method by prompting ChatGPT to detect generated texts from various large language models. Gehrmann et al. (2019) noted that large language models often predict the next word in a text sequence with high probability, which can be assessed through ranking, logarithms, and entropy.",
            "reference_string": "[273901392 | Nguyen-Son et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text",
            "venue": "USENIX Security Symposium",
            "year": 2024,
            "reference_count": 58,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.09407, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2151516184",
                    "name": "Mazal Bethany"
                },
                {
                    "authorId": "2279750613",
                    "name": "Brandon Wherry"
                },
                {
                    "authorId": "2279757012",
                    "name": "Emet Bethany"
                },
                {
                    "authorId": "8842079",
                    "name": "Nishant Vishwamitra"
                },
                {
                    "authorId": "71756373",
                    "name": "Peyman Najafirad"
                }
            ],
            "abstract": "With the recent proliferation of Large Language Models (LLMs), there has been an increasing demand for tools to detect machine-generated text. The effective detection of machine-generated text face two pertinent problems: First, they are severely limited in generalizing against real-world scenarios, where machine-generated text is produced by a variety of generators, including but not limited to GPT-4 and Dolly, and spans diverse domains, ranging from academic manuscripts to social media posts. Second, existing detection methodologies treat texts produced by LLMs through a restrictive binary classification lens, neglecting the nuanced diversity of artifacts generated by different LLMs. In this work, we undertake a systematic study on the detection of machine-generated text in real-world scenarios. We first study the effectiveness of state-of-the-art approaches and find that they are severely limited against text produced by diverse generators and domains in the real world. Furthermore, t-SNE visualizations of the embeddings from a pretrained LLM's encoder show that they cannot reliably distinguish between human and machine-generated text. Based on our findings, we introduce a novel system, T5LLMCipher, for detecting machine-generated text using a pretrained T5 encoder combined with LLM embedding sub-clustering to address the text produced by diverse generators and domains in the real world. We evaluate our approach across 9 machine-generated text systems and 9 domains and find that our approach provides state-of-the-art generalization ability, with an average increase in F1 score on machine-generated text of 19.6\\% on unseen generators and domains compared to the top performing existing approaches and correctly attributes the generator of text with an accuracy of 93.6\\%.",
            "corpus_id": 267027819,
            "sentences": [
                {
                    "corpus_id": "267027819",
                    "title": "Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text",
                    "text": "Finally, there exists a set of techniques that engage in the fine-tuning of language models specifically for the detection of text generated by machines. \n\nOne of the subsets of statistical features leveraged in detecting machine-generated text revolves around term frequencies within text samples. Text authored by humans typically aligns with Zipf's Law, where the most frequent word in humanwritten text has roughly twice the frequency of the second most frequent word, and nearly three times that of the third most frequent word and so on [43]. Another feature-based method for detecting machine-generated text focuses on clarity and coherence. Earlier models like GPT-2 have shown that in longer outputs, there is a higher likelihood that inconsistencies, redundancy, or logical errors will emerge [53]. Some works build classifiers on bag-of-words features and TF-IDF [24] to analyze shorter text sequences. Other works such as Fr\u00f6hling et al. [25] combine several features, including lack of syntactic and lexical diversity, repetitiveness, and extract linguistic features such as named entities and part-of-speech distributions to build a neural network classifier on this information. However, a notable limitation of these statistical and feature-based methods is their potential vulnerability to more advanced language models, which continually improve in generating human-like text patterns and linguistic diversity. \n\nZero-shot methods for detecting machine-generated text have garnered attention due to the lack of need to train models on large datasets. Works such as those by Gehrmann et al. [27] find that human-written texts exhibit a broader range of word choices, whereas machine-generated texts tend to be more constrained and predictable and calculate entropy on text sequences to help distinguish machine-generated text from human-generated text. Other works used simple logprobability thresholding, where LLM token probabilities are averaged and a threshold is determined [56]. Recent works like DetectGPT [40] take this idea further by introducing the observation that the curvature of a model's log probability function is typically more negative for machine-generated text compared to human-written text.",
                    "score": 0.636115404127882,
                    "section_title": "Background",
                    "char_start_offset": 8339,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 153
                        },
                        {
                            "start": 156,
                            "end": 298
                        },
                        {
                            "start": 299,
                            "end": 548
                        },
                        {
                            "start": 549,
                            "end": 648
                        },
                        {
                            "start": 649,
                            "end": 808
                        },
                        {
                            "start": 809,
                            "end": 913
                        },
                        {
                            "start": 914,
                            "end": 1193
                        },
                        {
                            "start": 1194,
                            "end": 1428
                        },
                        {
                            "start": 1431,
                            "end": 1568
                        },
                        {
                            "start": 1569,
                            "end": 1869
                        },
                        {
                            "start": 1870,
                            "end": 2001
                        },
                        {
                            "start": 2002,
                            "end": 2231
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 543,
                            "end": 547,
                            "matchedPaperCorpusId": "36455310"
                        },
                        {
                            "start": 803,
                            "end": 807,
                            "matchedPaperCorpusId": "202734604"
                        },
                        {
                            "start": 874,
                            "end": 878,
                            "matchedPaperCorpusId": "220936111"
                        },
                        {
                            "start": 950,
                            "end": 954,
                            "matchedPaperCorpusId": "233563022"
                        },
                        {
                            "start": 1608,
                            "end": 1612,
                            "matchedPaperCorpusId": "182952848"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.955078125
                }
            ],
            "relevance_judgement": 0.955078125,
            "relevance_judgment_input_expanded": "# Title: Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text\n# Venue: USENIX Security Symposium\n# Authors: Mazal Bethany, Brandon Wherry, Emet Bethany, Nishant Vishwamitra, Peyman Najafirad\n## Abstract\nWith the recent proliferation of Large Language Models (LLMs), there has been an increasing demand for tools to detect machine-generated text. The effective detection of machine-generated text face two pertinent problems: First, they are severely limited in generalizing against real-world scenarios, where machine-generated text is produced by a variety of generators, including but not limited to GPT-4 and Dolly, and spans diverse domains, ranging from academic manuscripts to social media posts. Second, existing detection methodologies treat texts produced by LLMs through a restrictive binary classification lens, neglecting the nuanced diversity of artifacts generated by different LLMs. In this work, we undertake a systematic study on the detection of machine-generated text in real-world scenarios. We first study the effectiveness of state-of-the-art approaches and find that they are severely limited against text produced by diverse generators and domains in the real world. Furthermore, t-SNE visualizations of the embeddings from a pretrained LLM's encoder show that they cannot reliably distinguish between human and machine-generated text. Based on our findings, we introduce a novel system, T5LLMCipher, for detecting machine-generated text using a pretrained T5 encoder combined with LLM embedding sub-clustering to address the text produced by diverse generators and domains in the real world. We evaluate our approach across 9 machine-generated text systems and 9 domains and find that our approach provides state-of-the-art generalization ability, with an average increase in F1 score on machine-generated text of 19.6\\% on unseen generators and domains compared to the top performing existing approaches and correctly attributes the generator of text with an accuracy of 93.6\\%.\n## Background\nFinally, there exists a set of techniques that engage in the fine-tuning of language models specifically for the detection of text generated by machines. \n\nOne of the subsets of statistical features leveraged in detecting machine-generated text revolves around term frequencies within text samples. Text authored by humans typically aligns with Zipf's Law, where the most frequent word in humanwritten text has roughly twice the frequency of the second most frequent word, and nearly three times that of the third most frequent word and so on [43]. Another feature-based method for detecting machine-generated text focuses on clarity and coherence. Earlier models like GPT-2 have shown that in longer outputs, there is a higher likelihood that inconsistencies, redundancy, or logical errors will emerge [53]. Some works build classifiers on bag-of-words features and TF-IDF [24] to analyze shorter text sequences. Other works such as Fr\u00f6hling et al. [25] combine several features, including lack of syntactic and lexical diversity, repetitiveness, and extract linguistic features such as named entities and part-of-speech distributions to build a neural network classifier on this information. However, a notable limitation of these statistical and feature-based methods is their potential vulnerability to more advanced language models, which continually improve in generating human-like text patterns and linguistic diversity. \n\nZero-shot methods for detecting machine-generated text have garnered attention due to the lack of need to train models on large datasets. Works such as those by Gehrmann et al. [27] find that human-written texts exhibit a broader range of word choices, whereas machine-generated texts tend to be more constrained and predictable and calculate entropy on text sequences to help distinguish machine-generated text from human-generated text. Other works used simple logprobability thresholding, where LLM token probabilities are averaged and a threshold is determined [56]. Recent works like DetectGPT [40] take this idea further by introducing the observation that the curvature of a model's log probability function is typically more negative for machine-generated text compared to human-written text.",
            "reference_string": "[267027819 | Bethany et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 60,
            "citation_count": 153,
            "influential_citation_count": 45,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.05130",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.05130, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1993226927",
                    "name": "Guangsheng Bao"
                },
                {
                    "authorId": "2257125588",
                    "name": "Yanbin Zhao"
                },
                {
                    "authorId": "2272668",
                    "name": "Zhiyang Teng"
                },
                {
                    "authorId": "2145500840",
                    "name": "Linyi Yang"
                },
                {
                    "authorId": "2256997888",
                    "name": "Yue Zhang"
                }
            ],
            "abstract": "Large language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. To build trustworthy AI systems, it is imperative to distinguish between machine-generated and human-authored content. The leading zero-shot detector, DetectGPT, showcases commendable performance but is marred by its intensive computational costs. In this paper, we introduce the concept of conditional probability curvature to elucidate discrepancies in word choices between LLMs and humans within a given context. Utilizing this curvature as a foundational metric, we present **Fast-DetectGPT**, an optimized zero-shot detector, which substitutes DetectGPT's perturbation step with a more efficient sampling step. Our evaluations on various datasets, source models, and test conditions indicate that Fast-DetectGPT not only surpasses DetectGPT by a relative around 75% in both the white-box and black-box settings but also accelerates the detection process by a factor of 340, as detailed in Table 1. See \\url{https://github.com/baoguangsheng/fast-detect-gpt} for code, data, and results.",
            "corpus_id": 263831345,
            "sentences": [
                {
                    "corpus_id": "263831345",
                    "title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
                    "text": "Our research centers on zero-shot detection of machine-generated text, under the premise that our model has not undergone training using any machine-generated text. This distinguishes our approach from conventional supervised methods, which commonly employ discriminative training strategies to acquire specific syntactic or semantic attributes customized for machine-generated text. In contrast, our zero-shot methodology capitalizes on the inherent capabilities of large language models to identify anomalies that function as markers of machine-generated content. \n\nThe White-box Setting. Conventional zero-shot methodologies often operate under the assumption that the source model responsible for generating machine-generated text is accessible. We refer to this context as the white-box setting, where the primary goal is to distinguish machine-generated texts produced by the source model from those generated by humans. In this white-box setting, our detection decisions are dependent on the source model, but it is not mandatory to possess detailed knowledge of the source model's architecture and parameters. For instance, within the white-box framework, a system like DetectGPT utilizes the OpenAI API to identify text generated by GPT-3, without requiring extensive knowledge of the inner workings of GPT-3. \n\nThe Black-box Setting. In real-world situations, there could be instances where we lack knowledge about the specific source models employed for content generation. This necessitates the development of a versatile detector capable of identifying texts generated by a variety of automated systems. We term this scenario the black-box setting, where the objective is to differentiate between machinegenerated texts produced by diverse, unidentified models and those composed by humans. In this context, the term \"black box\" signifies that we lack access to information about the source model or any details pertaining to it. \n\nEvaluation Metric (AUROC). Instead of measuring the detection accuracy with a specific threshold (\u03f5 in Figure 2), we measure the detection accuracy in the area under the receiver operating characteristic (AUROC), profiling the detectors on the whole spectrum of the thresholds. AUROC ranges from 0.0 to 1.0, mathematically denoting the probability of a random machine-generated text having a higher predicted probability of being machine-generated than a random human-written text.",
                    "score": 0.6853582512752884,
                    "section_title": "A ZERO-SHOT DETECTION TASK AND SETTINGS",
                    "char_start_offset": 24848,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 164
                        },
                        {
                            "start": 165,
                            "end": 383
                        },
                        {
                            "start": 384,
                            "end": 565
                        },
                        {
                            "start": 568,
                            "end": 590
                        },
                        {
                            "start": 591,
                            "end": 749
                        },
                        {
                            "start": 750,
                            "end": 926
                        },
                        {
                            "start": 927,
                            "end": 1117
                        },
                        {
                            "start": 1118,
                            "end": 1318
                        },
                        {
                            "start": 1321,
                            "end": 1343
                        },
                        {
                            "start": 1344,
                            "end": 1484
                        },
                        {
                            "start": 1485,
                            "end": 1616
                        },
                        {
                            "start": 1617,
                            "end": 1803
                        },
                        {
                            "start": 1804,
                            "end": 1942
                        },
                        {
                            "start": 1945,
                            "end": 1971
                        },
                        {
                            "start": 1972,
                            "end": 2222
                        },
                        {
                            "start": 2223,
                            "end": 2426
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95458984375
                }
            ],
            "relevance_judgement": 0.95458984375,
            "relevance_judgment_input_expanded": "# Title: Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature\n# Venue: International Conference on Learning Representations\n# Authors: Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, Yue Zhang\n## Abstract\nLarge language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. To build trustworthy AI systems, it is imperative to distinguish between machine-generated and human-authored content. The leading zero-shot detector, DetectGPT, showcases commendable performance but is marred by its intensive computational costs. In this paper, we introduce the concept of conditional probability curvature to elucidate discrepancies in word choices between LLMs and humans within a given context. Utilizing this curvature as a foundational metric, we present **Fast-DetectGPT**, an optimized zero-shot detector, which substitutes DetectGPT's perturbation step with a more efficient sampling step. Our evaluations on various datasets, source models, and test conditions indicate that Fast-DetectGPT not only surpasses DetectGPT by a relative around 75% in both the white-box and black-box settings but also accelerates the detection process by a factor of 340, as detailed in Table 1. See \\url{https://github.com/baoguangsheng/fast-detect-gpt} for code, data, and results.\n## A ZERO-SHOT DETECTION TASK AND SETTINGS\nOur research centers on zero-shot detection of machine-generated text, under the premise that our model has not undergone training using any machine-generated text. This distinguishes our approach from conventional supervised methods, which commonly employ discriminative training strategies to acquire specific syntactic or semantic attributes customized for machine-generated text. In contrast, our zero-shot methodology capitalizes on the inherent capabilities of large language models to identify anomalies that function as markers of machine-generated content. \n\nThe White-box Setting. Conventional zero-shot methodologies often operate under the assumption that the source model responsible for generating machine-generated text is accessible. We refer to this context as the white-box setting, where the primary goal is to distinguish machine-generated texts produced by the source model from those generated by humans. In this white-box setting, our detection decisions are dependent on the source model, but it is not mandatory to possess detailed knowledge of the source model's architecture and parameters. For instance, within the white-box framework, a system like DetectGPT utilizes the OpenAI API to identify text generated by GPT-3, without requiring extensive knowledge of the inner workings of GPT-3. \n\nThe Black-box Setting. In real-world situations, there could be instances where we lack knowledge about the specific source models employed for content generation. This necessitates the development of a versatile detector capable of identifying texts generated by a variety of automated systems. We term this scenario the black-box setting, where the objective is to differentiate between machinegenerated texts produced by diverse, unidentified models and those composed by humans. In this context, the term \"black box\" signifies that we lack access to information about the source model or any details pertaining to it. \n\nEvaluation Metric (AUROC). Instead of measuring the detection accuracy with a specific threshold (\u03f5 in Figure 2), we measure the detection accuracy in the area under the receiver operating characteristic (AUROC), profiling the detectors on the whole spectrum of the thresholds. AUROC ranges from 0.0 to 1.0, mathematically denoting the probability of a random machine-generated text having a higher predicted probability of being machine-generated than a random human-written text.",
            "reference_string": "[263831345 | Bao et al. | 2023 | Citations: 153]"
        },
        {
            "title": "A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 63,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.02898, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2354295905",
                    "name": "Lele Cao"
                }
            ],
            "abstract": "Advances in AI-generated content have led to wide adoption of large language models, diffusion-based visual generators, and synthetic audio tools. However, these developments raise critical concerns about misinformation, copyright infringement, security threats, and the erosion of public trust. In this paper, we explore an extensive range of methods designed to detect and mitigate AI-generated textual, visual, and audio content. We begin by discussing motivations and potential impacts associated with AI-based content generation, including real-world risks and ethical dilemmas. We then outline detection techniques spanning observation-based strategies, linguistic and statistical analysis, model-based pipelines, watermarking and fingerprinting, as well as emergent ensemble approaches. We also present new perspectives on robustness, adaptation to rapidly improving generative architectures, and the critical role of human-in-the-loop verification. By surveying state-of-the-art research and highlighting case studies in academic, journalistic, legal, and industrial contexts, this paper aims to inform robust solutions and policymaking. We conclude by discussing open challenges, including adversarial transformations, domain generalization, and ethical concerns, thereby offering a holistic guide for researchers, practitioners, and regulators to preserve content authenticity in the face of increasingly sophisticated AI-generated media.",
            "corpus_id": 277595847,
            "sentences": [
                {
                    "corpus_id": "277595847",
                    "title": "A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content",
                    "text": "One popular approach leverages an external LLM to classify whether a piece of text is AI-generated or not [12]. With carefully crafted prompts, these zero-shot methods can achieve moderate accuracy quickly. However, sensitivity to prompt design and the adversarial gap between the LLM detector and the text generator are common issues [22]. \n\n2) Linguistic and statistical signatures: Traditional stylometric features (e.g., function words, syntax complexity, average phrase length) have long been used in authorship attribution [23], [24]. More modern detection focuses on computing perplexity or log-likelihood using reference language models, observing that LLM-generated text tends to show distinctive probability distributions. Additionally, specialized white-box methods can measure rank ordering of tokens if the generating model is partially known [25]. \n\n3) Supervised classification (training-based): Labeled corpora of AI versus human text enable fine-tuning of large pre-trained transformers like RoBERTa or T5 to discriminate synthetic text [26], [27]. Researchers improve robustness with adversarial training sets that contain paraphrased or AIgenerated passages shifted in style. Tools like GPTZero [28] and RADAR [29] exemplify advanced supervised detectors. However, assembling high-quality, representative training data remains a challenge, especially as new generator architectures emerge frequently. \n\n4) Watermarking for AI text: Cooperative watermarking modifies text generation at token selection time, embedding an imperceptible pattern in the distribution of words or punctuation [30], [31]. A verifier can detect such patterns after the fact. While promising for major industrial LLMs that adopt the standard, watermarking fails if malicious or open-source models do not embed it, or if paraphrasing disrupts the signal [32], [33].",
                    "score": 0.6484977737294655,
                    "section_title": "B. Approaches 1) LLM prompting and zero-shot methods:",
                    "char_start_offset": 6867,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 111
                        },
                        {
                            "start": 112,
                            "end": 206
                        },
                        {
                            "start": 207,
                            "end": 340
                        },
                        {
                            "start": 343,
                            "end": 540
                        },
                        {
                            "start": 541,
                            "end": 732
                        },
                        {
                            "start": 733,
                            "end": 861
                        },
                        {
                            "start": 864,
                            "end": 1065
                        },
                        {
                            "start": 1066,
                            "end": 1194
                        },
                        {
                            "start": 1195,
                            "end": 1274
                        },
                        {
                            "start": 1275,
                            "end": 1419
                        },
                        {
                            "start": 1422,
                            "end": 1616
                        },
                        {
                            "start": 1617,
                            "end": 1668
                        },
                        {
                            "start": 1669,
                            "end": 1857
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 106,
                            "end": 110,
                            "matchedPaperCorpusId": "260379022"
                        },
                        {
                            "start": 529,
                            "end": 533,
                            "matchedPaperCorpusId": "256274849"
                        },
                        {
                            "start": 535,
                            "end": 539,
                            "matchedPaperCorpusId": "267068867"
                        },
                        {
                            "start": 1214,
                            "end": 1218,
                            "matchedPaperCorpusId": "258960584"
                        },
                        {
                            "start": 1229,
                            "end": 1233,
                            "matchedPaperCorpusId": "259501842"
                        },
                        {
                            "start": 1605,
                            "end": 1609,
                            "matchedPaperCorpusId": "256194179"
                        },
                        {
                            "start": 1846,
                            "end": 1850,
                            "matchedPaperCorpusId": "263831179"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95458984375
                }
            ],
            "relevance_judgement": 0.95458984375,
            "relevance_judgment_input_expanded": "# Title: A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content\n# Venue: arXiv.org\n# Authors: Lele Cao\n## Abstract\nAdvances in AI-generated content have led to wide adoption of large language models, diffusion-based visual generators, and synthetic audio tools. However, these developments raise critical concerns about misinformation, copyright infringement, security threats, and the erosion of public trust. In this paper, we explore an extensive range of methods designed to detect and mitigate AI-generated textual, visual, and audio content. We begin by discussing motivations and potential impacts associated with AI-based content generation, including real-world risks and ethical dilemmas. We then outline detection techniques spanning observation-based strategies, linguistic and statistical analysis, model-based pipelines, watermarking and fingerprinting, as well as emergent ensemble approaches. We also present new perspectives on robustness, adaptation to rapidly improving generative architectures, and the critical role of human-in-the-loop verification. By surveying state-of-the-art research and highlighting case studies in academic, journalistic, legal, and industrial contexts, this paper aims to inform robust solutions and policymaking. We conclude by discussing open challenges, including adversarial transformations, domain generalization, and ethical concerns, thereby offering a holistic guide for researchers, practitioners, and regulators to preserve content authenticity in the face of increasingly sophisticated AI-generated media.\n## B. Approaches 1) LLM prompting and zero-shot methods:\nOne popular approach leverages an external LLM to classify whether a piece of text is AI-generated or not [12]. With carefully crafted prompts, these zero-shot methods can achieve moderate accuracy quickly. However, sensitivity to prompt design and the adversarial gap between the LLM detector and the text generator are common issues [22]. \n\n2) Linguistic and statistical signatures: Traditional stylometric features (e.g., function words, syntax complexity, average phrase length) have long been used in authorship attribution [23], [24]. More modern detection focuses on computing perplexity or log-likelihood using reference language models, observing that LLM-generated text tends to show distinctive probability distributions. Additionally, specialized white-box methods can measure rank ordering of tokens if the generating model is partially known [25]. \n\n3) Supervised classification (training-based): Labeled corpora of AI versus human text enable fine-tuning of large pre-trained transformers like RoBERTa or T5 to discriminate synthetic text [26], [27]. Researchers improve robustness with adversarial training sets that contain paraphrased or AIgenerated passages shifted in style. Tools like GPTZero [28] and RADAR [29] exemplify advanced supervised detectors. However, assembling high-quality, representative training data remains a challenge, especially as new generator architectures emerge frequently. \n\n4) Watermarking for AI text: Cooperative watermarking modifies text generation at token selection time, embedding an imperceptible pattern in the distribution of words or punctuation [30], [31]. A verifier can detect such patterns after the fact. While promising for major industrial LLMs that adopt the standard, watermarking fails if malicious or open-source models do not embed it, or if paraphrasing disrupts the signal [32], [33].",
            "reference_string": "[277595847 | Cao | 2025 | Citations: 0]"
        },
        {
            "title": "ConDA: Contrastive Domain Adaptation for AI-generated Text Detection",
            "venue": "International Joint Conference on Natural Language Processing",
            "year": 2023,
            "reference_count": 59,
            "citation_count": 47,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.03992",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.03992, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2144903149",
                    "name": "Amrita Bhattacharjee"
                },
                {
                    "authorId": "40899329",
                    "name": "Tharindu Kumarage"
                },
                {
                    "authorId": "11064745",
                    "name": "Raha Moraffah"
                },
                {
                    "authorId": "38746648",
                    "name": "Huan Liu"
                }
            ],
            "abstract": "Large language models (LLMs) are increasingly being used for generating text in a variety of use cases, including journalistic news articles. Given the potential malicious nature in which these LLMs can be used to generate disinformation at scale, it is important to build effective detectors for such AI-generated text. Given the surge in development of new LLMs, acquiring labeled training data for supervised detectors is a bottleneck. However, there might be plenty of unlabeled text data available, without information on which generator it came from. In this work we tackle this data problem, in detecting AI-generated news text, and frame the problem as an unsupervised domain adaptation task. Here the domains are the different text generators, i.e. LLMs, and we assume we have access to only the labeled source data and unlabeled target data. We develop a Contrastive Domain Adaptation framework, called ConDA, that blends standard domain adaptation techniques with the representation power of contrastive learning to learn domain invariant representations that are effective for the final unsupervised detection task. Our experiments demonstrate the effectiveness of our framework, resulting in average performance gains of 31.7% from the best performing baselines, and within 0.8% margin of a fully supervised detector. All our code and data is available at https://github.com/AmritaBh/ConDA-gen-text-detection.",
            "corpus_id": 261660497,
            "sentences": [
                {
                    "corpus_id": "261660497",
                    "title": "ConDA: Contrastive Domain Adaptation for AI-generated Text Detection",
                    "text": "Generated Text Detection The burgeoning progress in the generation capabilities of large language models has led to a corresponding increase in research and development efforts in the field of detection. Several recent efforts look at methods, varying from simple feature-based classifiers to fine-tuned language model-based detectors, in order to classify whether a piece of input text is human-written or AI-generated (Ippolito et al., 2019;Gehrmann et al., 2019;Mitchell et al., 2023), along with methods that specifically focus on AIgenerated news (Zellers et al., 2019;Bogaert et al., 2022). A related direction of work is that of authorship attribution (AA). While older AA methods focused on human authors, more recent efforts (Uchendu et al., 2020;Munir et al., 2021) build models to identify the generator for a particular input text. Recent work also shows how AI-generated text can deceive state-of-the-art AA models (Jones et al., 2022), thus making the task of detecting such text even more important.",
                    "score": 0.6137121822168683,
                    "section_title": "Related Work",
                    "char_start_offset": 3926,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 596
                        },
                        {
                            "start": 597,
                            "end": 664
                        },
                        {
                            "start": 665,
                            "end": 843
                        },
                        {
                            "start": 844,
                            "end": 1014
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 443,
                            "end": 465,
                            "matchedPaperCorpusId": "233296145"
                        },
                        {
                            "start": 552,
                            "end": 574,
                            "matchedPaperCorpusId": "168169824"
                        },
                        {
                            "start": 756,
                            "end": 775,
                            "matchedPaperCorpusId": "233189559"
                        },
                        {
                            "start": 928,
                            "end": 948,
                            "matchedPaperCorpusId": "247595180"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95458984375
                }
            ],
            "relevance_judgement": 0.95458984375,
            "relevance_judgment_input_expanded": "# Title: ConDA: Contrastive Domain Adaptation for AI-generated Text Detection\n# Venue: International Joint Conference on Natural Language Processing\n# Authors: Amrita Bhattacharjee, Tharindu Kumarage, Raha Moraffah, Huan Liu\n## Abstract\nLarge language models (LLMs) are increasingly being used for generating text in a variety of use cases, including journalistic news articles. Given the potential malicious nature in which these LLMs can be used to generate disinformation at scale, it is important to build effective detectors for such AI-generated text. Given the surge in development of new LLMs, acquiring labeled training data for supervised detectors is a bottleneck. However, there might be plenty of unlabeled text data available, without information on which generator it came from. In this work we tackle this data problem, in detecting AI-generated news text, and frame the problem as an unsupervised domain adaptation task. Here the domains are the different text generators, i.e. LLMs, and we assume we have access to only the labeled source data and unlabeled target data. We develop a Contrastive Domain Adaptation framework, called ConDA, that blends standard domain adaptation techniques with the representation power of contrastive learning to learn domain invariant representations that are effective for the final unsupervised detection task. Our experiments demonstrate the effectiveness of our framework, resulting in average performance gains of 31.7% from the best performing baselines, and within 0.8% margin of a fully supervised detector. All our code and data is available at https://github.com/AmritaBh/ConDA-gen-text-detection.\n## Related Work\nGenerated Text Detection The burgeoning progress in the generation capabilities of large language models has led to a corresponding increase in research and development efforts in the field of detection. Several recent efforts look at methods, varying from simple feature-based classifiers to fine-tuned language model-based detectors, in order to classify whether a piece of input text is human-written or AI-generated (Ippolito et al., 2019;Gehrmann et al., 2019;Mitchell et al., 2023), along with methods that specifically focus on AIgenerated news (Zellers et al., 2019;Bogaert et al., 2022). A related direction of work is that of authorship attribution (AA). While older AA methods focused on human authors, more recent efforts (Uchendu et al., 2020;Munir et al., 2021) build models to identify the generator for a particular input text. Recent work also shows how AI-generated text can deceive state-of-the-art AA models (Jones et al., 2022), thus making the task of detecting such text even more important.",
            "reference_string": "[261660497 | Bhattacharjee et al. | 2023 | Citations: 47]"
        },
        {
            "title": "A Certified Robust Watermark For Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.19708, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2323563488",
                    "name": "Xianheng Feng"
                },
                {
                    "authorId": "2279667983",
                    "name": "Jian Liu"
                },
                {
                    "authorId": "2249553637",
                    "name": "Kui Ren"
                },
                {
                    "authorId": "2331522408",
                    "name": "Chun Chen"
                }
            ],
            "abstract": "The effectiveness of watermark algorithms in AI-generated text identification has garnered significant attention. Concurrently, an increasing number of watermark algorithms have been proposed to enhance the robustness against various watermark attacks. However, these watermark algorithms remain susceptible to adaptive or unseen attacks. To address this issue, to our best knowledge, we propose the first certified robust watermark algorithm for large language models based on randomized smoothing, which can provide provable guarantees for watermarked text. Specifically, we utilize two different models respectively for watermark generation and detection and add Gaussian and Uniform noise respectively in the embedding and permutation space during the training and inference stages of the watermark detector to enhance the certified robustness of our watermark detector and derive certified radius. To evaluate the empirical robustness and certified robustness of our watermark algorithm, we conducted comprehensive experiments. The results indicate that our watermark algorithm shows comparable performance to baseline algorithms while our algorithm can derive substantial certified robustness, which means that our watermark can not be removed even under significant alterations.",
            "corpus_id": 272987651,
            "sentences": [
                {
                    "corpus_id": "272987651",
                    "title": "A Certified Robust Watermark For Large Language Models",
                    "text": "AI Generated Text Identification. While large language models are applied to various scenarios with their increasingly powerful text generation capabilities, such as story generation [52], article translation [59] and code assistance [53], the potential risks they bring [7,28,50] have made the identi- Direct detection methods often identify AI-generated text by uncovering distinguishing characteristics between AIgenerated and human-written texts or utilizing an extensive dataset of AI-generated text to train a accurate text classifier for AI-generated content. For example, regarding the first kind of method, Hamed and Wu [1] discovered that the bigram similarity of texts generated by ChatGPT is higher than that of human-written texts and developed a corresponding detection method based on this finding. Mitchell et al. found that AIgenerated texts tend to occupy the negative curvature region of the model's log-probability function. Based on this observation they proposed DetectGPT [37]. Building on Mitchell et al.'s approach, Bao et al. [2] improved the perturbation step in their method, significantly enhancing both its accuracy and speed. However, with the rapid advancement in both the size of model and text generation capabilities of LLMs, the gap between AI-generated texts and human-written texts has been narrowing [16,35,38,39]. As a result, methods based on text characteristics are becoming increasingly less effective.For the second type of method, Mindner et al. [36] employed multidimensional text feature extraction approaches to construct a classifier, with the best classifier outperforming GPTZero [14] in terms of F1-score. Chen et al. [6] and Liu et al. [32] utilized the advanced language understanding capabilities of pretrained LLMs [33,43], finetuning them as binary classifiers on various text datasets for AI-generated text detection. While these methods perform well on their respective test datasets, their effectiveness may be limited when applied to texts generated by newly emerging models. Likewise, as the capabilities of large language models continue to advance, their effectiveness remains a question. \n\nWatermarking, as an alternative method for AI-generated text identification, is generally more effective, versatile, and interpretable compared to direct detection.",
                    "score": 0.7349849781229936,
                    "section_title": "Related work",
                    "char_start_offset": 31054,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 33
                        },
                        {
                            "start": 34,
                            "end": 566
                        },
                        {
                            "start": 567,
                            "end": 813
                        },
                        {
                            "start": 814,
                            "end": 944
                        },
                        {
                            "start": 945,
                            "end": 1000
                        },
                        {
                            "start": 1001,
                            "end": 1156
                        },
                        {
                            "start": 1157,
                            "end": 1353
                        },
                        {
                            "start": 1354,
                            "end": 1658
                        },
                        {
                            "start": 1659,
                            "end": 1876
                        },
                        {
                            "start": 1877,
                            "end": 2037
                        },
                        {
                            "start": 2038,
                            "end": 2153
                        },
                        {
                            "start": 2156,
                            "end": 2320
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 183,
                            "end": 187,
                            "matchedPaperCorpusId": "247585187"
                        },
                        {
                            "start": 274,
                            "end": 277,
                            "matchedPaperCorpusId": "247450984"
                        },
                        {
                            "start": 277,
                            "end": 280,
                            "matchedPaperCorpusId": "218781040"
                        },
                        {
                            "start": 995,
                            "end": 999,
                            "matchedPaperCorpusId": "256274849"
                        },
                        {
                            "start": 1349,
                            "end": 1352,
                            "matchedPaperCorpusId": "269738126"
                        },
                        {
                            "start": 1492,
                            "end": 1496,
                            "matchedPaperCorpusId": "260775600"
                        },
                        {
                            "start": 1776,
                            "end": 1779,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95361328125
                }
            ],
            "relevance_judgement": 0.95361328125,
            "relevance_judgment_input_expanded": "# Title: A Certified Robust Watermark For Large Language Models\n# Venue: arXiv.org\n# Authors: Xianheng Feng, Jian Liu, Kui Ren, Chun Chen\n## Abstract\nThe effectiveness of watermark algorithms in AI-generated text identification has garnered significant attention. Concurrently, an increasing number of watermark algorithms have been proposed to enhance the robustness against various watermark attacks. However, these watermark algorithms remain susceptible to adaptive or unseen attacks. To address this issue, to our best knowledge, we propose the first certified robust watermark algorithm for large language models based on randomized smoothing, which can provide provable guarantees for watermarked text. Specifically, we utilize two different models respectively for watermark generation and detection and add Gaussian and Uniform noise respectively in the embedding and permutation space during the training and inference stages of the watermark detector to enhance the certified robustness of our watermark detector and derive certified radius. To evaluate the empirical robustness and certified robustness of our watermark algorithm, we conducted comprehensive experiments. The results indicate that our watermark algorithm shows comparable performance to baseline algorithms while our algorithm can derive substantial certified robustness, which means that our watermark can not be removed even under significant alterations.\n## Related work\nAI Generated Text Identification. While large language models are applied to various scenarios with their increasingly powerful text generation capabilities, such as story generation [52], article translation [59] and code assistance [53], the potential risks they bring [7,28,50] have made the identi- Direct detection methods often identify AI-generated text by uncovering distinguishing characteristics between AIgenerated and human-written texts or utilizing an extensive dataset of AI-generated text to train a accurate text classifier for AI-generated content. For example, regarding the first kind of method, Hamed and Wu [1] discovered that the bigram similarity of texts generated by ChatGPT is higher than that of human-written texts and developed a corresponding detection method based on this finding. Mitchell et al. found that AIgenerated texts tend to occupy the negative curvature region of the model's log-probability function. Based on this observation they proposed DetectGPT [37]. Building on Mitchell et al.'s approach, Bao et al. [2] improved the perturbation step in their method, significantly enhancing both its accuracy and speed. However, with the rapid advancement in both the size of model and text generation capabilities of LLMs, the gap between AI-generated texts and human-written texts has been narrowing [16,35,38,39]. As a result, methods based on text characteristics are becoming increasingly less effective.For the second type of method, Mindner et al. [36] employed multidimensional text feature extraction approaches to construct a classifier, with the best classifier outperforming GPTZero [14] in terms of F1-score. Chen et al. [6] and Liu et al. [32] utilized the advanced language understanding capabilities of pretrained LLMs [33,43], finetuning them as binary classifiers on various text datasets for AI-generated text detection. While these methods perform well on their respective test datasets, their effectiveness may be limited when applied to texts generated by newly emerging models. Likewise, as the capabilities of large language models continue to advance, their effectiveness remains a question. \n\nWatermarking, as an alternative method for AI-generated text identification, is generally more effective, versatile, and interpretable compared to direct detection.",
            "reference_string": "[272987651 | Feng et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Studying the impacts of pre-training using ChatGPT-generated text on downstream tasks",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 60,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.05668",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.05668, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313731991",
                    "name": "Sarthak Anand"
                }
            ],
            "abstract": "In recent times, significant advancements have been witnessed in the field of language models, particularly with the emergence of Large Language Models (LLMs) that are trained on vast amounts of data extracted from internet archives. These LLMs, such as ChatGPT, have become widely accessible, allowing users to generate text for various purposes including articles, essays, jokes, and poetry. Given that LLMs are trained on a diverse range of text sources, encompassing platforms like Reddit and Twitter, it is foreseeable that future training datasets will also incorporate text generated by previous iterations of the models themselves. In light of this development, our research aims to investigate the influence of artificial text in the pre-training phase of language models. Specifically, we conducted a comparative analysis between a language model, RoBERTa, pre-trained using CNN/DailyMail news articles, and ChatGPT, which employed the same articles for its training and evaluated their performance on three downstream tasks as well as their potential gender bias, using sentiment analysis as a metric. Through a series of experiments, we demonstrate that the utilization of artificial text during pre-training does not have a significant impact on either the performance of the models in downstream tasks or their gender bias. In conclusion, our findings suggest that the inclusion of text generated by LLMs in their own pre-training process does not yield substantial effects on the subsequent performance of the models in downstream tasks or their potential gender bias.",
            "corpus_id": 261697402,
            "sentences": [
                {
                    "corpus_id": "261697402",
                    "title": "Studying the impacts of pre-training using ChatGPT-generated text on downstream tasks",
                    "text": "Watermarking in text generation is a technique that introduces hidden signals into generated text, which can be used to detect whether the text is artificial. In this approach, the language model generates a probability distribution for the next word based on a given sequence of words. The next word is then randomly selected from this distribution. The watermarking technique operates by pseudo-randomly sampling words from the distribution, thereby enabling the detection of text generated by the language model.\n\nIn a recent study conducted by Kirchenbauer et al. [33], the effectiveness of the watermarking technique was evaluated on a multi-billion parameter model from the Open Pre-trained Transformer (OPT) family. The proposed method involved the selection of randomized sets of green tokens (allowed tokens) and red tokens (restricted tokens) during the generation process. The use of green tokens was promoted while sampling, while red tokens were discouraged. This approach facilitated the detection of artificial text by counting the occurrences of red tokens. In a similar direction, Ope-nAI has also expressed intentions to incorporate watermarking into its language models.\n\nAs outlined in a blog post by Scott Aaronson 6 , OpenAI plans to use cryptographic pseudo-random functions to sample words and generate detectors using cryptographic keys. This approach aims to embed watermarks into the language model, allowing for the identification of artificial text.\n\nIn practice, evading watermarking detection in language models can be achieved through various techniques, such as inserting or deleting words randomly, rearranging sentence order, or paraphrasing text. These methods pose challenges to the effectiveness of watermarking as a detection mechanism. Additionally, the responsibility of embedding watermarking lies with the developer of the language model, and only they have the ability to build the corresponding detection models. This limitation means that watermarking can only mitigate the potential harms of language models to a certain extent, as it relies on the actions and choices of the developer.",
                    "score": 0.7450437969617514,
                    "section_title": "Watermarking in Language Models",
                    "char_start_offset": 18120,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95166015625
                }
            ],
            "relevance_judgement": 0.95166015625,
            "relevance_judgment_input_expanded": "# Title: Studying the impacts of pre-training using ChatGPT-generated text on downstream tasks\n# Venue: arXiv.org\n# Authors: Sarthak Anand\n## Abstract\nIn recent times, significant advancements have been witnessed in the field of language models, particularly with the emergence of Large Language Models (LLMs) that are trained on vast amounts of data extracted from internet archives. These LLMs, such as ChatGPT, have become widely accessible, allowing users to generate text for various purposes including articles, essays, jokes, and poetry. Given that LLMs are trained on a diverse range of text sources, encompassing platforms like Reddit and Twitter, it is foreseeable that future training datasets will also incorporate text generated by previous iterations of the models themselves. In light of this development, our research aims to investigate the influence of artificial text in the pre-training phase of language models. Specifically, we conducted a comparative analysis between a language model, RoBERTa, pre-trained using CNN/DailyMail news articles, and ChatGPT, which employed the same articles for its training and evaluated their performance on three downstream tasks as well as their potential gender bias, using sentiment analysis as a metric. Through a series of experiments, we demonstrate that the utilization of artificial text during pre-training does not have a significant impact on either the performance of the models in downstream tasks or their gender bias. In conclusion, our findings suggest that the inclusion of text generated by LLMs in their own pre-training process does not yield substantial effects on the subsequent performance of the models in downstream tasks or their potential gender bias.\n## Watermarking in Language Models\nWatermarking in text generation is a technique that introduces hidden signals into generated text, which can be used to detect whether the text is artificial. In this approach, the language model generates a probability distribution for the next word based on a given sequence of words. The next word is then randomly selected from this distribution. The watermarking technique operates by pseudo-randomly sampling words from the distribution, thereby enabling the detection of text generated by the language model.\n\nIn a recent study conducted by Kirchenbauer et al. [33], the effectiveness of the watermarking technique was evaluated on a multi-billion parameter model from the Open Pre-trained Transformer (OPT) family. The proposed method involved the selection of randomized sets of green tokens (allowed tokens) and red tokens (restricted tokens) during the generation process. The use of green tokens was promoted while sampling, while red tokens were discouraged. This approach facilitated the detection of artificial text by counting the occurrences of red tokens. In a similar direction, Ope-nAI has also expressed intentions to incorporate watermarking into its language models.\n\nAs outlined in a blog post by Scott Aaronson 6 , OpenAI plans to use cryptographic pseudo-random functions to sample words and generate detectors using cryptographic keys. This approach aims to embed watermarks into the language model, allowing for the identification of artificial text.\n\nIn practice, evading watermarking detection in language models can be achieved through various techniques, such as inserting or deleting words randomly, rearranging sentence order, or paraphrasing text. These methods pose challenges to the effectiveness of watermarking as a detection mechanism. Additionally, the responsibility of embedding watermarking lies with the developer of the language model, and only they have the ability to build the corresponding detection models. This limitation means that watermarking can only mitigate the potential harms of language models to a certain extent, as it relies on the actions and choices of the developer.",
            "reference_string": "[261697402 | Anand | 2023 | Citations: 0]"
        },
        {
            "title": "A Practical Examination of AI-Generated Text Detectors for Large Language Models",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 68,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.05139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2334353491",
                    "name": "Brian Tufts"
                },
                {
                    "authorId": "150345512",
                    "name": "Xuandong Zhao"
                },
                {
                    "authorId": "2257344711",
                    "name": "Lei Li"
                }
            ],
            "abstract": "The proliferation of large language models has raised growing concerns about their misuse, particularly in cases where AI-generated text is falsely attributed to human authors. Machine-generated content detectors claim to effectively identify such text under various conditions and from any language model. This paper critically evaluates these claims by assessing several popular detectors (RADAR, Wild, T5Sentinel, Fast-DetectGPT, PHD, LogRank, Binoculars) on a range of domains, datasets, and models that these detectors have not previously encountered. We employ various prompting strategies to simulate practical adversarial attacks, demonstrating that even moderate efforts can significantly evade detection. We emphasize the importance of the true positive rate at a specific false positive rate (TPR@FPR) metric and demonstrate that these detectors perform poorly in certain settings, with TPR@.01 as low as 0%. Our findings suggest that both trained and zero-shot detectors struggle to maintain high sensitivity while achieving a reasonable true positive rate.",
            "corpus_id": 274581321,
            "sentences": [
                {
                    "corpus_id": "274581321",
                    "title": "A Practical Examination of AI-Generated Text Detectors for Large Language Models",
                    "text": "Large language models (LLMs) are becoming increasingly accessible and powerful, leading to numerous beneficial applications (Touvron et al., 2023;Achiam et al., 2023). However, they also pose risks if used maliciously, such as generating fake news articles, facilitating academic plagiarism or spam content (Feng et al., 2024;Zellers et al., 2019b;Perkins, 2023;Fraser et al., 2024). The potential for misuse of LLMs has become a significant concern for major tech corporations, particularly in light of the 2024 elections in the united states. At the Munich Security Conference on February 16th, 2024, these companies pledged to combat misleading machine-generated content, acknowledging the potential of AI to deceptively influence electoral outcomes (Accord, 2024). As a result, there is a growing need to develop reliable methods for differentiating between LLM-generated and human-written content. To ensure the effectiveness and accountability of LLM detection methods, continuous evaluation of popular techniques is crucial. \n\nMany methods have been released recently that claim to have a strong ability to detect the difference between AI-generated and human-generated texts. These detectors primarily fall into three categories: trained detectors, zero-shot detectors, and watermarking techniques (Yang et al., 2023b;Ghosal et al., 2023;Tang et al., 2023). Trained detectors utilize datasets of human and AI-generated texts and train a binary classification model to detect the source of a text (Zellers et al., 2019b;Hovy, 2016;Hu et al., 2023;Tian and Cui, 2023;Verma et al., 2024). Zero-shot detection utilizes a language model's inherent traits to identify text it generates, without explicit training for detection tasks other than calibrating a threshold for detection in some cases (Gehrmann et al., 2019;Mitchell et al., 2023;Bao et al., 2024;Yang et al., 2023a;Venkatraman et al., 2024).",
                    "score": 0.7566256074483333,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 168,
                            "end": 383
                        },
                        {
                            "start": 384,
                            "end": 544
                        },
                        {
                            "start": 545,
                            "end": 768
                        },
                        {
                            "start": 769,
                            "end": 902
                        },
                        {
                            "start": 903,
                            "end": 1031
                        },
                        {
                            "start": 1034,
                            "end": 1183
                        },
                        {
                            "start": 1184,
                            "end": 1365
                        },
                        {
                            "start": 1366,
                            "end": 1593
                        },
                        {
                            "start": 1594,
                            "end": 1905
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 326,
                            "end": 348,
                            "matchedPaperCorpusId": "168169824"
                        },
                        {
                            "start": 348,
                            "end": 362,
                            "matchedPaperCorpusId": "257166266"
                        },
                        {
                            "start": 1306,
                            "end": 1326,
                            "matchedPaperCorpusId": "264439179"
                        },
                        {
                            "start": 1326,
                            "end": 1346,
                            "matchedPaperCorpusId": "264439054"
                        },
                        {
                            "start": 1504,
                            "end": 1527,
                            "matchedPaperCorpusId": "168169824"
                        },
                        {
                            "start": 1527,
                            "end": 1538,
                            "matchedPaperCorpusId": "5315990"
                        },
                        {
                            "start": 1538,
                            "end": 1554,
                            "matchedPaperCorpusId": "259501842"
                        },
                        {
                            "start": 1573,
                            "end": 1592,
                            "matchedPaperCorpusId": "258865787"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94873046875
                }
            ],
            "relevance_judgement": 0.94873046875,
            "relevance_judgment_input_expanded": "# Title: A Practical Examination of AI-Generated Text Detectors for Large Language Models\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Brian Tufts, Xuandong Zhao, Lei Li\n## Abstract\nThe proliferation of large language models has raised growing concerns about their misuse, particularly in cases where AI-generated text is falsely attributed to human authors. Machine-generated content detectors claim to effectively identify such text under various conditions and from any language model. This paper critically evaluates these claims by assessing several popular detectors (RADAR, Wild, T5Sentinel, Fast-DetectGPT, PHD, LogRank, Binoculars) on a range of domains, datasets, and models that these detectors have not previously encountered. We employ various prompting strategies to simulate practical adversarial attacks, demonstrating that even moderate efforts can significantly evade detection. We emphasize the importance of the true positive rate at a specific false positive rate (TPR@FPR) metric and demonstrate that these detectors perform poorly in certain settings, with TPR@.01 as low as 0%. Our findings suggest that both trained and zero-shot detectors struggle to maintain high sensitivity while achieving a reasonable true positive rate.\n## Introduction\nLarge language models (LLMs) are becoming increasingly accessible and powerful, leading to numerous beneficial applications (Touvron et al., 2023;Achiam et al., 2023). However, they also pose risks if used maliciously, such as generating fake news articles, facilitating academic plagiarism or spam content (Feng et al., 2024;Zellers et al., 2019b;Perkins, 2023;Fraser et al., 2024). The potential for misuse of LLMs has become a significant concern for major tech corporations, particularly in light of the 2024 elections in the united states. At the Munich Security Conference on February 16th, 2024, these companies pledged to combat misleading machine-generated content, acknowledging the potential of AI to deceptively influence electoral outcomes (Accord, 2024). As a result, there is a growing need to develop reliable methods for differentiating between LLM-generated and human-written content. To ensure the effectiveness and accountability of LLM detection methods, continuous evaluation of popular techniques is crucial. \n\nMany methods have been released recently that claim to have a strong ability to detect the difference between AI-generated and human-generated texts. These detectors primarily fall into three categories: trained detectors, zero-shot detectors, and watermarking techniques (Yang et al., 2023b;Ghosal et al., 2023;Tang et al., 2023). Trained detectors utilize datasets of human and AI-generated texts and train a binary classification model to detect the source of a text (Zellers et al., 2019b;Hovy, 2016;Hu et al., 2023;Tian and Cui, 2023;Verma et al., 2024). Zero-shot detection utilizes a language model's inherent traits to identify text it generates, without explicit training for detection tasks other than calibrating a threshold for detection in some cases (Gehrmann et al., 2019;Mitchell et al., 2023;Bao et al., 2024;Yang et al., 2023a;Venkatraman et al., 2024).",
            "reference_string": "[274581321 | Tufts et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Ghostbuster: Detecting Text Ghostwritten by Large Language Models",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 30,
            "citation_count": 60,
            "influential_citation_count": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.15047",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.15047, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2143413953",
                    "name": "V. Verma"
                },
                {
                    "authorId": "1988380666",
                    "name": "Eve Fleisig"
                },
                {
                    "authorId": "1391385991",
                    "name": "Nicholas Tomlin"
                },
                {
                    "authorId": "38666915",
                    "name": "D. Klein"
                }
            ],
            "abstract": "We introduce Ghostbuster, a state-of-the-art system for detecting AI-generated text.Our method works by passing documents through a series of weaker language models, running a structured search over possible combinations of their features, and then training a classifier on the selected features to predict whether documents are AI-generated.Crucially, Ghostbuster does not require access to token probabilities from the target model, making it useful for detecting text generated by black-box or unknown models.In conjunction with our model, we release three new datasets of human- and AI-generated text as detection benchmarks in the domains of student essays, creative writing, and news articles. We compare Ghostbuster to several existing detectors, including DetectGPT and GPTZero, as well as a new RoBERTa baseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is 5.9 F1 higher than the best preexisting model. It also outperforms all previous approaches in generalization across writing domains (+7.5 F1), prompting strategies (+2.1 F1), and language models (+4.4 F1). We also analyze our system\u2019s robustness to a variety of perturbations and paraphrasing attacks, and evaluate its performance on documents by non-native English speakers.",
            "corpus_id": 258865787,
            "sentences": [
                {
                    "corpus_id": "258865787",
                    "title": "Ghostbuster: Detecting Text Ghostwritten by Large Language Models",
                    "text": "We introduce Ghostbuster, a state-of-the-art system for detecting AI-generated text.Our method works by passing documents through a series of weaker language models, running a structured search over possible combinations of their features, and then training a classifier on the selected features to predict whether documents are AI-generated.Crucially, Ghostbuster does not require access to token probabilities from the target model, making it useful for detecting text generated by black-box or unknown models.In conjunction with our model, we release three new datasets of human- and AI-generated text as detection benchmarks in the domains of student essays, creative writing, and news articles. We compare Ghostbuster to several existing detectors, including DetectGPT and GPTZero, as well as a new RoBERTa baseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is 5.9 F1 higher than the best preexisting model. It also outperforms all previous approaches in generalization across writing domains (+7.5 F1), prompting strategies (+2.1 F1), and language models (+4.4 F1). We also analyze our system\u2019s robustness to a variety of perturbations and paraphrasing attacks, and evaluate its performance on documents by non-native English speakers.",
                    "score": 0.6100916446386753,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9482421875
                }
            ],
            "relevance_judgement": 0.9482421875,
            "relevance_judgment_input_expanded": "# Title: Ghostbuster: Detecting Text Ghostwritten by Large Language Models\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: V. Verma, Eve Fleisig, Nicholas Tomlin, D. Klein\n## Abstract\nWe introduce Ghostbuster, a state-of-the-art system for detecting AI-generated text.Our method works by passing documents through a series of weaker language models, running a structured search over possible combinations of their features, and then training a classifier on the selected features to predict whether documents are AI-generated.Crucially, Ghostbuster does not require access to token probabilities from the target model, making it useful for detecting text generated by black-box or unknown models.In conjunction with our model, we release three new datasets of human- and AI-generated text as detection benchmarks in the domains of student essays, creative writing, and news articles. We compare Ghostbuster to several existing detectors, including DetectGPT and GPTZero, as well as a new RoBERTa baseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is 5.9 F1 higher than the best preexisting model. It also outperforms all previous approaches in generalization across writing domains (+7.5 F1), prompting strategies (+2.1 F1), and language models (+4.4 F1). We also analyze our system\u2019s robustness to a variety of perturbations and paraphrasing attacks, and evaluate its performance on documents by non-native English speakers.\n",
            "reference_string": "[258865787 | Verma et al. | 2023 | Citations: 60]"
        },
        {
            "title": "Detecting Stylistic Fingerprints of Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 30,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.01659, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1405506735",
                    "name": "Yehonatan Bitton"
                },
                {
                    "authorId": "2348468222",
                    "name": "Elad Bitton"
                },
                {
                    "authorId": "2348465980",
                    "name": "Shai Nisan"
                }
            ],
            "abstract": "Large language models (LLMs) have distinct and consistent stylistic fingerprints, even when prompted to write in different writing styles. Detecting these fingerprints is important for many reasons, among them protecting intellectual property, ensuring transparency regarding AI-generated content, and preventing the misuse of AI technologies. In this paper, we present a novel method to classify texts based on the stylistic fingerprints of the models that generated them. We introduce an LLM-detection ensemble that is composed of three classifiers with varied architectures and training data. This ensemble is trained to classify texts generated by four well-known LLM families: Claude, Gemini, Llama, and OpenAI. As this task is highly cost-sensitive and might have severe implications, we want to minimize false-positives and increase confidence. We consider a prediction as valid when all three classifiers in the ensemble unanimously agree on the output classification. Our ensemble is validated on a test set of texts generated by Claude, Gemini, Llama, and OpenAI models, and achieves extremely high precision (0.9988) and a very low false-positive rate (0.0004). Furthermore, we demonstrate the ensemble's ability to distinguish between texts generated by seen and unseen models. This reveals interesting stylistic relationships between models. This approach to stylistic analysis has implications for verifying the originality of AI-generated texts and tracking the origins of model training techniques.",
            "corpus_id": 276773353,
            "sentences": [
                {
                    "corpus_id": "276773353",
                    "title": "Detecting Stylistic Fingerprints of Large Language Models",
                    "text": "The task of detecting AI-generated text is a classification task. In most cases, it is usually treated as a binary classification problem -AI or human -but could also be a multi-class problem if one wishes to differentiate between specific AI text generators. The task becomes even more complex when a text is written with a combination of several AI models, or using AI and human authors, requiring the identification of each contributor. [20] In most cases, detecting AI-generated text is done using supervised learning frameworks. This requires labeled examples to train a classifier. The main metrics that are used are true-positive rates, true-negative rates, accuracy and F1 score. Classical approaches applied feature extraction on the texts in order to quantify stylistic or syntactic features such as perplexity, burstiness, and word density. These features are then fed into machine learning algorithms such as logistic regression or SVM (which provide an advantage of explainability), or into \"deep\" neural networks such as LSTMs. More modern approaches use pre-trained language models such as BERT, and fine-tune them for the specific classification task. This eliminates the need to manually define a list of features, and overall presents a much higher accuracy compared to feature-based methods, mainly because this approach is able to capture more general elements and nuances of the writing style, and is less dependent on a specific domain or topic. [20,21,22,23] Approaches that rely solely on capturing the differences between AI and human texts operate under the assumption that such differences exist; however, in reality, the differences between machine and human text may diminish over time as these systems evolve. A \"watermarking\" approach has recently emerged in an attempt to overcome these shortcomings. \"Watermarks\" can be added to the text during the generation phase (generative watermarking) or after it (edit-based watermarking). [22] However, this approach has its own limitations, such as lower effectiveness on factual responses (as there are fewer options to control the output), and higher sensitivity to small edits of the watermarked text. 3",
                    "score": 0.7176560559012303,
                    "section_title": "AI Forensics",
                    "char_start_offset": 2398,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 65
                        },
                        {
                            "start": 66,
                            "end": 259
                        },
                        {
                            "start": 260,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 533
                        },
                        {
                            "start": 534,
                            "end": 587
                        },
                        {
                            "start": 588,
                            "end": 687
                        },
                        {
                            "start": 688,
                            "end": 851
                        },
                        {
                            "start": 852,
                            "end": 1041
                        },
                        {
                            "start": 1042,
                            "end": 1167
                        },
                        {
                            "start": 1168,
                            "end": 1467
                        },
                        {
                            "start": 1468,
                            "end": 1739
                        },
                        {
                            "start": 1740,
                            "end": 1832
                        },
                        {
                            "start": 1833,
                            "end": 1963
                        },
                        {
                            "start": 1964,
                            "end": 2182
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1472,
                            "end": 1475,
                            "matchedPaperCorpusId": "275994170"
                        },
                        {
                            "start": 1475,
                            "end": 1478,
                            "matchedPaperCorpusId": "273547059"
                        },
                        {
                            "start": 1478,
                            "end": 1481,
                            "matchedPaperCorpusId": "276142840"
                        },
                        {
                            "start": 1964,
                            "end": 1968,
                            "matchedPaperCorpusId": "273547059"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94775390625
                }
            ],
            "relevance_judgement": 0.94775390625,
            "relevance_judgment_input_expanded": "# Title: Detecting Stylistic Fingerprints of Large Language Models\n# Venue: arXiv.org\n# Authors: Yehonatan Bitton, Elad Bitton, Shai Nisan\n## Abstract\nLarge language models (LLMs) have distinct and consistent stylistic fingerprints, even when prompted to write in different writing styles. Detecting these fingerprints is important for many reasons, among them protecting intellectual property, ensuring transparency regarding AI-generated content, and preventing the misuse of AI technologies. In this paper, we present a novel method to classify texts based on the stylistic fingerprints of the models that generated them. We introduce an LLM-detection ensemble that is composed of three classifiers with varied architectures and training data. This ensemble is trained to classify texts generated by four well-known LLM families: Claude, Gemini, Llama, and OpenAI. As this task is highly cost-sensitive and might have severe implications, we want to minimize false-positives and increase confidence. We consider a prediction as valid when all three classifiers in the ensemble unanimously agree on the output classification. Our ensemble is validated on a test set of texts generated by Claude, Gemini, Llama, and OpenAI models, and achieves extremely high precision (0.9988) and a very low false-positive rate (0.0004). Furthermore, we demonstrate the ensemble's ability to distinguish between texts generated by seen and unseen models. This reveals interesting stylistic relationships between models. This approach to stylistic analysis has implications for verifying the originality of AI-generated texts and tracking the origins of model training techniques.\n## AI Forensics\nThe task of detecting AI-generated text is a classification task. In most cases, it is usually treated as a binary classification problem -AI or human -but could also be a multi-class problem if one wishes to differentiate between specific AI text generators. The task becomes even more complex when a text is written with a combination of several AI models, or using AI and human authors, requiring the identification of each contributor. [20] In most cases, detecting AI-generated text is done using supervised learning frameworks. This requires labeled examples to train a classifier. The main metrics that are used are true-positive rates, true-negative rates, accuracy and F1 score. Classical approaches applied feature extraction on the texts in order to quantify stylistic or syntactic features such as perplexity, burstiness, and word density. These features are then fed into machine learning algorithms such as logistic regression or SVM (which provide an advantage of explainability), or into \"deep\" neural networks such as LSTMs. More modern approaches use pre-trained language models such as BERT, and fine-tune them for the specific classification task. This eliminates the need to manually define a list of features, and overall presents a much higher accuracy compared to feature-based methods, mainly because this approach is able to capture more general elements and nuances of the writing style, and is less dependent on a specific domain or topic. [20,21,22,23] Approaches that rely solely on capturing the differences between AI and human texts operate under the assumption that such differences exist; however, in reality, the differences between machine and human text may diminish over time as these systems evolve. A \"watermarking\" approach has recently emerged in an attempt to overcome these shortcomings. \"Watermarks\" can be added to the text during the generation phase (generative watermarking) or after it (edit-based watermarking). [22] However, this approach has its own limitations, such as lower effectiveness on factual responses (as there are fewer options to control the output), and higher sensitivity to small edits of the watermarked text. 3",
            "reference_string": "[276773353 | Bitton et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Red Teaming Language Model Detectors with Language Models",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 47,
            "citation_count": 56,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1162/tacl_a_00639",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.19713, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2987927",
                    "name": "Zhouxing Shi"
                },
                {
                    "authorId": "2108927851",
                    "name": "Yihan Wang"
                },
                {
                    "authorId": "2065089223",
                    "name": "Fan Yin"
                },
                {
                    "authorId": "2143737082",
                    "name": "Xiangning Chen"
                },
                {
                    "authorId": "2782886",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "1793529",
                    "name": "Cho-Jui Hsieh"
                }
            ],
            "abstract": "The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users. To prevent the potentially deceptive usage of LLMs, recent work has proposed algorithms to detect LLM-generated text and protect LLMs. In this paper, we investigate the robustness and reliability of these LLM detectors under adversarial attacks. We study two types of attack strategies: 1) replacing certain words in an LLM\u2019s output with their synonyms given the context; 2) automatically searching for an instructional prompt to alter the writing style of the generation. In both strategies, we leverage an auxiliary LLM to generate the word replacements or the instructional prompt. Different from previous works, we consider a challenging setting where the auxiliary LLM can also be protected by a detector. Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems. Code is available at https://github.com/shizhouxing/LLM-Detector-Robustness.",
            "corpus_id": 258987266,
            "sentences": [
                {
                    "corpus_id": "258987266",
                    "title": "Red Teaming Language Model Detectors with Language Models",
                    "text": "Detectors for AI-generated text. Recent detectors for AI-generated text mostly fall into three categories. First, classifier-based detectors are trained with labeled data to distinguish human-written text and AI-generated text. For example, the AI Text Classifier developed by OpenAI (OpenAI, 2023a) is a fine-tuned language model. Second, watermarking methods introduce distinct patterns into AI-generated text, allowing for its identification. Among them, Kirchenbauer et al. (2023) randomly partition the vocabulary into a greenlist and a redlist during the generation, where the division is based on the hash of the previously generated tokens. The language model only uses words in the greenlists, and thereby the generated text has a different pattern compared to human-written text which does not consider such greenlists and redlists. Third, De-tectGPT (Mitchell et al., 2023) uses the likelihood of the generated text for the detection, as they find that text generated by language models tends to reside in the negative curvature region of the log probability function. Consequently, they define a curvature-based criterion for the detection. \n\nMethods for red-teaming detectors. As the detectors emerge, several concurrent works showed that the detectors may be evaded to some extent, typically by paraphrasing the text (Sadasivan et al., 2023;Krishna et al., 2023). However, they need additional paraphrasing models which are typically unprotected models that are much weaker than the original LLM. Besides paraphrasing, Kirchenbauer et al. (2023) also discussed attacks against watermarking detectors with word substitutions generated by a masked language model such as T5 (Raffel et al., 2020) which is a relatively weaker language model and tends to generate results with lower quality, and thus it may generate attacks with lower quality. On the other hand, Chakraborty et al. (2023) analyzed the possibilities of the detection given sufficiently many samples. \n\nAdversarial examples in NLP. Word substitution is a commonly used strategy in generating textual adversarial examples (Alzantot et al., 2018;Ren et al., 2019;Jin et al., 2020).",
                    "score": 0.6548822015827422,
                    "section_title": "Related Work",
                    "char_start_offset": 4423,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 32
                        },
                        {
                            "start": 33,
                            "end": 106
                        },
                        {
                            "start": 107,
                            "end": 227
                        },
                        {
                            "start": 228,
                            "end": 331
                        },
                        {
                            "start": 332,
                            "end": 445
                        },
                        {
                            "start": 446,
                            "end": 648
                        },
                        {
                            "start": 649,
                            "end": 842
                        },
                        {
                            "start": 843,
                            "end": 1079
                        },
                        {
                            "start": 1080,
                            "end": 1152
                        },
                        {
                            "start": 1155,
                            "end": 1189
                        },
                        {
                            "start": 1190,
                            "end": 1377
                        },
                        {
                            "start": 1378,
                            "end": 1510
                        },
                        {
                            "start": 1511,
                            "end": 1854
                        },
                        {
                            "start": 1855,
                            "end": 1976
                        },
                        {
                            "start": 1979,
                            "end": 2007
                        },
                        {
                            "start": 2008,
                            "end": 2155
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 458,
                            "end": 484,
                            "matchedPaperCorpusId": "256194179"
                        },
                        {
                            "start": 1533,
                            "end": 1559,
                            "matchedPaperCorpusId": "256194179"
                        },
                        {
                            "start": 1686,
                            "end": 1707,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9453125
                }
            ],
            "relevance_judgement": 0.9453125,
            "relevance_judgment_input_expanded": "# Title: Red Teaming Language Model Detectors with Language Models\n# Venue: Transactions of the Association for Computational Linguistics\n# Authors: Zhouxing Shi, Yihan Wang, Fan Yin, Xiangning Chen, Kai-Wei Chang, Cho-Jui Hsieh\n## Abstract\nThe prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users. To prevent the potentially deceptive usage of LLMs, recent work has proposed algorithms to detect LLM-generated text and protect LLMs. In this paper, we investigate the robustness and reliability of these LLM detectors under adversarial attacks. We study two types of attack strategies: 1) replacing certain words in an LLM\u2019s output with their synonyms given the context; 2) automatically searching for an instructional prompt to alter the writing style of the generation. In both strategies, we leverage an auxiliary LLM to generate the word replacements or the instructional prompt. Different from previous works, we consider a challenging setting where the auxiliary LLM can also be protected by a detector. Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems. Code is available at https://github.com/shizhouxing/LLM-Detector-Robustness.\n## Related Work\nDetectors for AI-generated text. Recent detectors for AI-generated text mostly fall into three categories. First, classifier-based detectors are trained with labeled data to distinguish human-written text and AI-generated text. For example, the AI Text Classifier developed by OpenAI (OpenAI, 2023a) is a fine-tuned language model. Second, watermarking methods introduce distinct patterns into AI-generated text, allowing for its identification. Among them, Kirchenbauer et al. (2023) randomly partition the vocabulary into a greenlist and a redlist during the generation, where the division is based on the hash of the previously generated tokens. The language model only uses words in the greenlists, and thereby the generated text has a different pattern compared to human-written text which does not consider such greenlists and redlists. Third, De-tectGPT (Mitchell et al., 2023) uses the likelihood of the generated text for the detection, as they find that text generated by language models tends to reside in the negative curvature region of the log probability function. Consequently, they define a curvature-based criterion for the detection. \n\nMethods for red-teaming detectors. As the detectors emerge, several concurrent works showed that the detectors may be evaded to some extent, typically by paraphrasing the text (Sadasivan et al., 2023;Krishna et al., 2023). However, they need additional paraphrasing models which are typically unprotected models that are much weaker than the original LLM. Besides paraphrasing, Kirchenbauer et al. (2023) also discussed attacks against watermarking detectors with word substitutions generated by a masked language model such as T5 (Raffel et al., 2020) which is a relatively weaker language model and tends to generate results with lower quality, and thus it may generate attacks with lower quality. On the other hand, Chakraborty et al. (2023) analyzed the possibilities of the detection given sufficiently many samples. \n\nAdversarial examples in NLP. Word substitution is a commonly used strategy in generating textual adversarial examples (Alzantot et al., 2018;Ren et al., 2019;Jin et al., 2020).",
            "reference_string": "[258987266 | Shi et al. | 2023 | Citations: 56]"
        },
        {
            "title": "AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 35,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.07700, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2266505035",
                    "name": "Zhen Guo"
                },
                {
                    "authorId": "2266834515",
                    "name": "Shangdi Yu"
                }
            ],
            "abstract": "Large language models (LLMs) have opened up enormous opportunities while simultaneously posing ethical dilemmas. One of the major concerns is their ability to create text that closely mimics human writing, which can lead to potential misuse, such as academic misconduct, disinformation, and fraud. To address this problem, we present AuthentiGPT, an efficient classifier that distinguishes between machine-generated and human-written texts. Under the assumption that human-written text resides outside the distribution of machine-generated text, AuthentiGPT leverages a black-box LLM to denoise input text with artificially added noise, and then semantically compares the denoised text with the original to determine if the content is machine-generated. With only one trainable parameter, AuthentiGPT eliminates the need for a large training dataset, watermarking the LLM's output, or computing the log-likelihood. Importantly, the detection capability of AuthentiGPT can be easily adapted to any generative language model. With a 0.918 AUROC score on a domain-specific dataset, AuthentiGPT demonstrates its effectiveness over other commercial algorithms, highlighting its potential for detecting machine-generated text in academic settings.",
            "corpus_id": 265157663,
            "sentences": [
                {
                    "corpus_id": "265157663",
                    "title": "AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising",
                    "text": "Large language models (LLMs) have opened up enormous opportunities while simultaneously posing ethical dilemmas. One of the major concerns is their ability to create text that closely mimics human writing, which can lead to potential misuse, such as academic misconduct, disinformation, and fraud. To address this problem, we present AuthentiGPT, an efficient classifier that distinguishes between machine-generated and human-written texts. Under the assumption that human-written text resides outside the distribution of machine-generated text, AuthentiGPT leverages a black-box LLM to denoise input text with artificially added noise, and then semantically compares the denoised text with the original to determine if the content is machine-generated. With only one trainable parameter, AuthentiGPT eliminates the need for a large training dataset, watermarking the LLM's output, or computing the log-likelihood. Importantly, the detection capability of AuthentiGPT can be easily adapted to any generative language model. With a 0.918 AUROC score on a domain-specific dataset, AuthentiGPT demonstrates its effectiveness over other commercial algorithms, highlighting its potential for detecting machine-generated text in academic settings.",
                    "score": 0.6050454069353515,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9443359375
                }
            ],
            "relevance_judgement": 0.9443359375,
            "relevance_judgment_input_expanded": "# Title: AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising\n# Venue: arXiv.org\n# Authors: Zhen Guo, Shangdi Yu\n## Abstract\nLarge language models (LLMs) have opened up enormous opportunities while simultaneously posing ethical dilemmas. One of the major concerns is their ability to create text that closely mimics human writing, which can lead to potential misuse, such as academic misconduct, disinformation, and fraud. To address this problem, we present AuthentiGPT, an efficient classifier that distinguishes between machine-generated and human-written texts. Under the assumption that human-written text resides outside the distribution of machine-generated text, AuthentiGPT leverages a black-box LLM to denoise input text with artificially added noise, and then semantically compares the denoised text with the original to determine if the content is machine-generated. With only one trainable parameter, AuthentiGPT eliminates the need for a large training dataset, watermarking the LLM's output, or computing the log-likelihood. Importantly, the detection capability of AuthentiGPT can be easily adapted to any generative language model. With a 0.918 AUROC score on a domain-specific dataset, AuthentiGPT demonstrates its effectiveness over other commercial algorithms, highlighting its potential for detecting machine-generated text in academic settings.\n",
            "reference_string": "[265157663 | Guo et al. | 2023 | Citations: 11]"
        },
        {
            "title": "Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2023,
            "reference_count": 47,
            "citation_count": 37,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.13808",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.13808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2145124944",
                    "name": "Yu Fu"
                },
                {
                    "authorId": "2140188860",
                    "name": "Deyi Xiong"
                },
                {
                    "authorId": "49265991",
                    "name": "Yue Dong"
                }
            ],
            "abstract": "To mitigate potential risks associated with language models (LMs), recent AI detection research proposes incorporating watermarks into machine-generated text through random vocabulary restrictions and utilizing this information for detection. In this paper, we show that watermarking algorithms designed for LMs cannot be seamlessly applied to conditional text generation (CTG) tasks without a notable decline in downstream task performance. To address this issue, we introduce a simple yet effective semantic-aware watermarking algorithm that considers the characteristics of conditional text generation with the input context. Compared to the baseline watermarks, our proposed watermark yields significant improvements in both automatic and human evaluations across various text generation models, including BART and Flan-T5, for CTG tasks such as summarization and data-to-text generation. Meanwhile, it maintains detection ability with higher z-scores but lower AUC scores, suggesting the presence of a detection paradox that poses additional challenges for watermarking CTG.",
            "corpus_id": 260164516,
            "sentences": [
                {
                    "corpus_id": "260164516",
                    "title": "Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy",
                    "text": "Automatic Detection The detection of AI-generated text, particularly in the context of large language models (LLMs), has recently attracted significant research interest (Bakhtin et al. 2019;Schuster et al. 2020;Fr\u00f6hling and Zubiaga 2021;Sadasivan et al. 2023;Mitchell et al. 2023). Previous approaches have primarily focused on leveraging the perplexi-ties of generated texts for detection. For example, Solaiman et al. (2019) utilized a classifier to evaluate the total log probability of the text, using it as a means to determine whether the content originated from a machine. Building on this premise, Mitchell et al. (2023) further validated that the log probability of machine-generated text diminishes upon perturbation, while the log probability of human-written text remains unpredictable when perturbed. \n\nWatermarking There has been a recent emergence of watermarking specific patterns into language models for AI detection. Zhao, Wang, and Li (2023)",
                    "score": 0.6723599368141275,
                    "section_title": "Related Work",
                    "char_start_offset": 5305,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 282
                        },
                        {
                            "start": 283,
                            "end": 391
                        },
                        {
                            "start": 392,
                            "end": 580
                        },
                        {
                            "start": 581,
                            "end": 814
                        },
                        {
                            "start": 817,
                            "end": 936
                        },
                        {
                            "start": 937,
                            "end": 962
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 170,
                            "end": 191,
                            "matchedPaperCorpusId": "182952342"
                        },
                        {
                            "start": 191,
                            "end": 212,
                            "matchedPaperCorpusId": "211204954"
                        },
                        {
                            "start": 212,
                            "end": 238,
                            "matchedPaperCorpusId": "233563022"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94287109375
                }
            ],
            "relevance_judgement": 0.94287109375,
            "relevance_judgment_input_expanded": "# Title: Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Yu Fu, Deyi Xiong, Yue Dong\n## Abstract\nTo mitigate potential risks associated with language models (LMs), recent AI detection research proposes incorporating watermarks into machine-generated text through random vocabulary restrictions and utilizing this information for detection. In this paper, we show that watermarking algorithms designed for LMs cannot be seamlessly applied to conditional text generation (CTG) tasks without a notable decline in downstream task performance. To address this issue, we introduce a simple yet effective semantic-aware watermarking algorithm that considers the characteristics of conditional text generation with the input context. Compared to the baseline watermarks, our proposed watermark yields significant improvements in both automatic and human evaluations across various text generation models, including BART and Flan-T5, for CTG tasks such as summarization and data-to-text generation. Meanwhile, it maintains detection ability with higher z-scores but lower AUC scores, suggesting the presence of a detection paradox that poses additional challenges for watermarking CTG.\n## Related Work\nAutomatic Detection The detection of AI-generated text, particularly in the context of large language models (LLMs), has recently attracted significant research interest (Bakhtin et al. 2019;Schuster et al. 2020;Fr\u00f6hling and Zubiaga 2021;Sadasivan et al. 2023;Mitchell et al. 2023). Previous approaches have primarily focused on leveraging the perplexi-ties of generated texts for detection. For example, Solaiman et al. (2019) utilized a classifier to evaluate the total log probability of the text, using it as a means to determine whether the content originated from a machine. Building on this premise, Mitchell et al. (2023) further validated that the log probability of machine-generated text diminishes upon perturbation, while the log probability of human-written text remains unpredictable when perturbed. \n\nWatermarking There has been a recent emergence of watermarking specific patterns into language models for AI detection. Zhao, Wang, and Li (2023)",
            "reference_string": "[260164516 | Fu et al. | 2023 | Citations: 37]"
        },
        {
            "title": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 55,
            "citation_count": 20,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.16617",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.16617, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145114723",
                    "name": "Zhijie Deng"
                },
                {
                    "authorId": "2162081759",
                    "name": "Hongcheng Gao"
                },
                {
                    "authorId": "2188993538",
                    "name": "Yibo Miao"
                },
                {
                    "authorId": "46702482",
                    "name": "Hao Zhang"
                }
            ],
            "abstract": "The detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires querying the source LLM with hundreds of its perturbations. This paper aims to bridge this gap. Concretely, we propose to incorporate a Bayesian surrogate model, which allows us to select typical samples based on Bayesian uncertainty and interpolate scores from typical samples to other samples, to improve query efficiency. Empirical results demonstrate that our method significantly outperforms existing approaches under a low query budget. Notably, when detecting the text generated by LLaMA family models, our method with just 2 or 3 queries can outperform DetectGPT with 200 queries.",
            "corpus_id": 258947640,
            "sentences": [
                {
                    "corpus_id": "258947640",
                    "title": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model",
                    "text": "Large language models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022;OpenAI, 2022;Touvron et al., 2023) have the impressive ability to replicate human language patterns, producing text that appears coherent, well-written, and persuasive, although the generated text may contain factual errors and unsupported quotations. As LLMs are increasingly used to simplify writing and presentation tasks, some individuals regrettably have misused LLMs for nefarious purposes, such as creating convincing fake news articles or engaging in cheating, which can have significant social consequences. Mitigating these negative impacts has become a pressing issue for the community. \n\nThe LLM-generated texts are highly articulate, posing a significant challenge for humans in identifying them (Gehrmann et al., 2019b). Fortunately, it is shown that machine learning tools can be leveraged to recognize the watermarks underlying the texts. Some methods (e.g., OpenAI, 2023b) involve training supervised classifiers, which, yet, suffer from overfitting to the training data and ineffectiveness to generalize to new test data. Zero-shot LLM-generated text detection approaches bypass these issues by leveraging the source LLM to detect its samples (Solaiman et al., 2019;Gehrmann et al., 2019b;Ippolito et al., 2020). They usually proceed by inspecting the average per-token log probability of the candidate text, but the practical detection performance can be unsatisfactory. \n\nDetectGPT (Mitchell et al., 2023) is a recent method that achieves improved zero-shot detection efficacy by exploring the probability curvature of LLMs. It generates multiple perturbations of the candidate text and scores them using the source LLM to define detection statistics. It can detect texts generated by GPT-2 (Radford et al., 2019) and GPT-NeoX-20B (Black et al., 2022). Yet, DetectGPT relies on hundreds of queries to the source LLM to estimate the local probability curvature surrounding one single candidate passage.",
                    "score": 0.7628792424197927,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 323
                        },
                        {
                            "start": 324,
                            "end": 588
                        },
                        {
                            "start": 589,
                            "end": 669
                        },
                        {
                            "start": 672,
                            "end": 806
                        },
                        {
                            "start": 807,
                            "end": 926
                        },
                        {
                            "start": 927,
                            "end": 1111
                        },
                        {
                            "start": 1112,
                            "end": 1302
                        },
                        {
                            "start": 1303,
                            "end": 1461
                        },
                        {
                            "start": 1464,
                            "end": 1616
                        },
                        {
                            "start": 1617,
                            "end": 1743
                        },
                        {
                            "start": 1744,
                            "end": 1844
                        },
                        {
                            "start": 1845,
                            "end": 1993
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 29,
                            "end": 49,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 781,
                            "end": 805,
                            "matchedPaperCorpusId": "182952848"
                        },
                        {
                            "start": 1256,
                            "end": 1279,
                            "matchedPaperCorpusId": "182952848"
                        },
                        {
                            "start": 1279,
                            "end": 1301,
                            "matchedPaperCorpusId": "218560609"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9423828125
                }
            ],
            "relevance_judgement": 0.9423828125,
            "relevance_judgment_input_expanded": "# Title: Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Zhijie Deng, Hongcheng Gao, Yibo Miao, Hao Zhang\n## Abstract\nThe detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires querying the source LLM with hundreds of its perturbations. This paper aims to bridge this gap. Concretely, we propose to incorporate a Bayesian surrogate model, which allows us to select typical samples based on Bayesian uncertainty and interpolate scores from typical samples to other samples, to improve query efficiency. Empirical results demonstrate that our method significantly outperforms existing approaches under a low query budget. Notably, when detecting the text generated by LLaMA family models, our method with just 2 or 3 queries can outperform DetectGPT with 200 queries.\n## Introduction\nLarge language models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022;OpenAI, 2022;Touvron et al., 2023) have the impressive ability to replicate human language patterns, producing text that appears coherent, well-written, and persuasive, although the generated text may contain factual errors and unsupported quotations. As LLMs are increasingly used to simplify writing and presentation tasks, some individuals regrettably have misused LLMs for nefarious purposes, such as creating convincing fake news articles or engaging in cheating, which can have significant social consequences. Mitigating these negative impacts has become a pressing issue for the community. \n\nThe LLM-generated texts are highly articulate, posing a significant challenge for humans in identifying them (Gehrmann et al., 2019b). Fortunately, it is shown that machine learning tools can be leveraged to recognize the watermarks underlying the texts. Some methods (e.g., OpenAI, 2023b) involve training supervised classifiers, which, yet, suffer from overfitting to the training data and ineffectiveness to generalize to new test data. Zero-shot LLM-generated text detection approaches bypass these issues by leveraging the source LLM to detect its samples (Solaiman et al., 2019;Gehrmann et al., 2019b;Ippolito et al., 2020). They usually proceed by inspecting the average per-token log probability of the candidate text, but the practical detection performance can be unsatisfactory. \n\nDetectGPT (Mitchell et al., 2023) is a recent method that achieves improved zero-shot detection efficacy by exploring the probability curvature of LLMs. It generates multiple perturbations of the candidate text and scores them using the source LLM to define detection statistics. It can detect texts generated by GPT-2 (Radford et al., 2019) and GPT-NeoX-20B (Black et al., 2022). Yet, DetectGPT relies on hundreds of queries to the source LLM to estimate the local probability curvature surrounding one single candidate passage.",
            "reference_string": "[258947640 | Deng et al. | 2023 | Citations: 20]"
        },
        {
            "title": "AI Content Self-Detection for Transformer-based Large Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 21,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.17289, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2277214778",
                    "name": "Antonio Junior Alves Caiado"
                },
                {
                    "authorId": "2277214594",
                    "name": "Michael Hahsler"
                }
            ],
            "abstract": "$ $The usage of generative artificial intelligence (AI) tools based on large language models, including ChatGPT, Bard, and Claude, for text generation has many exciting applications with the potential for phenomenal productivity gains. One issue is authorship attribution when using AI tools. This is especially important in an academic setting where the inappropriate use of generative AI tools may hinder student learning or stifle research by creating a large amount of automatically generated derivative work. Existing plagiarism detection systems can trace the source of submitted text but are not yet equipped with methods to accurately detect AI-generated text. This paper introduces the idea of direct origin detection and evaluates whether generative AI systems can recognize their output and distinguish it from human-written texts. We argue why current transformer-based models may be able to self-detect their own generated text and perform a small empirical study using zero-shot learning to investigate if that is the case. Results reveal varying capabilities of AI systems to identify their generated text. Google's Bard model exhibits the largest capability of self-detection with an accuracy of 94\\%, followed by OpenAI's ChatGPT with 83\\%. On the other hand, Anthropic's Claude model seems to be not able to self-detect.",
            "corpus_id": 266690717,
            "sentences": [
                {
                    "corpus_id": "266690717",
                    "title": "AI Content Self-Detection for Transformer-based Large Language Models",
                    "text": "While detection of AI-generated content can be important in many settings, the emergence of generative AI creates especially complicated ethical challenges for academic integrity. Much work had already been done to detect plagiarism, which can lead to students not learning by copying assignment solutions or researchers taking credit for someone else's work and ideas. \n\nAI-generated content creates a new challenge since it does not directly copy existing content but generates new text. Traditional methods that identify similarities between a new document and a database of existing documents may fall short of distinguishing AI-generated content from new human work. Large language models aim to create natural, human-like text, making it increasingly hard to differentiate generated from human-created text. \n\nMany tools to detect AI-generated text are now offered. Some popular tools geared toward educators are Copyleaks AI Content Detector, Crossplag, GPTZero, Hugging Face OpenAI Detector, Originality.ai, Turnitin AI Detection and ZeroGPT. The list of detectors and their capability is constantly changing following the fast-paced changes seen in the development of large language models. \n\nMost tools are based on detecting artifacts of the text generation process, including word choice, writing style, sentence length, and many more. A report by Open AI [9] lays out three AI content detection strategies, including a simple classifier learned from scratch, a classifier resulting from fine-tuning an existing language model, or using the probabilities assigned by the model to strings. Many existing tools follow the first two approaches. For example, the Hugging Face Open AI detector is a transformer-based classifier that is fine-tuned to detect GPT-2 text. Self-detection introduced in this paper is most closely related to the third approach. However, it does not require access to the model parameters to assess probabilities. It relies on the model itself to perform the detection.",
                    "score": 0.6316391711236907,
                    "section_title": "B. Detection of AI-generated Text",
                    "char_start_offset": 4271,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 369
                        },
                        {
                            "start": 372,
                            "end": 489
                        },
                        {
                            "start": 490,
                            "end": 671
                        },
                        {
                            "start": 672,
                            "end": 813
                        },
                        {
                            "start": 816,
                            "end": 871
                        },
                        {
                            "start": 872,
                            "end": 1015
                        },
                        {
                            "start": 1016,
                            "end": 1050
                        },
                        {
                            "start": 1051,
                            "end": 1199
                        },
                        {
                            "start": 1202,
                            "end": 1347
                        },
                        {
                            "start": 1348,
                            "end": 1600
                        },
                        {
                            "start": 1601,
                            "end": 1653
                        },
                        {
                            "start": 1654,
                            "end": 1775
                        },
                        {
                            "start": 1776,
                            "end": 1862
                        },
                        {
                            "start": 1863,
                            "end": 1947
                        },
                        {
                            "start": 1948,
                            "end": 2003
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94189453125
                }
            ],
            "relevance_judgement": 0.94189453125,
            "relevance_judgment_input_expanded": "# Title: AI Content Self-Detection for Transformer-based Large Language Models\n# Venue: arXiv.org\n# Authors: Antonio Junior Alves Caiado, Michael Hahsler\n## Abstract\n$ $The usage of generative artificial intelligence (AI) tools based on large language models, including ChatGPT, Bard, and Claude, for text generation has many exciting applications with the potential for phenomenal productivity gains. One issue is authorship attribution when using AI tools. This is especially important in an academic setting where the inappropriate use of generative AI tools may hinder student learning or stifle research by creating a large amount of automatically generated derivative work. Existing plagiarism detection systems can trace the source of submitted text but are not yet equipped with methods to accurately detect AI-generated text. This paper introduces the idea of direct origin detection and evaluates whether generative AI systems can recognize their output and distinguish it from human-written texts. We argue why current transformer-based models may be able to self-detect their own generated text and perform a small empirical study using zero-shot learning to investigate if that is the case. Results reveal varying capabilities of AI systems to identify their generated text. Google's Bard model exhibits the largest capability of self-detection with an accuracy of 94\\%, followed by OpenAI's ChatGPT with 83\\%. On the other hand, Anthropic's Claude model seems to be not able to self-detect.\n## B. Detection of AI-generated Text\nWhile detection of AI-generated content can be important in many settings, the emergence of generative AI creates especially complicated ethical challenges for academic integrity. Much work had already been done to detect plagiarism, which can lead to students not learning by copying assignment solutions or researchers taking credit for someone else's work and ideas. \n\nAI-generated content creates a new challenge since it does not directly copy existing content but generates new text. Traditional methods that identify similarities between a new document and a database of existing documents may fall short of distinguishing AI-generated content from new human work. Large language models aim to create natural, human-like text, making it increasingly hard to differentiate generated from human-created text. \n\nMany tools to detect AI-generated text are now offered. Some popular tools geared toward educators are Copyleaks AI Content Detector, Crossplag, GPTZero, Hugging Face OpenAI Detector, Originality.ai, Turnitin AI Detection and ZeroGPT. The list of detectors and their capability is constantly changing following the fast-paced changes seen in the development of large language models. \n\nMost tools are based on detecting artifacts of the text generation process, including word choice, writing style, sentence length, and many more. A report by Open AI [9] lays out three AI content detection strategies, including a simple classifier learned from scratch, a classifier resulting from fine-tuning an existing language model, or using the probabilities assigned by the model to strings. Many existing tools follow the first two approaches. For example, the Hugging Face Open AI detector is a transformer-based classifier that is fine-tuned to detect GPT-2 text. Self-detection introduced in this paper is most closely related to the third approach. However, it does not require access to the model parameters to assess probabilities. It relies on the model itself to perform the detection.",
            "reference_string": "[266690717 | Caiado et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Sarang at DEFACTIFY 4.0: Detecting AI-Generated Text Using Noised Data and an Ensemble of DeBERTa Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 18,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.16857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2346209847",
                    "name": "Avinash Trivedi"
                },
                {
                    "authorId": "46505742",
                    "name": "S. Sangeetha"
                }
            ],
            "abstract": "This paper presents an effective approach to detect AI-generated text, developed for the Defactify 4.0 shared task at the fourth workshop on multimodal fact checking and hate speech detection. The task consists of two subtasks: Task-A, classifying whether a text is AI generated or human written, and Task-B, classifying the specific large language model that generated the text. Our team (Sarang) achieved the 1st place in both tasks with F1 scores of 1.0 and 0.9531, respectively. The methodology involves adding noise to the dataset to improve model robustness and generalization. We used an ensemble of DeBERTa models to effectively capture complex patterns in the text. The result indicates the effectiveness of our noise-driven and ensemble-based approach, setting a new standard in AI-generated text detection and providing guidance for future developments.",
            "corpus_id": 276574900,
            "sentences": [
                {
                    "corpus_id": "276574900",
                    "title": "Sarang at DEFACTIFY 4.0: Detecting AI-Generated Text Using Noised Data and an Ensemble of DeBERTa Models",
                    "text": "Large Language Models (LLMs), such as ChatGPT [1], are really good at writing long pieces of text that sound very human. While these developments have various beneficial applications, they also raise concerns about potential misuse, such as the automatic creation of fake news articles and academic contents [2]. To address these risks, various algorithms have been developed to detect AI-generated text, which include watermarking techniques [3], tools like GPTZero [4], DetectGPT [5], and OpenAI's text classifier [6]. \n\nThe task of detecting AI-generated text is inherently challenging, as recent research [7] highlights the increasing sophistication of newer, more capable LLMs. Early studies demonstrated that humans struggle to tell if something was written by a computer or a human. Given the ethical implications and the complexity of the problem, creating robust detection systems remains an active area of research. The Defactify 4.0 shared task 1 [8], part of the fourth workshop on multimodal fact-checking and hate speech detection, featured two subtasks: Task-A focused on distinguishing between AI-generated and human-authored text, while Task-B aimed to identify the specific LLM responsible for generating the text. This paper proposes an ensemble based DeBERTa model, trained and validated on noisy dataset to make the model more robust. This work highlights how adding noise to the dataset makes the model remain resilient to disturbances. It captures features invariant under perturbations and demonstrates significantly improved robustness against such disturbances. \n\nThe rest of the paper is as follows. Section 2 contains related work, section 3 describes the dataset, section 4 describes our methodology, section 5 contains experimental results and section 6 includes conclusions and future work.",
                    "score": 0.7143309001647273,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 120
                        },
                        {
                            "start": 121,
                            "end": 312
                        },
                        {
                            "start": 313,
                            "end": 520
                        },
                        {
                            "start": 523,
                            "end": 682
                        },
                        {
                            "start": 683,
                            "end": 789
                        },
                        {
                            "start": 790,
                            "end": 925
                        },
                        {
                            "start": 926,
                            "end": 1232
                        },
                        {
                            "start": 1233,
                            "end": 1355
                        },
                        {
                            "start": 1356,
                            "end": 1458
                        },
                        {
                            "start": 1459,
                            "end": 1587
                        },
                        {
                            "start": 1590,
                            "end": 1626
                        },
                        {
                            "start": 1627,
                            "end": 1821
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9404296875
                }
            ],
            "relevance_judgement": 0.9404296875,
            "relevance_judgment_input_expanded": "# Title: Sarang at DEFACTIFY 4.0: Detecting AI-Generated Text Using Noised Data and an Ensemble of DeBERTa Models\n# Venue: arXiv.org\n# Authors: Avinash Trivedi, S. Sangeetha\n## Abstract\nThis paper presents an effective approach to detect AI-generated text, developed for the Defactify 4.0 shared task at the fourth workshop on multimodal fact checking and hate speech detection. The task consists of two subtasks: Task-A, classifying whether a text is AI generated or human written, and Task-B, classifying the specific large language model that generated the text. Our team (Sarang) achieved the 1st place in both tasks with F1 scores of 1.0 and 0.9531, respectively. The methodology involves adding noise to the dataset to improve model robustness and generalization. We used an ensemble of DeBERTa models to effectively capture complex patterns in the text. The result indicates the effectiveness of our noise-driven and ensemble-based approach, setting a new standard in AI-generated text detection and providing guidance for future developments.\n## Introduction\nLarge Language Models (LLMs), such as ChatGPT [1], are really good at writing long pieces of text that sound very human. While these developments have various beneficial applications, they also raise concerns about potential misuse, such as the automatic creation of fake news articles and academic contents [2]. To address these risks, various algorithms have been developed to detect AI-generated text, which include watermarking techniques [3], tools like GPTZero [4], DetectGPT [5], and OpenAI's text classifier [6]. \n\nThe task of detecting AI-generated text is inherently challenging, as recent research [7] highlights the increasing sophistication of newer, more capable LLMs. Early studies demonstrated that humans struggle to tell if something was written by a computer or a human. Given the ethical implications and the complexity of the problem, creating robust detection systems remains an active area of research. The Defactify 4.0 shared task 1 [8], part of the fourth workshop on multimodal fact-checking and hate speech detection, featured two subtasks: Task-A focused on distinguishing between AI-generated and human-authored text, while Task-B aimed to identify the specific LLM responsible for generating the text. This paper proposes an ensemble based DeBERTa model, trained and validated on noisy dataset to make the model more robust. This work highlights how adding noise to the dataset makes the model remain resilient to disturbances. It captures features invariant under perturbations and demonstrates significantly improved robustness against such disturbances. \n\nThe rest of the paper is as follows. Section 2 contains related work, section 3 describes the dataset, section 4 describes our methodology, section 5 contains experimental results and section 6 includes conclusions and future work.",
            "reference_string": "[276574900 | Trivedi et al. | 2025 | Citations: 0]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "273901392",
            "title": "SimLLM: Detecting Sentences Generated by Large Language Models Using Similarity between the Generation and its Re-generation",
            "text": "The methods previously used to detect text generated by large language models can be classified into three approaches. \n\nThe first strategy involves training models on large datasets to identify generated text characteristics, such as OpenAI's fine-tuning of the RoBERTa model (Solaiman et al., 2019). Some researchers have analyzed probability distributions in large language models' hidden layers (Wang et al., 2023), while others have used a paraphraser in a GAN to train the detector component (Hu et al., 2023). The intrinsic dimension of the embedding space from long texts has been estimated to understand the workings of these models better (Tulchinskii et al., 2023). Other approaches include building a proxy model to estimate generated text's perplexity (Wu et al., 2023), using positive-unlabeled learning (Tian et al., 2024) to improve performance on short text, and highlighting human text's coherence to spot machinegenerated text discrepancies (Liu et al., 2023). Some researchers have also incorporated top similarity texts from the training set into prompts and used in-context learning to boost detector and attacker capabilities (Koike et al., 2024). However, this approach is sensitive to out-of-distribution texts. \n\nWatermarking is another method where a language model is guided to generate text that meets specific criteria, acting as a watermark to identify generated content. For example, Kirchenbauer et al. (2023) instructed the model to use only a certain set of \"green\" words, avoiding the \"red\" ones. However, this method's downside is that it requires modifying the original models, which is impractical for real-world use, especially considering the proprietary nature of many large language models. \n\nThe third strategy involves zero-shot detection, where research identifies generated text without training. Bhattacharjee and Liu (2023) employed this method by prompting ChatGPT to detect generated texts from various large language models. Gehrmann et al. (2019) noted that large language models often predict the next word in a text sequence with high probability, which can be assessed through ranking, logarithms, and entropy.",
            "score": 0.8704169204463568,
            "section_title": "Related Work",
            "char_start_offset": 4732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 121,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1236
                },
                {
                    "start": 1239,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1733
                },
                {
                    "start": 1736,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2166
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 418,
                    "matchedPaperCorpusId": "264128397"
                },
                {
                    "start": 498,
                    "end": 515,
                    "matchedPaperCorpusId": "259501842"
                },
                {
                    "start": 649,
                    "end": 675,
                    "matchedPaperCorpusId": "259108779"
                },
                {
                    "start": 765,
                    "end": 782,
                    "matchedPaperCorpusId": "258865367"
                },
                {
                    "start": 818,
                    "end": 837,
                    "matchedPaperCorpusId": "258960584"
                },
                {
                    "start": 960,
                    "end": 978,
                    "matchedPaperCorpusId": "254877728"
                },
                {
                    "start": 1149,
                    "end": 1169,
                    "matchedPaperCorpusId": "260091573"
                },
                {
                    "start": 1416,
                    "end": 1442,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 1844,
                    "end": 1872,
                    "matchedPaperCorpusId": "260379022"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9462890625
        },
        {
            "corpus_id": "277322659",
            "title": "TempTest: Local Normalization Distortion and the Detection of Machine-generated Text",
            "text": "There are two main approaches to the detection of machine-generated text. The first is to train a supervised classifier, which are often excellent in domain but their performance can degrade heavily when the language model or text domain changes (Bao et al., 2024). Some recent works have begun to address this challenge (Li et al., 2024;Zhang et al., 2024), though still face the limitation of requiring training processes and data. The second, which we study in this article, is the zero-shot approach. State-of-the-art zeroshot approaches are discussed below. We also mention watermarking, which is the practice of modifying the output of a language model in order to leave specific statistical traces which are easy to detect, see, for example, Kirchenbauer et al. (2023). Watermarking works well, but only when incorporated directly into the language model. For a survey of efforts to detect machine-generated text see Ghosal et al. (2023). \n\nDetectGPT (Mitchell et al., 2023) evaluates a text w 1 \u2022 \u2022 \u2022 w T by first using a mask-filling model to generate perturbations w1 \u2022 \u2022 \u2022 wT of the original text which still convey the same meaning but are differently expressed. They compute the mean log-likelihood \u00b5 of these texts, along with the standard deviation \u03c3, and assign the text a score (P (w 1 \u2022 \u2022 \u2022 w T ) \u2212 \u00b5)/\u03c3. Thus, rather than comparing the log-likelihood of w 1 \u2022 \u2022 \u2022 w T to a global threshold, they compare it to the loglikelihood of similar texts. DetectGPT marked a very significant upgrade to vanilla log-likelihood in the detection of machine-generated text. \n\nNPR (Su et al., 2023) translates the main idea of De-tectGPT to log-rank. The method computes the ratio of the log-rank of w 1 \u2022 \u2022 \u2022 w T to the log rank of perturbations of the text, using the same perturbation model as DetectGPT.",
            "score": 0.8267089062204787,
            "section_title": "RELATED WORK",
            "char_start_offset": 6279,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1578
                },
                {
                    "start": 1581,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1811
                }
            ],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 264,
                    "matchedPaperCorpusId": "263831345"
                },
                {
                    "start": 321,
                    "end": 338,
                    "matchedPaperCorpusId": "269929955"
                },
                {
                    "start": 338,
                    "end": 357,
                    "matchedPaperCorpusId": "267938309"
                },
                {
                    "start": 749,
                    "end": 775,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 924,
                    "end": 944,
                    "matchedPaperCorpusId": "266982110"
                },
                {
                    "start": 958,
                    "end": 981,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1585,
                    "end": 1602,
                    "matchedPaperCorpusId": "259129463"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "275342724",
            "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks for AI generated text detection",
            "text": "There is a large and growing body of work focused on detection methods for AI-generated text. We review some of the main methods that appear in the literature, breaking them into four categories. The first category includes watermarking methods, which add hidden patterns to AI-generated text to make it identifiable. Zero-shot detectors rely on large language models (LLMs) pre-trained for next-token prediction to identify AI-generated content without additional training, using the next-token distributions of the text to generate statistics that separate human-and machine-generated examples. Other approaches involve fine-tuning language models and training them specifically for the detection classification task. Finally, LLMs have also been prompted to detect whether a text is AI-generated, asking them to complete the given prompt with the answer.",
            "score": 0.8109808600680237,
            "section_title": "Detection Methods",
            "char_start_offset": 4896,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 857
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "268793677",
            "title": "The Impact of Prompts on Zero-Shot Detection of AI-Generated Text",
            "text": "Recent years have seen significant advancements in the development of Large Language Models (LLMs) [1,2,3], and their practical applications have become widespread.Meanwhile, their potential misuse have raised significant concerns.For example, the generation of fake news and plagiarism using LLMs is a notable issue.Detectors that evaluate whether a given text is human-generated or AI-generated serve as a defense mechanism against such misuse.\n\nDetectors for AI-generated text can be broadly classified into three categories: a zero-shot detec-tor leveraging statistical properties [4,5,6,7,8,9,10,11], a detector employing supervised learning [12,13,14,15], and a detector utilizing watermarking [16,17].\n\nZero-shot detectors, such as DetectGPT [5], which do not require additional training, are designed in many methods using likelihood-based scores.A summary of zero-shot detectors is illustrated in Table 1.In other words, the zero-shot detection is carried out by replicating the likelihood at the generation phase.When using LLMs, we usually input prompts and utilize the generated output.However, at the detection phase, it is anticipated that reproducing likelihood Log-likelihood Detect using the log likelihood of the given text.",
            "score": 0.791175854730479,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 231
                },
                {
                    "start": 231,
                    "end": 317
                },
                {
                    "start": 317,
                    "end": 446
                },
                {
                    "start": 448,
                    "end": 708
                },
                {
                    "start": 710,
                    "end": 855
                },
                {
                    "start": 855,
                    "end": 914
                },
                {
                    "start": 914,
                    "end": 1023
                },
                {
                    "start": 1023,
                    "end": 1098
                },
                {
                    "start": 1098,
                    "end": 1242
                }
            ],
            "ref_mentions": [
                {
                    "start": 585,
                    "end": 588,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 588,
                    "end": 590,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 700,
                    "end": 704,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 749,
                    "end": 752,
                    "matchedPaperCorpusId": "256274849"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.958984375
        },
        {
            "corpus_id": "264172471",
            "title": "Unbiased Watermark for Large Language Models",
            "text": "The objective of detecting machine-generated text lies in discerning whether a given text has been produced by an algorithm or written by a human. Such detection is crucial to prevent misuse and a substantial body of research has explored this area [74,28,13,31,54,56,55,64]. However, the task has become increasingly challenging due to the continual improvement in language models and the advent of adversarial attacks [19,70,50]. The difference between this and text watermarking is that watermarking is employed to differentiate whether a text is generated by a particular model or provider, yet the detection of machine-generated text is not concerned with a specific model.",
            "score": 0.7870173863639282,
            "section_title": "A.5 Detecting machine-generated text",
            "char_start_offset": 30998,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 678
                }
            ],
            "ref_mentions": [
                {
                    "start": 249,
                    "end": 253,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 420,
                    "end": 424,
                    "matchedPaperCorpusId": "249993639"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91796875
        },
        {
            "corpus_id": "267548131",
            "title": "Permute-and-Flip: An optimally stable and watermarkable decoder for LLMs",
            "text": "Another major motivation of the work is to come up with a reliable method for detecting AI-generated text, so as to prevent LLM misuse. We briefly review two categories of existing work on this problem. \n\nPost-hoc detection. Post-hoc detection of LLM-generated text encompasses two main approaches: zero-shot detection and training-based detection. Zero-shot detection is characterized by its capacity to identify AI-generated text without needing specific training data, leveraging the inherent stylistic differences between human and machine writing. Techniques within this category, such as DetectGPT (Mitchell et al., 2023), PHD (Tulchinskii et al., 2023), DNA-GPT (Yang et al., 2024), and Fast-DetectGPT (Bao et al., 2024), utilize metrics like log-probability scores, n-gram frequencies, lower intrinsic dimensionality, and conditional probability to differentiate AI-generated content. In contrast, training-based detection involves fine-tuning pre-trained language models on datasets that consist of both human and LLM-generated texts to build a classifier. This method is exemplified by various systems, including commercial detection platforms (OpenAI, 2023a;GPTZero, 2023;ZeroGPT, 2023), and research projects (Chen et al., 2023b;Yu et al., 2023;Liu et al., 2023;Hu et al., 2023), which leverage the capabilities of large language models to effectively classify text origins. However, despite post-hoc detection's effectiveness in many cases, recent studies show detection methods' robustness is limited across different scenarios. They have proven fragile to adversarial attacks and biased against non-native English writers (Wolff, 2020;Sadasivan et al., 2023;Liang et al., 2023;Shi et al., 2024). Limitations in accuracy even led OpenAI to close their detector in July 2023 (OpenAI, 2023a). \n\nLLM watermarking. The watermarking approach provides a direct solution for AI text detection by intentionally embedding detectable signals or \"watermarks\" within the text.",
            "score": 0.7802019323159374,
            "section_title": "A.2 DETECT AI-GENERATED TEXT",
            "char_start_offset": 28272,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 202
                },
                {
                    "start": 205,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1804
                },
                {
                    "start": 1807,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1978
                }
            ],
            "ref_mentions": [
                {
                    "start": 604,
                    "end": 627,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 633,
                    "end": 659,
                    "matchedPaperCorpusId": "259108779"
                },
                {
                    "start": 669,
                    "end": 688,
                    "matchedPaperCorpusId": "258960101"
                },
                {
                    "start": 709,
                    "end": 727,
                    "matchedPaperCorpusId": "263831345"
                },
                {
                    "start": 1257,
                    "end": 1274,
                    "matchedPaperCorpusId": "264406273"
                },
                {
                    "start": 1274,
                    "end": 1290,
                    "matchedPaperCorpusId": "259501842"
                },
                {
                    "start": 1692,
                    "end": 1709,
                    "matchedPaperCorpusId": "258987266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91650390625
        },
        {
            "corpus_id": "273901392",
            "title": "SimLLM: Detecting Sentences Generated by Large Language Models Using Similarity between the Generation and its Re-generation",
            "text": "The rise of generative AI, especially large language models, has had a substantial impact across various applications. However, it also presents challenges, such as academic dishonesty and the spread of disinformation, stemming from the misuse of generated text. Therefore, our goal is to create a strategy to detect and mitigate the negative effects associated with the improper use of generated text. \n\nDetection of text generated by large language models uses three main techniques. Firstly, supervised learning methods (Solaiman et al., 2019;Wang et al., 2023;Hu et al., 2023;Wu et al., 2023) train classifiers on datasets of original and generated text, though this requires large volumes of training data. The zero-shot approach (Bhattacharjee and Liu, 2023;Mitchell et al., 2023) eliminates the need for training but is sensitive to out-of-distribution text. Recent research explores watermarking methodologies (Kirchenbauer et al., 2023) to force models to produce predefined words, aiding detection, but this requires modifying the models, which is impractical for proprietary models like ChatGPT. Previous studies mainly address non-analogous text with substantial differences between original and generated content. In contrast, we focus on analogous generated text, where changes to the original text are minimal. \n\nMotivation An AI model aims to extensively optimize original data to generate new data. This process often results in a significant disparity between the original and the generated data. When the model optimizes the generated data to create re-generated data, the already optimized nature of the generated data limits further optimization. As a result, the gap between the generated and re-generated data diminishes. To illustrate, we randomly selected a human sentence (h) from the Extreme Summarization (XSum) dataset (Narayan et al., 2018) (Figure 1). Then, a large language model, specifically ChatGPT (GPT 3.5-turbo), was tasked with generating a machine sentence (m ChatGP T ) conveying an opposite meaning to the original text.",
            "score": 0.7678299970107244,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 402
                },
                {
                    "start": 405,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1325
                },
                {
                    "start": 1328,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2062
                }
            ],
            "ref_mentions": [
                {
                    "start": 546,
                    "end": 564,
                    "matchedPaperCorpusId": "264128397"
                },
                {
                    "start": 564,
                    "end": 580,
                    "matchedPaperCorpusId": "259501842"
                },
                {
                    "start": 580,
                    "end": 596,
                    "matchedPaperCorpusId": "258865367"
                },
                {
                    "start": 764,
                    "end": 786,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 918,
                    "end": 945,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 1848,
                    "end": 1870,
                    "matchedPaperCorpusId": "215768182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95654296875
        },
        {
            "corpus_id": "258048481",
            "title": "On the Possibilities of AI-Generated Text Detection",
            "text": "Recent research has shown promising results in developing detection methods. Some of these methods use statistical approaches to identify differences in the linguistic patterns of human and machine-generated text. We survey the existing approaches here. \n\nTraditional approaches. They involve statistical outlier detection methods, which employ statistical metrics such as entropy, perplexity, and n-gram frequency to differentiate between human and machine-generated texts (Lavergne et al., 2008;Gehrmann et al., 2019). However, with the advent of ChatGPT (OpenAI) (OpenAI, 2023), a new innovative statistical detection methodology, DetectGPT (Mitchell et al., 2023), has been developed. It operates on the principle that text generated by the model tends to lie in the negative curvature areas of the model's log probability. DetectGPT (Mitchell et al., 2023) generates and compares multiple perturbations of model-generated text to determine whether the text is machine-generated or not based on the log probability of the original text and the perturbed versions. DetectGPT significantly outperforms the majority of the existing zero-shot methods for model sample detection with very high AUC scores (note that we use the terms AUROC and AUC interchangeably for presentation convenience). \n\nClassifier-based detectors. In contrast to statistical methods, classifier-based detectors are common in natural language detection paradigms, particularly in fake news and misinformation detection (Schildhauer, 2022;Zou & Ling, 2021). OpenAI has recently fine-tuned a GPT model (OpenAI, 2023) using data from Wikipedia, WebText, and internal human demonstration data to create a web interface for a discrimination task using text generated by 34 language models. This approach combines a classifier-based approach with a human evaluation component to determine whether a given text was machine-generated or not. These recent advancements in the field of detecting AI-generated text have significant implications for detecting and preventing the spread of misinformation and fake news, thereby contributing to the betterment of society (Schildhauer, 2022;Zou & Ling, 2021;Kshetri & Voas, 2022). \n\nWatermark-based identification.",
            "score": 0.7676900531016942,
            "section_title": "Background on AI-Generated Text Detectors and Related Works",
            "char_start_offset": 4175,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 253
                },
                {
                    "start": 256,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1292
                },
                {
                    "start": 1295,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2189
                },
                {
                    "start": 2192,
                    "end": 2223
                }
            ],
            "ref_mentions": [
                {
                    "start": 474,
                    "end": 497,
                    "matchedPaperCorpusId": "12098535"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9658203125
        },
        {
            "corpus_id": "258947640",
            "title": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model",
            "text": "Large language models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022;OpenAI, 2022;Touvron et al., 2023) have the impressive ability to replicate human language patterns, producing text that appears coherent, well-written, and persuasive, although the generated text may contain factual errors and unsupported quotations. As LLMs are increasingly used to simplify writing and presentation tasks, some individuals regrettably have misused LLMs for nefarious purposes, such as creating convincing fake news articles or engaging in cheating, which can have significant social consequences. Mitigating these negative impacts has become a pressing issue for the community. \n\nThe LLM-generated texts are highly articulate, posing a significant challenge for humans in identifying them (Gehrmann et al., 2019b). Fortunately, it is shown that machine learning tools can be leveraged to recognize the watermarks underlying the texts. Some methods (e.g., OpenAI, 2023b) involve training supervised classifiers, which, yet, suffer from overfitting to the training data and ineffectiveness to generalize to new test data. Zero-shot LLM-generated text detection approaches bypass these issues by leveraging the source LLM to detect its samples (Solaiman et al., 2019;Gehrmann et al., 2019b;Ippolito et al., 2020). They usually proceed by inspecting the average per-token log probability of the candidate text, but the practical detection performance can be unsatisfactory. \n\nDetectGPT (Mitchell et al., 2023) is a recent method that achieves improved zero-shot detection efficacy by exploring the probability curvature of LLMs. It generates multiple perturbations of the candidate text and scores them using the source LLM to define detection statistics. It can detect texts generated by GPT-2 (Radford et al., 2019) and GPT-NeoX-20B (Black et al., 2022). Yet, DetectGPT relies on hundreds of queries to the source LLM to estimate the local probability curvature surrounding one single candidate passage.",
            "score": 0.7628792424197927,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1461
                },
                {
                    "start": 1464,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 49,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 781,
                    "end": 805,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 1256,
                    "end": 1279,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 1279,
                    "end": 1301,
                    "matchedPaperCorpusId": "218560609"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9423828125
        },
        {
            "corpus_id": "274581321",
            "title": "A Practical Examination of AI-Generated Text Detectors for Large Language Models",
            "text": "Large language models (LLMs) are becoming increasingly accessible and powerful, leading to numerous beneficial applications (Touvron et al., 2023;Achiam et al., 2023). However, they also pose risks if used maliciously, such as generating fake news articles, facilitating academic plagiarism or spam content (Feng et al., 2024;Zellers et al., 2019b;Perkins, 2023;Fraser et al., 2024). The potential for misuse of LLMs has become a significant concern for major tech corporations, particularly in light of the 2024 elections in the united states. At the Munich Security Conference on February 16th, 2024, these companies pledged to combat misleading machine-generated content, acknowledging the potential of AI to deceptively influence electoral outcomes (Accord, 2024). As a result, there is a growing need to develop reliable methods for differentiating between LLM-generated and human-written content. To ensure the effectiveness and accountability of LLM detection methods, continuous evaluation of popular techniques is crucial. \n\nMany methods have been released recently that claim to have a strong ability to detect the difference between AI-generated and human-generated texts. These detectors primarily fall into three categories: trained detectors, zero-shot detectors, and watermarking techniques (Yang et al., 2023b;Ghosal et al., 2023;Tang et al., 2023). Trained detectors utilize datasets of human and AI-generated texts and train a binary classification model to detect the source of a text (Zellers et al., 2019b;Hovy, 2016;Hu et al., 2023;Tian and Cui, 2023;Verma et al., 2024). Zero-shot detection utilizes a language model's inherent traits to identify text it generates, without explicit training for detection tasks other than calibrating a threshold for detection in some cases (Gehrmann et al., 2019;Mitchell et al., 2023;Bao et al., 2024;Yang et al., 2023a;Venkatraman et al., 2024).",
            "score": 0.7566256074483333,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1031
                },
                {
                    "start": 1034,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1905
                }
            ],
            "ref_mentions": [
                {
                    "start": 326,
                    "end": 348,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 348,
                    "end": 362,
                    "matchedPaperCorpusId": "257166266"
                },
                {
                    "start": 1306,
                    "end": 1326,
                    "matchedPaperCorpusId": "264439179"
                },
                {
                    "start": 1326,
                    "end": 1346,
                    "matchedPaperCorpusId": "264439054"
                },
                {
                    "start": 1504,
                    "end": 1527,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 1527,
                    "end": 1538,
                    "matchedPaperCorpusId": "5315990"
                },
                {
                    "start": 1538,
                    "end": 1554,
                    "matchedPaperCorpusId": "259501842"
                },
                {
                    "start": 1573,
                    "end": 1592,
                    "matchedPaperCorpusId": "258865787"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94873046875
        },
        {
            "corpus_id": "267294743",
            "title": "Beyond Lexical Boundaries: LLM-Generated Text Detection for Romanian Digital Libraries",
            "text": "Despite the performance exhibited by DetectGPT and GPTZeo on English corpora, both models had extremely poor results on our Romanian dataset, which is described in the following section. As per the results of our preliminary experiments, GPTZero misdetected all human texts as AI-generated, while DetectGPT misclassified most AI texts as being written by humans. As outlined in the beginning, pre-existing solutions are primarily designed for high-resourced languages like English and exhibit limited performance or complete inoperability when applied to Romanian. As such, there is a need for specialized solutions to deal with the specificity of the Romanian language. \n\nZero-shot detection is showcased by Gehrmann [44] via the Giant Language Model Test Room (GLTR) study. The idea of the research is that LLMs generate from a limited subset of the true distribution of natural language for which they have high confidence. To test whether a text is machine-generated, the authors use three approaches: (1) compute the probability of the word, (2) compute the absolute rank of a word, and (3) compute the entropy of the predicted distribution. The first two steps evaluate whether a word was sampled from a model similar to the detection model; in contrast, the last step verifies whether the previously generated context is well-known to the detection system, such that it is sure of its next prediction. \n\nClassifier-based methods are widely spread in detection paradigms, while watermarkbased methods represent an innovative alternative to the above-mentioned methods [52]. In the early days, watermark methods were used in computer vision and image processing to ensure copyright protection [53]. Recently, Kirchenbauer [54] proposed in their study the use of watermarks with LLMs, incorporating signals in generated text, which is undetectable to human observers and can be detected with open source algorithms without access to the language model API or parameters for detection. It works by selecting a randomized set of \"green\" tokens before a word is generated and then softly promoting the use of green tokens during sampling. It requires, however, access to the language model while generating the text.",
            "score": 0.7563347115923463,
            "section_title": "Detection Mechanisms",
            "char_start_offset": 24196,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1408
                },
                {
                    "start": 1411,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2139
                },
                {
                    "start": 2140,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 718,
                    "end": 722,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 1698,
                    "end": 1702,
                    "matchedPaperCorpusId": "62101493"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.841796875
        },
        {
            "corpus_id": "273185492",
            "title": "AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text",
            "text": "The latter is more similar to our work as they also measure the novelty of generated n-grams and compare it to human-written text, however they differ from us in tow major ways: (1) they only find verbatim matches, whereas we also match to approximate, semantically similar blocks of text and (2) they compute the percentage of n-grams of a certain length in a text that can be found in the reference corpus, whereas we measure how much of the text can be reconstructed by mixing and matching a vast amount of existing text snippets of varying lengths from the web. \n\nMachine Text Detection: Detecting machine-generated text has been explored for several years using a variety of methods (Jawahar et al., 2020;Uchendu et al., 2021). Gehrmann et al. (2019) and Dugan et al. (2023) demonstrate that even humans tend to struggle to differentiate between text written by humans and machines, highlighting the need for automated detection solutions. Some approaches involve training a classifier in a supervised manner to identify machine-generated text (Bakhtin et al., 2019;Uchendu et al., 2020), while others use a zero-shot detection method (Solaiman et al., 2019;Ippolito et al., 2020). Additionally, there is research on bot detection through question answering (Wang et al., 2023;Chew & Baird, 2003). Recently, Mitchell et al. (2023b) introduced DetectGPT, a zero-shot method based on the hypothesis that texts produced by a large language model (LLM) are located at local maxima, and thus exhibit negative curvature, in the model's probability distribution. Follow-up work build on DetectGPT by making it faster (Bao et al., 2024) and proposing to use cross-detection when the target model is unknown (Mireshghallah et al., 2024). \n\nVarious strategies have been developed to detect machine-generated text in real-world settings. One notable approach is watermarking, which embeds algorithmically detectable patterns into the generated text while maintaining the quality and diversity of the language model's outputs.",
            "score": 0.7501842306944358,
            "section_title": "C RELATED WORK",
            "char_start_offset": 47540,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1733
                },
                {
                    "start": 1736,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 2019
                }
            ],
            "ref_mentions": [
                {
                    "start": 688,
                    "end": 710,
                    "matchedPaperCorpusId": "226237099"
                },
                {
                    "start": 710,
                    "end": 731,
                    "matchedPaperCorpusId": "237589233"
                },
                {
                    "start": 733,
                    "end": 755,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 760,
                    "end": 779,
                    "matchedPaperCorpusId": "255125274"
                },
                {
                    "start": 1071,
                    "end": 1092,
                    "matchedPaperCorpusId": "221835708"
                },
                {
                    "start": 1163,
                    "end": 1185,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 1282,
                    "end": 1301,
                    "matchedPaperCorpusId": "12757"
                },
                {
                    "start": 1313,
                    "end": 1336,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1615,
                    "end": 1633,
                    "matchedPaperCorpusId": "263831345"
                },
                {
                    "start": 1704,
                    "end": 1732,
                    "matchedPaperCorpusId": "268417138"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91259765625
        },
        {
            "corpus_id": "256274849",
            "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
            "text": "As large language models continue to improve, they will become increasingly attractive tools for replacing human writers in a variety of contexts, such as education, journalism, and art. While legitimate uses of language model technologies exist in all of these settings, teachers, readers, and consumers are likely to demand tools for verifying the human origin of certain content with high educational, societal, or artistic significance, particularly when factuality (and not just fluency) is crucial. \n\nIn light of these elevated stakes and the regular emergence of new large language models, we study the zero-shot machinegenerated text detection problem, in which we use only the raw log probabilities computed by a generative model to determine if a candidate passage was sampled from it. We identify a property of the log probability function computed by a wide variety of large language models, showing that a tractable approximation to the trace of the Hessian of the model's log probability function provides a useful signal for detecting model samples. Our experiments find that this signal is more discriminative than existing zero-shot detection methods and is competitive with bespoke detection models trained with millions of model samples. \n\nDetectGPT and Watermarking. One interpretation of the perturbation function is producing semantically similar rephrasings of the original passage. If these rephrasings are systematically lower-probability than the original passage, the model is exposing its bias toward the specific (and roughly arbitrary, by human standards) phrasing used. In other words, LLMs that do not perfectly imitate human writing essentially watermark themselves implicitly. Under this interpretation, efforts to manually add watermarking biases to model outputs (Aaronson, 2022;Kirchenbauer et al., 2023) may further improve the effectiveness of methods such as DetectGPT, even as LLMs continue to improve. \n\nLimitations. One limitation of probability-based methods for zero-shot machine-generated text detection (like Detect-GPT) is the white-box assumption that we can evaluate log probabilities of the model(s) in question. For models behind APIs that do provide probabilities (such as GPT-3), evaluating probabilities nonetheless costs money. Another assumption of DetectGPT is access to a reasonable perturbation function.",
            "score": 0.749295300916941,
            "section_title": "Discussion",
            "char_start_offset": 29476,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1256
                },
                {
                    "start": 1259,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1943
                },
                {
                    "start": 1946,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2163
                },
                {
                    "start": 2164,
                    "end": 2283
                },
                {
                    "start": 2284,
                    "end": 2364
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9580078125
        },
        {
            "corpus_id": "261697402",
            "title": "Studying the impacts of pre-training using ChatGPT-generated text on downstream tasks",
            "text": "Watermarking in text generation is a technique that introduces hidden signals into generated text, which can be used to detect whether the text is artificial. In this approach, the language model generates a probability distribution for the next word based on a given sequence of words. The next word is then randomly selected from this distribution. The watermarking technique operates by pseudo-randomly sampling words from the distribution, thereby enabling the detection of text generated by the language model.\n\nIn a recent study conducted by Kirchenbauer et al. [33], the effectiveness of the watermarking technique was evaluated on a multi-billion parameter model from the Open Pre-trained Transformer (OPT) family. The proposed method involved the selection of randomized sets of green tokens (allowed tokens) and red tokens (restricted tokens) during the generation process. The use of green tokens was promoted while sampling, while red tokens were discouraged. This approach facilitated the detection of artificial text by counting the occurrences of red tokens. In a similar direction, Ope-nAI has also expressed intentions to incorporate watermarking into its language models.\n\nAs outlined in a blog post by Scott Aaronson 6 , OpenAI plans to use cryptographic pseudo-random functions to sample words and generate detectors using cryptographic keys. This approach aims to embed watermarks into the language model, allowing for the identification of artificial text.\n\nIn practice, evading watermarking detection in language models can be achieved through various techniques, such as inserting or deleting words randomly, rearranging sentence order, or paraphrasing text. These methods pose challenges to the effectiveness of watermarking as a detection mechanism. Additionally, the responsibility of embedding watermarking lies with the developer of the language model, and only they have the ability to build the corresponding detection models. This limitation means that watermarking can only mitigate the potential harms of language models to a certain extent, as it relies on the actions and choices of the developer.",
            "score": 0.7450437969617514,
            "section_title": "Watermarking in Language Models",
            "char_start_offset": 18120,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95166015625
        },
        {
            "corpus_id": "270766717",
            "title": "Exploring Causal Mechanisms for Machine Text Detection Methods",
            "text": "The potential societal impact of competent language models has called for the need to discrimi-nate between their output and human-written texts (Solaiman et al., 2019;Goldstein et al., 2023).\n\nSince the release of a supervised classifier for GPT-2 with a 95% accuracy rate (Solaiman et al., 2019) in 2019, the task of detecting machine outputs has become severely more challenging: the new classifier for ChatGPT was reported to identify only 26% of AI-generated text as \"likely AI-written,\" while misclassifying human-written text as AI-written at a rate of 9% (Kirchner et al., 2023).Recently, DetectGPT (Mitchell et al., 2023) proposed a zeroshot detector that uses an approximation of the curvature of a language model's log probability function, outperforming existing zero-shot methods (Gehrmann et al., 2019) for detecting machinegenerated text and performing similarly or better than GPT-2 detectors.Watermarking (Abdelnabi and Fritz, 2021;Yang et al., 2022;Kirchenbauer et al., 2023;Yoo et al., 2023a,b) is another approach to identify machine-generated texts by encoding a secret message in the output of the language model.While there are research directions aimed at addressing the challenges to detection, such as robustness analysis of existing classifiers against paraphraser (Sadasivan et al., 2023;Krishna et al., 2023), there is a lack of fundamental analysis regarding the factors that impact the detection performance.We believe that conducting such an analysis could guide future directions toward a more reliable detection of machine texts.",
            "score": 0.7361988846727631,
            "section_title": "Related Works",
            "char_start_offset": 3922,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 194,
                    "end": 587
                },
                {
                    "start": 587,
                    "end": 909
                },
                {
                    "start": 909,
                    "end": 1135
                },
                {
                    "start": 1135,
                    "end": 1439
                },
                {
                    "start": 1439,
                    "end": 1563
                }
            ],
            "ref_mentions": [
                {
                    "start": 793,
                    "end": 816,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 922,
                    "end": 949,
                    "matchedPaperCorpusId": "221516138"
                },
                {
                    "start": 949,
                    "end": 967,
                    "matchedPaperCorpusId": "245144237"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89794921875
        },
        {
            "corpus_id": "272987651",
            "title": "A Certified Robust Watermark For Large Language Models",
            "text": "AI Generated Text Identification. While large language models are applied to various scenarios with their increasingly powerful text generation capabilities, such as story generation [52], article translation [59] and code assistance [53], the potential risks they bring [7,28,50] have made the identi- Direct detection methods often identify AI-generated text by uncovering distinguishing characteristics between AIgenerated and human-written texts or utilizing an extensive dataset of AI-generated text to train a accurate text classifier for AI-generated content. For example, regarding the first kind of method, Hamed and Wu [1] discovered that the bigram similarity of texts generated by ChatGPT is higher than that of human-written texts and developed a corresponding detection method based on this finding. Mitchell et al. found that AIgenerated texts tend to occupy the negative curvature region of the model's log-probability function. Based on this observation they proposed DetectGPT [37]. Building on Mitchell et al.'s approach, Bao et al. [2] improved the perturbation step in their method, significantly enhancing both its accuracy and speed. However, with the rapid advancement in both the size of model and text generation capabilities of LLMs, the gap between AI-generated texts and human-written texts has been narrowing [16,35,38,39]. As a result, methods based on text characteristics are becoming increasingly less effective.For the second type of method, Mindner et al. [36] employed multidimensional text feature extraction approaches to construct a classifier, with the best classifier outperforming GPTZero [14] in terms of F1-score. Chen et al. [6] and Liu et al. [32] utilized the advanced language understanding capabilities of pretrained LLMs [33,43], finetuning them as binary classifiers on various text datasets for AI-generated text detection. While these methods perform well on their respective test datasets, their effectiveness may be limited when applied to texts generated by newly emerging models. Likewise, as the capabilities of large language models continue to advance, their effectiveness remains a question. \n\nWatermarking, as an alternative method for AI-generated text identification, is generally more effective, versatile, and interpretable compared to direct detection.",
            "score": 0.7349849781229936,
            "section_title": "Related work",
            "char_start_offset": 31054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2153
                },
                {
                    "start": 2156,
                    "end": 2320
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 187,
                    "matchedPaperCorpusId": "247585187"
                },
                {
                    "start": 274,
                    "end": 277,
                    "matchedPaperCorpusId": "247450984"
                },
                {
                    "start": 277,
                    "end": 280,
                    "matchedPaperCorpusId": "218781040"
                },
                {
                    "start": 995,
                    "end": 999,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1349,
                    "end": 1352,
                    "matchedPaperCorpusId": "269738126"
                },
                {
                    "start": 1492,
                    "end": 1496,
                    "matchedPaperCorpusId": "260775600"
                },
                {
                    "start": 1776,
                    "end": 1779,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95361328125
        },
        {
            "corpus_id": "278327131",
            "title": "An End-to-End Model For Logits Based Large Language Models Watermarking",
            "text": "Large Language Models (LLMs) like ChatGPT (Achiam et al., 2023), Llama (Touvron et al., 2023;Dubey et al., 2024), and OPT (Zhang et al., 2022) have greatly improved the quality of AI-generated content (AIGC), broadening their applications in fields such as translation (Hendy et al., 2023), content creation (Ni et al., 2023), etc. However, such a rapid expansion has also raised concerns about copyright infringement, academic dishonesty, and unethical practices (Augenstein et al., 2024). These issues highlight the urgent need for reliable methods to distinguish between humanwritten text and LLM-generated content, ensuring digital integrity and combating misinformation (Barrett et al., 2023). \n\nNumerous approaches have been proposed to address LLM ethical concerns by detecting LLM-generated content. Passive detection methods focus on identifying unique properties of generated text, often through training binary classifiers (Bakhtin et al., 2019;Jawahar et al., 2020) or statistical techniques like DetectGPT (Mitchell et al., 2023), which compares the log probability value of a sentence with its perturbed version. However, as LLMs improve and the gap between generated and human-written text narrows, the effectiveness of these methods declines dramatically. In contrast, active detection methods, like embedding watermarks in generated text, are proving to be more robust alternatives. LLM watermarking methods fall into two main categories: logits-based and sampling-based. Specifically, logits-based methods (Kirchenbauer et al., 2023;Liu et al., 2024b;Huo et al., 2024) randomly divide the vocabulary into \"green\" and \"red\" lists by hashing preceding tokens as the seed. Then, perturbations are introduced to the logits that favor green list tokens in the generated text, and the proportion of the \"green\" tokens is used to distinguish whether a text is LLM-generated.",
            "score": 0.7287993307476698,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1885
                }
            ],
            "ref_mentions": [
                {
                    "start": 464,
                    "end": 489,
                    "matchedPaperCorpusId": "272064135"
                },
                {
                    "start": 956,
                    "end": 977,
                    "matchedPaperCorpusId": "226237099"
                },
                {
                    "start": 1019,
                    "end": 1042,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1524,
                    "end": 1551,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 1551,
                    "end": 1569,
                    "matchedPaperCorpusId": "263830310"
                },
                {
                    "start": 1569,
                    "end": 1586,
                    "matchedPaperCorpusId": "268041258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.935546875
        },
        {
            "corpus_id": "263829122",
            "title": "On the Zero-Shot Generalization of Machine-Generated Text Detectors",
            "text": "Research on detecting machine-generated text can be roughly divided into two categories: supervised training and zero-shot detection (To clarify, in the literature \"zero-shot\" usually means that the approach does not require training data, while our work focus on zero-shot generalization to the detection of a held-out generator). \n\nIn the case of supervised methods, Bakhtin et al. ( 2019) train an energy-based model to identify machine-generated text. Zellers et al. (2020) trainn a GROVER detector and finds that models exhibiting superior performance in generating neural disinformation are also highly effective in detecting their own generated content. Both Solaiman et al. (2019) and Ippolito et al. (2020) propose zero-shot approaches to detect machine-generated text and evaluate the capability of pretrained models. Liu et al. (2022) present a coherence-based contrastive learning model to detect the machine-generated text under low-resource scenario. Kirchenbauer et al. (2023) propose a watermarking method (Abdelnabi and Fritz, 2021) which introduces designed noise which is imperceptible to human readers. Mitchell et al. (2023) propose DetectGPT, a zeroshot method that utilizes a novel curvature-based criterion to determine whether a text is generated by a specific model. This approach has demonstrated superior detection capabilities compared to other existing zero-shot methods. While DetectGPT does not require training a separate detector, it relies on access to the generator LLM, which can be costly. Recently, Su et al. (2023) follow up the work of De-tectGPT and introduce two new zero-shot methods: DetectLLM-LRR and DetectLLM-NPR.",
            "score": 0.7276653990794515,
            "section_title": "A Related Work",
            "char_start_offset": 14054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 331
                },
                {
                    "start": 334,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1661
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.890625
        },
        {
            "corpus_id": "267627184",
            "title": "ACW: Enhancing Traceability of AI-Generated Codes Based on Watermarking",
            "text": "The development of large language models (LLMs) [44] marks a milestone in the progress of artificial intelligence. LLMs promote extensive applications across a variety of domains [26], one of which is code generation. Multiple AI models for effective code generation, such as ChatGPT [30], StarCoder [20] and CodeT5+ [41], are now widely adopted. While these AI models can potentially improve programmers' productivity significantly [27], they can certainly be misused as well. For instance, Langton [17] showed how to use ChatGPT to construct a ROT13 cipher-based ransomware [6] that could evade conventional defenses. Furthermore, certain users (e.g., students) may have incentives to pretend to be the author of certain AI-generated codes. Moreover, the code generated by AI models may be vulnerable [28,31,36]. It is thus desirable that we are able to determine whether a given piece of code is generated by AI, and furthermore, which AI model is the author, for traceability and accountability. \n\nExisting approaches on detecting AI-generated content mostly focus on images [7] and text [16]. In particular, those for text can be broadly categorized into two groups. The first group includes the so-called passive detectors [11,29,43], which build binary classifiers to distinguish AI-generated or human-written text. The second group focuses on Li et al. watermarking [1,16] which actively embeds hidden 'patterns' (a.k.a. watermark) into AI-generated text, and determines whether certain text is AI-generated or not based on the presence or absence of the hidden pattern. Applying existing watermarking techniques developed for AI-generated text to AI-generated codes is non-trivial due to the inherent difference between general text and computer codes. To the best of our knowledge, only Lee et al. [19] have attempted to detect AI-generated codes based on watermarking. They propose an approach called SWEET that extends WLLM [16], a pioneering work for watermarking text. The idea is to split a vocabulary into green tokens and red tokens and always select high-entropy green ones (and avoid red ones) during code generation.",
            "score": 0.7271100643288755,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2136
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 183,
                    "matchedPaperCorpusId": "257921533"
                },
                {
                    "start": 317,
                    "end": 321,
                    "matchedPaperCorpusId": "258685677"
                },
                {
                    "start": 803,
                    "end": 807,
                    "matchedPaperCorpusId": "235683109"
                },
                {
                    "start": 807,
                    "end": 810,
                    "matchedPaperCorpusId": "245220588"
                },
                {
                    "start": 810,
                    "end": 813,
                    "matchedPaperCorpusId": "257219623"
                },
                {
                    "start": 1079,
                    "end": 1082,
                    "matchedPaperCorpusId": "253254809"
                },
                {
                    "start": 1092,
                    "end": 1096,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 1233,
                    "end": 1236,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1374,
                    "end": 1377,
                    "matchedPaperCorpusId": "221516138"
                },
                {
                    "start": 1377,
                    "end": 1380,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 1936,
                    "end": 1940,
                    "matchedPaperCorpusId": "256194179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86767578125
        },
        {
            "corpus_id": "264439179",
            "title": "A Survey on Detection of LLMs-Generated Content",
            "text": "In this section, we delve into further details about the detection algorithms. Based on their distinguishing characteristics, existing detection methods can be categorized into three classes: 1) Trainingbased classifiers, which typically fine-tune a pretrained language model on collected binary data -both human and AI-generated text distributions. \n\n2) Zero-shot detectors leverage the intrinsic properties of typical LLMs, such as probability curves or representation spaces, to perform self-detection. \n\n3) Watermarking involves hiding identifying information within the generated text that can later be used to determine if the text came from a specific language model, rather than detecting AI-generated text in general. We summarize the representative approaches in Figure 3 as classified by the scenarios listed in Section 3.",
            "score": 0.7263669570524931,
            "section_title": "Detection Methodologies",
            "char_start_offset": 14768,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 349
                },
                {
                    "start": 352,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 833
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8935546875
        },
        {
            "corpus_id": "267094724",
            "title": "Fine-tuning Large Language Models for Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection",
            "text": "In the realm of distinguishing texts generated by large language models (LLMs) from those authored by humans, researchers have employed a variety of methods and tools. Broadly, these approaches can be categorized into two main types: black-box and white-box detection methods (Tang et al., 2023). Black-box detection relies on API-level access to LLMs, utilizing textual samples from both human and machine sources to train classification models (Dugan et al., 2020). The study by Guo et al. (2023) integrated existing question-and-answer datasets and leveraged fine-tuning of pre-trained models to investigate the characteristics and similarities between human-generated and AI-generated texts (Guo et al., 2023). \n\nIn the context of white-box detection, Kirchenbauer et al. ( 2023) introduced a novel approach involving the embedding of watermarks in the outputs of large language models to facilitate the detection of AI-generated text (Kirchenbauer et al., 2023). Additionally, a variety of tools and methodologies, including XGBoost, decision trees, and transformer-based models, have been evaluated for their efficacy in detecting texts produced by AI (Zaitsu and Jin, 2023). These techniques incorporate multiple stylistic measurement features to differentiate between AI-generated and human-generated texts (Shijaku and Canhasi, 2023). \n\nSpecific tools and techniques in this domain include the GLTR tool developed by Gehrmann et al. (2019), which analyzes the usage of rare words in texts to distinguish between those generated by the GPT-2 model and human writers (Gehrmann et al., 2019). The DetectGPT method posits that minor rewrites of LLM-generated texts tend to reduce the log probability under the model, a hypothesis that has been explored in depth (Mitchell et al., 2023). Furthermore, intrinsic dimension analysis, including methods like the Persistent Homology Dimension estimator (PHD), has been applied to distinguish between authentic texts and those generated artificially (Tulchinskii et al., 2023).",
            "score": 0.7248232384918813,
            "section_title": "Related Work",
            "char_start_offset": 2483,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1343
                },
                {
                    "start": 1346,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 2025
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97705078125
        },
        {
            "corpus_id": "269761924",
            "title": "Stylometric Watermarks for Large Language Models",
            "text": "Several notable attempts have already been made to detect machine-generated text, albeit without incorporating language and stylometric features like us.These efforts can generally be categorized into two primary groups, depending on their functional approach.\n\nPost-hoc detection.In this case, it is assumed that the LLM has already completed text generation some time in the past.The methods in this group attempt to detect the generated text without employing any watermark or other modifications to either the language model or the final output.Notable examples of this group are the classifier by OpenAI Kirchner et al. [2023], GPTZero Tian [2023] or DetectGPT [Mitchell et al., 2023] who uses the probability",
            "score": 0.7227070240021847,
            "section_title": "Background & Related Work",
            "char_start_offset": 5891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 153,
                    "end": 260
                },
                {
                    "start": 262,
                    "end": 281
                },
                {
                    "start": 281,
                    "end": 382
                },
                {
                    "start": 382,
                    "end": 549
                },
                {
                    "start": 549,
                    "end": 714
                }
            ],
            "ref_mentions": [
                {
                    "start": 666,
                    "end": 689,
                    "matchedPaperCorpusId": "256274849"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90966796875
        },
        {
            "corpus_id": "265506006",
            "title": "I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text",
            "text": "Potential harms of Large Language Models such as mass misinformation and plagiarism can be partially mitigated if there exists a reliable way to detect machine generated text. In this paper, we propose a new watermarking method to detect machine-generated texts. Our method embeds a unique pattern within the generated text, ensuring that while the content remains coherent and natural to human readers, it carries distinct markers that can be identified algorithmically. Specifically, we intervene with the token sampling process in a way which enables us to trace back our token choices during the detection phase. We show how watermarking affects textual quality and compare our proposed method with a state-of-the-art watermarking method in terms of robustness and detectability. Through extensive experiments, we demonstrate the effectiveness of our watermarking scheme in distinguishing between watermarked and non-watermarked text, achieving high detection rates while maintaining textual quality.",
            "score": 0.7207526592354454,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93359375
        },
        {
            "corpus_id": "267406364",
            "title": "LLM-Detector: Improving AI-Generated Chinese Text Detection with Open-Source LLM Instruction Tuning",
            "text": "Large language models (LLMs), such as ChatGPT, represent a significant milestone in the field of natural language processing (NLP). LLMs have been pre-trained on extensive text corpora, enabling them to generate texts that are contextually relevant and fluent. (Brown et al., 2020) However, the impressive capabilities of generative language models in text generation have also led to rising concerns about their possible misuse in various areas, including phishing, dissemination of misinformation, and academic dishonesty. Unfortunately, when human classifying AI-generated and human-written texts, human only slightly better than random guessing (Gehrmann et al., 2019). Therefore, our goal is to develop an automated system to classify AI-generated texts with the aim of mitigating its potential for misuse. \n\nThere has been some previous effort in detecting AIgenerated texts. First, Guo et al. (Guo et al., 2023b) finetuned RoBERTa to detect whether a certain text (English and Chinese) is generated by ChatGPT or written by human. However, the study conducted demonstrates that a limitation of supervised models is the potential occurrence of overfitting in-domain data, resulting in poor detection performance out-of-domain (OOD) (Chakraborty et al., 2023). The second is zero-shot classifier, DetectGPT (Mitchell et al., 2023), works under the assumption that AI-generated texts variations typically have lower model probability than the original, while human-written could go either way. As current zero-shot classifiers require input documents, with considerable length (exceeding 100 tokens) for the classifier to effectively capture contextual features of the text, for which in terms of classifying short sentences, their performances are relatively poor. Kirchenbauer et al. (Kirchenbauer et al., 2023) demonstrated how to incorporate a watermark using only the logarithmic credentials of each step to mark AIgenerated texts. While watermark-based detectors are an intriguing area of research, adding watermarks may affect the readability of the texts, and the removal of watermarks is also a challenge we need to address.",
            "score": 0.7176712642298846,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 811
                },
                {
                    "start": 814,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 281,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88916015625
        },
        {
            "corpus_id": "276773353",
            "title": "Detecting Stylistic Fingerprints of Large Language Models",
            "text": "The task of detecting AI-generated text is a classification task. In most cases, it is usually treated as a binary classification problem -AI or human -but could also be a multi-class problem if one wishes to differentiate between specific AI text generators. The task becomes even more complex when a text is written with a combination of several AI models, or using AI and human authors, requiring the identification of each contributor. [20] In most cases, detecting AI-generated text is done using supervised learning frameworks. This requires labeled examples to train a classifier. The main metrics that are used are true-positive rates, true-negative rates, accuracy and F1 score. Classical approaches applied feature extraction on the texts in order to quantify stylistic or syntactic features such as perplexity, burstiness, and word density. These features are then fed into machine learning algorithms such as logistic regression or SVM (which provide an advantage of explainability), or into \"deep\" neural networks such as LSTMs. More modern approaches use pre-trained language models such as BERT, and fine-tune them for the specific classification task. This eliminates the need to manually define a list of features, and overall presents a much higher accuracy compared to feature-based methods, mainly because this approach is able to capture more general elements and nuances of the writing style, and is less dependent on a specific domain or topic. [20,21,22,23] Approaches that rely solely on capturing the differences between AI and human texts operate under the assumption that such differences exist; however, in reality, the differences between machine and human text may diminish over time as these systems evolve. A \"watermarking\" approach has recently emerged in an attempt to overcome these shortcomings. \"Watermarks\" can be added to the text during the generation phase (generative watermarking) or after it (edit-based watermarking). [22] However, this approach has its own limitations, such as lower effectiveness on factual responses (as there are fewer options to control the output), and higher sensitivity to small edits of the watermarked text. 3",
            "score": 0.7176560559012303,
            "section_title": "AI Forensics",
            "char_start_offset": 2398,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2182
                }
            ],
            "ref_mentions": [
                {
                    "start": 1472,
                    "end": 1475,
                    "matchedPaperCorpusId": "275994170"
                },
                {
                    "start": 1475,
                    "end": 1478,
                    "matchedPaperCorpusId": "273547059"
                },
                {
                    "start": 1478,
                    "end": 1481,
                    "matchedPaperCorpusId": "276142840"
                },
                {
                    "start": 1964,
                    "end": 1968,
                    "matchedPaperCorpusId": "273547059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94775390625
        },
        {
            "corpus_id": "263831149",
            "title": "How Reliable Are AI-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts",
            "text": "The task of AI text detection is generally considered a binary classification task, with two classes: \n\n\"Human-written\" and \"AI-written.\" In the early days, several supervised learning-based classification methods were explored for detecting AIgenerated text, such as logistic regression and SVC (Ippolito et al., 2019). In contrast, GLTR (Gehrmann et al., 2019) employs a set of simple statistical tests to determine whether an input text sequence is AI-generated or not, making the method zero-shot. In recent years, fine-tuned PLM-based detectors have emerged as the state-of-the-art, including OpenAI's GPT2 detector (Solaiman et al., 2019;Jawahar et al., 2020;Zellers et al., 2019;Kumarage et al., 2023). With the rapid advancement of newer large language models, there is an increasing emphasis on the capabilities of few-shot or zero-shot detection and the interpretability of these detectors (Mitrovi\u0107 et al., 2023). Some new detectors include commercial products such as GPTZero (Tian, 2023), and OpenAI's detector (Kirchner et al., 2023). A recent high-performing zero-shot detection approach called DetectGPT (Mitchell et al., 2023) operates on the hypothesis that minor rewrites of AI-generated text would exhibit lower token log probabilities than the original sample. Watermarking on PLM-generated text is also an exciting approach gaining attention in the research community (Kirchenbauer et al., 2023). However, it assumes that the AI generator itself supports the implementation of watermarking, which reduces the practicality of the approach.",
            "score": 0.7143751208322138,
            "section_title": "Detection of AI-Generated Text",
            "char_start_offset": 4438,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 104,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1560
                }
            ],
            "ref_mentions": [
                {
                    "start": 665,
                    "end": 686,
                    "matchedPaperCorpusId": "168169824"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9365234375
        },
        {
            "corpus_id": "276574900",
            "title": "Sarang at DEFACTIFY 4.0: Detecting AI-Generated Text Using Noised Data and an Ensemble of DeBERTa Models",
            "text": "Large Language Models (LLMs), such as ChatGPT [1], are really good at writing long pieces of text that sound very human. While these developments have various beneficial applications, they also raise concerns about potential misuse, such as the automatic creation of fake news articles and academic contents [2]. To address these risks, various algorithms have been developed to detect AI-generated text, which include watermarking techniques [3], tools like GPTZero [4], DetectGPT [5], and OpenAI's text classifier [6]. \n\nThe task of detecting AI-generated text is inherently challenging, as recent research [7] highlights the increasing sophistication of newer, more capable LLMs. Early studies demonstrated that humans struggle to tell if something was written by a computer or a human. Given the ethical implications and the complexity of the problem, creating robust detection systems remains an active area of research. The Defactify 4.0 shared task 1 [8], part of the fourth workshop on multimodal fact-checking and hate speech detection, featured two subtasks: Task-A focused on distinguishing between AI-generated and human-authored text, while Task-B aimed to identify the specific LLM responsible for generating the text. This paper proposes an ensemble based DeBERTa model, trained and validated on noisy dataset to make the model more robust. This work highlights how adding noise to the dataset makes the model remain resilient to disturbances. It captures features invariant under perturbations and demonstrates significantly improved robustness against such disturbances. \n\nThe rest of the paper is as follows. Section 2 contains related work, section 3 describes the dataset, section 4 describes our methodology, section 5 contains experimental results and section 6 includes conclusions and future work.",
            "score": 0.7143309001647273,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 520
                },
                {
                    "start": 523,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1587
                },
                {
                    "start": 1590,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1821
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9404296875
        },
        {
            "corpus_id": "277999747",
            "title": "Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text",
            "text": "Detecting AI-generated text has gained significant attention in recent years, with numerous methods proposed to tackle this challenge. Typically, this task is framed as a binary classification problem, distinguishing human-written text from machine-generated content [2,3,4]. Existing approaches can be broadly categorized into three main types: supervised methods, unsupervised methods, and hybrid approaches. \n\nSupervised methods rely on labeled datasets to train detection models. Research in this area includes works like [5,6,2,7,8,9], which demonstrate that supervised approaches generally achieve strong performance. However, these methods are prone to overfitting, particularly when dealing with limited or domain-specific training data [10,11]. Unsupervised methods, such as zero-shot detection techniques [12,4,10,11,13,14], leverage the capabilities of pre-trained language models to classify text without task-specific training. Adversarial methods also fall within this category, focusing on evaluating detector robustness against perturbations. For example, [15] assesses the impact of character-level modifications like misspellings, using French as a case study. Similarly, [16] introduces DIPPER, a generative model trained to paraphrase paragraphs and evade detection. \n\nHybrid approaches combine feature-based methods with machine learning or neural models. They often utilize metrics such as word count, vocabulary richness, and readability scores, fused with machine learning or fine-tuned neural networks for detection [12,17,18,19,20]. Fusion and ensemble strategies have also been explored to enhance detection accuracy.",
            "score": 0.7105591505828129,
            "section_title": "Related Work",
            "char_start_offset": 2546,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 410
                },
                {
                    "start": 413,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1644
                }
            ],
            "ref_mentions": [
                {
                    "start": 267,
                    "end": 270,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 531,
                    "end": 533,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 745,
                    "end": 749,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 821,
                    "end": 824,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1072,
                    "end": 1076,
                    "matchedPaperCorpusId": "259129474"
                },
                {
                    "start": 1190,
                    "end": 1194,
                    "matchedPaperCorpusId": "257687440"
                },
                {
                    "start": 1545,
                    "end": 1548,
                    "matchedPaperCorpusId": "264953899"
                },
                {
                    "start": 1551,
                    "end": 1554,
                    "matchedPaperCorpusId": "260775600"
                },
                {
                    "start": 1554,
                    "end": 1557,
                    "matchedPaperCorpusId": "260887319"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89892578125
        },
        {
            "corpus_id": "270370917",
            "title": "Improving Logits-based Detector without Logits from Black-box LLMs",
            "text": "Large language models (LLMs) such as ChatGPT [1], GPT-4 [2], Llama [3][4][5] and Claude-3 [6] have profoundly impacted both industrial and academic domains, reshaping productivity across various  sectors including news reporting, story writing, and academic research [7]. Nevertheless, their misuse also raises concerns, particularly regarding the dissemination of fake news [8], the proliferation of malicious product reviews [9], and instances of plagiarism [10]. Instances of AI-synthesized scientific abstracts deluding scientists [11,12] have raised doubts about the reliability of scientific discourse. Accurate and reliable machine-generated text detection methods are necessary in order to address these issues [13][14][15][16][17]. \n\nMethods for detecting text generated by Large Language Models are broadly categorized into watermarking [18][19][20][21], training-based classifiers [22][23][24][25][26], and zero-shot detectors. Watermarking methods discreetly embed identifiable markers within the text output, striving to retain the model's linguistic integrity. However, this tactic is implementable solely by the model provider. Trainingbased classifiers, while effective, are costly and often lack the agility to adapt to new domains or model updates. Our emphasis is on zero-shot detectors that exploit the intrinsic differences between text written by machines and humans, offering the advantage of being generally training-free. \n\nMost zero-shot detectors primarily depend on analyzing model output logits for detection. Notably, DetectGPT [27] operates on probability divergence based upon principles of perturbation theory, while DNA-GPT [28] harnesses reprompting-based probability divergence, and Fast-DetectGPT [29] builds on variations in conditional probability distributions. In scenarios requiring the scrutiny of black-box models, these strategies commonly leverage a surrogate model to approximate the behavior of the target model.",
            "score": 0.709588683910685,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 740
                },
                {
                    "start": 743,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1446
                },
                {
                    "start": 1449,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1960
                }
            ],
            "ref_mentions": [
                {
                    "start": 267,
                    "end": 270,
                    "matchedPaperCorpusId": "255624599"
                },
                {
                    "start": 427,
                    "end": 430,
                    "matchedPaperCorpusId": "198147840"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "247450984"
                },
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "255187118"
                },
                {
                    "start": 539,
                    "end": 542,
                    "matchedPaperCorpusId": "255773668"
                },
                {
                    "start": 847,
                    "end": 851,
                    "matchedPaperCorpusId": "221516138"
                },
                {
                    "start": 859,
                    "end": 863,
                    "matchedPaperCorpusId": "259092330"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "266191652",
            "title": "Towards Optimal Statistical Watermarking",
            "text": "The prevalence of large language models (LLMs) in recent years makes it challenging and important to detect whether a human-like text is produced by the LLM system (Kirchenbauer et al., 2023a;Kuditipudi et al., 2023;Christ et al., 2023;Yoo et al., 2023;Fernandez et al., 2023;Fu et al., 2023;Wang et al., 2023;Yang et al., 2023;Liu et al., 2023;Zhao et al., 2023;Koike et al., 2023). On the one hand, some of the most advanced LLMs to date, such as GPT-4 (OpenAI, 2023a), are good at producing human-like texts, which might be hard to distinguish from human-generated texts even for humans in various scenarios. On the other hand, it is important to keep human-produced text datasets separated from machine-produced texts in order to avoid the spread of misleading information (Vincent, 2022) and the contamination of training datasets for future language models (Kuditipudi et al., 2023). \n\nTo detect machine-generated content, a recent line of work (Kirchenbauer et al., 2023a;Kuditipudi et al., 2023;Christ et al., 2023) proposes to inject statistical watermarks, a signal embedded within the generated texts which reveals the generation source, into texts. As discussed in Kuditipudi et al. (2023), there are three desirable properties of watermarking: 1. distortion-free: the watermark should not alert the distribution of the generated texts; 2. agnostic: the detector should not know the language model or the prompt; 3. robust: the detector should be able to detect the watermark even under slight perturbation of the generated texts. However, previously proposed methods are either heuristic or guaranteed by different, sub-optimal mathematical descriptions of the above properties, making it difficult to systematically evaluate the watermarking schemes and to draw useful statistical conclusions.",
            "score": 0.709499891215287,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 889
                },
                {
                    "start": 892,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1807
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93798828125
        },
        {
            "corpus_id": "269614196",
            "title": "Who Wrote This? The Key to Zero-Shot LLM-Generated Text Detection Is GECScore",
            "text": "The history of zero-shot methods for detecting LLM-generated text can be traced back to research on machine translation (MT) detection. These studies utilized linguistic features to determine whether a given text was generated by MT systems (Corston-Oliver et al., 2001;Arase and Zhou, 2013). Progress in the field has led to the development of a suite of zero-shot methods specifically designed for LLM-generated text detection. These methods base their calculations on the logits that are generated by the source model, including approaches such as Log-Likelihood (Solaiman et al., Corresponding author.",
            "score": 0.707915314154888,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 605
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 270,
                    "matchedPaperCorpusId": "890333"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78173828125
        },
        {
            "corpus_id": "270258466",
            "title": "Distortion-free Watermarks are not Truly Distortion-free under Watermark Key Collisions",
            "text": "In an era where artificial intelligence surpasses human capabilities in generating text, the authenticity and origin of such AI-generated content have become paramount concerns.Language model watermarking (Aaronson, 2022;Kirchenbauer et al., 2023;Christ et al., 2023;Kuditipudi et al., 2023;Hu et al., 2023a) provides a promising solution for distinguishing between human and machine-generated text.This technique secretly embeds a statistical signal into the generated text using a pseudo-random generator seeded with watermark keys.The embedded signal is then detected through a statistical hypothesis test, ensuring the traceability and verification of the text's origin.\n\nDistortion-free watermarks (Aaronson, 2022;Christ et al., 2023;Kuditipudi et al., 2023;Hu et al., 2023a) represent one of the most compelling techniques in language model watermarking.These watermarks are particularly valuable because they provably preserve the output distribution of the original language model.Specifically, the expected watermarked distribution with respect to the watermark keys remains identical to the original language model distribution, thus offering significant practical application potential.\n\nHowever, the pseudo-random nature of the watermark generator may lead to correlations between generated content when the watermark keys are identical (i.e., key collision).In extreme cases, such as when the prompt remains the same, key collisions can result in identical generated content, significantly limiting its application scenarios.For instance, when using GPT-4 to generate content, and Kuditipudi et al. (2023) adopted the inverse-sampling and Gumbel-reparametrization to modify the watermarked token distributions, where the watermark keys are based on the token position or a fixed key list respectively.Notice Christ et al. (2023)'s method encounters resilience challenges under modifications and lacks empirical evidence regarding its detectability.Meanwhile, Kuditipudi et al. (2023)'s detection process involves hundreds of resampling steps from the secret key distribution, proving inefficient for processing lengthy texts.Hu et al. (2023a) employed inverse-sampling and permute-reweight methods for watermarking.",
            "score": 0.7068313451004888,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 177,
                    "end": 399
                },
                {
                    "start": 399,
                    "end": 534
                },
                {
                    "start": 534,
                    "end": 674
                },
                {
                    "start": 676,
                    "end": 860
                },
                {
                    "start": 860,
                    "end": 989
                },
                {
                    "start": 989,
                    "end": 1197
                },
                {
                    "start": 1199,
                    "end": 1371
                },
                {
                    "start": 1371,
                    "end": 1538
                },
                {
                    "start": 1538,
                    "end": 1814
                },
                {
                    "start": 1814,
                    "end": 1961
                },
                {
                    "start": 1961,
                    "end": 2138
                },
                {
                    "start": 2138,
                    "end": 2228
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83056640625
        },
        {
            "corpus_id": "272827661",
            "title": "ESPERANTO: Evaluating Synthesized Phrases to Enhance Robustness in AI Detection for Text Origination",
            "text": "The identification of machine-generated text has been an active field of study preceding the unveiling of LLMs [49,50]. \n\nThe emergence of LLMs has heightened the urgency and priority of devising effective techniques for the identification of synthetic content. Broadly speaking, AI-text detection techniques can be classified into four categories: statistical [13,51,52], information retrieval [32,53,54], zero-shot [22,55], and watermarking [28,29,30,31,56] methods. Statistical methods involve analyzing the distribution of linguistic patterns in a text to extract statistical features, which are subsequently used to determine whether the text is human-written or AI-generated. Building on the fact that most language models tend to sample from the head of the distribution, Gehrmann et al. [13]. introduce a statistical approach that incorporates three tests: the probability of the word, the absolute rank of a word, and the entropy of the predicted distribution. These tests enable them to quantify the likelihood that a generated word is derived from the top of the distribution and to evaluate whether the previously generated context is recognized by the detection system. The research conducted by Crothers et al. [57] demonstrates that despite the fact that neural network features outperform statistical features, the integration of statistical features can enhance the robustness against particular adversarial attacks. By leveraging information retrieval principles, Krishna et al. [32] suggest a defense against paraphrase attacks through the retrieval of earlier-created AI-text. Their approach involves storing all LLM-generated texts in a database and then searching the entire database for a text that approximately matches the content of the input query. However, retrieval-based detection methods require maintaining a substantial database of LLM-generated texts, and querying this database to find matches can be excessively time-consuming. \n\nIn an alternative approach to detecting AI-generated text, researchers have made attempts to utilize LLMs to compel them to identify the content that they have generated themselves in a zero-shot manner. Based on the assumption that the ChatGPT [42] model make fewer modifications to LLM-generated texts compared to human-written texts, Zhu et al. [22] develop a zero-shot and black-box detection method.",
            "score": 0.704131721743239,
            "section_title": "Related work",
            "char_start_offset": 8219,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 122,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1963
                },
                {
                    "start": 1966,
                    "end": 2169
                },
                {
                    "start": 2170,
                    "end": 2370
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 115,
                    "matchedPaperCorpusId": "13394455"
                },
                {
                    "start": 115,
                    "end": 118,
                    "matchedPaperCorpusId": "6889400"
                },
                {
                    "start": 365,
                    "end": 368,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 395,
                    "end": 399,
                    "matchedPaperCorpusId": "257687440"
                },
                {
                    "start": 402,
                    "end": 405,
                    "matchedPaperCorpusId": "259244038"
                },
                {
                    "start": 417,
                    "end": 421,
                    "matchedPaperCorpusId": "266164139"
                },
                {
                    "start": 421,
                    "end": 424,
                    "matchedPaperCorpusId": "260379022"
                },
                {
                    "start": 443,
                    "end": 447,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 1225,
                    "end": 1229,
                    "matchedPaperCorpusId": "247451060"
                },
                {
                    "start": 1497,
                    "end": 1501,
                    "matchedPaperCorpusId": "257687440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87060546875
        },
        {
            "corpus_id": "258762215",
            "title": "Large Language Models can be Guided to Evade AI-Generated Text Detection",
            "text": "In recent years, the research community has developed a wide range of detectors for AI-generated contents. In general, these detectors can be classified into three categories: training-based, statistical, and watermarking methods. Training-based methods treat the detection problem as a binary classification task, where neural networks are trained using AI-generated text and human-written text. Early studies utilized classifiers to identify fake reviews (Hovy, 2016) and fake news (Zellers et al., 2019). More recently, researchers have trained classifiers using text generated by LLMs, such as the GPT-3.5 detector (Guo et al., 2023) and GPT-2 detector (Solaiman et al., 2019). Statistical methods, on the other hand, focus on zero-shot detection without any additional training overhead. These methods seek to distinguish between human-written text and AI-generated text based on the statistical characteristics of text, such as the statistical irregularities in measures like entropy (Lavergne et al., 2008), perplexity (Beresneva, 2016) and token rank (Gehrmann et al., 2019). A recent method, DetectGPT (Mitchell et al., 2023), exploits the phenomenon that AI-generated text tends to lie in the negative curvature regions of log probability of text. The watermarking methods involve modifying the LLM's text generation process to imprint specific patterns on the generated text, such that it can be detected (Abdelnabi & Fritz, 2021;Grinbaum & Adomaitis, 2022;Kirchenbauer et al., 2023). Although the proposed method SICO primarily focuses on the first two types of detection methods, it can also help evade watermarking when acted as an external paraphraser, as shown in Appendix F. \n\nRecent studies have found that paraphrasing can evade these detectors, which trains an additional neural network to rewrite the original AI-generated text (Krishna et al., 2023;Sadasivan et al., 2023). In contrast, SICO eliminates the need for extra models or training steps.",
            "score": 0.6986523235479467,
            "section_title": "Related works 2.1 AI-generated text detection",
            "char_start_offset": 2098,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 1969
                }
            ],
            "ref_mentions": [
                {
                    "start": 484,
                    "end": 506,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 990,
                    "end": 1013,
                    "matchedPaperCorpusId": "12098535"
                },
                {
                    "start": 1026,
                    "end": 1043,
                    "matchedPaperCorpusId": "1175726"
                },
                {
                    "start": 1059,
                    "end": 1082,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 1416,
                    "end": 1441,
                    "matchedPaperCorpusId": "221516138"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "259075804",
            "title": "Distinguishing Human Generated Text From ChatGPT Generated Text Using Machine Learning",
            "text": "At first tweets were generated by different language models. Then different machine learning methods were used with tf-idf and Bag of Words techniques.Sankar Sadasivan et al. [18] evaluated the effectiveness of several existing approaches for detecting AI-generated text including rule-based methods, statistical methods, and machine learningbased methods.They discover that although these techniques can be effective in identifying some sorts of AI-generated text, they are frequently open to adversarial attempts that trick them into thinking the material is human-generated. A lightweight neural network-based paraphraser was developed and applied it to the AI-generated texts.John Kirchenbauer et al. [19] introduced a watermarking method. This method add a small amount of noise to the weights of the LLM during training.The noise is made with the intention of encoding a distinct watermark signal that can later be decoded by a watermark detector.The GPT-2 and GPT-3 language models are used to demonstrate the utility of their watermarking technology. The watermark can be found even after fine-tuning the LLM on fresh data and that it is resistant to a variety of attacks, including gradient masking and weight perturbations. In a paper Kalpesh Krishna et al. [20] created a substantial amount of AI-generated text samples using a number of cutting-edge language models, such as GPT-3 and T5. The efficiency of several rule-based and machine learning-based text-derived AI detectors is then assessed using the created samples. A retrieval-based defensive method was proposed that depends on determining the text's original author. The suggested technique operates by maintaining a database of known AI-generated text samples and their associated original sources, and by comparing any new text samples against this database to identify probable sources. The authors demonstrate that the suggested retrieval-based defensive mechanism is successful in identifying material that has been paraphrased by AI, with detection a good accuracy. Souradip Chakraborty et al. [21] proposed multiple possibilities that can detect AI generated texts including some statistical methods and several machine learning algorithms. In most of the papers GPT-2 or previous version of GPT-3 were used.",
            "score": 0.698561448561721,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 6551,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2219
                },
                {
                    "start": 2220,
                    "end": 2287
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82373046875
        },
        {
            "corpus_id": "270063331",
            "title": "Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via Code Rewriting",
            "text": "Detecting AI-generated (synthetic) text has been studied before the emergency of LLM.The main works on synthetic text detection follow two lines of research.One research line formulates detection as a binary classification problem by collecting synthetic texts from generative models and training a supervised model based on pre-trained transformers [19,51,53] or other neural models [3,43].Another research line detects synthetic text by designing zero-shot metrics.These metrics measure the relationship between a given text and the text distribution of generative models.Gehrmann et al. [11] claim that synthetic texts are sampled from the head of generative models' distribution, so the average log probability score under the generative model G of a given text, i.e., 1   =1 ( G (  | 1 , ...,   \u22121 )), can be a simple and effective zero-shot metric (called GLTR).Su et al. [40] proposed two metrics, LRR and NPR, where the former combines log-rand and loglikelihood to better magnify the differences between human-written and machine-generated text, while the latter is primarily based on the idea that the log-rank of machine-generated texts should be more sensitive to smaller perturbations.Mitchell et al. [27] further improved the GLTR metrics by proposing that synthetic texts tend to occupy negative curvature regions of the model's log probability function, i.e., the local maximum of the generative model's distribution.They use the average token probability disparity between the given text and perturbed texts to detect whether the given text is located at the negative curvature regions of   ().Overall, all of these methods converge on the same idea: LLMs tend to generate tokens with higher confidence.For tokens generated by LLMs themselves, there will be assigned a higher log probability.Therefore, effective differentiation can be achieved by statistically analyzing token log probabilities.\n\nIn addition to detection methods, some approaches have explored adding watermarks to the generated text during the generation process behind LLM services [21,50].Through watermark extraction,  it becomes feasible to discern whether a given text originates from a model.",
            "score": 0.698289542016289,
            "section_title": "Detection of Synthetic Text",
            "char_start_offset": 9882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 85,
                    "end": 157
                },
                {
                    "start": 157,
                    "end": 391
                },
                {
                    "start": 391,
                    "end": 467
                },
                {
                    "start": 467,
                    "end": 574
                },
                {
                    "start": 574,
                    "end": 868
                },
                {
                    "start": 868,
                    "end": 1198
                },
                {
                    "start": 1198,
                    "end": 1433
                },
                {
                    "start": 1433,
                    "end": 1611
                },
                {
                    "start": 1611,
                    "end": 1720
                },
                {
                    "start": 1720,
                    "end": 1809
                },
                {
                    "start": 1809,
                    "end": 1913
                },
                {
                    "start": 1915,
                    "end": 2077
                },
                {
                    "start": 2077,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 350,
                    "end": 354,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 354,
                    "end": 357,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 357,
                    "end": 360,
                    "matchedPaperCorpusId": "222377949"
                },
                {
                    "start": 387,
                    "end": 390,
                    "matchedPaperCorpusId": "221835708"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74951171875
        },
        {
            "corpus_id": "260351507",
            "title": "Three Bricks to Consolidate Watermarks for Large Language Models",
            "text": "Discerning between generated and natural texts is increasingly challenging. In this context, watermarking emerges as a promising technique for ascribing text to a specific generative model. It alters the sampling generation process to leave an invisible trace in the output, facilitating later detection. This research consolidates watermarks for large language models based on three theoretical and empirical considerations. First, we introduce new statistical tests that offer robust theoretical guarantees which remain valid even at low false-positive rates (less than 10\u22126). Second, we compare the effectiveness of watermarks using classical benchmarks in the field of natural language processing, gaining insights into their real-world applicability. Third, we develop advanced detection schemes for scenarios where access to the LLM is available, as well as multi-bit watermarking.",
            "score": 0.6966463737624835,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9736328125
        },
        {
            "corpus_id": "259075804",
            "title": "Distinguishing Human Generated Text From ChatGPT Generated Text Using Machine Learning",
            "text": "To detect AI generated texts multiple approaches are proposed. Traditionally Sebastian Gehrmann et al. [11] proposed a statistical method to distinguish machine and human generated text. The paper introduces a tool named GLTR. The GLTR tool is a 6-gram character-level statistical language model model, which is trained on a large corpus of text data. The tool uses this model to calculate the probability of each character in the generated text, and then shows any character that has a low probability of occurring in the training corpus. Anton Bakhtin et al. [12] proposed a Energy-based model (EBM) to discriminate machine generated text. EBM is also a statistical model which finds an energy function from given data. The authors used a comparatively larger dataset collected from human to machine conversation. Eric Mitchell et al. [13] proposed a zero shot learning method called DetectGPT. This method detects whether text is machine-written or not by calculating log probabilities computed by the model of interest. On text samples generated by the GPT-2, the research team conducted an study [14]. Atsumu Harada et al. [15] gathered two datasets, one with sentences produced by humans, the other with sentences written by both humans and machines. The cosine similarity between sentence pairs was then calculated as a measure of text consistency. Finally, based on the cosine similarity ratings of the sentences, they classified them as either human-written or human and machine-written using machine learning methods. Sandra Mitrovic et al. [16] proposed a transformer based model to detect chatgpt generated texts. To determine if a text was produced by ChatGPT or a person, the paper's authors developed a machine learning methodology. The model was trained on a dataset of 10,000 text samples that were classified as either human-or ChatGPT-generated. It is based on a combination of text-based and user-based attributes. Tiziano Fagni et al. [17] proposed a method to detect deepFake tweets. At first tweets were generated by different language models.",
            "score": 0.6957573437701943,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 4545,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2066
                }
            ],
            "ref_mentions": [
                {
                    "start": 1128,
                    "end": 1132,
                    "matchedPaperCorpusId": "233263687"
                },
                {
                    "start": 1956,
                    "end": 1960,
                    "matchedPaperCorpusId": "220936111"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84228515625
        },
        {
            "corpus_id": "273228395",
            "title": "Signal Watermark on Large Language Models",
            "text": "Its primary function is to predict if text was generated by a GPT-2 model. Mitchell et al. [30] introduced DetectGPT, a method for zero-shot detection of machine-generated text based on the curvature of the language model's probability function. This approach does not require training a separate classifier or collecting a dataset of real or generated texts. Guo et al. [31] focused on understanding how ChatGPT compares to human experts. They developed a RoBERTa Detector and conducted extensive experiments to effectively detect whether text is generated by ChatGPT or humans, providing a comprehensive analysis of the differences between machine and human-generated content. Alongside academic research, there are several commercial tools that have emerged to address the challenge of detecting modelgenerated text. These include GPTZero [32], Writer [33], AITextClassifier [34], Copyleaks [35], and Sapling [36]. These commercial tools complement the academic models by providing accessible, user-friendly platforms for a wide range of users.",
            "score": 0.6952235743637337,
            "section_title": "B. Detection for Model-generated Text",
            "char_start_offset": 13277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1047
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93994140625
        },
        {
            "corpus_id": "268357168",
            "title": "DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for Identifying Large Language Model Generated Text",
            "text": "Recent endeavors have been directed towards developing classifiers aimed at differentiating between LLM-generated text and human-written text.The prevailing approach entails the collection and labeling of LLM-generated and humanwritten texts, followed by the training of a binary classifier through supervised learning.Although the efficacy of these classifiers has yet to be fully established, some preliminary analyses have been reported [8,9].One study [9] elucidated three distinct methods, substantiated with examples, to circumvent the GPTZero [7] classifier detection.Another investigation [8] conducted a direct assessment of GPTZero's accuracy, uncovering inconsistencies in its ability to detect human-written text.Moreover, classifier-based LLM-generated text detectors commonly necessitate a substantial character count to perform detection accurately.For instance, GPTZero [7] required a minimum of 250 characters to initiate detection.Looking ahead, OpenAI is planning a cryptography-based watermarking system for ChatGPTgenerated text detection [15], although no definitive work has been disclosed as of yet.Zero-shot learning-based methods have also demonstrated some advancement.For example, Cer et al. [16] reported an increment in AUROC from 1% to 14% compared to other zero-shot detection strategies across various datasets; however, the accuracy might still fall short in real-world applications concerning text generated by models.\n\nA method has been proposed for detecting LLM-generated texts based on text watermarking [11], which involves watermarking the text by modifying the LLMs (sensitive tokens are defined and excluded from the output of the LLMs).In contrast, our proposed DeepTextMark does not necessitate access to or modifications of the LLM.Distinct from model-dependent methods, DeepTextMark exhibits a model-independent feature, enabling its application to any text.Moreover, DeepTextMark employs a substantially more compact architecture with about 50 million parameters, whereas the method in [11] necessitates billions of parameters to implement the watermarking process.",
            "score": 0.6939171684531236,
            "section_title": "Text Source Detection for Large Language Models",
            "char_start_offset": 6349,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 142,
                    "end": 319
                },
                {
                    "start": 319,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 575
                },
                {
                    "start": 575,
                    "end": 725
                },
                {
                    "start": 725,
                    "end": 864
                },
                {
                    "start": 864,
                    "end": 949
                },
                {
                    "start": 949,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1196
                },
                {
                    "start": 1196,
                    "end": 1453
                },
                {
                    "start": 1455,
                    "end": 1680
                },
                {
                    "start": 1680,
                    "end": 1778
                },
                {
                    "start": 1778,
                    "end": 1905
                },
                {
                    "start": 1905,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 1543,
                    "end": 1547,
                    "matchedPaperCorpusId": "256194179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.958984375
        },
        {
            "corpus_id": "267294743",
            "title": "Beyond Lexical Boundaries: LLM-Generated Text Detection for Romanian Digital Libraries",
            "text": "Due to the outstanding performance of the generative language models in producing qualitative texts extremely similar to human writings, machine-generated detection algorithms became a necessity in the research field. Despite the release of various detection systems, none of them have proven to be foolproof. This subsection explores the recent state-of-the-art detection methods and algorithms. \n\nAccording to Chakraborty [11], recent research in MGT detection can be split into different categories, namely, statistical approaches, classification-based detection models, zero-shot detection, LLM fine-tuning-based detection methods, and watermark-based identification. \n\nStatistical approaches use metrics like entropy, perplexity, or n-gram frequency to distinguish machine-generated and human content [43,44]. More recent studies proposed DetectGPT [45], which states the artificially generated text tends to lie in the negative curvature of the log-likelihood. The model uses Gradient Boosting [46], a Machine Learning (ML) technique that trains multiple models sequentially and combines them to produce a more accurate model, which outperforms other zero-shot methods with high AUC [47] scores. The algorithm extracts a set of 35 features from the input text, such as sentence length, punctuation usage, and the frequency of particular words and phrases. These features are fed to the Gradient Boosting model that predicts if the text is GPT-generated. The model achieved an F1-score of 98.6%. Even though the DetectGPT algorithm is detailed in the Mitchell [45] paper, the implementation was not open sourced nor publicly available until recently. For this reason, the research community created a public implementation (https://github.com/BurhanUlTayyab/DetectGPT, accessed on 8 January 2024) of the algorithm. The approach preserved the idea from the original paper; however, there are a few differences in the feature extraction and the model training process. In the public implementation, 20 features are extracted from the input text, which includes the frequency of particular words and phrases, sentence length, punctuation usage, and the presence of certain characters.",
            "score": 0.691441812378234,
            "section_title": "Detection Mechanisms",
            "char_start_offset": 20258,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 671
                },
                {
                    "start": 674,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2186
                }
            ],
            "ref_mentions": [
                {
                    "start": 806,
                    "end": 810,
                    "matchedPaperCorpusId": "12098535"
                },
                {
                    "start": 810,
                    "end": 813,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 1000,
                    "end": 1004,
                    "matchedPaperCorpusId": "39450643"
                },
                {
                    "start": 1189,
                    "end": 1193,
                    "matchedPaperCorpusId": "13806304"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9345703125
        },
        {
            "corpus_id": "256274849",
            "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
            "text": "The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.",
            "score": 0.6903242279002464,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "258865409",
            "title": "Who Wrote this Code? Watermarking for Code Generation",
            "text": "Instead, many works still prioritize a detection problem on normal text (Solaiman et al., 2019;Ippolito et al., 2020;Guo et al., 2023;Tian and Cui, 2023;OpenAI, 2023b;Yu et al., 2023;Gehrmann et al., 2019;Mitchell et al., 2023;Yang et al., 2023). While these post-hoc detection methods (i.e., no control during the text generation) have demonstrated powerful performance in the many domain of natural language tasks, their application to programming language remains unexplored. \n\nContrary to the post-hoc detection methods, another line of research for detecting machinegenerated text has gained attention: Watermarkingbased methods, which embed a hidden signal within the generated text (Kirchenbauer et al., 2023a,b;Kuditipudi et al., 2023;Wang et al., 2023). For example, a method proposed in Kirchenbauer et al. (2023a) -which we refer to as WLLM (Watermarking for Large Language Models) -randomly divides the entire vocabulary into two groups (i.e., the green list and the red list) at each generation step and enhance the probability of green list tokens to be sampled. By adding scalar values to the logits of a green list tokens, the model favors generating tokens from the green list rather than the red one. To detect the watermark in a text, we count the number of green tokens and check whether this number is statistically significant (through hypothesis testing) to conclude whether the model output is generated without knowledge of the green-red rule. \n\nWhile both watermarking-based methods and post-hoc detection methods work well in many language generation tasks, we observe that these performances do not transfer well to code generation tasks, for example, in Figure 1. In other words, it is much more challenging to embed watermarks in a detectable way without impairing the code functionality. We attribute this to the nature of extremely low entropy4 of code generation.",
            "score": 0.6903091600920651,
            "section_title": "Introduction",
            "char_start_offset": 1513,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 478
                },
                {
                    "start": 481,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1468
                },
                {
                    "start": 1471,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1896
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 227,
                    "matchedPaperCorpusId": "256274849"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8369140625
        },
        {
            "corpus_id": "266369103",
            "title": "A Survey on the Detection and Impacts of Deepfakes in Visual, Audio, and Textual Formats",
            "text": "One way to detect whether a text is generated by a machine is to use the generative models themselves, such as GPT-2 or Grover, without any additional fine-tuning. These models can recognise the output of similar generative models [239]. Autoregressive models, such as GPT-2, GPT-3, and Grover, predict the next word in a sequence based on prior words, creating a unique statistical pattern in their generated text. \n\nThese patterns, which may include specific phrases, repetitions, or syntax, can be used to differentiate machine-generated text from human-written text. Thus, even without specific fine-tuning, detection algorithms can take advantage of these patterns to identify the content that is likely to be generated by these models [48], [237], [240]. \n\nIn their 2019 study, Solaiman et al. [237] proposed a baseline zero-shot approach that leveraged the total log probability produced by a transformer-based language generative model to discern machine-generated text from human-authored content. Their system operates by determining a threshold value: if a text's total log probability, as determined by the GPT-2 model, is nearer to the average probability of machine-generated texts than human-authored texts, it classifies the text as machine-generated. However, their previously mentioned simple classifier approach proved to be more accurate [237]. Zellers et al. [240] examined the challenges posed by artificially generated text, particularly in the context of fake news. They introduced Grover, a language model constructed similarly to GPT-2 but specifically designed for both generating and detecting fabricated news articles. Grover's approach is based on the idea that knowing how a text is generated is the key to detecting it. To achieve this, Grover leveraged its ability to produce synthetic text, which helps it spot patterns that are common in machine-generated content. This allows Grover to identify materials that are likely to be produced using similar models. Grover was specifically trained to identify fake news and distinguish it from other zero-shot models. It uses statistical patterns learnt from its training data to detect anomalies that may indicate machine-generated text.",
            "score": 0.6881543730266563,
            "section_title": "3) METHODS BASED ON ZERO SHOT MODELS",
            "char_start_offset": 87987,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 760
                },
                {
                    "start": 763,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2095
                },
                {
                    "start": 2096,
                    "end": 2216
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 236,
                    "matchedPaperCorpusId": "252907813"
                },
                {
                    "start": 741,
                    "end": 745,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.951171875
        },
        {
            "corpus_id": "258714683",
            "title": "Watermarking Text Generated by Black-Box Language Models",
            "text": "In this paper, we propose a watermarking framework for injecting authentication watermarks into text generated from black-box language models. The motivation is to enable third-parties who employ black-box language model services (e.g., APIs) to autonomously inject watermarks in their generated text for the purposes of detection and authentication. Extensive experiments on text datasets with different languages and topics (Generality) have demonstrated that the watermark retains a connection to the original semantics (Fidelity), making it challenging for adversaries to remove the watermark without affecting the integrity of the original content (Robustness). We hope our method can provide new insights for generated text detection and inspire more future work.",
            "score": 0.6871184516422593,
            "section_title": "CONCLUSION",
            "char_start_offset": 48462,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 769
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89453125
        },
        {
            "corpus_id": "263831345",
            "title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
            "text": "Our research centers on zero-shot detection of machine-generated text, under the premise that our model has not undergone training using any machine-generated text. This distinguishes our approach from conventional supervised methods, which commonly employ discriminative training strategies to acquire specific syntactic or semantic attributes customized for machine-generated text. In contrast, our zero-shot methodology capitalizes on the inherent capabilities of large language models to identify anomalies that function as markers of machine-generated content. \n\nThe White-box Setting. Conventional zero-shot methodologies often operate under the assumption that the source model responsible for generating machine-generated text is accessible. We refer to this context as the white-box setting, where the primary goal is to distinguish machine-generated texts produced by the source model from those generated by humans. In this white-box setting, our detection decisions are dependent on the source model, but it is not mandatory to possess detailed knowledge of the source model's architecture and parameters. For instance, within the white-box framework, a system like DetectGPT utilizes the OpenAI API to identify text generated by GPT-3, without requiring extensive knowledge of the inner workings of GPT-3. \n\nThe Black-box Setting. In real-world situations, there could be instances where we lack knowledge about the specific source models employed for content generation. This necessitates the development of a versatile detector capable of identifying texts generated by a variety of automated systems. We term this scenario the black-box setting, where the objective is to differentiate between machinegenerated texts produced by diverse, unidentified models and those composed by humans. In this context, the term \"black box\" signifies that we lack access to information about the source model or any details pertaining to it. \n\nEvaluation Metric (AUROC). Instead of measuring the detection accuracy with a specific threshold (\u03f5 in Figure 2), we measure the detection accuracy in the area under the receiver operating characteristic (AUROC), profiling the detectors on the whole spectrum of the thresholds. AUROC ranges from 0.0 to 1.0, mathematically denoting the probability of a random machine-generated text having a higher predicted probability of being machine-generated than a random human-written text.",
            "score": 0.6853582512752884,
            "section_title": "A ZERO-SHOT DETECTION TASK AND SETTINGS",
            "char_start_offset": 24848,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1942
                },
                {
                    "start": 1945,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2222
                },
                {
                    "start": 2223,
                    "end": 2426
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95458984375
        },
        {
            "corpus_id": "263605550",
            "title": "On the Generalization of Training-based ChatGPT Detection Methods",
            "text": "For open-source language models such as GPT-2 (Solaiman et al., 2019), andLLaMa (Touvron et al., 2023), since their model parameters are publicly available, information such as model probability scores can be leveraged for detection. For example, DetectGPT (Mitchell et al., 2023) assumes that LLMs always generate the texts with high probability scores. Thus, it manipulates the candidate texts (by editing or paraphrasing) to check whether the model gives a lower probability score. \n\nBesides, there are watermarking strategies (Kirchenbauer et al., 2023) which intervene the text generation process to inject watermarks into the generated texts to make them identifiable. \n\nDetecting ChatGPT generated texts is also an important task because of the extraordinary prevalence of social-wide usage of ChatGPT. However, many previously mentioned methods are not applicable due to the lack of access to ChatGPT's model & probability scores. Therefore, plenty of works leverage the Training-based Methods (Guo et al., 2023;OpenAI, 2019;Chen et al., 2023), to train classification models to predict whether a text x is human-written or ChatGPT generated: \n\nPreprint. where D H and D C represent the collected human and ChatGPT texts, respectively. Besides, there are \"similarity-based\" methods, such as GPT-Pat (Yu et al., 2023) and DNA-GPT (Yang et al., 2023) to compare the similarity of a text x with its ChatGPT re-generated texts. Besides, \"scorebased methods\" such as GPT-Zero (GPTZero.com) and GLTR (Gehrmann et al., 2019) detection ChatGPT texts based on their specific traits. More details of these methods are in Appendix D.",
            "score": 0.6823054439655665,
            "section_title": "OPEN-SOURCE LANGUAGE MODEL DETECTION AND CHATGPT DETECTION",
            "char_start_offset": 5115,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 484
                },
                {
                    "start": 487,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1150
                },
                {
                    "start": 1153,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1630
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8203125
        },
        {
            "corpus_id": "262465111",
            "title": "From Text to Source: Results in Detecting Large Language Model-Generated Content",
            "text": "Detecting AI-generated text is a recent and rapidly growing area of research (Jawahar et al., 2020). Although Sadasivan et al. (2023) demonstrated a theoretical impossibility of distinguishing between human-written and machine-generated when the total variation (TV) norm between the two is low, a more recent study by Chakraborty et al. (2023) showed that detection is still possible given enough samples. \n\nPopular methods to detect AI-generated text can be grouped into three categories: 1) Using statistical features of text such as perplexity, n-grams, entropy, etc. (Gehrmann et al., 2019;Mitchell et al., 2023). 2) Watermarking generated text which was first demonstrated by Atallah et al. (2001) who embedded a watermark bit in the syntactic structure of the text. More recently, Kirchenbauer et al. (2023) used the LLM's output log probability at each generation step to embed a watermark based on \"green/red\" token list where an LLM will have an artificially increased likelihood of selecting tokens from the \"green\" list. Other work on watermarking include (Fernandez et al., 2023;Christ et al., 2023). 3) Classifier-based approaches which use a classifier trained on a dataset containing both human-written and machine-generated text to detect LM-generated text (Zellers et al., 2019;Solaiman et al., 2019;Uchendu et al., 2020;Fagni et al., 2021;Antoun et al., 2021;Guo et al., 2023;Mitrovi\u0107 et al., 2023). This approach is vulnerable against adversarial text mimicking, among others, Wikipedia style and informative (Antoun et al., 2023). \n\nWe highlight recent work by Mireshghallah et al. (2023) that studies cross-model detection and detector transferability by examining the effect of using classifier models other than the generator itself to detect machine-generated text.",
            "score": 0.6809070532051753,
            "section_title": "Related Works",
            "char_start_offset": 4247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 406
                },
                {
                    "start": 409,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1551
                },
                {
                    "start": 1554,
                    "end": 1790
                }
            ],
            "ref_mentions": [
                {
                    "start": 77,
                    "end": 99,
                    "matchedPaperCorpusId": "226237099"
                },
                {
                    "start": 572,
                    "end": 595,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 682,
                    "end": 703,
                    "matchedPaperCorpusId": "37687669"
                },
                {
                    "start": 1274,
                    "end": 1296,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 1318,
                    "end": 1339,
                    "matchedPaperCorpusId": "221835708"
                },
                {
                    "start": 1339,
                    "end": 1358,
                    "matchedPaperCorpusId": "220936111"
                },
                {
                    "start": 1358,
                    "end": 1378,
                    "matchedPaperCorpusId": "229923851"
                },
                {
                    "start": 1529,
                    "end": 1550,
                    "matchedPaperCorpusId": "259129474"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "258865367",
            "title": "LLMDet: A Third Party Large Language Models Generated Text Detection Tool",
            "text": "Recently, the emergence of ChatGPT1 has heralded a \"Cambrian Explosion\" for generative large language models (LLMs). GPT-4 (OpenAI, 2023), Bard2 , PaLM-2 (Anil et al., 2023), and other LLMs from internet companies are currently flourishing, while open-source communities are witnessing a proliferation of open-source models like LLaMA (Touvron et al., 2023a), OPT (Liu et al., 2021), ChatGLM (Du et al., 2022). These models are capable of generating coherent, fluent, and meaningful text. However, the formidable text generation capabilities of generative language models have also raised concerns about their potential misuse in domains such as phishing, spreading false information, and academic fraud. Additionally, with the application of products like ChatGPT, the future abundance of machine-generated text data has the potential to contaminate genuine humangenerated data (Hataya et al., 2022), altering the data ecosystem of the real world. \n\nAccordingly, the study of practical content generation detection tools has attracted widespread attention from the community. Recently, the primary focus of research is on approaching the text detection problem as a binary classification task to distinguish machine-generated text and humanauthored text, making it hard to assign responsibility to a specific model or its provider. Nevertheless, Watermarking (Kirchenbauer et al., 2023) methods necessitate altering the text generation process, leading to a compromise in the quality of the generated content. Techniques like GPT-zero3 , Detect-GPT (Mitchell et al., 2023), and the classifier in OpenAI (OpenAI, 2023) require access to the deployed model, thereby resulting in high cost and intractability for third parties. \n\nThus, a practical LLM detection tool should possess the following capabilities, which are also the objectives of our method: Specificity: Merely focusing on identifying human and machinegenerated text is insufficient for duty attribution. There is a pressing need for the ability to recognize the specific model responsible for generating the text. Safety: Ensuring model security and mitigating potential risks require a detection method that does not require accessing model parameters.",
            "score": 0.6808730979911986,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1725
                },
                {
                    "start": 1728,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2216
                }
            ],
            "ref_mentions": [
                {
                    "start": 392,
                    "end": 409,
                    "matchedPaperCorpusId": "247519241"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "258740888",
            "title": "Smaller Language Models are Better Black-box Machine-Generated Text Detectors",
            "text": "The problem of machine-generated text detection has already been studied for multiple years using a variety of different approaches (Ippolito et al., 2020;Jawahar et al., 2020;Uchendu et al., 2020, 2021): Both Gehrmann et al. (2019) and Dugan et al. (2022) have found that humans generally struggle to distinguish between human-and machine-generated text, thereby motivating the development of automatic solutions. Among those, some methods aim to detect machinegenerated text by training a classifier in a supervised manner (Bakhtin et al., 2019;Uchendu et al., 2020), while others perform detection in a zero-shot manner (Solaiman et al., 2019;Ippolito et al., 2020). There is also a line of work that relies on bot detection through question answering (Wang et al., 2023;Chew and Baird, 2003), which is outside the scope of this paper. \n\nMost recently, Mitchell et al. (2023) introduced the zero-shot method DetectGPT, which is based on the hypothesis that texts generated from a LLM lie on local maxima, and therefore negative curvature, of the model's probability distribution. Other strategies have been proposed to enable the detection of machinegenerated text in the wild. Particularly through efforts on the side of the LLM provider, more powerful detection methods can be devised. One such method is watermarking, which injects algorithmically detectable patterns into the released text while ideally preserving the quality and diversity of language model outputs. Watermarks for natural language have already been proposed by Atallah et al. (2001) and have since been adapted for outputs of neural language models (Fang et al., 2017;Ziegler et al., 2019). Notable recent attempts for transformer based language models include work by Abdelnabi and Fritz (2021), who propose an adversarial watermarking transformer (AWT). While this watermarking method is dependent on the model architecture, Kirchenbauer et al. (2023) propose a watermark that can be applied to texts generated by any common autoregressive language model.",
            "score": 0.6800612239636035,
            "section_title": "Related Work",
            "char_start_offset": 20734,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 838
                },
                {
                    "start": 841,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 2033
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 155,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 155,
                    "end": 176,
                    "matchedPaperCorpusId": "226237099"
                },
                {
                    "start": 176,
                    "end": 196,
                    "matchedPaperCorpusId": "221835708"
                },
                {
                    "start": 547,
                    "end": 568,
                    "matchedPaperCorpusId": "221835708"
                },
                {
                    "start": 646,
                    "end": 668,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 1537,
                    "end": 1558,
                    "matchedPaperCorpusId": "37687669"
                },
                {
                    "start": 1625,
                    "end": 1644,
                    "matchedPaperCorpusId": "10494183"
                },
                {
                    "start": 1644,
                    "end": 1665,
                    "matchedPaperCorpusId": "202537031"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82373046875
        },
        {
            "corpus_id": "267365030",
            "title": "Detecting Multimedia Generated by Large AI Models: A Survey",
            "text": "There are existing surveys providing a detailed overview of current detection strategies and benchmarks [18]- [20]. Specifically, [18] categorizes the detection methods into three groups: training-based, zero-shot, and watermarking. Works in each group are further distinguished according to their detection scenarios such as black-box and whitebox. The authors in [19] categorize the detection of LLMgenerated content based on the techniques applied by detectors (e.g., adversarial learning, watermarking methods, and human-assisted methods) and the dependency of detectors on training (e.g., zero-shot detectors, fine-tuned detectors). [20] divides the existing literature into two parts: methods designed to detect LLM-generated text (e.g., watermarkingbased, fine-tune-based, and zero-shot) and methods designed to evade detection (e.g., paraphrasing attacks, spoofing attacks). Although these organizational strategies apply to recent detection methods, their taxonomy may not be sufficient for adapting to new or evolving detection techniques. To this end, we provide a novel taxonomy based on Pure Detection and Beyond Detection.",
            "score": 0.6798586151917345,
            "section_title": "Text",
            "char_start_offset": 17239,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1136
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8447265625
        },
        {
            "corpus_id": "270370917",
            "title": "Improving Logits-based Detector without Logits from Black-box LLMs",
            "text": "Detection of LLMs-Generated Text. The burgeoning capabilities of advanced large language models (LLMs) underscore the imperative for robust methodologies aimed at detecting these models. Specifically, the detection is to distinguish whether a given text originates from a language model on the condition that the model is known (White-box) [27] or unknown (Black-box) [30]. The earlier work focused on feature-based methods, like [31][32][33][34]. While in the era of LLMs, the training-based methods [31,[35][36][37] are aroused to counter with LLMs's strong ability to produce high-quality text. They usually involve training a binary classifier using text generated by AI or humans. Besides, zero-shot detectors leverage the inherent statistical feature differences between LLMs and humangenerated text without requiring training, including probability curvature (DetectGPT [27]), N-gram divergence (DNA-GPT [28]), and conditional probability curvature (Fast-DetectGPT [29]), the editing distance of the output [38], and style representations [39], enhancing their ability to adapt to new data distributions and source models. \n\nBlack-box Detection. Given the proprietary nature of the latest LLMs [1,2,6], there is a critical need for effective black-box detection methods. Present techniques falter when direct access to the source model is restricted. The training-based methods, like OpenAI text classifier [40], GPTZero [22], G3detector [41], and GPT-Sentinel [42] usually closely adhere to the specific distributions of text domains and source models during training, thereby lacking generalization ability and robustness on model updates. For zero-shot methods [43, 38, 27-29, 44, 45, 34, 46, 14, 47-49] in the blackbox detection settings, they usually rely on a surrogate model for scoring. However, the efficacy of these surrogate models often falls short compared to white-box detection, where access to the source model is available.",
            "score": 0.6795135878417188,
            "section_title": "Related Work",
            "char_start_offset": 5389,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1947
                }
            ],
            "ref_mentions": [
                {
                    "start": 438,
                    "end": 442,
                    "matchedPaperCorpusId": "4324753"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "273185492",
            "title": "AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text",
            "text": "The latter is more similar to our work as they also measure the novelty of generated n-grams and compare it to human-written text, however they differ from us in tow major ways: (1) they only find verbatim matches, whereas we also match to approximate, semantically similar blocks of text and (2) they compute the percentage of n-grams of a certain length in a text that can be found in the reference corpus, whereas we measure how much of the text can be reconstructed by mixing and matching a vast amount of existing text snippets of varying lengths from the web. \n\nMachine Text Detection: Detecting machine-generated text has been explored for several years using a variety of methods (Jawahar et al., 2020;Uchendu et al., 2021). Gehrmann et al. (2019) and Dugan et al. (2023) demonstrate that even humans tend to struggle to differentiate between text written by humans and machines, highlighting the need for automated detection solutions. Some approaches involve training a classifier in a supervised manner to identify machine-generated text (Bakhtin et al., 2019;Uchendu et al., 2020), while others use a zero-shot detection method (Solaiman et al., 2019;Ippolito et al., 2020). Additionally, there is research on bot detection through question answering (Wang et al., 2023;Chew & Baird, 2003). Recently, Mitchell et al. (2023b) introduced DetectGPT, a zero-shot method based on the hypothesis that texts produced by a large language model (LLM) are located at local maxima, and thus exhibit negative curvature, in the model's probability distribution. Follow-up work build on DetectGPT by making it faster (Bao et al., 2024) and proposing to use cross-detection when the target model is unknown (Mireshghallah et al., 2024).",
            "score": 0.6794002324189862,
            "section_title": "RELATED WORK",
            "char_start_offset": 32069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1733
                }
            ],
            "ref_mentions": [
                {
                    "start": 688,
                    "end": 710,
                    "matchedPaperCorpusId": "226237099"
                },
                {
                    "start": 710,
                    "end": 731,
                    "matchedPaperCorpusId": "237589233"
                },
                {
                    "start": 733,
                    "end": 755,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 760,
                    "end": 779,
                    "matchedPaperCorpusId": "255125274"
                },
                {
                    "start": 1071,
                    "end": 1092,
                    "matchedPaperCorpusId": "221835708"
                },
                {
                    "start": 1163,
                    "end": 1185,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 1282,
                    "end": 1301,
                    "matchedPaperCorpusId": "12757"
                },
                {
                    "start": 1313,
                    "end": 1336,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1615,
                    "end": 1633,
                    "matchedPaperCorpusId": "263831345"
                },
                {
                    "start": 1704,
                    "end": 1732,
                    "matchedPaperCorpusId": "268417138"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.873046875
        },
        {
            "corpus_id": "273653962",
            "title": "Segmenting Watermarked Texts From Language Models",
            "text": "Watermarking is a technique that involves embedding nearly unnoticeable statistical signals within generated content to help trace its source. This work focuses on a scenario where an untrusted third-party user sends prompts to a trusted language model (LLM) provider, who then generates a text from their LLM with a watermark. This setup makes it possible for a detector to later identify the source of the text if the user publishes it. The user can modify the generated text by substitutions, insertions, or deletions. Our objective is to develop a statistical method to detect if a published text is LLM-generated from the perspective of a detector. We further propose a methodology to segment the published text into watermarked and non-watermarked sub-strings. The proposed approach is built upon randomization tests and change point detection techniques. We demonstrate that our method ensures Type I and Type II error control and can accurately identify watermarked sub-strings by finding the corresponding change point locations. To validate our technique, we apply it to texts generated by several language models with prompts extracted from Google's C4 dataset and obtain encouraging numerical results. We release all code publicly at https://github.com/doccstat/llm-watermark-cpd.",
            "score": 0.6790145018353734,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.904296875
        },
        {
            "corpus_id": "274131225",
            "title": "Debiasing Watermarks for Large Language Models via Maximal Coupling",
            "text": "1. The language model provider shares a random watermark key \u03be with the detector. 2. The user sends a prompt P to the language model provider. \n\n3. The language model provider generates a text of length l, denoted by w 1:l = (w 1 , ..., w l ), given the prompt P and the watermark key \u03be. 4. The user sends the text w1:n , which may be either (i) (an edited version of) the generated text w 1:l = (w 1 , ..., w l ) or (ii) a piece of text independent of w 1:l (e.g., text that the user wrote without using the watermarked language model). 5. Using the watermark key \u03be, the detector determines if w1:n contains the embedded watermark. \n\nThis protocol implies that a watermarking scheme has two parts: (1) a decoding (or sampling) algorithm that can embed identifiable statistical signals into the generated text, and (2) a detection algorithm that can detect the watermark signal in the received text. In this remainder of this section, we introduce every aspect of our proposed watermark, including different detection methods, asymptotic theoretical analysis on detection boundaries, and practical considerations. \n\nSetup. For clarity in illustrating the watermarking scheme, we first describe a simplified setup for the communication protocol and delay the adaptation in practice to Section 3.3. In particular, we assume that: (1) the text generation has a predefined length n; (2) the language model provider randomly generates the watermark key \u03be = (G 1:n , \u03b6 1:n ) and shares it with the detector: G 1:n = (G 1 , . . .",
            "score": 0.6774870578979785,
            "section_title": "Algorithm 2 Communication Protocol",
            "char_start_offset": 15988,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 2
                },
                {
                    "start": 3,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 142
                },
                {
                    "start": 145,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1113
                },
                {
                    "start": 1116,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1522
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.845703125
        },
        {
            "corpus_id": "259092330",
            "title": "Undetectable Watermarks for Language Models",
            "text": "Approaches for detecting AI-generated text largely fall into two categories. Watermarking schemes alter the output of a language model in a way that a corresponding detection algorithm can identify. Post-hoc detectors leave the output of the model unchanged and instead identify AI-generated text using existing differences between natural language and the model's output. \n\nPost-hoc detectors. The simplest post-hoc detectors use natural heuristics to distinguish between human-and AI-generated text. These heuristics include relative entropy scoring [LUY08], perplexity [Ber16], and other statistical methods [GSR19]; see [Ber16] for a survey of such methods. Other post-hoc detectors (e.g., [ZHR + 19, MLK + 23, Tia23, KAAL23]) are themselves models, specifically trained for this binary classification task. Unfortunately, these heuristic and model-based methods lack formal guarantees, and it's possible to train a model to transform AI-generated text in a way that evades them; see, e.g., [KSK + 23, SKB + 23]. For example, [KSK + 23] trains a model to paraphrase text output by language models, fooling common post-hoc detectors such as GPTZero [Tia23], DetectGPT [MLK + 23], and the detector developed by OpenAI [KAAL23]. Furthermore, simple tricks such as instructing the model in the prompt to write a response that evades a detector, or varying the model's parameters (e.g., increasing the temperature and frequency/presence penalties for GPT-4), fool [Tia23]. [CBZ + 23] prove that as AI-generated text more closely resembles natural text, post-hoc detectors will need longer text samples. \n\nSee [JAML20] for more comprehensive background on post-hoc detection of AI-generated text and attacks. \n\nLanguage watermarking schemes. Several schemes (e.g., [AF21, QZL + 23, YAJK23, MZ23]) involve using an ML model in the watermarking algorithm itself.",
            "score": 0.6772785753711936,
            "section_title": "Related Work",
            "char_start_offset": 4687,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 372
                },
                {
                    "start": 375,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1601
                },
                {
                    "start": 1604,
                    "end": 1706
                },
                {
                    "start": 1709,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1858
                }
            ],
            "ref_mentions": [
                {
                    "start": 552,
                    "end": 559,
                    "matchedPaperCorpusId": "12098535"
                },
                {
                    "start": 572,
                    "end": 579,
                    "matchedPaperCorpusId": "1175726"
                },
                {
                    "start": 624,
                    "end": 631,
                    "matchedPaperCorpusId": "1175726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92724609375
        },
        {
            "corpus_id": "266163914",
            "title": "Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection",
            "text": "With the advances of large language models (LLMs), there have been a plethora of research on detecting AI-generated content (Becker et al., 2023;Crothers et al., 2022;Jawahar et al., 2020;Mireshghallah et al., 2023;Su et al., 2023;Wang et al., 2023a). Current AI-generated text detection methods can be in general categorized into three categories (Lu et al., 2023): statistical (Gehrmann et al., 2019), watermarking (Kirchenbauer et al., 2023), and training-based (Gall\u00e9 et al., 2021). \n\nThe statistical approach analyzes measures like entropy, perplexity, and n-gram frequency to identify statistical irregularities (Gehrmann et al., 2019;Lu et al., 2023), which rely on statistical characteristics of the text to differentiate between AIgenerated and human-written content. The watermarking methods (Wilson et al., 2014) imprint specific patterns on generated text. Soft watermarking (Kirchenbauer et al., 2023) presents a novel statistical test designed to detect the presence of a watermark, providing interpretable pvalues as evidence, and additionally, establishing an information-theoretic framework that enables the analysis of the watermark's sensitivity. Early training-based detection studies focused on identifying fake reviews (Hovy, 2016) and fake news (Zellers et al., 2019). Recent research employed LLM-generated texts for training classifiers, such as ArguGPT (Liu et al., 2023a), CheckGPT (Liu et al., 2023b), and G3Detector (Zhan et al., 2023). Notably, Guo et al. (2023) fine-tuned the RoBERTa-QA and RoBERTa-single models on the HC3 dataset and achieved high detection accuracy.",
            "score": 0.6760434315088938,
            "section_title": "Related Work",
            "char_start_offset": 27916,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 486
                },
                {
                    "start": 489,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1601
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 188,
                    "matchedPaperCorpusId": "226237099"
                },
                {
                    "start": 379,
                    "end": 402,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 618,
                    "end": 641,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 802,
                    "end": 823,
                    "matchedPaperCorpusId": "6169182"
                },
                {
                    "start": 1241,
                    "end": 1253,
                    "matchedPaperCorpusId": "5315990"
                },
                {
                    "start": 1268,
                    "end": 1290,
                    "matchedPaperCorpusId": "168169824"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9072265625
        },
        {
            "corpus_id": "270703139",
            "title": "Detecting AI-Generated Text: Factors Influencing Detectability with Current Methods",
            "text": "Large language models (LLMs) have advanced to a point that even humans have difficulty discerning whether a text was generated by another human, or by a computer. However, knowing whether a text was produced by human or artificial intelligence (AI) is important to determining its trustworthiness, and has applications in many domains including detecting fraud and academic dishonesty, as well as combating the spread of misinformation and political propaganda. The task of AI-generated text (AIGT) detection is therefore both very challenging, and highly critical. In this survey, we summarize stateof-the art approaches to AIGT detection, including watermarking, statistical and stylistic analysis, and machine learning classification. We also provide information about existing datasets for this task. Synthesizing the research findings, we aim to provide insight into the salient factors that combine to determine how \u201cdetectable\u201d AIGT text is under different scenarios, and to make practical recommendations for future work towards this significant technical and societal challenge.",
            "score": 0.6734783001349617,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9677734375
        },
        {
            "corpus_id": "270559810",
            "title": "Watermarking Language Models with Error Correcting Codes",
            "text": "As language model capabilities improve, there are corresponding potential harms such as the creation of misinformation Zellers et al. (2019) and propaganda Solaiman et al. (2019). To mitigate such harms, a first step is to detect and filter content. A popular approach to reliably detecting AI generated content is to add a watermark (see e.g., Kirchenbauer et al., 2023;Kuditipudi et al., 2023;Aaronson and Kirchner, 2022;Christ et al., 2023;Hu et al., 2024;Huo et al., 2024, etc), a hidden signal embedded in the output that biases generation towards specific patterns that are undetectable to humans. \n\nWe consider the detection setting from the model-provider 's perspective: the detection algorithm receives (user or machine-generated) text as input, but no further metadata such as prompts or generation parameters. We explore how to introduce statistical signals into the generation process to reliably classify text as watermarked. We aim to design an effective watermark with the following properties, inspired by, e.g., (Kuditipudi et al., 2023;Kirchenbauer et al., 2023;Piet et al., 2023). The watermark should not decrease the quality of the outputs of the language model. The watermark should have high detection power, be detectable with only a small number of tokens. The watermark should be robust to watermarking attacks and perturbations, such as edits and deletions of significant fractions of the text, translation attacks, paraphrasing, etc. \n\nPrior watermarking schemes achieve many but not all of these qualities. For instance, Kuditipudi et al. (2023) introduces distortion-free watermarks that do not modify the output distribution in expectation, but their detection method is computationally expensive due to the use of permutation",
            "score": 0.6724920489414631,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 603
                },
                {
                    "start": 606,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1462
                },
                {
                    "start": 1465,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1758
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 140,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 345,
                    "end": 371,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 443,
                    "end": 459,
                    "matchedPaperCorpusId": "264172471"
                },
                {
                    "start": 1055,
                    "end": 1081,
                    "matchedPaperCorpusId": "256194179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "268680797",
            "title": "TrustAI at SemEval-2024 Task 8: A Comprehensive Analysis of Multi-domain Machine Generated Text Detection Techniques",
            "text": "Recent works on LLM-generated1 text detection has shown promising results.Statistical methods are used to detect the LLM-generated text by utilizing the entropy (Shen et al., 2023), and N-gram frequency (Tassopoulou et al., 2021).Some other studies uses the fact that language models assign high probability for the repeated sentences which is often AI model generated and ranks the AI model generated sentence Krishna et al. (2022).In a study, OpenAI has trained a classifier to detect LLMgenerated text using the RoBERTa-based model (Solaiman et al., 2019).\n\nSome of the widely-used methods adopted the GPT detectors such as OpenAI detection classifier2 , GPTZero3 , and ZeroGPT4 .Another variant is DetectGPT (Mitchell et al., 2023), which works on the assumption of LLM-generated text lies in the negative curvature region of the log-likelihood.Using this approach, DetectGPT perturbs the input text using masked language models, such as BERT (Devlin et al., 2018), BART (Lewis et al., 2019), T5 (Raffel et al., 2019) and compare the log probability of the text and masked filled variants.Similarly, few works utilized the different decoding strategies including top-k, nucleus, and temperature sampling to generate the text from GPT2 and BERT based models employed to perform binary classification to label text as human-written or machine generated (Ippolito et al., 2020).\n\nRecently, watermarking methods have been used in enterprises to protect the intellectual properties and fair use of the generation models.However these techniques simplify the detection of the LLMgenerated output text by synonym replacement over generated outputs and text level posthoc lexical substitutions (Li et al., 2023;Sadasivan et al., 2023), and soft watermarking was introduced in (Kirchenbauer et al., 2023) using green and red token lists.Hidden space operations were also introduced by injecting secret signals into the probability vector of each target token (Zhao et al., 2023).",
            "score": 0.6724134071263113,
            "section_title": "Related Work",
            "char_start_offset": 3650,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 74,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 433
                },
                {
                    "start": 433,
                    "end": 559
                },
                {
                    "start": 561,
                    "end": 683
                },
                {
                    "start": 683,
                    "end": 849
                },
                {
                    "start": 849,
                    "end": 1093
                },
                {
                    "start": 1093,
                    "end": 1379
                },
                {
                    "start": 1381,
                    "end": 1519
                },
                {
                    "start": 1519,
                    "end": 1832
                },
                {
                    "start": 1832,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 229,
                    "matchedPaperCorpusId": "229923802"
                },
                {
                    "start": 411,
                    "end": 432,
                    "matchedPaperCorpusId": "248887396"
                },
                {
                    "start": 712,
                    "end": 735,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1355,
                    "end": 1378,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 1690,
                    "end": 1707,
                    "matchedPaperCorpusId": "265352433"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63525390625
        },
        {
            "corpus_id": "257921485",
            "title": "To ChatGPT, or not to ChatGPT: That is the question!",
            "text": "This section provides an overview of current research on distinguishing AI-generated text from human-generated text. \n\nTo categorize most automated machine learning-based detection methods for synthetic texts, we followed OpenAI's classification [33], which divides these methods into three main categories, which are: i) Simple Classifiers [18], [33], ii) Zero-shot detection techniques [22], [25], [39], and iii) Fine-tuning based detection [26]. Simple Classifiers fall under the category of black-box detection techniques, whereas zeroshot and fine-tuning-based detection techniques come under the umbrella of white-box detection techniques. There exists other approaches that do not fit into these three categories; however, they are still significant and merit consideration. These alternative methods include testing ChatGPT-generated text against various plagiarism tools [20], designing a Deep Neural Network-based AI detection tool [6], a samplingbased approach [16], and online detection tools [1], [3], [10], [13], [17], [28], [29], [36], [40]. In the following sections, we will analyze the existing approaches belonging to the aforementioned categories and alternative methods, focusing on their effectiveness in detecting AI-generated text.",
            "score": 0.6724100314462085,
            "section_title": "II. RELATED WORKS",
            "char_start_offset": 4062,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 119,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1255
                }
            ],
            "ref_mentions": [
                {
                    "start": 400,
                    "end": 404,
                    "matchedPaperCorpusId": "168169824"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73876953125
        },
        {
            "corpus_id": "260164516",
            "title": "Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy",
            "text": "Automatic Detection The detection of AI-generated text, particularly in the context of large language models (LLMs), has recently attracted significant research interest (Bakhtin et al. 2019;Schuster et al. 2020;Fr\u00f6hling and Zubiaga 2021;Sadasivan et al. 2023;Mitchell et al. 2023). Previous approaches have primarily focused on leveraging the perplexi-ties of generated texts for detection. For example, Solaiman et al. (2019) utilized a classifier to evaluate the total log probability of the text, using it as a means to determine whether the content originated from a machine. Building on this premise, Mitchell et al. (2023) further validated that the log probability of machine-generated text diminishes upon perturbation, while the log probability of human-written text remains unpredictable when perturbed. \n\nWatermarking There has been a recent emergence of watermarking specific patterns into language models for AI detection. Zhao, Wang, and Li (2023)",
            "score": 0.6723599368141275,
            "section_title": "Related Work",
            "char_start_offset": 5305,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 962
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 191,
                    "matchedPaperCorpusId": "182952342"
                },
                {
                    "start": 191,
                    "end": 212,
                    "matchedPaperCorpusId": "211204954"
                },
                {
                    "start": 212,
                    "end": 238,
                    "matchedPaperCorpusId": "233563022"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94287109375
        },
        {
            "corpus_id": "260334887",
            "title": "Towards Codable Watermarking for Injecting Multi-Bits Information to LLMs",
            "text": "Detecting-based Methods. This kind of method aims to detect whether a text is generated by a language model by formulating the task as a binary classification problem (Bakhtin et al., 2019;Jawahar et al., 2020;OpenAI, 2019). ( 1) One way to achieve this goal is to first collect outputs from a specific LLM along with human-written texts, and then use them to train a binary classifier Mitrovi\u0107 et al. (2023). For example, OpenAI (2019) fine-tune a RoBERTa classification model to detect whether a text is generated by GPT-2 model or not. Later, Mitrovi\u0107 et al. (2023) collect data produced by ChatGPT OpenAI (2022) and fine-tune a DistilBERT model (Sanh et al., 2019) to distinguish texts written by humans and that generated by ChatGPT. OpenAI (2023) create a more advanced classifier that can detect texts generated by a variety of diverse LLMs. \n\n(2) Besides explicitly training deep classifiers, some other methods achieve detection by exploring the statistical differences between LLM-generated texts and human-written texts (Solaiman et al., 2019;Wu et al., 2023). GPTZero (Tian, 2023) is a popular tool to identify machine-generated texts by calculating the perplexity and burstiness scores of texts and comparing them with specific thresholds. \n\nDetectGPT (Mitchell et al., 2023) claims that when a machine-generated text is perturbed, its log probability will always show a decreasing pattern, which is not the same for human-written texts. \n\nBased on this finding, it defines a log probability-based metric to help detect texts generated by LLMs. Recently, there have been some works (Koike et al., 2023;Yang et al., 2023a) exploring the detection of machine-generated text in specific domains. However, all these detection methods face the risk of becoming increasingly ineffective in the future, as the LLMs will be consistently improved to behave more and more like humans, which finally makes detection impractical (Sadasivan et al., 2023).",
            "score": 0.6714272142331874,
            "section_title": "A SUPPLEMENT TO RELATED WORK",
            "char_start_offset": 27760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 848
                },
                {
                    "start": 851,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1252
                },
                {
                    "start": 1255,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1955
                }
            ],
            "ref_mentions": [
                {
                    "start": 386,
                    "end": 408,
                    "matchedPaperCorpusId": "256416337"
                },
                {
                    "start": 546,
                    "end": 568,
                    "matchedPaperCorpusId": "256416337"
                },
                {
                    "start": 1595,
                    "end": 1615,
                    "matchedPaperCorpusId": "260091573"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91845703125
        },
        {
            "corpus_id": "267294743",
            "title": "Beyond Lexical Boundaries: LLM-Generated Text Detection for Romanian Digital Libraries",
            "text": "It requires, however, access to the language model while generating the text. \n\nAnother detection approach based on text classification was proposed by OpenAI (https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text, accessed on 8 January 2024) and consisted in fine-tuning a GPT model with data from Wikipedia, WebText [55], and human input data to create an interface for a discrimination task using outputs produced by 34 language models. Their approach combined the classifier-based method with human evaluation to determine if a text was artificially generated. Nonetheless, this approach has some limitations. The text must have at least 1000 characters, and it was primarily trained on English corpora, making it inappropriate for multilingual use cases. Its authors recommend using the classifier only for English text since it performs significantly worse in other languages. Based on the preliminary evaluations of a set of English texts, the model correctly identifies 26% of AI-written text (true positives) and incorrectly labels human-authored text as AI text for 9% of the texts (false positives). \n\nSimpler classifier methods involve ML models such as XGBoost [56]. In their approach, the input features are based on the TF-IDF score and hand-crafted linguistic features based on characters and punctuation. The authors achieved an F1-score of 99% for detecting ChatGPT text. However, as a limitation, it can easily perform overfitting due to sample bias, requiring a large training dataset to overcome this drawback. \n\nAmong the various detection methods, we can also include fine-tuning language models for binary classification [2]. Solaiman et al. [2] used a sequence classifier model based on RoBERTa-base and RoBERTa-large, which achieved 95% accuracy on a GPT2 dataset detection. The advantage of this method is the bidirectionality, which allows discriminative classification models to be more powerful for detection than generative classification models. \n\nHowever, despite the numerous studies targeting MGT detection, Krishna et al. [57] highlight that paraphrased text escapes the existent detectors, including watermarking, DetectGPT, or GPTZero, with an important drop in performance.",
            "score": 0.667640569778096,
            "section_title": "Detection Mechanisms",
            "char_start_offset": 26336,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 80,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1550
                },
                {
                    "start": 1553,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1996
                },
                {
                    "start": 1999,
                    "end": 2231
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8330078125
        },
        {
            "corpus_id": "258714683",
            "title": "Watermarking Text Generated by Black-Box Language Models",
            "text": "Statistical Discrepancy Detection. Several methods distinguish between generated and human-written text by identifying statistical discrepancies between them, as exemplified by two recent tools: GPTZero [40] and DetectGPT [18]. GPTZero uses perplexity and burstiness to tell apart human-written and generated text, as language models tend to produce more predictable and consistent text based on the patterns they learned from training data, resulting in lower perplexity scores for generated text. DetectGPT exploits the negative curvature regions of a model's log probability function to identify generated text by comparing the log probability of unperturbed and perturbed text variations. However, as language models are constantly improving and becoming more sophisticated, these heuristic features struggle to achieve robustness and generalization. Deep Learning-based Detection. Deep learning-based methods rely on gathering human-written and generated samples to train classifiers. Recently, OpenAI fine-tuned a GPT model for this discrimination task using a dataset comprising paired human and AI-generated texts on identical topics [25]. Similarly, Guo et al. [10] fine-tuned a text classifier based on pre-trained autoencoding LLMs (e.g., RoBERTa) by collecting the Human ChatGPT Comparison Corpus (HC3). Deep learning-based methods exhibit strong performance under the training data distribution, but they are susceptible to adversarial attacks, lack interpretability, and struggle to provide reliable judgments in human-AI collaboration scenarios. Watermark-based Detection. Kirchenbauer et al. [12] proposed the watermarking framework for white-box language models. The watermarking operates by randomly selecting a random set of \"greenlist\" words from the model's vocabulary and softly encouraging the use of these \"greenlist\" words by interfering with the sampling process at each generation step. The watermark can be detected by testing the following null hypothesis,  0 : The text sequence is generated with no knowledge of the selection rule of \"greenlist\" words. \n\nIf the null hypothesis is rejected, it can be concluded that the text was generated by the given model. This method is suitable for model owners who have access to the model's output probability distribution and can interfere with the sampling process.",
            "score": 0.6669843102203198,
            "section_title": "Recent Generated Text Detection Methods",
            "char_start_offset": 9898,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 35,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2083
                },
                {
                    "start": 2086,
                    "end": 2189
                },
                {
                    "start": 2190,
                    "end": 2338
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97021484375
        },
        {
            "corpus_id": "267068867",
            "title": "Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text",
            "text": "Successful efforts to spot machine-generated text show promise on early models whose generation output is not convincingly human. However, with the rise of transformer models for language modeling (Radford et al., 2019;Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023), primitive mechanisms to detect machinegenerated text are rendered useless. While one approach is to record (Krishna et al., 2023) or watermark all generated text (Kirchenbauer et al., 2023) . Detection of Machine-Generated Text from ChatGPT. Our detection approach using Binoculars is highly accurate at separating machine-generated and human-written samples from News, Creative Writing and Student Essay datasets with a false positive rate of 0.01%. Binoculars, based on open-source Falcon models with no finetuning, outperforms commercial detection systems, such as GPTZero, as well as open-source detectors -even though both of these baselines are specifically tuned to detect ChatGPT (Verma et al., 2023;Tian, 2023a). Our approach operates entirely in a zero-shot setting and has not been tuned on ChatGPT specifically. \n\nthe generative model. \n\nInstead, the recent spread of machine-generated text, especially via ChatGPT, has led to a flurry of work on post-hoc detection approaches that can be used to detect machine text without cooperation from model owners. These detectors can be separated into two main groups. The first is trained detection models, where a pretrained language model backbone is finetuned for the binary classification task of detection (Solaiman et al., 2019;Zellers et al., 2019;Yu et al., 2023;Zhan et al., 2023), including techniques like adversarial training (Hu et al., 2023) and abstention (Tian et al., 2023). Alternatively, instead of finetuning the whole backbone, a linear classifier can be fit on top of frozen learned features, which allows for the inclusion of commercial API outputs (Verma et al., 2023). \n\nThe second category of approaches comprises statistical signatures that are characteristic of machine-generated text.",
            "score": 0.6668482581658476,
            "section_title": "The LLM Detection Landscape",
            "char_start_offset": 1829,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1932
                },
                {
                    "start": 1935,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 238,
                    "matchedPaperCorpusId": "258048481"
                },
                {
                    "start": 446,
                    "end": 473,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 1573,
                    "end": 1594,
                    "matchedPaperCorpusId": "168169824"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8037109375
        },
        {
            "corpus_id": "273482525",
            "title": "Beyond Binary: Towards Fine-Grained LLM-Generated Text Detection via Role Recognition and Involvement Measurement",
            "text": "Current detection methods can be broadly categorized into three types based on the features they rely on [22]: watermarking-based detection methods, statistical outlier detection methods, and finetuning classifiers. (i) The watermark-based methods require embedding the signals that are invisible to humans into the AI-generated text and then detecting them based on these invisible token-level secret markers [28,31]. However, this method not only requires pre-editing that is not applicable to open-source models [42] but also affects the quality of model generation due to the insertion of watermarks [40,48]. (ii) Statistical outlier detection methods focus on distinguishing whether a text is written by GPT based on the human features contained in the text. They adopt features ranging from shallows (entropy [21,30], n-gram frequencies [2], and perplexity [7]) to deeps such as using the absolute rank [21], the Log Likelihood Ratio Ranking (LRR) by complementing Log Rank [47] and the model's log probability in regions of negative curvature (DetectGPT) [38]. (iii) Supervised fine-tuning classifiers, trained on annotated data [3,26,45], have shown effectiveness in detecting LLM-generated text across domains, such as news [29,61], social media (e.g., Twitter) [19,43], and academic papers [59]. \n\nHowever, these classifiers often overfit to specific domains, leading to poor performance on out-of-distribution data [12,51], and their capabilities significantly degrade when applied to unseen datasets from different domains [32]. Thus, assessing the transferability of detection models is essential for their practical application across diverse datasets and domains. In our work, we evaluate detection methods-including zero-shot LLMs, supervised feature-based, and PLM-based models-on our novel tasks and dataset. Our empirical validation provides insights into improving detection models for LLM-generated content, particularly with respect to enhancing generalization and robustness in diverse, real-world scenarios.",
            "score": 0.6666068277408461,
            "section_title": "Detection methods",
            "char_start_offset": 3993,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1305
                },
                {
                    "start": 1308,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 2031
                }
            ],
            "ref_mentions": [
                {
                    "start": 410,
                    "end": 414,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 414,
                    "end": 417,
                    "matchedPaperCorpusId": "273650112"
                },
                {
                    "start": 604,
                    "end": 608,
                    "matchedPaperCorpusId": "267938448"
                },
                {
                    "start": 608,
                    "end": 611,
                    "matchedPaperCorpusId": "257496757"
                },
                {
                    "start": 815,
                    "end": 819,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 819,
                    "end": 822,
                    "matchedPaperCorpusId": "12098535"
                },
                {
                    "start": 843,
                    "end": 846,
                    "matchedPaperCorpusId": "4324753"
                },
                {
                    "start": 863,
                    "end": 866,
                    "matchedPaperCorpusId": "1175726"
                },
                {
                    "start": 909,
                    "end": 913,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 980,
                    "end": 984,
                    "matchedPaperCorpusId": "259129463"
                },
                {
                    "start": 1062,
                    "end": 1066,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1139,
                    "end": 1142,
                    "matchedPaperCorpusId": "226237099"
                },
                {
                    "start": 1233,
                    "end": 1237,
                    "matchedPaperCorpusId": "261557118"
                },
                {
                    "start": 1237,
                    "end": 1240,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 1271,
                    "end": 1275,
                    "matchedPaperCorpusId": "220936111"
                },
                {
                    "start": 1275,
                    "end": 1278,
                    "matchedPaperCorpusId": "270562338"
                },
                {
                    "start": 1300,
                    "end": 1304,
                    "matchedPaperCorpusId": "260091554"
                },
                {
                    "start": 1430,
                    "end": 1433,
                    "matchedPaperCorpusId": "221835708"
                },
                {
                    "start": 1535,
                    "end": 1539,
                    "matchedPaperCorpusId": "269922444"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8818359375
        },
        {
            "corpus_id": "265157663",
            "title": "AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising",
            "text": "Large Language Models (LLMs) have significantly transformed the field of artificial intelligence, offering vast opportunities, but also raising important ethical questions. One of the main issues is their capacity to generate text that closely resembles human writing. Given their extensive scale and accessibility, if misused, LLMs could not only amplify harms such as disinformation and fake news [1], but also undermine academic integrity by assisting unauthorized content generation like essay-writing for students. Such instances highlight the need to differentiate between content generated by machines and that created by humans [2,3,4,5,6,7]. \n\nIn the past, various methods have been developed to tackle the challenge of detecting machinegenerated text [8,9,10,11,12,13,14,15]. One common strategy involves training separate classifiers on datasets containing both human and machine-generated texts with labels [16,17]. However, the effectiveness of these supervised classifiers often requires a large amount of training data, therefore increasing the cost associated with the training procedure. Another set of evaluation techniques relies on computing the log-likelihood of the language models [18,19]. However, obtaining the parameters of these models can often be challenging, as they may be inaccessible through chat interfaces or commercial APIs. Watermarking LLM outputs could also offer proactive detection of machine-generated text, maintaining trust and transparency [20,21,22]. However, this technique needs a balance between imperceptibility and detectability, while also addressing potential attacks on the watermarks to ensure the integrity of the detection process [23]. \n\nTable 1: AUROC scores on the aggregated PubMedQA and GPT generated QA datasets. AuthentiGPT outperforms zero-shot GPT-3.5 and GPT-4, and surpasses GPTZero and Originality.AI.",
            "score": 0.6662196581271796,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1693
                },
                {
                    "start": 1696,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1870
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 402,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 639,
                    "end": 641,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 645,
                    "end": 647,
                    "matchedPaperCorpusId": "256655912"
                },
                {
                    "start": 761,
                    "end": 764,
                    "matchedPaperCorpusId": "4324753"
                },
                {
                    "start": 764,
                    "end": 766,
                    "matchedPaperCorpusId": "1175726"
                },
                {
                    "start": 766,
                    "end": 769,
                    "matchedPaperCorpusId": "219756231"
                },
                {
                    "start": 919,
                    "end": 923,
                    "matchedPaperCorpusId": "3091"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "265658859",
            "title": "New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking",
            "text": "The advancement of Large Language Models (LLMs) like GPT-4 and Llama-2 has heralded a new era in natural language processing, offering unprecedented capabilities in generating human-like text (OpenAI, 2023;Touvron et al., 2023;Chowdhery et al., 2022). However, this advancement also brings forth a unique challenge: ensuring the integrity and traceability of machine-generated content (Clark et al., 2021;Mora-Cantallops et al., 2021). These concerns have led to the development of many watermarking techniques for LLMs, aimed at embedding identifiable markers into generated text without compromising its quality or readability (Kirchenbauer et al., 2023a;b;Takezawa et al., 2023;Zhao et al., 2023a;Christ et al., 2023;Yoo et al., 2023;Zhao et al., 2023b). \n\nWatermarking in the context of LLMs is a relatively new and rapidly evolving field. The primary objective is to embed a non-obtrusive, detectable marker within the text generated by these models, enabling the identification of the source model and potentially deterring misuse such as plagiarism or misinformation. Recent advances have introduced sophisticated techniques aimed at embedding watermarks seamlessly into the language model's output through black-box approaches, ensuring minimal impact on the text's quality without access to the original model's weights. However, the effectiveness and subtlety of these methods remain under scrutiny (Tang et al., 2023). Key concerns include the detectability of these watermarks by third parties, their potential to degrade text quality, and the challenge of maintaining the watermark's integrity across different contexts and content types. Prior work has used metrics such as perplexity (Kirchenbauer et al., 2023a), n-gram log diversity (Kirchenbauer et al., 2023b), or BLEU scores for machine-translation Figure 1: Can watermarked outputs from large language models be distinguished with a black-box approach?",
            "score": 0.665804014369747,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1923
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9189453125
        },
        {
            "corpus_id": "266369103",
            "title": "A Survey on the Detection and Impacts of Deepfakes in Visual, Audio, and Textual Formats",
            "text": "Watermark-based identification has emerged as an intriguing paradigm in the field of text detection [250], [251]. Initially used in image copyright protection [252], it was applied in language with the advent of syntax tree manipulations, a concept introduced by Atallah et al. in 2001 [253]. Meral et al. [254] further explored this avenue in 2009, contributing to the development of watermarking techniques in language. \n\nThis approach was recently revolutionised by Kirchenbauer et al. [255] In 2023, they introduced a novel approach by developing a watermarking method tailored for Large Language Models (LLMs). This technique involves manipulating the LLM's logits at each step to embed watermarks, with tokens categorised into 'green' and 'red' lists. A watermarked LLM favours tokens from the 'green' list during text generation, forming distinguishable watermark patterns. The authenticity of these watermarks is verified using a specific hash function. This innovative approach has the potential to improve copyright protection and content authentication, enabling secure communication, and offering new research avenues in language privacy and digital rights management. However, a study by Sadasivan et al. in 2023 [256] underscored the complex challenge involved in reliably detecting AI-generated texts. The researchers designed a paraphraser based on a neural network to modify the outputs of the AI generative models. This paraphraser is intended to bypass a variety of detectors, such as watermarking systems and zero-shot classifiers. Furthermore, this research highlighted the susceptibility of watermarked Large Language Models (LLMs) to spoofing attacks. Additionally, in another 2023 study, Krishna et al. [257] explored the vulnerability of AI-generated text detectors to paraphrase. They found that although detection methods such as watermarking techniques, zero-shot, classifiers, and fine-tuned LLMs had reduced accuracy when tested against paraphrased texts, retrieval-based methods were an effective countermeasure. This underscores the need for the continuous development of detection strategies to keep up with evolving evasion techniques and the potential value of retrieval-based defence.",
            "score": 0.6653099185705786,
            "section_title": "5) METHODS BASED ON WATERMARKING",
            "char_start_offset": 100592,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 421
                },
                {
                    "start": 424,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2220
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 164,
                    "matchedPaperCorpusId": "62101493"
                },
                {
                    "start": 306,
                    "end": 311,
                    "matchedPaperCorpusId": "1192689"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92919921875
        },
        {
            "corpus_id": "273877389",
            "title": "Distinguishing LLM-Generated from Human-Written Code by Contrastive Learning",
            "text": "Large language models are continuously iterated, which leads to dramatically improved performance on many language-related tasks and the ability to generate convincing text [13,64,110]. The surprisingly strong capabilities of ChatGPT have raised many interests and concerns. For example, people are either curious about how close ChatGPT is to human experts or are worried about the potential risks brought by LLMs like ChatGPT. Therefore, to ensure the responsible and ethical use of LLM-generated content, it is necessary to propose approaches to identify whether a given piece of content is generated by LLMs. Yang et al. [103] summarize existing methods for LLM-generated content detection into three categories: 1) Training-based methods [10,29,39,54,73,88,97,107,109], which usually involve fine-tuning a pre-trained language model on a constructed dataset of both human-written and LLM-generated content; 2) Zero-shot methods [45,63,64,102], which utilize intrinsic properties of typical LLMs, such as probability curve [64] and N-gram divergence [102], for self-detection; 3) Watermarking involves concealing information within the generated text, which enables the identification of the text's source. Our proposed approach falls within the category of training-based methods. Diverging from existing models that primarily focus on detecting text generated by LLMs, utilizing data sources such as Wikipedia [30] and student essays [91], our work concentrates on identifying program code generated by LLMs. \n\nIn the domain of detecting code generated by LLMs, Pan et al. [79] conduct an empirical study using a dataset derived from fundamental Python programming problems to evaluate the effectiveness of several AIGC detectors [25,29,64,71,81] in detecting AI-generated code. Their findings reveal the inadequate performance of these detectors in identifying AI-generated code, highlighting the urgent need for more reliable detection methods, particularly in educational contexts. The recently released work by Nguyen et al. [66], independent and concurrent to our work, proposes a CodeBERT-based classifier named GPTSniffer that is similar to our idea.",
            "score": 0.6643182783484713,
            "section_title": "LLM-generated Content Detection",
            "char_start_offset": 60019,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1514
                },
                {
                    "start": 1517,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2163
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 177,
                    "matchedPaperCorpusId": "256409866"
                },
                {
                    "start": 750,
                    "end": 753,
                    "matchedPaperCorpusId": "259501842"
                },
                {
                    "start": 933,
                    "end": 937,
                    "matchedPaperCorpusId": "257687440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89013671875
        },
        {
            "corpus_id": "264426339",
            "title": "DetectGPT-SC: Improving Detection of Text Generated by Large Language Models through Self-Consistency with Masked Predictions",
            "text": "Recent research has shown promising results in the development of detection methods. The existing detectors are built on the assumption that there is a distributional difference between human-generated texts and AI-generated texts. These differences are typically identified by training classifiers or using statistical information. \n\nClassifier-based detectors. Classifier-based detectors are commonly used in natural language processing detection paradigms, especially in fake news and misinformation detection [3]. Guo et al. [4] proposed the ChatGPT Detector, where they initially constructed a dataset consisting of ChatGPT conversations with human questions and answers, and trained a text detection classifier based on this dataset. The use of these methods requires substantial data collection and incurs the cost of training these classifier models. \n\nStatistical-based detectors. Statistical-based detectors utilize statistical metrics such as entropy, perplexity, and n-gram frequency to differentiate between human-generated and AI-generated texts [1]. \n\nSome other work is based on watermark-based detectors. In previous research, watermarks have been applied in the field of image processing and computer vision to protect copyrighted content and prevent intellectual property theft [6]. \n\nRecently, with the emergence of ChatGPT, the work by Kirchenbauer et al. [7] demonstrated how to incorporate a watermark using only the logarithmic credentials of each step to mark AI-generated texts. While watermark-based detectors are an intriguing area of research, adding watermarks may affect the readability of the texts, and the removal of watermarks is also a challenge we need to address. \n\nHowever, with the emergence of ChatGPT, an innovative statistical detection method called DetectGPT [5] has been developed. Its principle is that text generated by the model typically resides in the negative curvature region of the model's log probability. DetectGPT [5] generates and compares multiple variants of model-generated texts to determine whether the texts are machine-generated based on the log probabilities of the original texts and these variants. \n\nDetectGPT [5] outperforms the vast majority of existing zero-shot methods in terms of model sample detection, achieving very high AUC. It is based on this concept that DetectGPT-SC was proposed.",
            "score": 0.6642230949070014,
            "section_title": "Related Work",
            "char_start_offset": 3773,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 332
                },
                {
                    "start": 335,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1064
                },
                {
                    "start": 1067,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1301
                },
                {
                    "start": 1304,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1701
                },
                {
                    "start": 1704,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2166
                },
                {
                    "start": 2169,
                    "end": 2303
                },
                {
                    "start": 2304,
                    "end": 2363
                }
            ],
            "ref_mentions": [
                {
                    "start": 1297,
                    "end": 1300,
                    "matchedPaperCorpusId": "62101493"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "270766723",
            "title": "NCL-UoR at SemEval-2024 Task 8: Fine-tuning Large Language Models for Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection",
            "text": "Researchers have employed a variety of methods and tools to detect AI-generated texts.Broadly, these approaches can be categorized into two main types: black-box and white-box detection methods (Tang et al., 2023).Black-box detection relies on API-level access to LLMs, utilizing textual samples from both human and machine sources to train classification models (Dugan et al., 2020).The study by Guo et al. (2023) integrated existing questionand-answer datasets and leveraged fine-tuning of pre-trained models to investigate the characteristics and similarities between human-generated and AI-generated texts.\n\nAs for white-box detection, Kirchenbauer et al. (2023) introduced a novel approach involving the embedding of watermarks in the outputs of LLMs to facilitate the detection of AI-generated text.Additionally, a variety of tools and methodologies, including XGBoost, decision trees, and transformerbased models, have been evaluated for their efficacy in detecting texts produced by AI (Zaitsu and Jin, 2023).These techniques incorporate multiple stylistic measurement features to differentiate between AI-generated and human-generated texts (Shijaku and Canhasi, 2023).Specific tools and techniques in this domain include the GLTR tool developed by Gehrmann et al. (2019), which analyzes the usage of rare words in texts to distinguish between those generated by the GPT-2 model and human writers.The DetectGPT method posits that minor rewrites of LLM-generated texts tend to reduce the log probability under the model, a hypothesis that has been explored in depth (Mitchell et al., 2023).Furthermore, intrinsic dimension analysis, including methods like the Persistent Homology Dimension estimator (PHD), has been applied to distinguish between authentic texts and those generated artificially (Tulchinskii et al., 2023).Detectors specifically designed for certain LLMs, such as the GROVER detector for the GROVER model (Zellers et al., 2019) and the RoBERTa detector using the RoBERTa model (Liu et al., 2019), also play a significant role in this field.",
            "score": 0.6638916864401634,
            "section_title": "Related Work",
            "char_start_offset": 2279,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 86,
                    "end": 214
                },
                {
                    "start": 214,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 610
                },
                {
                    "start": 612,
                    "end": 805
                },
                {
                    "start": 805,
                    "end": 1017
                },
                {
                    "start": 1017,
                    "end": 1178
                },
                {
                    "start": 1178,
                    "end": 1406
                },
                {
                    "start": 1406,
                    "end": 1598
                },
                {
                    "start": 1598,
                    "end": 1831
                },
                {
                    "start": 1831,
                    "end": 2065
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.935546875
        },
        {
            "corpus_id": "263776375",
            "title": "AI meets AI: Artificial Intelligence and Academic Integrity - A Survey on Mitigating AI-Assisted Cheating in Computing Education",
            "text": "OpenAI has launched a tool, the OpenAI AI Text Classifier [32], that attempts to distinguish between human-written and AI-generated text. The classifier isn't particularly accurate, with a success rate of around 26%. However, when used together with other methods, it could be helpful in preventing AI text generators from being abused. The OpenAI AI Text Classifier was trained on text from 34 text-generating systems from five different organizations, including OpenAI itself. The training data was paired with similar humanwritten text from sources such as Wikipedia, websites extracted from links shared on Reddit, and a set of \"human demonstrations\" collected for a previous OpenAI text-generating system. However, this tool is far from mature. For instance, it requires a minimum of 1,000 characters, or about 150 to 250 words, to work on any text, and it doesn't detect plagiarism. It also may misclassify texts written in a language other than English, due to its English-centric dataset. Besides OpenAI's efforts, other tools have emerged to detect AI-generated text. For example, GPTZero, developed by a Princeton University student, uses criteria including \"perplexity\" (the complexity of text) and \"burstiness\" (the variations of sentences) to detect whether text might be AI-written [33]. Plagiarism detector Turnitin is also developing its own AI-generated text detector [34]. \n\nMost current tools for detecting AI-generated text employ features based on the vocabulary, grammar, and style<strike>s</strike> of sentences. Our research team is currently working on an interesting project that performs the detection by looking at logic flows within full paragraphs or even entire papers. A recent paper [46] proposed a novel method to embed watermarks in text generated by large language models with negligible impact on the quality of generated text. However, as the detecting techniques improve, so will the text-generating AI, thus creating a cat-and-mouse game like the one between cyber criminals and security researchers.",
            "score": 0.6637690893566295,
            "section_title": "UTILIZING AI TO DETECT AI-GENERATED TEXT",
            "char_start_offset": 7503,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1390
                },
                {
                    "start": 1393,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 2040
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.876953125
        },
        {
            "corpus_id": "273650112",
            "title": "Where Am I From? Identifying Origin of LLM-generated Content",
            "text": "Watermarking. Watermarking serves as a crucial technique for embedding unique identifiers or signatures into digital signals or content to protect intellectual property or verify authenticity. In the context of language models (LMs), this technique is crucial for safeguarding creators' rights and ensuring the legitimacy of generated text. Early rulebased approaches work in text watermarking involved embedding watermarks within a sentence's syntactic structure, as proposed in the post-hoc strategy by Atallah et al. (2001). Subsequently, synonym substitution techniques (Topkara et al., 2006) and semantic combination patterns for watermark construction (Xiang et al., 2021) were introduced for natural language text. Venugopal et al. (2011), were the first to propose using bit information for watermarking, while He et al. (2022) introduced the innovative concept of context-conditional textual watermarking. However, applying post-hoc watermarks to LMs faces challenges due to the diverse and versatile nature of output generation. To address this, Fang et al. (2017) proposed restricting generative models to produce tokens only from an \"allowed\" set. Unfortunately, this constraint often leads to lowquality output, especially in low-entropy cases. \n\nDetection of AI-generated Content. With the proliferation of large language models capable of generating high-quality and natural text, detecting the usage of machine-generated text has become increasingly important. Early methods employed natural heuristics and statistical analyses, while entropy scoring (Lavergne et al., 2008) uti-lized local syntactic and semantic consistency of short phrases to identify AI-generated sentences. GLTR (Gehrmann et al., 2019) assessed the expected probability of individual tokens, setting a threshold for detection. DetectGPT (Mitchell et al., 2023) observed that paragraphs created by AI often reside in the negative curvature of the log probability of texts. Some detectors, like a tool released by OpenAI (OpenAI, 2023b), incur additional training costs by using a fine-tuned GPT model to discern whether a sentence is human-written or AIgenerated.",
            "score": 0.6633045908653048,
            "section_title": "Related Work",
            "char_start_offset": 2725,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 13
                },
                {
                    "start": 14,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1257
                },
                {
                    "start": 1260,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2150
                }
            ],
            "ref_mentions": [
                {
                    "start": 505,
                    "end": 526,
                    "matchedPaperCorpusId": "37687669"
                },
                {
                    "start": 574,
                    "end": 596,
                    "matchedPaperCorpusId": "3061822"
                },
                {
                    "start": 722,
                    "end": 745,
                    "matchedPaperCorpusId": "15210695"
                },
                {
                    "start": 819,
                    "end": 835,
                    "matchedPaperCorpusId": "252367820"
                },
                {
                    "start": 1056,
                    "end": 1074,
                    "matchedPaperCorpusId": "10494183"
                },
                {
                    "start": 1567,
                    "end": 1590,
                    "matchedPaperCorpusId": "12098535"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88623046875
        },
        {
            "corpus_id": "258960319",
            "title": "Detection of GPT-4 Generated Text in Higher Education: Combining Academic Judgement and Software to Identify Generative AI Tool Misuse",
            "text": "Other services including Netus AI, and stealthwriter openly promoting themselves as being able to paraphrase Gen-AI content to evade detection by other services. Netus AI state 'As AI content generation evolves, we continue to stay ahead of the curve' (Netus AI, n.d.). This suggests that alongside the development of detection tools, equally we will see the emphasis on evasion of these tools increase. \n\nCurrent methods used to identify machine-generated text can be broadly categorized into three groups: (1) Identifying the presence of 'watermarked' content; \n\n(2) Statistical outlier detection methods which seek out irregularities in the text generated; and (3) Classifiers that have been trained to distinguish between text generated by a machine and text written by a human (Krishna et al., 2023). Whilst watermarking is being promoted as a present solution to increase detection capabilities (Kirchenbauer et al., 2023). Sadasivan et al. (2023) demonstrated that the application of paraphrasing tools on top of generative text production can essentially invalidate the use of detection software, leading to the conclusion that even with watermarked text, highly sophisticated detection software can be easily evaded. A study by Mitchell et al. (2023) determined that before using a paraphrasing tool, DetectGPT accurately identified 70.3% of model-generated sequences from GPT2-XL. However, after using a paraphrasing tool to manipulate the content, the detection dropped to only 4.6%. It is critical to note that this was achieved with nominal semantic alterations. Solaiman et al. (2019) and Ippolito et al. (2020) also presented tools for detecting machine-generated text, although their applicability to newer LLMs remains untested. \n\nThese studies have explored various detection methods and their effectiveness in identifying AI-generated content, and although some detection techniques show promise, the constant advancements in AI models necessitate ongoing investigation and adaptation of detection strategies. The limited success shown of both human participants and detection tools in accurately detecting Gen-AI content underscores the threat to academic integrity in HEIs, and the need for an increased understanding of how detection works in practice.",
            "score": 0.6625108185411189,
            "section_title": "AI-generated content and its detection",
            "char_start_offset": 16424,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 562
                },
                {
                    "start": 565,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1745
                },
                {
                    "start": 1748,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2274
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7177734375
        },
        {
            "corpus_id": "259092330",
            "title": "Undetectable Watermarks for Language Models",
            "text": "With the rise in the use of artificial models that churn out human-like text, there's also an increase in the potential for misuse. Imagine a student employing a language model to effortlessly write her \"Machine Learning 101\" homework or conjuring up tear-jerking emails to beg professors for easier exams. That's when the need arises to distinguish between texts penned by a language model and those crafted by human hands. The go-to method of employing a heuristic test to determine if a text was AI-generated, however, grows increasingly fragile as large language models (LLMs) advance. Even the cutting-edge detectors, like GPTZero [Tia23], can be outsmarted with cleverly crafted prompts. \n\nUltimately, as LLM outputs move closer to becoming identical to human-generated text, this approach becomes hopeless. It is already very hard to tell, for instance, that the previous paragraph was written by such a model. To overcome this problem, it is reasonable to consider intentionally modifying the model to embed watermarks into the text. Recent work of [KGW + 23] introduced such watermarks in the context of LLMs. However, existing watermarking schemes come with a cost: To plant a useful watermark, the distribution of texts the model generates has to be noticeably changed. In fact, for existing schemes it is possible for the user to distinguish between outputs of the original model and of the watermarked one, and it is hence possible that the quality of text degrades. \n\nWe show how to plant watermarks with the following properties, stated informally, in any LLM. \n\n1. (Undetectability) It is computationally infeasible to distinguish between the original and the watermarked models, even when the user is allowed to make many adaptive queries. In particular, the quality of generated text remains identical.",
            "score": 0.6618515506497789,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1479
                },
                {
                    "start": 1482,
                    "end": 1575
                },
                {
                    "start": 1578,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1820
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91455078125
        },
        {
            "corpus_id": "265456189",
            "title": "Machine-Generated Text Detection using Deep Learning",
            "text": "to discern the features differentiating humangenerated text from machine-generated text. Let's delve into each of these approaches. \n\nRecent LLMs like ChatGPT can generate text to compose essays, describe art in detail, create AI art prompts, have philosophical conversations, and even code for you. To detect such intricate patterns, a new methodology was proposed in DetectGPT, where they discovered how LLMs operate where the models tend to create text that falls into specific patterns, particularly in regions where the model's calculations show negative curvature. DetectGPT utilizes log probabilities from the LLM and random perturbations that it generates to determine whether the text is machine-generated (Mitchell et al., 2023). It is particularly good at identifying fake news articles made by models like GPT-NeoX and outperforms most zero-shot methods. They test LLM performance with six diverse datasets: XSum for fake news detection, SQuAD for academic essays, Reddit WritingPrompts for creative writing, WMT16 in English and German, and Pub-MedQA for long-form expert answers. The future work that authors in DetectGPT wanted to explore was to see how watermarking algorithms worked with detection algorithms like DetectGPT. (Guo et al., 2023) proposed an HC3 (Human ChatGPT Comparison Corpus) dataset, which consists of nearly 40K questions and their corresponding human/ChatGPT answers, and developed a ChatGPT detection model to differentiate human and Chatgpt generated text. The detector is primarily created by fine-tuning RoBERTa model on the dataset, and the authors propose two methods for training it. The first method uses only the pure answered text, while the second method utilizes the question-answer text pairs for joint model training. \n\nIn 2019, (Solaiman et al., 2019) built a detector for GPT2-generated output by fine-tuning RoBERTa using outputs from the largest GPT2 Model, which was with 1.5 billion parameters and was capable of detecting if the text was a machinegenerated text or not. They conducted this research by using RoBERTa-base with 125 million parameters and RoBERTa-large with 356 million parameters as the foundation for their sequence classifier.",
            "score": 0.6611357677939304,
            "section_title": "Numerous investigations have been undertaken",
            "char_start_offset": 2678,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 131
                },
                {
                    "start": 134,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1769
                },
                {
                    "start": 1772,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2202
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8828125
        },
        {
            "corpus_id": "259108779",
            "title": "Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts",
            "text": "Modern large language models (LLMs) generate human-looking texts increasingly well, which may also lead to worrisome consequences [Fagni et al., 2021, Adelani et al., 2020, Stokel-Walker, 2022]. Hence, the ability to detect AI-generated texts (artificial text detection, ATD) becomes crucial for media, education, politics, creative industries and other spheres of human social activities. A straightforward idea would be to train a classifier to detect artificial text; many such classifiers exist [Zellers et al., 2019, Gehrmann et al., 2019, Solaiman et al., 2019], but most of them are designed to detect samples of individual generation models, either using the model itself [Mitchell et al., 2023] or training on a dataset of its generations. This leads to poor generalization to new models and unknown data domains. Another idea, known as watermarking, is to inject some detectable artifacts into model generations; for instance, Kirchenbauer et al. [2023] propose to intentionally inject a statistical skew that can be detected in a text sample. However, later works showed that watermark detectors can be broken by adversarial attacks, e.g., by text perturbations or paraphrasing [He et al., 2023]. Since text generation is constantly evolving, Sadasivan et al. [2023] claim that perfect artificial text detection is impossible; Krishna et al. [2023] address this statement and propose a retrieval-based detector that could be implemented by text generation service providers: they should store the hash value of every text generated by their model and retrieve it by request. This approach works even for a perfect text generator indistinguishable from human writing, but it does not apply to publicly available models, and plenty of them already exist. In this work, we show that the intrinsic dimension of text samples can serve as a helpful score function allowing to separate artificial and generated texts in a very general setting, without additional knowledge about the generator. The only assumption is that generation is good enough to create fluent grammatical samples of length \u2248 200 words.",
            "score": 0.65994213875731,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2111
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 149,
                    "matchedPaperCorpusId": "220936111"
                },
                {
                    "start": 149,
                    "end": 171,
                    "matchedPaperCorpusId": "198147840"
                },
                {
                    "start": 499,
                    "end": 520,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 520,
                    "end": 543,
                    "matchedPaperCorpusId": "182952848"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9267578125
        },
        {
            "corpus_id": "265157663",
            "title": "AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising",
            "text": "GPT-3.5 (zero-shot) 0.721 GPT-4 (zero-shot) 0.577 GPTZero 0.797 Originality.AI 0.906 AuthentiGPT 0.918 \n\nIn this paper, we introduce AuthentiGPT, a novel classification algorithm designed to distinguish between machine-generated and human-written text. Unlike previous methods that rely on extensive labeled data, watermarking, or log-likelihood calculations, AuthentiGPT leverages the language model itself for detection. With the assumption that human-written text resides outside the distribution of machine-generated text [24], the algorithm first introduces synthetic noise to the input text and utilizes a black-box LLM to denoise the text at a noise level. Then, by comparing the denoised text with the original text at the semantic level, a lightweight classifier (with only one free parameter) that is trained on a small set of examples can effectively determine the text was generated by a machine or a human. Our experimental results (Table 1) show that AuthentiGPT outperforms existing methods in detecting machine-generated content within PubMedQA and generated QA datasets, demonstrating the effectiveness of the algorithm. An important advantage of AuthentiGPT is its adaptability. As LLMs continue to improve, AuthentiGPT can be easily adapted to them with minimal effort and modification. \n\nWhile expanding the capabilities of LLMs, it becomes increasingly crucial to address the ethical considerations surrounding their use. AuthentiGPT represents a valuable advancement in this regard and improves language models' responsible and ethical application. \n\n2 Related Works",
            "score": 0.6587267607857041,
            "section_title": "AUROC Scores",
            "char_start_offset": 1911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 105,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1305
                },
                {
                    "start": 1308,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1570
                },
                {
                    "start": 1573,
                    "end": 1588
                }
            ],
            "ref_mentions": [
                {
                    "start": 526,
                    "end": 530,
                    "matchedPaperCorpusId": "127986954"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90185546875
        },
        {
            "corpus_id": "258832418",
            "title": "G3Detector: General GPT-Generated Text Detector",
            "text": "(also known as ChatGPT) and GPT4. This inadequacy is particularly pronounced within professional spheres, including, but not limited to, the legal and medical sectors, where these detectors frequently misclassify machine-generated content as human-authored text. \n\nTo address this prevalent issue, we present a simple yet effective detection methodology capable of efficaciously identifying synthetic text across a diverse range of domains. Specifically, we carefully select the backbone model as well as training corpus, thereby establishing a robust foundation for the success of our approach. Furthermore, the performance of our detector exhibits remarkable consistency across different model architectures and decoding strategies. This characteristic ensures that our detection technique retains its robustness irrespective of the variations in synthetic text generation methods, thereby strengthening its potential application within an expansive context. \n\nThe main contributions of our empirical research are summarized as follows: \n\n\u2022 We introduce an innovative framework dedicated to the identification of text generated by machines, exhibiting superior performance over an array of widely-utilized detection tools. \n\n\u2022 The robustness of our proposed detection system is confirmed through rigorous empirical analysis, demonstrating its resilience across numerous domains and emergent generative models. Furthermore, our detector exhibits resilience against a highly sophisticated technique aimed at evading detection. \n\n\u2022 Our methodology proves complementary to watermarking-based detection algorithms, providing a viable alternative solution in situations where a detection-evasion tactic compromises the effectiveness of watermark-based detection approaches.",
            "score": 0.657310865440706,
            "section_title": "Detectors",
            "char_start_offset": 2636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 262
                },
                {
                    "start": 265,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 960
                },
                {
                    "start": 963,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1526
                },
                {
                    "start": 1529,
                    "end": 1769
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78076171875
        },
        {
            "corpus_id": "273547059",
            "title": "Scalable watermarking for identifying large language model outputs",
            "text": "Large language models (LLMs) have enabled the generation of high-quality synthetic text, often indistinguishable from human-written content, at a scale that can markedly affect the nature of the information ecosystem [1][2][3] . Watermarking can help identify synthetic text and limit accidental or deliberate misuse 4 , but has not been adopted in production systems owing to stringent quality, detectability and computational efficiency requirements. Here we describe SynthID-Text, a production-ready text watermarking scheme that preserves text quality and enables high detection accuracy, with minimal latency overhead. SynthID-Text does not affect LLM training and modifies only the sampling procedure; watermark detection is computationally efficient, without using the underlying LLM. To enable watermarking at scale, we develop an algorithm integrating watermarking with speculative sampling, an efficiency technique frequently used in production systems 5 . Evaluations across multiple LLMs empirically show that SynthID-Text provides improved detectability over comparable methods, and standard benchmarks and human side-by-side ratings indicate no change in LLM capabilities. To demonstrate the feasibility of watermarking in large-scale-production systems, we conducted a live experiment that assessed feedback from nearly 20 million Gemini 6 responses, again confirming the preservation of text quality. We hope that the availability of SynthID-Text 7 will facilitate further development of watermarking and responsible use of LLM systems. \n\nLarge language models (LLMs) are widely adopted tools for synthetic text generation, finding applications in language-based assistants, code generation, writing support and various other domains. As LLMs advance in quality, coherence, coverage and expertise, it can become difficult to distinguish synthetically generated text from human-written text [1][2][3] . Given the widespread use of LLMs in education, software development and web content generation, identification and attribution of LLM text is critical to ensure safe and responsible use of the technology [8][9][10][11] . \n\nMultiple strategies have emerged to address this problem. One is a retrieval-based approach, which involves keeping a growing record of all generated texts and checking against it for matches 12 . This requires scale and coordination, and raises privacy concerns as it requires accessing and storing all LLM interactions.",
            "score": 0.6566738081117697,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2138
                },
                {
                    "start": 2141,
                    "end": 2198
                },
                {
                    "start": 2199,
                    "end": 2337
                },
                {
                    "start": 2338,
                    "end": 2462
                }
            ],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 220,
                    "matchedPaperCorpusId": "221562332"
                },
                {
                    "start": 220,
                    "end": 223,
                    "matchedPaperCorpusId": "235694265"
                },
                {
                    "start": 223,
                    "end": 226,
                    "matchedPaperCorpusId": "249674779"
                },
                {
                    "start": 1906,
                    "end": 1909,
                    "matchedPaperCorpusId": "221562332"
                },
                {
                    "start": 1909,
                    "end": 1912,
                    "matchedPaperCorpusId": "235694265"
                },
                {
                    "start": 1912,
                    "end": 1915,
                    "matchedPaperCorpusId": "249674779"
                },
                {
                    "start": 2122,
                    "end": 2125,
                    "matchedPaperCorpusId": "271448069"
                },
                {
                    "start": 2125,
                    "end": 2128,
                    "matchedPaperCorpusId": "259341801"
                },
                {
                    "start": 2128,
                    "end": 2132,
                    "matchedPaperCorpusId": "252118610"
                },
                {
                    "start": 2132,
                    "end": 2136,
                    "matchedPaperCorpusId": "268363462"
                },
                {
                    "start": 2333,
                    "end": 2335,
                    "matchedPaperCorpusId": "257687440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9580078125
        },
        {
            "corpus_id": "270562290",
            "title": "Applying Ensemble Methods to Model-Agnostic Machine-Generated Text Detection",
            "text": "In this paper, we study the problem of detecting machine-generated text when the large language model (LLM) it is possibly derived from is unknown. We do so by apply ensembling methods to the outputs from DetectGPT classifiers (Mitchell et al. 2023), a zero-shot model for machine-generated text detection which is highly accurate when the generative (or base) language model is the same as the discriminative (or scoring) language model. We find that simple summary statistics of DetectGPT sub-model outputs yield an AUROC of 0.73 (relative to 0.61) while retaining its zero-shot nature, and that supervised learning methods sharply boost the accuracy to an AUROC of 0.94 but require a training dataset. This suggests the possibility of further generalisation to create a highly-accurate, model-agnostic machine-generated text detector.",
            "score": 0.6566040103581422,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9052734375
        },
        {
            "corpus_id": "265067266",
            "title": "DEMASQ: Unmasking the ChatGPT Wordsmith",
            "text": "In general, automated machine learning techniques for identifying synthetic texts can be categorized into three primary groups: i) Basic classifiers [23], [50], ii) Zero-shot detection approaches [31], [36], [59], and iii) Detection methods based on fine-tuning [37]. Basic classifiers. The first category, simple classifiers, includes various detection techniques. For instance, OpenAI's logistic regression model [50] was trained on TF-IDF, unigram, and bigram features and specifically analyzed text generated by GPT-2. Through exploration of different generation strategies and model parameters, it was discovered that these classifiers can achieve accuracy levels of up to 97%. However, detecting shorter outputs proves more challenging for these models compared to longer ones. Another simple classifier proposed by Guo et al. [23] analyzed linguistic and stylistic characteristics of responses from ChatGPT and human experts. Unfortunately, these detection models were ineffective in identifying ChatGPT-generated text due to the imbalanced nature of the dataset they were trained on, failing to capture all of ChatGPT's text generation styles [42]. In a separate study, Kushnareva et al. [31] trained a logistic regression model using Topological Data Analysis (TDA) to extract interpretable topological features, such as the number of connected components, edges, and cycles in the graph, for recognizing artificial text. However, this approach is unlikely to be effective for ChatGPT as it was not specifically tested on that particular model, but instead was conducted on datasets from WebText & GPT-2, Amazon Reviews & GPT-2, RealNews & GROVER (FakeNews) [59]. Furthermore, prior work [42] tested the authors [31] designed model and got an accuracy of 25.1%. \n\nZero-shot detection. Zero-shot detection techniques also includes a variety of approaches for identifying AI-generated text. For instance, Mitchell et al. [36] introduced a method that leverages the log probabilities of the generative model to detect AI-generated text. However, this approach is primarily tailored to GPT-2 prompts and our evaluation indicates that it does not generalize well to GPT-3 models.",
            "score": 0.655951699067995,
            "section_title": "VII. RELATED WORKS",
            "char_start_offset": 51667,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1770
                },
                {
                    "start": 1773,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2183
                }
            ],
            "ref_mentions": [
                {
                    "start": 208,
                    "end": 212,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 1667,
                    "end": 1671,
                    "matchedPaperCorpusId": "168169824"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8486328125
        },
        {
            "corpus_id": "272880811",
            "title": "Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness",
            "text": "The past few years have witnessed tremendous advances in Large Language Models (LLMs). These models such as ChatGPT (OpenAI, 2022), PaLM (Chowdhery et al., 2023), GPT-4 (OpenAI, 2023) can now generate text of supreme quality, demonstrating exceptional performance in various fields like question answering, news reporting, and story writing. The increasing capability of LLMs to produce human-like text at high efficiency, however, also raises concerns about their misuse for malicious purposes, e.g., phishing (Panda et al., 2024), disinformation (Jiang et al., 2024), and academic dishonesty (Perkins, 2023). The effective detection Figure 1: Histograms of token cohesiveness distributions for 500 human-written and 500 LLM-generated articles. Human-written articles are sampled from XSum (Narayan et al., 2018), and LLM-generated articles are produced by prompting four source models with the first 30 tokens of each human-written article. The calculation of token cohesiveness will be detailed in Section 3.2. \n\nof LLM-generated text therefore becomes a vital principle to ensure the responsible use of LLMs. \n\nLLM-generated text detection is typically formulated as a binary classification task, i.e., to classify if a piece of text is generated by a particular source LLM or written by human (Tang et al., 2024). Current solutions roughly fall into two categories: supervised classifiers and zero-shot classifiers. Supervised classifiers are trained from labeled data and thus may overfit to their specific training domains (Wang et al., 2023b). Zero-shot classifiers, in contrast, are entirely training-free, making them less prone to domain-specific degradation and typically generalizing better (Zhu et al., 2023). \n\nMost existing zero-shot detectors are developed based on the generation probabilities (or their variations) of the source model, assuming that LLMgenerated text aligns better with these probabilities (Gehrmann et al., 2019;Mitchell et al., 2023).",
            "score": 0.655359984724904,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1013
                },
                {
                    "start": 1016,
                    "end": 1112
                },
                {
                    "start": 1115,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1723
                },
                {
                    "start": 1726,
                    "end": 1972
                }
            ],
            "ref_mentions": [
                {
                    "start": 137,
                    "end": 161,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 548,
                    "end": 568,
                    "matchedPaperCorpusId": "263152773"
                },
                {
                    "start": 594,
                    "end": 609,
                    "matchedPaperCorpusId": "257166266"
                },
                {
                    "start": 791,
                    "end": 813,
                    "matchedPaperCorpusId": "215768182"
                },
                {
                    "start": 1298,
                    "end": 1317,
                    "matchedPaperCorpusId": "257496757"
                },
                {
                    "start": 1704,
                    "end": 1722,
                    "matchedPaperCorpusId": "266164139"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93310546875
        },
        {
            "corpus_id": "258987266",
            "title": "Red Teaming Language Model Detectors with Language Models",
            "text": "Detectors for AI-generated text. Recent detectors for AI-generated text mostly fall into three categories. First, classifier-based detectors are trained with labeled data to distinguish human-written text and AI-generated text. For example, the AI Text Classifier developed by OpenAI (OpenAI, 2023a) is a fine-tuned language model. Second, watermarking methods introduce distinct patterns into AI-generated text, allowing for its identification. Among them, Kirchenbauer et al. (2023) randomly partition the vocabulary into a greenlist and a redlist during the generation, where the division is based on the hash of the previously generated tokens. The language model only uses words in the greenlists, and thereby the generated text has a different pattern compared to human-written text which does not consider such greenlists and redlists. Third, De-tectGPT (Mitchell et al., 2023) uses the likelihood of the generated text for the detection, as they find that text generated by language models tends to reside in the negative curvature region of the log probability function. Consequently, they define a curvature-based criterion for the detection. \n\nMethods for red-teaming detectors. As the detectors emerge, several concurrent works showed that the detectors may be evaded to some extent, typically by paraphrasing the text (Sadasivan et al., 2023;Krishna et al., 2023). However, they need additional paraphrasing models which are typically unprotected models that are much weaker than the original LLM. Besides paraphrasing, Kirchenbauer et al. (2023) also discussed attacks against watermarking detectors with word substitutions generated by a masked language model such as T5 (Raffel et al., 2020) which is a relatively weaker language model and tends to generate results with lower quality, and thus it may generate attacks with lower quality. On the other hand, Chakraborty et al. (2023) analyzed the possibilities of the detection given sufficiently many samples. \n\nAdversarial examples in NLP. Word substitution is a commonly used strategy in generating textual adversarial examples (Alzantot et al., 2018;Ren et al., 2019;Jin et al., 2020).",
            "score": 0.6548822015827422,
            "section_title": "Related Work",
            "char_start_offset": 4423,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1152
                },
                {
                    "start": 1155,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1976
                },
                {
                    "start": 1979,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2155
                }
            ],
            "ref_mentions": [
                {
                    "start": 458,
                    "end": 484,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 1533,
                    "end": 1559,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 1686,
                    "end": 1707,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9453125
        },
        {
            "corpus_id": "273901404",
            "title": "Context-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models",
            "text": "Large Language Models (LLMs) show their power on text generations but their formidable power may be used for unethical purposes such as plagiarism (Augenstein et al., 2023). Current research injects watermarks into LLMs' generated texts, thereby enabling people to distinguish between LLM-generated text and human-written text. Recent watermark methods inject a unique signature into LLM-generated text, which can only be perceived by watermarking methods, facilitating the detection of whether a text was generated by LLMs. \n\nCurrent watermark methods mainly inject the signature according to \"green-red list\" (Ren et al., 2023): they divide vocabulary into green/red lists, regard words in green lists as the unique signature, and encourage to generate green tokens, which is easy to be recognized. The methods can be divided into two categories: (1) Token sampling biasing-based watermark forces LLMs to select only green tokens during generation. EXP-Edit (Kuditipudi et al., 2023) intervenes in the sampling process of each token. However, forcing LLMs to sample green tokens restricts the semantic richness of LLM-generated text, thus undermining its text quality. (2) To improve the generation quality, researchers further propose token probability biasing-based watermark, which enriches the semantics of watermarked texts by introducing a bias to the probability distribution to softly encourage generating green tokens instead of restricting to select green tokens. Takezawa et al. (2023) proposed NS-Mark to constrain the frequency of biasing. Wu et al. (2023) introduced DiPMark to approximate the biased probability distribution to the original one. These methods mitigate the impact of biasing on text quality and ensure superior text quality. \n\nThe above methods narrow the gap in text quality between the watermarked text and the unwatermarked text but lack robustness against paraphrasing attacks. Paraphrasing attacks (Krishna et al., 2024) use language models to modify the watermarked text to evade the unique signature of the watermarked text.",
            "score": 0.6543769534179502,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 524
                },
                {
                    "start": 527,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1757
                },
                {
                    "start": 1760,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2064
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9638671875
        },
        {
            "corpus_id": "259202884",
            "title": "The Age of Synthetic Realities: Challenges and Opportunities",
            "text": "They generated machine-paraphrased text and evaluated human performance in detecting these generated texts. The study showed that humans could not accurately detect GPT-3 paraphrased text, with accuracy only slightly above random (53%). \n\nIn response to concerns about AI-generated text, several proprietary tools have emerged to address the detection of AI-authored content. One such tool is GPTZero, which has gained attention in the media as a promising method for identifying AI-generated text [193]. However, we were unable to locate the source code or a scientific article detailing their approach. Similarly, numerous applications have been developed claiming to detect AI-generated text, such as GPTkit4 , Illuminarty5 , OpenAI's AI Text Classifier 6 , and AICheatCheck7 . However, many of these tools lack comprehensive studies on the reliability of their detection methods. \n\nIn an effort to facilitate the detection of ChatGPTgenerated content, Yu et al. [194] released a large dataset specifically designed for the identification of ChatGPTwritten abstracts. This dataset includes over 35,000 synthetic abstracts generated by ChatGPT, comprising fully generated texts, polished outputs, and mixtures of humanwritten and machine-generated abstracts. Additionally, the dataset contains more than 15,000 human-written abstracts for comparison. The results of their detection experiments demonstrated the ability to identify content that was entirely generated by ChatGPT. However, the task becomes more challenging when the generated text is mixed with human-written content. This work provides a valuable dataset into the complexities of detecting machine-generated text, particularly in scenarios involving a combination of human and AI-authored content. \n\nA potential solution to address the misuse of LLMs is the use of text watermarks [195,196]. Grinbaum et al. [195] argue that machine-generated long texts should include a watermark to indicate their source and ensure transparency. In [196], Kirchenbauer et al. propose embedding watermarks by modifying the sampling rules of nextword prediction. They use a hash function and pseudorandom generator to assign random colors (green and red) to words in the vocabulary. During next-word prediction, words from the red list are prohibited from appearing.",
            "score": 0.6538969995850042,
            "section_title": "B) Detection Methods",
            "char_start_offset": 53271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 236
                },
                {
                    "start": 239,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2233
                },
                {
                    "start": 2234,
                    "end": 2317
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69287109375
        },
        {
            "corpus_id": "273162522",
            "title": "Is Your Paper Being Reviewed by an LLM? Investigating AI Text Detectability in Peer Review",
            "text": "AI generated text detection. Early approaches to AI-generated text detection predominantly framed the task as a binary classification problem, where the objective was to determine whether a given text was human-written or machine-generated [13,[19][20][21]. For instance, Solaiman et al. [22] employed a bag-of-words model combined with logistic regression to distinguish GPT-2 generated web articles from human-authored content. Several studies focused on fine-tuning pre-trained language models such as RoBERTa [23] to improve detection accuracy [11,24,25]. These detectors leveraged the internal representations of language models to differentiate human and machine text. Concurrently, researchers explored zero-shot detection techniques that avoided the need for additional training, with methods relying on features like perplexity or entropy to detect machine-generated text [11,26]. Moreover, work by Zellers et al. [24] introduced neural networks specifically trained to spot AI-generated misinformation, while Ippolito et al. [26] further advanced zero-shot detection using likelihood-based methods. Another line of research highlighted the use of linguistic patterns, syntactic structures, and word distributions to identify AI-generated content without direct fine-tuning, thus improving adaptability to new models [11,25]. More recent studies explored watermarking and cryptographic signals embedded in LLM-generated text to enhance detection, such as the method proposed by Mitchell et al. [13], which enables proactive identification.",
            "score": 0.6534426572436366,
            "section_title": "A Related Work",
            "char_start_offset": 13231,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1548
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 244,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 248,
                    "end": 252,
                    "matchedPaperCorpusId": "226237099"
                },
                {
                    "start": 252,
                    "end": 256,
                    "matchedPaperCorpusId": "220936111"
                },
                {
                    "start": 548,
                    "end": 552,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 552,
                    "end": 555,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 555,
                    "end": 558,
                    "matchedPaperCorpusId": "221835708"
                },
                {
                    "start": 881,
                    "end": 885,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 885,
                    "end": 888,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 923,
                    "end": 927,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 1035,
                    "end": 1039,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 1326,
                    "end": 1330,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 1330,
                    "end": 1333,
                    "matchedPaperCorpusId": "221835708"
                },
                {
                    "start": 1503,
                    "end": 1507,
                    "matchedPaperCorpusId": "256274849"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91259765625
        },
        {
            "corpus_id": "267413235",
            "title": "Detection of Machine-Generated Text: Literature Survey",
            "text": "To recognize and detect AI-generated text, several machine learning models and algorithms have been developed. Fine-tuned BERT, Bag-of-Words, Histogram-of-Likelihood Ranks, and Total Probability are among the most famous models [10]. For this objective, logistic regression trained on term frequency-inverse document frequency (tf-idf) characteristics has also been used [23]. Another option is to use Support Vector Machine classifiers, which assess text features such as semantics and grammar. While these classifiers are simple to deploy and need little pre-training, altering them requires extensive re-training [15]. \n\nDue to significant developments in natural language generation models such as GPT-3, the identification of artificially created text, as opposed to human-written material, has become an increasingly essential study subject. The capacity to distinguish between human and machinegenerated material is critical in a variety of applications, including spam detection, identifying false news, and assuring the legitimacy of online information. To overcome this issue, machine learning approaches have been used to find minor patterns and traits that distinguish produced text from human-written material. \n\nThere are possible detrimental effects of false news created by sophisticated language models and provide defense techniques [48]. Hence, the significance of establishing powerful detection methods to safeguard the integrity of online data should not be undermined. The authors advise investing in research to better understand the risks and biases prevalent in cutting-edge language models. \n\nMany researchers studied the effectiveness of existing detection methods in detecting artificially generated language, with a particular emphasis on the text generated by Open AI's GPT-3 [23]. They discovered that current models had variable degrees of effectiveness in recognizing machinegenerated text, with some models doing well and others struggling. \n\nOne of the methods offered in 2020 for detecting machine-generated text using \"BERTscore\" [22]. BERTscore is an evaluation measure that uses BERT, a strong language representation model, to quantify the semantic similarity between generated and reference text. The scientists discovered that BERTscore-based classifiers could distinguish between human-written and machine-generated text with high detection accuracy.",
            "score": 0.6532221916222865,
            "section_title": "Text Detection Using Machine Learning",
            "char_start_offset": 11757,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1975
                },
                {
                    "start": 1978,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2238
                },
                {
                    "start": 2239,
                    "end": 2394
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 232,
                    "matchedPaperCorpusId": "222419308"
                },
                {
                    "start": 371,
                    "end": 375,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 1807,
                    "end": 1811,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 2068,
                    "end": 2072,
                    "matchedPaperCorpusId": "207870319"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93994140625
        },
        {
            "corpus_id": "272689061",
            "title": "PersonaMark: Personalized LLM watermarking for model protection and user attribution",
            "text": "Under the idea of adding watermarks in the process of generating text using language models, Kirchenbauer et al. [12] proposed a digital watermarking framework for large models that can embed signals that are invisible to humans but can be detected by algorithms in the generated text to mitigate the potential harm of large models. This framework achieves watermark embedding and detection by selecting a set of \"green\" markers and prioritizing their use during the generation process. Lee et al. [17] proposed a code generation watermarking method called SWEET, which improves the ability to detect AI generated code text by removing low entropy segments during watermark generation and detection, while significantly improving code quality preservation, surpassing all baseline methods, including post detection methods. Hu et al. [9] proposed an unbiased watermarking technique that enables the tracking and attribution of model generated text without affecting the output quality of large models (LLMs), while ensuring that users cannot detect the presence of watermarks, providing a new perspective for responsible AI development. \n\nDiPmark [35] protects data by embedding hidden information in the text generated by large models, while maintaining the original data distribution unchanged, achieving concealment, efficiency, and elasticity, making it a powerful solution for text attribution and quality protection. Yoo et al. [38] proposed a method called \"Multi bit Watermark Allocation by Position\" (MPAC), which embeds traceable multi bit information in the process of generating large models to address the problem of abuse in AI generated text. It effectively embeds and extracts long messages (>=32 bits) without sacrificing text quality, and has certain resistance to attacks. Wang et al. [32] proposed a novel encoding text watermarking method called Balance Marking, which is used to embed multi bit information in text generated by large models (LLMs). This method balances the probabilities of available and unavailable vocabulary by introducing a surrogate language model, effectively improving the quality and diversity of generated text without significantly increasing computational complexity. Kirchenbauer et al. [14] explored the reliability of watermarking technology in detecting and recording text generated by Large Models (LLMs) in real-world scenarios through empirical analysis.",
            "score": 0.6504244669385844,
            "section_title": "LLM watermarking",
            "char_start_offset": 13318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1136
                },
                {
                    "start": 1139,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2217
                },
                {
                    "start": 2218,
                    "end": 2411
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 117,
                    "matchedPaperCorpusId": "256194179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91748046875
        },
        {
            "corpus_id": "276287904",
            "title": "Modification and Generated-Text Detection: Achieving Dual Detection Capabilities for the Outputs of LLM by Watermark",
            "text": "The development of large language models (LLMs) has raised concerns about potential misuse. One practical solution is to embed a watermark in the text, allowing ownership verification through watermark extraction. Existing methods primarily focus on defending against modification attacks, often neglecting other spoofing attacks. For example, attackers can alter the watermarked text to produce harmful content without compromising the presence of the watermark, which could lead to false attribution of this malicious content to the LLM. This situation poses a serious threat to the LLMs service providers and highlights the significance of achieving modification detection and generated-text detection simultaneously. Therefore, we propose a technique to detect modifications in text for unbiased watermark which is sensitive to modification. We introduce a new metric called ``discarded tokens\", which measures the number of tokens not included in watermark detection. When a modification occurs, this metric changes and can serve as evidence of the modification. Additionally, we improve the watermark detection process and introduce a novel method for unbiased watermark. Our experiments demonstrate that we can achieve effective dual detection capabilities: modification detection and generated-text detection by watermark.",
            "score": 0.6503856443750888,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91015625
        },
        {
            "corpus_id": "277595847",
            "title": "A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content",
            "text": "One popular approach leverages an external LLM to classify whether a piece of text is AI-generated or not [12]. With carefully crafted prompts, these zero-shot methods can achieve moderate accuracy quickly. However, sensitivity to prompt design and the adversarial gap between the LLM detector and the text generator are common issues [22]. \n\n2) Linguistic and statistical signatures: Traditional stylometric features (e.g., function words, syntax complexity, average phrase length) have long been used in authorship attribution [23], [24]. More modern detection focuses on computing perplexity or log-likelihood using reference language models, observing that LLM-generated text tends to show distinctive probability distributions. Additionally, specialized white-box methods can measure rank ordering of tokens if the generating model is partially known [25]. \n\n3) Supervised classification (training-based): Labeled corpora of AI versus human text enable fine-tuning of large pre-trained transformers like RoBERTa or T5 to discriminate synthetic text [26], [27]. Researchers improve robustness with adversarial training sets that contain paraphrased or AIgenerated passages shifted in style. Tools like GPTZero [28] and RADAR [29] exemplify advanced supervised detectors. However, assembling high-quality, representative training data remains a challenge, especially as new generator architectures emerge frequently. \n\n4) Watermarking for AI text: Cooperative watermarking modifies text generation at token selection time, embedding an imperceptible pattern in the distribution of words or punctuation [30], [31]. A verifier can detect such patterns after the fact. While promising for major industrial LLMs that adopt the standard, watermarking fails if malicious or open-source models do not embed it, or if paraphrasing disrupts the signal [32], [33].",
            "score": 0.6484977737294655,
            "section_title": "B. Approaches 1) LLM prompting and zero-shot methods:",
            "char_start_offset": 6867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 861
                },
                {
                    "start": 864,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1419
                },
                {
                    "start": 1422,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1857
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 110,
                    "matchedPaperCorpusId": "260379022"
                },
                {
                    "start": 529,
                    "end": 533,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "267068867"
                },
                {
                    "start": 1214,
                    "end": 1218,
                    "matchedPaperCorpusId": "258960584"
                },
                {
                    "start": 1229,
                    "end": 1233,
                    "matchedPaperCorpusId": "259501842"
                },
                {
                    "start": 1605,
                    "end": 1609,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 1846,
                    "end": 1850,
                    "matchedPaperCorpusId": "263831179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95458984375
        },
        {
            "corpus_id": "266369103",
            "title": "A Survey on the Detection and Impacts of Deepfakes in Visual, Audio, and Textual Formats",
            "text": "In the context of the rapid advancement of Large Language Models (LLMs), the ability to produce highly accurate and contextually relevant text is becoming an increasing concern in online security. The sophisticated text generation capabilities of these models can potentially facilitate harmful activities on online platforms, emphasising the importance of detecting AI-generated texts as a mitigation strategy. Several detection methodologies have been developed, each with varying degrees of effectiveness influenced by factors such as the architecture of the language model, the decoding strategy, the length of the text and the availability of raw log probabilities from the LLMs. \n\nSimple classifiers offer a basic, yet sometimes limited, approach, particularly with shorter texts and complex generation strategies. Zero-shot models provide a robust detection strategy by exploiting unique statistical patterns in machine-generated text; however, they are susceptible to adversarial actions or different generation strategies. Finetuned language models represent another approach that provides a more promising route for enhancing the detection accuracy. However, their effectiveness can be influenced by the sampling method and the length of the generated sequences. \n\nWatermarking, a more recent development, has emerged as a unique method for detection. This involves embedding watermarks in the logits of an LLM during text generation, forming a pattern that can potentially identify synthetic text. Despite the promise of watermarking, its vulnerability to spoofing/paraphrasing attacks and evasion techniques presents significant challenges that warrant further research. Interestingly, despite being distinct research domains, bot detection and AI-generated text detection overlap. Both fields aim to distinguish between human-and machine-driven activities in their respective domains. Bot detection primarily identifies automated accounts or actions on online platforms, whereas AI-generated text detection seeks to discern between machine-generated and human-generated content. \n\nThe detection methodologies developed for bot detection, such as identifying unique behavioural patterns, could provide valuable insights and potential strategies for AI-generated text detection. Furthermore, techniques such as frequency analysis, sentiment analysis, and behavioural pattern detection from bot detection can be adapted for AI-based text detection. However, recognising that synthetic text and bot activities represent different aspects of online manipulation is also crucial. Although the detection strategies share similarities, the specifics of each domain require customised methodologies designed to address distinct challenges and characteristics.",
            "score": 0.6464026632967861,
            "section_title": "D. DETECTING SYNTHETIC TEXT AND THEIR OVERLAP WITH BOT DETECTION",
            "char_start_offset": 114719,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1272
                },
                {
                    "start": 1275,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2091
                },
                {
                    "start": 2094,
                    "end": 2289
                },
                {
                    "start": 2290,
                    "end": 2458
                },
                {
                    "start": 2459,
                    "end": 2586
                },
                {
                    "start": 2587,
                    "end": 2763
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "265506006",
            "title": "I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text",
            "text": "Transformer based Large Language Models (LLMs) (Vaswani et al., 2023) such as ChatGPT, Llama2 (Touvron et al., 2023) are able to generate texts that closely resemble human authored texts. For instance, Clark et al. (2021) report that untrained humans are not able to distinguish between texts generated by GPT-3 and texts authored by humans. As we train larger models with more parameters on an ever-expanding corpora, their capabilities in generating human-like text are likely to increase (Hoffmann et al., 2022). With their incredible performance in text generation, they become effective tools for automating monotonous text based tasks such as summarization and translation (Radford et al., 2019). \n\nHowever, these LLMs pose various threats to society because they can be also used for bad causes such as generating credible-sounding misinformation (Pan et al., 2023), creating fake product reviews (Adelani et al., 2019) and academic plagiarism (Dehouche, 2021). Recent studies have discovered that even though LLM-generated responses may sound convincing, they can be frequently incorrect (Lin et al., 2022). \n\nThe potential negative consequences associated with LLMs can be reduced significantly if a reliable detection system is in place to differentiate between machine-generated and human-written texts. A number of researchers focused on this important problem and proposed various approaches training a classifier (Guo et al., 2023), detecting based on linguistic features (Guo et al., 2023) and log probabilities and perturbations (Mitchell et al., 2023). Data driven methods such as training classifiers requires wide range of data with different styles, sources, and languages. More importantly non-watermarking detectors are generally found to be ineffective (Krishna et al., 2023). Moreover, existing perplexity based detectors are biased against non-native English writers (Liang et al., 2023), raising ethical concerns about their usage in real-life applications. \n\nIn this paper we propose a novel model-agnostic watermarking method to detect machine generated text.",
            "score": 0.6462168433606612,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1115
                },
                {
                    "start": 1118,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1983
                },
                {
                    "start": 1986,
                    "end": 2087
                }
            ],
            "ref_mentions": [
                {
                    "start": 951,
                    "end": 967,
                    "matchedPaperCorpusId": "233676730"
                },
                {
                    "start": 1096,
                    "end": 1114,
                    "matchedPaperCorpusId": "237532606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9267578125
        },
        {
            "corpus_id": "258615690",
            "title": "Bot or Human? Detecting ChatGPT Imposters with A Single Question",
            "text": "Since its introduction, Large Language Models (LLMs) have become widely used and raised public concerns about potential misuse. For instance, students may use ChatGPT (OpenAI, 2023) to complete written assignments, making it difficult for instructors to accurately assess student learning. As a result, there is a growing need to detect whether a piece of text was written by ChatGPT. To tackle this problem, DetectGPT (Mitchell et al., 2023) proposes a solution by comparing the log probabilities of the original passage with that of the perturbations of the same passage. The hypothesis behind this method is that minor rewrites of text generated by the model would likely result in lower log probabilities compared to the original sample, while minor rewrites of text written by humans may result in either higher or lower log probabilities. Another line of study model this problem as binary classification problem and fine-tune another model using supervised data (Bakhtin et al., 2019). Most recently, Mitrovi\u0107 et al. (2023) fine-tunes a Transformer-based model and uses it to make predictions, which are then explained using SHAP (Lundberg & Lee, 2017). Another area of research focuses on adding watermarks to AI-generated text in order to facilitate their identification, which involves imprinting specific patterns on the text to make it easier to detect (Zhao et al., 2023). Soft watermarking, as proposed by Kirchenbauer et al. (2023), involves dividing tokens into green and red lists in order to create these patterns. When generating text, a watermarked LLM is more likely to select a token from the green list, which is determined by the prefix token. These watermarks are often subtle and difficult for humans to notice. \n\nHowever, as demonstrated in Sadasivan et al. (2023a), a range of detection methods, including watermarking schemes, neural network-based detectors, and zero-shot classifiers, can be easily defeated by paraphrasing attacks. These attacks involve applying a light paraphraser to text generated by a language model.",
            "score": 0.6457240941668633,
            "section_title": "LLM offline detection.",
            "char_start_offset": 5238,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1737
                },
                {
                    "start": 1740,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2052
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9384765625
        },
        {
            "corpus_id": "273654899",
            "title": "DeTeCtive: Detecting AI-generated Text via Multi-Level Contrastive Learning",
            "text": "Existing methods for AI-generated text detection generally fall into the following three categories: (i) Watermarking methods: watermarking methods, which include rule based [5,30,59] and deep learning based [17,62] methods, involve embedding specific markers into AI-generated content, which can later be used to verify its source. The soft watermarking method [32] is an inference-time framework that involves grouping the vocabulary and decoding the next token preferentially. [23] proposes a method of adding watermarks by embedding backdoors triggered by special inputs into the model. UPV [41] is an unforgeable and publicly verifiable algorithm ensuring security against forgery and unauthorized detection attempts. (ii) Statistical methods: applying statistical metrics like entropy as thresholds to distinguish AI-generated text from human-written text. HowkGPT [63] identifies text origins by comparing perplexity scores of human-written and ChatGPT [6,69] generated text. DetectGPT [47] utilizes the structural properties of the LLM's probability density for zero-shot detection of AI-generated text. Similarly, DetectLLM [56] employs normalized perturbation log-ranks for identification, exhibiting less sensitivity to perturbations. (iii) Supervised learning methods: GPT-Sentinel [11] incorporates a binary classifier into RoBERTa [43] and T5 [51], which are directly trained on specific datasets. RADAR [27] employs an adversarial learning approach. By continually iterating to improve the detector and generator (both of which are LLMs), RADAR performs well in detecting both original and paraphrased AI-generated text. [55] utilizes contrastive learning to learn style representations on human-written text and uses the learned representations to identify different sources in a few-shot manner. Building on SCL [24] framework, CoCo [42] incorporates coherency information into the text representation, enhancing the ability to detect AI-generated text under resource-constrained conditions. \n\nContrastive learning for NLP.",
            "score": 0.6454433919178341,
            "section_title": "Introduction",
            "char_start_offset": 6551,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 2008
                },
                {
                    "start": 2011,
                    "end": 2040
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 177,
                    "matchedPaperCorpusId": "6540783"
                },
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "11028118"
                },
                {
                    "start": 180,
                    "end": 183,
                    "matchedPaperCorpusId": "3061822"
                },
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "233307237"
                },
                {
                    "start": 362,
                    "end": 366,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 960,
                    "end": 963,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 963,
                    "end": 966,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 993,
                    "end": 997,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1357,
                    "end": 1361,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1418,
                    "end": 1422,
                    "matchedPaperCorpusId": "259501842"
                },
                {
                    "start": 1636,
                    "end": 1640,
                    "matchedPaperCorpusId": "266977496"
                },
                {
                    "start": 1850,
                    "end": 1854,
                    "matchedPaperCorpusId": "264406273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8681640625
        },
        {
            "corpus_id": "258967971",
            "title": "Baselines for Identifying Watermarked Large Language Models",
            "text": "Recent methods such as DetectGPT and GPTZero distinguish between machine-generated and human-written text by analyzing their statistical discrepancies (Tian, 2023;Mitchell et al., 2023). DetectGPT compares the log probability computed by a model on unperturbed text and perturbed variations, leveraging the observation that text sampled from a LLM generally occupy negative curvature regions of the model's log probability function. GPTZero instead uses perplexity and burstiness to distinguish human from machine text, with lower perplexity and burstiness indicating a greater likelihood of machine-generated text. However, these heuristics do not generalize and are often fallible. \n\nDetection by Learning Classifiers Several papers have proposed to train classifiers to distinguish between AI and human generated text. During the initial GPT-2 release, OpenAI trained a RoBERTa classifier to detect GPT-2 generated text with 95% accuracy (Solaiman et al., 2019). More recently, OpenAI fine-tuned a GPT model on a dataset of machine-generated and human texts focusing on the same topic, with a true positive identification rate of 26% (Ope-nAI, 2023). Similarly, Guo et al. (2023) collected the Human ChatGPT Comparison Corpuse (HC3) and fine-tuned RoBERTa for the detection task. \n\nNotably, the capabilities of such classifiers decrease as machine-generated text becomes increasingly human-like. Sadasivan et al. (2023) show theoretically that for sufficiently advanced language models, machine-generated text detectors offer only a marginal improvement over random classifiers. Moreover, such methods are prone to adversarial attacks and are not robust to out-of-distribution text. \n\nWatermarking Large Language Models An alternative to detecting of machine-generated text using statistical discrepancies and learned classifiers is the concept of watermarks. Watermarks are hidden patterns in machinegenerated text that are imperceptible to humans, but algorithmically identifiable as synthetic. Natural language watermarks long predate the development of LLMs, relying on methods such as synonym substitution, as well as syntactic and semantic transformations (Topkara et al., 2005).",
            "score": 0.6440418152462469,
            "section_title": "Related Work Generated Text Detection Via Statistical Discrepancies",
            "char_start_offset": 1073,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1685
                },
                {
                    "start": 1688,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2188
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "267750204",
            "title": "Machine-generated Text Localization",
            "text": "The importance of detecting machine-generated text has risen due to the risk of producing factual inaccuracies (Lin et al., 2022) and the potential for its use in misinformation, such as propaganda or monetization (Zhang et al., 2023). Existing detection methods can primarily be categorized into two types: metric-based methods and modelbased methods. Metrics-base methods (Solaiman et al., 2019;Gehrmann et al., 2019;Mitchell et al., 2023;Su et al., 2023;Wang et al., 2023b) rely on extracting distinguishable features from text using the target language model. Specifically, Solaiman et al. (2019) apply log probability to identify whether a document is generated by LLMs or humans. Gehrmann et al. (2019) employ the absolute rank of each token as the evaluation metrics. Recent studies (Mitchell et al., 2023;Su et al., 2023;Bao et al., 2023) have shown that minor modifications to machine-generated text usually result in lower log probability under the model than the original text, a pattern not observed with humanwritten text. Thus, these methods introduce perturbations to the input text, measuring the discrepancy between the original and perturbed texts. \n\nModel-based methods (Solaiman et al., 2019;Guo et al., 2023;Ippolito et al., 2020;Bhattacharjee et al., 2023) involve training specific classifiers on annotated corpora to classify input text directly. This kind of method is particular useful for detecting text generated by black-box or unknown models. For example, Solaiman et al. (2019) finetuned a RoBERTa (Liu et al., 2019) model based on outputs from GPT-series models. Guo et al. (2023) developed their approach using the HC3 (Guo et al., 2023) dataset. \n\nTo improve the generalization capabilities of these detectors, Verma et al. (2024) extracted features from text using a series of language models and trained a classifier to categorize these features.",
            "score": 0.6433251641458023,
            "section_title": "Related Work",
            "char_start_offset": 6398,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1166
                },
                {
                    "start": 1169,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1679
                },
                {
                    "start": 1682,
                    "end": 1882
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 129,
                    "matchedPaperCorpusId": "237532606"
                },
                {
                    "start": 214,
                    "end": 234,
                    "matchedPaperCorpusId": "247628067"
                },
                {
                    "start": 397,
                    "end": 419,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 419,
                    "end": 441,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 441,
                    "end": 457,
                    "matchedPaperCorpusId": "259129463"
                },
                {
                    "start": 686,
                    "end": 708,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 790,
                    "end": 813,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 813,
                    "end": 829,
                    "matchedPaperCorpusId": "259129463"
                },
                {
                    "start": 1229,
                    "end": 1251,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 1251,
                    "end": 1278,
                    "matchedPaperCorpusId": "261660497"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90478515625
        },
        {
            "corpus_id": "256274849",
            "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
            "text": "As in prior work (Jawahar et al., 2020), we study the machine-generated text detection problem as a binary classification problem. Specifically, we aim to classify whether a candidate passage was generated by a particular source model. While several works have investigated methods for training a second deep network to detect machine-generated text, such an approach has several shortcomings, including a tendency to overfit to the topics it was trained on as well as the need to train a new model for each new source model that is released. We therefore consider the zero-shot version of machine-generated text detection, where we use the source model itself, without fine-tuning or adaptation of any kind, to detect its own samples. The most common method for zero-shot machine-generated text detection is evaluating the average per-token log probability of the generated text and thresholding (Solaiman et al., 2019;Gehrmann et al., 2019;Ippolito et al., 2020). However, this zeroth-order approach to detection ignores the local structure of the learned probability function around a candidate passage, which we find contains useful information about the source of a passage. \n\nThis paper poses a simple hypothesis: minor rewrites of model-generated text tend to have lower log probability under the model than the original sample, while minor rewrites of human-written text may have higher or lower log probability than the original sample. In other words, unlike human-written text, model-generated text tends to lie in areas where the log probability function has negative curvature (for example, near local maxima of the log probability). We empirically verify this hypothesis, and find that it holds true across a diverse body of LLMs, even when the minor rewrites, or perturbations, come from alternative language models. We leverage this observation to build DetectGPT, a zero-shot method for automated machine-generated text detection. To test if a passage came from a source model p \u03b8 , DetectGPT compares the log probability of the candidate passage under p \u03b8 with the average log probability of several perturbations of the passage under p \u03b8 (generated with, e.g., T5; Raffel et al. (2020)).",
            "score": 0.6426978517422193,
            "section_title": "\ud83e\udd14 from other source",
            "char_start_offset": 1443,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2206
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 39,
                    "matchedPaperCorpusId": "226237099"
                },
                {
                    "start": 920,
                    "end": 942,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 942,
                    "end": 964,
                    "matchedPaperCorpusId": "218560609"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "266977496",
            "title": "Few-Shot Detection of Machine-Generated Text using Style Representations",
            "text": "For example, Kirchenbauer et al. (2023) encode a watermark in generated text by splitting a model's vocabulary into so-called red and green lists, encouraging tokens from the green list to be sampled during decoding more frequently than tokens from the red list. Because this line of work requires direct access to a model and its vocabulary, the approach is most relevant for models deployed by an organization through an API (He et al., 2022). However, adversaries may decline to generate watermarked text, for example, by using their own LLM, or remove the watermark from API-generated text, for example, by paraphrasing, which has emerged as an effective mechanism to circumvent the detection of watermarked text (Krishna et al., 2023). We conduct a study comparing the proposed approach to watermarking in Appendix E. \n\nMuch of the recent work on detecting machine-generated text has focused on training a classifier using datasets of human-and machine-generated text (Jawahar et al., 2020), requiring access to a model's predictive distribution for comparison with other distributions (Mitchell et al., 2023), or directly using a model to detect its own outputs (Zellers et al., 2019). Other work has looked at patterns of repetition typical of LLM to rank documents according to their likelihoods of being machine-generated (Gall\u00e9 et al., 2021). In other recent work, the reliability of machine-text detectors in general has been questioned on theoretical grounds (Sadasivan et al., 2023). Zero-shot detection of machine-generated text is also a difficult task for human discriminators. Indeed, Dugan et al. (2020) demonstrated that human annotators had difficulty pinpointing changepoints between passages of documents composed by humans and those composed by machines, while Maronikolakis et al. (2021) find that automatic detectors can outperform humans in certain settings. \n\nGiven the limitations of discriminative classifiers at detecting machine-written documents, one might wonder whether generative approaches would be more robust.",
            "score": 0.6419683385212789,
            "section_title": "INTRODUCTION",
            "char_start_offset": 5915,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1884
                },
                {
                    "start": 1887,
                    "end": 2047
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 39,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 717,
                    "end": 739,
                    "matchedPaperCorpusId": "257687440"
                },
                {
                    "start": 973,
                    "end": 995,
                    "matchedPaperCorpusId": "226237099"
                },
                {
                    "start": 1091,
                    "end": 1114,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1168,
                    "end": 1190,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 1602,
                    "end": 1621,
                    "matchedPaperCorpusId": "221854248"
                },
                {
                    "start": 1784,
                    "end": 1811,
                    "matchedPaperCorpusId": "235097388"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84326171875
        },
        {
            "corpus_id": "273228395",
            "title": "Signal Watermark on Large Language Models",
            "text": "1) Watermark Methods: Recent advancements in watermarking techniques for language models have been crucial in ensuring text authenticity and model protection. Abdelnabi and Fritz [20] pioneered the Adversarial Watermarking Transformer (AWT), marking a significant step in embedding binary messages into text and demonstrating resilience against multiple attacks. He et al. [21] developed CATER, a conditional watermarking framework that secures text generation APIs against imitation attacks while preserving word distribution integrity. Zhao, Li, and Wang [22] introduced Distillation-Resistant Watermarking (DRW), a method effective in protecting NLP models from distillation theft and maintaining accuracy. Kirchenbauer et al. [23] proposed a framework for embedding watermarks in language models that are detectable with minimal impact on text quality. In a similar vein, Zhao, Wang, and Li's GINSEW technique [24] innovatively embeds secret signals in the probability vector of decoding steps, enhancing IP infringement detection. Peng et al. [25] developed EmbMarker for EaaS, embedding watermarks in trigger words for effective copyright verification. Yoo et al. [26] explored robust multi-bit watermarking, focusing on error resistance and enhanced payload. Lastly, Li et al. [27] introduced a novel strategy for embedding watermarks in the weight quantization process of large language models, ensuring model security without compromising operational modes. Together, these contributions represent a diverse and evolving approach to watermarking in the rapidly advancing field of language models. \n\n2) Detectors: The advancement in language models has prompted the development of various detection tools for identifying model-generated text. Gehrmann [28] developed GLTR, a tool to assist non-experts in detecting texts generated by models. It uses statistical methods to identify generation artifacts and has been shown to improve human detection rates of fake text significantly. GPT2-Detector developed by OpenAI [29] is a RoBERTa base fine-tuned with outputs from the 1.5Bparameter GPT-2 model. Its primary function is to predict if text was generated by a GPT-2 model.",
            "score": 0.6412364792038093,
            "section_title": "B. Detection for Model-generated Text",
            "char_start_offset": 11169,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1605
                },
                {
                    "start": 1608,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2107
                },
                {
                    "start": 2108,
                    "end": 2182
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 183,
                    "matchedPaperCorpusId": "221516138"
                },
                {
                    "start": 373,
                    "end": 377,
                    "matchedPaperCorpusId": "252367820"
                },
                {
                    "start": 1170,
                    "end": 1174,
                    "matchedPaperCorpusId": "259129912"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97705078125
        },
        {
            "corpus_id": "273482129",
            "title": "Unveiling Large Language Models Generated Texts: A Multi-Level Fine-Grained Detection Framework",
            "text": "Large language models has revolutionized numerous industries with their exceptional text generation capabilities [18]. However, these advancements also pose risks of misuse. Overreliance on LLMs can facilitate the spread of misinformation [19] and lead to unfair competition, threatening integrity in academic and societal contexts [20,21]. Therefore, timely detection of AI-generated text is crucial to mitigate these adverse impacts. \n\nExisting detection methods are categorized into metricbased and model-based approaches. Metric-based methods quantify linguistic features, such as stylometric analysis and perplexity scoring, to determine text origin. Gehrmann et al. [22] introduced GLTR, which uses metrics like word probability and entropy for detection, while Solaiman et al. [23] proposed a zero-shot detection method leveraging pre-trained models like GPT-2 or GROVER. However, as LLMs advance, these methods have become less effective. To address these limitations, more advanced and sophisticated detection techniques have emerged. Mitchell et al. [24] presented DetectGPT, a zero-shot method utilizing probability curvature analysis, and Tian et al. [25] developed the Multiscale Positive-Unlabeled (MPU) framework to enhance detection across varying text lengths. While metricbased approaches rely on linguistic analysis, model-based methods use advanced machine learning to classify text as human or AI-generated. These often involve fine-tuning language models or developing specialized architectures. For instance, Guo et al. [26] fine-tuned a RoBERTa model to detect ChatGPT-generated text, while Wang et al. [27] introduced SeqXGPT, a sentence-level method using logprobability lists and self-attention networks for enhanced detection. Liu et al. [28] proposed COCO, which improves detection in low-resource settings by combining an entity coherence graph with contrastive learning. However, most methods focus on binary classification, which limits their application. As LLMs are frequently used for text expansion and refinement, fine-grained detection is increasingly necessary for practical use. \n\nTo address this gap, we propose a novel LLM involvement prediction algorithm.",
            "score": 0.6403543821670494,
            "section_title": "AI-generated Content Detection",
            "char_start_offset": 7092,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 435
                },
                {
                    "start": 438,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2118
                },
                {
                    "start": 2121,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 117,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 332,
                    "end": 336,
                    "matchedPaperCorpusId": "257166266"
                },
                {
                    "start": 336,
                    "end": 339,
                    "matchedPaperCorpusId": "232099112"
                },
                {
                    "start": 1060,
                    "end": 1064,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1766,
                    "end": 1770,
                    "matchedPaperCorpusId": "264406273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.978515625
        },
        {
            "corpus_id": "273849981",
            "title": "Understanding the Effects of Human-written Paraphrases in LLM-generated Text Detection",
            "text": "Large language models (LLMs) have become essential in Natural Language Processing (NLP) thanks to their advanced capabilities for text processing and generation, which is achieved through analysis of patterns and relationships between words and sentences using transformer models [42]. Consequently, LLMs have had a significant impact on Natural Language Generation (NLG), as they have provided improved capacity for automatically generating high quality text [4]. \n\nWhile the advancement of LLMs in the context of NLG has aided tasks such as machine translation [38] and text summarization [40], it has also given rise to undesired social problems, including intentional malicious usage, ethical concerns and information inaccuracy. This has brought about the need for researching the development of methods for automated LLM-generated text detection which distinguishes if a text is human-or LLM-generated [12]. Currently, there are 2 major streams of LLM-generated text detectors: (i) zero-shot classifiers [25,33], which identify LLM-generated text based on the pattern and characteristics of the input, and (ii) watermark detectors [17], which rely on detecting the presence of watermarks which are imprinted into the text during the generation process [16], and are effective in the cases where the watermarks have been added by the LLM. The detectors then examine the input, classifying it as LLM-generated if the level of watermarking exceeds a set threshold, or as human-generated otherwise. \n\nBoth kinds of detectors have demonstrated excellent performance in LLM-generated text detection. However, research testing these detectors has primarily focused on datasets involving texts which are exclusively generated by humans or by LLMs. There can be, however, more complicated cases, such as paraphrased texts, which have been seldom considered in previous research. Paraphrasing is defined as the rewriting of context in a simpler and shorter form [8]; an LLM-generated text which is then paraphrased by humans leads to modified texts where the statistical properties of watermarks in the LLM-generated text is no longer identifiable.",
            "score": 0.6393146762033723,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1500
                },
                {
                    "start": 1503,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2144
                }
            ],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 284,
                    "matchedPaperCorpusId": "267152841"
                },
                {
                    "start": 563,
                    "end": 567,
                    "matchedPaperCorpusId": "255942578"
                },
                {
                    "start": 591,
                    "end": 595,
                    "matchedPaperCorpusId": "256416014"
                },
                {
                    "start": 908,
                    "end": 912,
                    "matchedPaperCorpusId": "233563022"
                },
                {
                    "start": 1258,
                    "end": 1262,
                    "matchedPaperCorpusId": "256194179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9638671875
        },
        {
            "corpus_id": "259129463",
            "title": "DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text",
            "text": "With the rapid progress of large language models (LLMs) and the huge amount of text they generated, it becomes more and more impractical to manually distinguish whether a text is machine-generated. Given the growing use of LLMs in social media and education, it prompts us to develop methods to detect machine-generated text, preventing malicious usage such as plagiarism, misinformation, and propaganda. Previous work has studied several zero-shot methods, which require no training data. These methods achieve good performance, but there is still a lot of room for improvement. In this paper, we introduce two novel zero-shot methods for detecting machine-generated text by leveraging the log rank information. One is called DetectLLM-LRR, which is fast and efficient, and the other is called DetectLLM-NPR, which is more accurate, but slower due to the need for perturbations. Our experiments on three datasets and seven language models show that our proposed methods improve over the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover, DetectLLM-NPR needs fewer perturbations than previous work to achieve the same level of performance, which makes it more practical for real-world use. We also investigate the efficiency--performance trade-off based on users preference on these two measures and we provide intuition for using them in practice effectively. We release the data and the code of both methods in https://github.com/mbzuai-nlp/DetectLLM",
            "score": 0.6385516203623102,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95849609375
        },
        {
            "corpus_id": "258714683",
            "title": "Watermarking Text Generated by Black-Box Language Models",
            "text": "Recent advances in large language models (LLMs) have enabled them to reach human-level proficiency across numerous professional and academic tasks [24,26,33]. One of the most impressive examples is OpenAI's ChatGPT [23], which has demonstrated remarkable prowess in answering questions, composing emails, essays, and even generating code. However, this impressive ability to create human-like text with remarkable efficiency has ignited apprehension regarding the potential abuse of LLMs for malicious purposes [6,7,15,31], such as phishing, disinformation campaigns, and academic dishonesty. Several countries and institutions have imposed bans on ChatGPT, citing concerns about privacy breaches, ideological influences, and academic dishonesty [19,20]. Additionally, media outlets have cautioned the public regarding the possibility of misleading information generated by LLMs [9]. These growing concerns have cast a shadow on the positive applications of LLMs. Therefore, detecting and authenticating generated text becomes crucial to ensure the responsible and secure use of LLMs. \n\nA prevalent solution is passive detection [1, 10,18,25,28,40], where a text classifier, usually fine-tuned on a pretrained language model like RoBERTa [16] and GPT-2 [30], is adopted to distinguish between generated and human-written text. However, these learning-based methods perform well only when the input data share a similar distribution with the training data, thereby limiting their applicability to specific domains. Moreover, as LLMs advance rapidly and human reliance on generated content grows, the line between human-written and generated text will gradually become more indistinct. For example, in the evaluations on a \"challenge set\" of English texts, OpenAI's text classifier only identifies 26% of generated text [25]. Besides, these classifiers are vulnerable to adversarial attacks [11,29,38] and are biased against non-native language writers [14], causing more false positives and negatives. \n\nTo achieve more reliable detection, Kirchenbauer et al. [12] proposed a watermark-based detection method for white-box language model usage scenarios.",
            "score": 0.6375514295840754,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 2000
                },
                {
                    "start": 2003,
                    "end": 2153
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 154,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 514,
                    "end": 516,
                    "matchedPaperCorpusId": "256229858"
                },
                {
                    "start": 516,
                    "end": 519,
                    "matchedPaperCorpusId": "256655912"
                },
                {
                    "start": 519,
                    "end": 522,
                    "matchedPaperCorpusId": "254530623"
                },
                {
                    "start": 1253,
                    "end": 1257,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1889,
                    "end": 1893,
                    "matchedPaperCorpusId": "202539059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94287109375
        },
        {
            "corpus_id": "276574900",
            "title": "Sarang at DEFACTIFY 4.0: Detecting AI-Generated Text Using Noised Data and an Ensemble of DeBERTa Models",
            "text": "This paper presents an effective approach to detect AI-generated text, developed for the Defactify 4.0 shared task at the fourth workshop on multimodal fact checking and hate speech detection. The task consists of two subtasks: Task-A, classifying whether a text is AI generated or human written, and Task-B, classifying the specific large language model that generated the text. Our team (Sarang) achieved the 1st place in both tasks with F1 scores of 1.0 and 0.9531, respectively. The methodology involves adding noise to the dataset to improve model robustness and generalization. We used an ensemble of DeBERTa models to effectively capture complex patterns in the text. The result indicates the effectiveness of our noise-driven and ensemble-based approach, setting a new standard in AI-generated text detection and providing guidance for future developments.",
            "score": 0.6370986505216137,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88623046875
        },
        {
            "corpus_id": "257631570",
            "title": "Can AI-Generated Text be Reliably Detected?",
            "text": "In order to aid this, a lot of recent research focuses on detecting AI-generated texts. \n\nSeveral detection works study this problem as a binary classification problem (OpenAI, 2019;Jawahar et al., 2020;Mitchell et al., 2023;Bakhtin et al., 2019;Fagni et al., 2020) and use neural network-based detectors. For example, OpenAI fine-tunes RoBERTa-based (Liu et al., 2019) GPT-2 detector models to distinguish between non-AI generated and GPT-2 generated texts (OpenAI, 2019). This requires such a detector to be fine-tuned with supervision on each new LLM for reliable detection. Another stream of work focuses on zero-shot AI text detection without any additional training overhead (Solaiman et al., 2019;Ippolito et al., 2019;Gehrmann et al., 2019). These works evaluate the expected per-token log probability of texts and perform thresholding to detect AI-generated texts. Mitchell et al. (2023) observe that AI-generated passages tend to lie in negative curvature of log probability of texts. They propose DetectGPT, a zero-shot LLM text detection method, to leverage this observation. Since these approaches rely on a neural network for their detection, they can be vulnerable to adversarial and poisoning attacks (Goodfellow et al., 2014;Sadasivan et al., 2023;Kumar et al., 2022;Wang et al., 2022). Another line of work aims to watermark AI-generated texts to ease their detection (Atallah et al., 2001;Wilson et al., 2014;Kirchenbauer et al., 2023a;Zhao et al., 2023). Watermarking eases the detection of LLM output text by imprinting specific patterns on them. Soft watermarking proposed in Kirchenbauer et al. (2023a) partitions tokens into \"green\" and \"red\" lists, as they define, to help create these patterns. A watermarked LLM samples a token, with high probability, from the green list determined by a pseudo-random generator seeded by its prefix token.",
            "score": 0.6363394865509243,
            "section_title": "Introduction",
            "char_start_offset": 1866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 90,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1866
                }
            ],
            "ref_mentions": [
                {
                    "start": 1284,
                    "end": 1302,
                    "matchedPaperCorpusId": "246634569"
                },
                {
                    "start": 1386,
                    "end": 1408,
                    "matchedPaperCorpusId": "37687669"
                },
                {
                    "start": 1408,
                    "end": 1428,
                    "matchedPaperCorpusId": "6169182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9140625
        },
        {
            "corpus_id": "267027819",
            "title": "Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text",
            "text": "Finally, there exists a set of techniques that engage in the fine-tuning of language models specifically for the detection of text generated by machines. \n\nOne of the subsets of statistical features leveraged in detecting machine-generated text revolves around term frequencies within text samples. Text authored by humans typically aligns with Zipf's Law, where the most frequent word in humanwritten text has roughly twice the frequency of the second most frequent word, and nearly three times that of the third most frequent word and so on [43]. Another feature-based method for detecting machine-generated text focuses on clarity and coherence. Earlier models like GPT-2 have shown that in longer outputs, there is a higher likelihood that inconsistencies, redundancy, or logical errors will emerge [53]. Some works build classifiers on bag-of-words features and TF-IDF [24] to analyze shorter text sequences. Other works such as Fr\u00f6hling et al. [25] combine several features, including lack of syntactic and lexical diversity, repetitiveness, and extract linguistic features such as named entities and part-of-speech distributions to build a neural network classifier on this information. However, a notable limitation of these statistical and feature-based methods is their potential vulnerability to more advanced language models, which continually improve in generating human-like text patterns and linguistic diversity. \n\nZero-shot methods for detecting machine-generated text have garnered attention due to the lack of need to train models on large datasets. Works such as those by Gehrmann et al. [27] find that human-written texts exhibit a broader range of word choices, whereas machine-generated texts tend to be more constrained and predictable and calculate entropy on text sequences to help distinguish machine-generated text from human-generated text. Other works used simple logprobability thresholding, where LLM token probabilities are averaged and a threshold is determined [56]. Recent works like DetectGPT [40] take this idea further by introducing the observation that the curvature of a model's log probability function is typically more negative for machine-generated text compared to human-written text.",
            "score": 0.636115404127882,
            "section_title": "Background",
            "char_start_offset": 8339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 156,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1428
                },
                {
                    "start": 1431,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2231
                }
            ],
            "ref_mentions": [
                {
                    "start": 543,
                    "end": 547,
                    "matchedPaperCorpusId": "36455310"
                },
                {
                    "start": 803,
                    "end": 807,
                    "matchedPaperCorpusId": "202734604"
                },
                {
                    "start": 874,
                    "end": 878,
                    "matchedPaperCorpusId": "220936111"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "233563022"
                },
                {
                    "start": 1608,
                    "end": 1612,
                    "matchedPaperCorpusId": "182952848"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.955078125
        },
        {
            "corpus_id": "273653962",
            "title": "Segmenting Watermarked Texts From Language Models",
            "text": "With the increasing use of large language models in recent years, it has become essential to differentiate between text generated by these models and text written by humans. Some of the most advanced LLMs, such as GPT-4, Llama 3, and Gemini, are very good at producing human-like texts, which could be challenging to distinguish from human-generated texts, even for humans. However, it is crucial to distinguish between human-produced texts and machine-produced texts to prevent the spread of misleading information, improper use of LLM-based tools in education, model extraction attacks through distillation, and the contamination of training datasets for future language models. \n\nWatermarking is a principled method for embedding nearly unnoticeable statistical signals into text generated by LLMs, enabling provable detection of LLM-generated content from its human-written (b) We develop a systematic statistical method to segment texts into watermarked and nonwatermarked sub-strings. We have also investigated the theoretical and finite sample performance of this methodology. As far as we know, this problem has not been studied in recent literature on generating and detecting watermarked texts from LLMs. \n\n2 Problem setup",
            "score": 0.6358809449234848,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 680
                },
                {
                    "start": 683,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1214
                },
                {
                    "start": 1217,
                    "end": 1232
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9287109375
        },
        {
            "corpus_id": "267759853",
            "title": "GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick",
            "text": "Machine-generated text detection can be roughly categorized into three approaches. \n\nZero-shot Methods. This approach, known as \"model self-detection,\" necessitates full access to the language model and utilizes statistical measures such as perplexity and entropy. Noteworthy contributions include  (Li et al., 2023;Tulchinskii et al., 2023). A drawback of training-based methods is their potential over-fitting to specific datasets and models. \n\nWatermarking Techniques. Recent advancements in hidden signal watermarking in texts can be categorized into post-edited and decoding-based watermarking. Post-edited watermarking involves text formatting or lexical changes (Brassil et al., 2002;Sato et al., 2023;He et al., 2022;Yoo et al., 2023a), while decoding-based watermarking in the era of large language models (LLMs) embeds statistical signals during decoding. Notable techniques in this domain include Kirchenbauer et al. (2023)'s red-green list and Zhao et al. (2023)'s robust watermarking. Unbiased watermarks, which preserve the original token distributions, have been explored by Kuditipudi et al. (2023) and Hu et al. (2023). Additionally, multi-bit watermarking, which embeds complex information, is examined by Wang et al. (2023) and Yoo et al. (2023b). Several techniques for embedding watermarks include text formatting, as demonstrated by Por et al. (2012) andRizzo et al. (2016), and context-aware lexical substitution, as explored by Yang et al. (2021). Syntactic modifications are discussed by Atallah et al. (2001) and Meral et al. (2009). Training data watermarking is addressed by Liu et al. (2023b) andTang et al. (2023). A publicly detectable watermark is proposed by Fairoze et al. (2023), while leveraging semantic meaning for robustness is examined by Ren et al. (2023).",
            "score": 0.6351643639440091,
            "section_title": "Related Work",
            "char_start_offset": 4842,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 85,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 444
                },
                {
                    "start": 447,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1797
                }
            ],
            "ref_mentions": [
                {
                    "start": 669,
                    "end": 691,
                    "matchedPaperCorpusId": "6540783"
                },
                {
                    "start": 908,
                    "end": 934,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 1355,
                    "end": 1376,
                    "matchedPaperCorpusId": "17690312"
                },
                {
                    "start": 1513,
                    "end": 1534,
                    "matchedPaperCorpusId": "37687669"
                },
                {
                    "start": 1539,
                    "end": 1558,
                    "matchedPaperCorpusId": "1192689"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90380859375
        },
        {
            "corpus_id": "267740518",
            "title": "WatME: Towards Lossless Watermarking Through Lexical Redundancy",
            "text": "Early works on AI-generated text detection develop post-hoc detection methods to analyze machinegenerated text by treating the problem as a binary classification task (OpenAI, 2019;Jawahar et al., 2020;Mitchell et al., 2023). For instance, Ope-nAI has fine-tuned RoBERTa (Liu et al., 2019) to distinguish between human and GPT-2 generated texts (OpenAI, 2019). However, existing detectors are found to be fragile against adversarial attacks (Wolff, 2020) and biased towards non-native English writers (Liang et al., 2023). Moreover, as LLMs continue to advance, their generated outputs more closely resemble human-written text, rendering these methods progressively less effective. \n\nOn the other side, watermarking, traditionally a copyright marking method (Adi et al., 2018;Rouhani et al., 2018), involves developers, users, and regulatory entities. Developers choose an algorithm to subtly embed hidden modifications into data, which can be altered during user transmission. Regulatory bodies can later extract this information to trace and regulate AI-generated content (Atallah et al., 2001;Wilson et al., 2014;Hacker et al., 2023). In the context of natural languages, watermarking typically involves modifying content or structure. For example, rule-based methods (Stefan et al., 2000) or carefully designed neural encoders (Yang et al., 2022;Ueoka et al., 2021) encrypt messages into text, which are then extracted using the corresponding rules and neural decoder. The discrete nature of natural language, however, presents a considerable challenge to this approach, as modifications can unintentionally degrade text quality or alter its intended meaning. \n\nFor the detection of LLM-generated texts, a pioneering watermarking technique (Kirchenbauer et al., 2023) partitions tokens into 'green' and 'red' lists, biases output distribution towards 'green' tokens, and creates patterns that are detectable yet imperceptible to humans. Nevertheless, while yielding promising detection results, these methods may still degrade the textual quality and be vulnerable to the paraphrase attack.",
            "score": 0.635160991323355,
            "section_title": "Related Work",
            "char_start_offset": 6145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1663
                },
                {
                    "start": 1666,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 202,
                    "matchedPaperCorpusId": "226237099"
                },
                {
                    "start": 1074,
                    "end": 1096,
                    "matchedPaperCorpusId": "37687669"
                },
                {
                    "start": 1096,
                    "end": 1116,
                    "matchedPaperCorpusId": "6169182"
                },
                {
                    "start": 1116,
                    "end": 1136,
                    "matchedPaperCorpusId": "256615207"
                },
                {
                    "start": 1331,
                    "end": 1350,
                    "matchedPaperCorpusId": "245144237"
                },
                {
                    "start": 1350,
                    "end": 1369,
                    "matchedPaperCorpusId": "233307237"
                },
                {
                    "start": 1744,
                    "end": 1771,
                    "matchedPaperCorpusId": "256194179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6240234375
        },
        {
            "corpus_id": "269292978",
            "title": "SemEval-2024 Task 8: Multidomain, Multimodel and Multilingual Machine-Generated Text Detection",
            "text": "The proliferation of Large Language Models (LLMs) has led to a significant increase in the volume of machine-generated text (MGT) across a wide range of domains.This rise has sparked concerns regarding the potential for misuse in fields such as journalism, education, academia, etc (Uchendu et al., 2023;Crothers et al., 2023).Moreover, it poses challenges to maintaining information integrity and ensuring accurate information dissemination.As such, the ability to accurately distinguish between human-written content and machinegenerated content has become paramount for identifying potential misuse (Jawahar et al., 2020;Stiff and Johansson, 2022;Macko et al., 2023).\n\nIn response to these challenges, we are introducing a shared task that focuses on the detection of machine-generated text across multiple generators, domains, and languages.We are providing largescale evaluation datasets for three subtasks with the primary goals of fostering extensive research in MGT detection, advancing the development of automated systems for detecting MGT, and reducing instances of misuse: Subtask A: Human vs. Machine Classification.The goal of this subtask is to accurately classify a text as either produced by a human or generated by a machine.This is the basic, but one of the most common use-cases of MGT detection systems for preventing the misuse of LLMs.This task is divided into two tracks: (i) The monolingual track, which focuses solely on English texts; and (ii) The multilingual track, which involves texts in a variety of languages, thereby expanding the diversity and complexity beyond existing benchmarks.\n\nSubtask B: Multi-Way Generator Detection.This task aims to pinpoint the exact source of a text, i.e., determine whether it originated from a human or a specific LLM (GPT-3, GPT-3.5, GPT-4, Cohere, DALL-E, or BLOOMz).",
            "score": 0.6351320096128343,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 161,
                    "end": 327
                },
                {
                    "start": 327,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 670
                },
                {
                    "start": 672,
                    "end": 845
                },
                {
                    "start": 845,
                    "end": 1129
                },
                {
                    "start": 1129,
                    "end": 1243
                },
                {
                    "start": 1243,
                    "end": 1358
                },
                {
                    "start": 1358,
                    "end": 1617
                },
                {
                    "start": 1619,
                    "end": 1660
                },
                {
                    "start": 1660,
                    "end": 1835
                }
            ],
            "ref_mentions": [
                {
                    "start": 282,
                    "end": 304,
                    "matchedPaperCorpusId": "252992519"
                },
                {
                    "start": 602,
                    "end": 624,
                    "matchedPaperCorpusId": "226237099"
                },
                {
                    "start": 624,
                    "end": 650,
                    "matchedPaperCorpusId": "245454767"
                },
                {
                    "start": 650,
                    "end": 669,
                    "matchedPaperCorpusId": "264300609"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87158203125
        },
        {
            "corpus_id": "261101245",
            "title": "Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities",
            "text": "We first discuss the task of detecting AI-generated language. Being able to generate AI-generated text is helpful to flag potentially malicious content, for example in the context of misinformation (Zhou et al., 2023) as well as plagiarism for student essay writing and journalism (Mitchell et al., 2023). To achieve this, various methods have been proposed in the literature (Tang et al., 2023), some of which we will discuss in the following. \n\nWatermarking The detection of watermarking refers to injecting a watermark into machinegenerated content which can be algorithmically detected whilst being unrecognizable to the human reader. One use case involves circumventing data contamination arising from automatic translation. In this context, Venugopal et al. (2011) suggested the integration of bit-level watermarks into machine-translated outputs, allowing for subsequent detection in a post-hoc manner. Kirchenbauer et al. ( 2023) later expand upon this idea, formulating a watermarking algorithm for LLMgenerated context. Their methodology encourages LLMs to generate a series of watermarked words, enabling the statistical detection of watermarks in any subsequent LLM-generated content. This approach, however, necessitates modifications to the output distribution to achieve its purpose. Hence, He et al. (2022) introduce a method of conditional synonym replacement, designed to augment the stealthiness of textual watermarks without inducing a shift in the output distribution. Alternatively, Christ et al. (2023) present an undetectable watermarking algorithm that relies on the empirical entropy of the generated output. Their method maintains the original output distribution, offering a formal guarantee of this preservation. However, previous work has found that watermarking can be defeated through paraphrasing input texts (Krishna et al., 2023;Sadasivan et al., 2023) Discriminating approaches The problem of detecting synthetically generated context can be approached as a binary classification task. This strategy was adopted by OpenAI in response to the potential misuse of GPT-2 for spreading misinformation.",
            "score": 0.6341421368047072,
            "section_title": "Preventing misuse of LLMs via content detection",
            "char_start_offset": 41546,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 444
                },
                {
                    "start": 447,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2132
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 217,
                    "matchedPaperCorpusId": "257633591"
                },
                {
                    "start": 281,
                    "end": 304,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 747,
                    "end": 770,
                    "matchedPaperCorpusId": "15210695"
                },
                {
                    "start": 1306,
                    "end": 1322,
                    "matchedPaperCorpusId": "252367820"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6630859375
        },
        {
            "corpus_id": "265552122",
            "title": "MARKMyWORDS: Analyzing and Evaluating Language Model Watermarks",
            "text": "The capabilities of large language models have grown significantly in recent years and so too have concerns about their misuse. It is important to be able to distinguish machine-generated text from human-authored content. Prior works have proposed numerous schemes to watermark text, which would benefit from a systematic evaluation framework. This work focuses on LLM output watermarking techniques - as opposed to image or model watermarks - and proposes Mark My Words, a comprehensive benchmark for them under different natural language tasks. We focus on three main metrics: quality, size (i.e., the number of tokens needed to detect a watermark), and tamper resistance (i.e., the ability to detect a watermark after perturbing marked text). Current watermarking techniques are nearly practical enough for real-world use: Kirchenbauer et al. [33]'s scheme can watermark models like Llama 2 7B-chat or Mistral-7B-Instruct with no perceivable loss in quality on natural language tasks, the watermark can be detected with fewer than 100 tokens, and their scheme offers good tamper resistance to simple perturbations. However, they struggle to efficiently watermark code generations. We publicly release our benchmark (https://github.com/wagner-group/MarkMyWords).",
            "score": 0.6322203565700668,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93212890625
        },
        {
            "corpus_id": "273228395",
            "title": "Signal Watermark on Large Language Models",
            "text": "With the advancement of Large Language Models (LLMs) such as ChatGPT [1], their capabilities have become increasingly sophisticated, enabling them to perform a wide range of tasks with remarkable efficiency and accuracy. Demonstrating formidable abilities, these models can answer scientific and mathematical queries, understand historical events and biographies, and offer assistance with daily life. Even though language models may occasionally produce illusions or incorrect answers, the potential and impact of LLMs remain significant. However, this enhanced functionality also presents new challenges in terms of security and potential misuse. For instance, there's a risk of these models being used to create fake news or media rumors, and students might leverage them to generate assignments, thereby impacting academic fairness. Consequently, there is a growing need for reliable methods to identify model-generated text to effectively address and mitigate these emerging concerns. \n\nNumerous open-source and commercial tools have been developed to detect model-generated text, such as GPT2-Detector [29], RoBERTa-QA [31], DetectGPT [30], GPTZero [32], Writer [33], and others. However, there are three significant limitations to these methods. First, their probability predictions cannot provide conclusive certainty and should only be regarded as a reference. Second, most current approaches rely on methods that differentiate human text from modelgenerated text based on variations in perplexity and burstiness. Third, methods like DetectGPT and RoBERTa-QA, which require multiple text-generation process and model retraining, consume substantial computational resources and time when detecting model-generated text. As LLMs advance, the generated text increasingly resembles human writing, making these methods based on variance in perplexity between human and model-generated text less applicable. Therefore, embedding watermarks in text generated by LLMs presents a faster, more stable, and reliable method of verification. \n\nWatermarks in Large Language Models need to meet three arXiv:2410.06545v1 [cs.CR] 9 Oct 2024 Fig. 2: This figure illustrates an example of embedding a signal watermark while generating text by LLMs.",
            "score": 0.631835975267425,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2037
                },
                {
                    "start": 2040,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2238
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.947265625
        },
        {
            "corpus_id": "266690717",
            "title": "AI Content Self-Detection for Transformer-based Large Language Models",
            "text": "While detection of AI-generated content can be important in many settings, the emergence of generative AI creates especially complicated ethical challenges for academic integrity. Much work had already been done to detect plagiarism, which can lead to students not learning by copying assignment solutions or researchers taking credit for someone else's work and ideas. \n\nAI-generated content creates a new challenge since it does not directly copy existing content but generates new text. Traditional methods that identify similarities between a new document and a database of existing documents may fall short of distinguishing AI-generated content from new human work. Large language models aim to create natural, human-like text, making it increasingly hard to differentiate generated from human-created text. \n\nMany tools to detect AI-generated text are now offered. Some popular tools geared toward educators are Copyleaks AI Content Detector, Crossplag, GPTZero, Hugging Face OpenAI Detector, Originality.ai, Turnitin AI Detection and ZeroGPT. The list of detectors and their capability is constantly changing following the fast-paced changes seen in the development of large language models. \n\nMost tools are based on detecting artifacts of the text generation process, including word choice, writing style, sentence length, and many more. A report by Open AI [9] lays out three AI content detection strategies, including a simple classifier learned from scratch, a classifier resulting from fine-tuning an existing language model, or using the probabilities assigned by the model to strings. Many existing tools follow the first two approaches. For example, the Hugging Face Open AI detector is a transformer-based classifier that is fine-tuned to detect GPT-2 text. Self-detection introduced in this paper is most closely related to the third approach. However, it does not require access to the model parameters to assess probabilities. It relies on the model itself to perform the detection.",
            "score": 0.6316391711236907,
            "section_title": "B. Detection of AI-generated Text",
            "char_start_offset": 4271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 813
                },
                {
                    "start": 816,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1199
                },
                {
                    "start": 1202,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2003
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94189453125
        },
        {
            "corpus_id": "249635702",
            "title": "An Empirical Study of AI Generated Text Detection Tools",
            "text": "Using scalar controls, DIPPER's paraphrased results can be tailored to vocabulary and sentence structure. Extensive testing proved that DIPPER's paraphrase of AI-generated text could evade watermarking techniques and GPTZero, DetectGPT, and OpenAI's text classifier. The detection accuracy of DetectGPT was decreased from 70.3% to 4.6% while maintaining a false positive rate of 1% when DIPPER was used to paraphrase text generated by three well-known big language models, one of which was GPT3.5-davinci-003. These rephrases were impressive since they didn't alter the original text's meaning. The study developed a straightforward defense mechanism to safeguard AI-generated text identification from paraphrase-based attacks. Language model API providers were required to get semantically identical texts for this defense strategy to work. To find sequences comparable to the candidate text, the algorithm looked through a collection of already generated sequences. A 15-million-generation database derived from a finely tuned T5-XXL model confirmed the efficacy of this defense strategy. The software identified Paraphrased generations in 81% to 97% of test cases, demonstrating its efficacy. Remarkably, only 1% of human-written sequences were incorrectly labeled as AI-generated by the software. The project made its code, models, and data publicly available to pave the way for additional work on detecting and protecting AI-generated text. \n\nOpenAI [21], an AI research company, compared manual and automatic ML-based synthetic text recognition methods. Utilizing models trained on GPT-2 datasets enhances the inherent authenticity of the text created by GPT-2, hence facilitating human evaluators' identification of erroneous datasets. Consequently, the team evaluated a rudimentary logistic regression model, a detection model based on finetuning, and a detection model employing zero-shot learning. A logistic regression model was trained using TFIDF, unigram, and bigram features and evaluated using various generating processes and model parameters afterward. The most basic classifiers demonstrated an accuracy rate of 97% or higher. Models need help in identifying shorter outputs.",
            "score": 0.6313021280752451,
            "section_title": "Literature Review",
            "char_start_offset": 6896,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1446
                },
                {
                    "start": 1449,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2195
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.859375
        },
        {
            "corpus_id": "258999304",
            "title": "Challenges and Remedies to Privacy and Security in AIGC: Exploring the Potential of Privacy Computing, Blockchain, and Beyond",
            "text": "The flourishing of AIGC has brought more options for human beings to work, study and create. However, as the characteristics of AIGC become closer and closer to humancreated content, it is increasingly difficult to tell if certain works are generated by AI. For this reason, there has been a significant amount of work trying to distinguish them from each other. How to improve the identifiability of AIGC is also an important part to ensure the safety of generated content. \n\nThe rapid improvement of language models has raised the specter of abuse of text generation systems. GLTR [71] is a tool to support humans in detecting whether a text was generated by a model. It applies a suite of baseline statistical methods that can detect generation artifacts across common sampling schemes. A feature-based detection method of automated language models is proposed in [65] . It uses carefully crafted features that attempt to model intrinsic differences between human and machine text. Guo et al. [79] conducted comprehensive human evaluations and linguistic analyses of ChatGPT-generated content compared with that of humans, where many interesting results are revealed. Kumarage et al. [111] present a novel algorithm using stylometric signals to aid in detecting AI-generated tweets. Their extensive experiments demonstrate that the stylometric features are effective in augmenting the state-of-the-art AI-generated text detectors. Yu et al. [234] initially investigate the possible negative impact of ChatGPT on academia, and present a large-scale CHatGPT-writtEn AbsTract (CHEAT) to support the development of detection algorithms. Liu et al. [139] test existing AIGC detectors and build a detector using SVMs and RoBERTa. Results suggest that a RoBERTa fine-tuned with the training set of ArguGPT achieves above 90% accuracy in both essay-and sentence-level classification. Kirchenbauer et al. [107] proposed a watermark to detect whether the text is generated by AI models. They have only tested it on Meta's smaller open source language model, OPT-6.7B, so its effectiveness on larger, widely used language models like ChatGPT is unknown. Overall, these detection methods still have some drawbacks at present.",
            "score": 0.6309178661581877,
            "section_title": "Identifiability.",
            "char_start_offset": 69532,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 474
                },
                {
                    "start": 477,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2216
                }
            ],
            "ref_mentions": [
                {
                    "start": 867,
                    "end": 871,
                    "matchedPaperCorpusId": "233563022"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7880859375
        },
        {
            "corpus_id": "266369103",
            "title": "A Survey on the Detection and Impacts of Deepfakes in Visual, Audio, and Textual Formats",
            "text": "It uses statistical patterns learnt from its training data to detect anomalies that may indicate machine-generated text. According to researchers, Grover has consistently performed well across different model sizes and against various models, proving its ability to identify artificially generated content. However, its effectiveness can be weakened by adversarial actions or generation strategies that differ significantly from those it has been trained. \n\nGehrmann et al. [232] developed GLTR, a statistical tool designed for the detection and visualisation of machinegenerated texts. This tool exploits the unique next-word prediction distributions found in machine-generated text, which are different from those found in human-written content. Using the capabilities of language models such as BERT [47] and GPT-2 [48], the GLTR assesses the likelihood of each subsequent word in a text based on the predictions of these models and visually presents this information. This technique helps to detect and understand the specific characteristics of the pattern of machine-generated text. Despite GLTR's utility, it has some limitations. Its effectiveness can be compromised when applied to texts generated by different or more advanced models. It may struggle with very short or overly complex texts, and its performance may vary according to the sampling methods used by the generative models [229]. Furthermore, GLTR may be vulnerable to adversarial attacks designed to mislead the detection process. \n\nA study conducted by Mitchell et al. [241] in 2023 introduced DetectGPT as an innovative technique to distinguish between machine-generated text and human-written. This approach is based on the concept of 'probability curvature', which helps to identify the differences between the two types of text. DetectGPT has successfully detected outputs from various generative models, including GPT versions and transformer-based models, by examining the shape of the predictive probability distribution for the next word in a sequence. However, this method has two limitations. First, DetectGPT's computational load may pose challenges, particularly with larger datasets or real-time detection scenarios. Second, the effectiveness of the method relies heavily on the availability of raw log probabilities from the large language models it is trying to detect, which may limit its practicality when such probabilities are inaccessible or when dealing with new or proprietary models.",
            "score": 0.6302473297507245,
            "section_title": "3) METHODS BASED ON ZERO SHOT MODELS",
            "char_start_offset": 90083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 455
                },
                {
                    "start": 458,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1503
                },
                {
                    "start": 1506,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2203
                },
                {
                    "start": 2204,
                    "end": 2480
                }
            ],
            "ref_mentions": [
                {
                    "start": 818,
                    "end": 822,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9404296875
        },
        {
            "corpus_id": "258987266",
            "title": "Red Teaming Language Model Detectors with Language Models",
            "text": "We consider a large language model G that conditions on an input context or prompt X and generates an output text Y = G(X). We use upper-case characters to denote a sequence of tokens. For example, X = [x 1 , x 2 , ..., x m ], where m is the sequence length. The model G is protected by a detector f (Y) \u2208 [0, 1] that predicts whether Y is generated by an LLM, where a higher detection score f (Y) means that Y is more likely to be LLMgenerated. We use \u03c4 to denote a detection threshold such that Y is considered LLM-generated if f (Y) \u2265 \u03c4 . \n\nIn this work, we consider three categories of detectors: (1) classifier-based detectors, (2) watermarking detectors, and (3) likelihood-based detectors. For classifier-based detectors, a text classifier f (Y) is trained on a labeled dataset with G-generated and human-written texts. For watermarking detectors, G is modified from a base generator G 0 with a watermarking mechanism W , denoted as G = W (G 0 ), and a watermark detector f (Y) is constructed to predict whether Y is generated by the watermarked LLM G. Specifically, we consider the watermarking mechanism in Kirchenbauer et al. (2023). For likelihood-based detectors, they estimate f (Y) by comparing the log probabilities of Y and several random perturbations of Y. Specifically, we consider DetectGPT (Mitchell et al., 2023). We consider a model G as protected if there is a detector f (Y) in place to protect the model from inappropriate usage. \n\nTo stress test the reliability and robustness of those detectors in this setting, we develop redteaming techniques to generate texts that can down-grade a detector using an LLM that is also protected by this detector. We consider attacks by output perturbation and input perturbation respectively: \n\n\u2022 Output perturbation perturbs the original output Y and generates a perturbed output Y \u2032 .",
            "score": 0.6293771329908325,
            "section_title": "Settings and Overview",
            "char_start_offset": 8295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 541
                },
                {
                    "start": 544,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1455
                },
                {
                    "start": 1458,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1755
                },
                {
                    "start": 1758,
                    "end": 1849
                }
            ],
            "ref_mentions": [
                {
                    "start": 1116,
                    "end": 1142,
                    "matchedPaperCorpusId": "256194179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9345703125
        },
        {
            "corpus_id": "272827661",
            "title": "ESPERANTO: Evaluating Synthesized Phrases to Enhance Robustness in AI Detection for Text Origination",
            "text": "Based on the assumption that the ChatGPT [42] model make fewer modifications to LLM-generated texts compared to human-written texts, Zhu et al. [22] develop a zero-shot and black-box detection method. This approach generates revised versions of a text using ChatGPT and measures the similarity between the original text and its revised version. They use the criterion that a higher similarity score suggests a higher probability of the text being LLM-generated to assess whether a text is AI-generated. In another research effort, Bhattacharjee and Liu [55] assess the zero-shot performance of ChatGPT by providing it with a simple prompt along with the text to be classified in the task of distinguishing between human-written and AI-generated text. They test this approach on samples from 19 models, ranging from an 82M-parameter model to a 1.6B-parameter model, as found in the TuringBench dataset [16]. Their findings indicate that although ChatGPT has difficulty identifying AI-generated text, it performs effectively on human-written text. \n\nTo investigate the reliability of existing AI-text detectors, numerous studies have been dedicated to designing prompts that may allow LLMs to generate texts capable of evading detection. In one such work, Kumarage et al. [58] present a framework named \"EScaPe\", which directs pre-trained language models (PLMs) to circumvent AI-generated-text detectors using a universal evasive prompt. The EScaPe framework involves initially crafting a specific evasive prompt for a particular PLM through prompt tuning and then capitalizes on the transferability of soft prompts to transfer the evasive prompt from one PLM to another. In a related study, Lu et al. [59] propose \"SICO\", an in-context learning approach that iteratively replaces words and sentences within the in-context examples to assist LLMs in generating text that can evade detection. The substitution procedure is directed by a proxy detector. The authors demonstrate that, in addition to reducing the effectiveness of existing AI text detectors, SICO decreases the likelihood of being recognized by humans. Kirchenbauer et al. [28] present a watermarking strategy designed to make synthetic text detectable even in short token spans.",
            "score": 0.6286710935617369,
            "section_title": "Related work",
            "char_start_offset": 10389,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1045
                },
                {
                    "start": 1048,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2240
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 148,
                    "matchedPaperCorpusId": "266164139"
                },
                {
                    "start": 553,
                    "end": 557,
                    "matchedPaperCorpusId": "260379022"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83203125
        },
        {
            "corpus_id": "267759853",
            "title": "GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick",
            "text": "The emergence of large language models (LLMs), exemplified by GPT-4 (OpenAI, 2023a), has enabled the generation of remarkably human-like content, facilitating tasks such as writing (Shanahan and Clarke, 2023), coding (Chen et al., 2021), and fostering creativity. However, this technological advancement brings forth the potential for malicious applications, including social engineering (Mirsky et al., 2023), fake news fabrication (Zellers et al., 2019), and academic dishonesty. Consequently, the need for effective detection of machine-generated texts has become increasingly critical. \n\nVarious strategies have been proposed to distinguish machine-generated texts from human- written texts, and decoding-based watermarking has emerged as a highly effective approach. This technique embeds subtle patterns into the text during the decoding stage of LLM, which designated algorithms can identify. The GumbelMax-trickbased watermark (GM watermark), introduced by Aaronson and Kirchner (2023) as their Exponential watermark, is a prominent example within this category, known for its exceptional detectability and low perplexity for generated text. However, a critical limitation of this method is its tendency to produce identical outputs for the same prompt, which could adversely affect both the diversity of the model's outputs and the overall user experience, as illustrated in Figure 1. \n\nTo address the challenge of generating diverse outputs of the GM watermark, our analysis delves into the core mechanism of decoding-based watermarks. We discover that these watermarks share a cohesive framework, as illustrated in Figure 3. The primary cause of uniform completions for identical prompts is traced back to the deterministic nature of both the Decoder and Pseudo-random functions in the GM watermark. To mitigate this, we propose two strategies to introduce variability into the Decoder function and one strategy to the Pseudorandom function: 1) Implement a drop mechanism with a predefined probability d p , enabling direct sampling from the language model without watermark insertion. 2) Replace the \"argmax\" operation in GumbelMax watermark with \"sampling from softmax\" with temperature \u03c4 .",
            "score": 0.6286451113722014,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1393
                },
                {
                    "start": 1396,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2203
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92138671875
        },
        {
            "corpus_id": "271310245",
            "title": "Watermark Smoothing Attacks against Language Models",
            "text": "Detecting whether a text is generated by language models is critical in domains like fraud detection, fake news identification, and plagiarism prevention. A common approach is watermarking, where subtle patterns are embedded in the generated text for later detection [1][2][3][4]. Watermarking has gained traction in both academia and industry [5] as a key safeguard for language model applications. While various watermarking techniques exist, they share a core principle: favoring certain tokens over others (detailed in Section 2). \n\nIn this work, we identify key scenarios where watermarks fail and introduce a novel watermark removal attack that exploits this weakness, revealing fundamental limitations in existing watermarking schemes. \n\nEffectiveness of watermarks. We say a watermark is effective if (i) the watermarked text maintains high quality, comparable to those generated from the corresponding un-watermarked model, and (ii) the detector reliably identifies watermark traces, i.e., it can identify watermarked text without making a large error. We analytically and empirically show that these aspects are in tension: better text quality often implies lower watermark detectability, and vice versa. Moreover, both are connected through the model's confidence in generating output. We explain the high-level idea as follows (see more detail in Section 3). \n\nGiven a prefix, when the model is confident about the output token, watermarking has negligible impact on the output. In this case, the watermark trace is not obvious. Conversely, when the model is not confident, watermarking makes the model tend to select certain tokens (that are originally unlikely to get sampled) over others, making watermark trace more detectable while degrading the text quality. \n\nSmoothing Attack. Leveraging this insight, we propose the Smoothing Attack for watermark removal. For each prefix, the attack first identifies if the output token contains the watermark trace, by estimating the target watermarked model's confidence in this output. If the confidence is low, then we replace the token with a freshly sampled one (see more detail in Section 4), removing watermark traces while maintaining text quality; otherwise, if the confidence is high, then we retain the watermarked model's output.",
            "score": 0.6280308542409565,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 742
                },
                {
                    "start": 745,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1370
                },
                {
                    "start": 1373,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1776
                },
                {
                    "start": 1779,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 344,
                    "end": 347,
                    "matchedPaperCorpusId": "273547059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "258960101",
            "title": "DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text",
            "text": "Task Definition. Following the same setting as the previous DetectGPT (Mitchell et al., 2023), we aim to detect whether a given text is generated from a known1 language model. We formulate the detection as a binary classification task. Given a text sequence S = [s 1 , ..., s L ], where L is the sequence length, and a specific language model M like GPT-4, the goal is to classify whether S is generated from the machine distribution M or from the human distribution H. In the black-box setting, we only have access to the output text generated by the M given arbitrary input, while in the white-box setting, we additionally have access to the model output probability p(s l+1 |s 1:l ) for each token at position l. \n\nFormally, given a sequence S = [s 1 , ..., s L ], we define a truncate rate \u03b3 for splitting the sequence into two parts: X = [s 1 , ..., s \u2308\u03b3L\u2309 ], and Y 0 = [s \u2308\u03b3L\u2309+1 , ..., s L ]. Next, we ask the LLMs to continue generating the remaining sequences purely based on X, and the generated results are denoted by Y \u2032 \u223c M (\u2022|X). In practice, we sample the new results for K times (refer to a principled choice of \n\nOur method is based on the hypothesis that the text generation process M of the machine typically maximizes the log probability function log p(s l+1 |s 1 , s 2 , . . . , s l ) throughout the generation, while humans' generation process is different. In other words, the thought process of human writing does not simply follow the likelihood maximization criterion. We find that this discrepancy between machine and human is especially enormous when conditioned on the preceding text X, and we state this hypothesis formally as:",
            "score": 0.6278024600518176,
            "section_title": "METHODOLOGY",
            "char_start_offset": 8531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1126
                },
                {
                    "start": 1129,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1656
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.759765625
        },
        {
            "corpus_id": "273638693",
            "title": "Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors",
            "text": "Machine-generated Text Detection. The widespread adoption of large language models (LLMs) underscores the necessity for reliable methodologies to detect the texts generated by these models. \n\nThe detection is aiming to ascertain if a given text generated by a language model on the condition that the model is known (white-box) or unknown (black-box) (Wu et al., 2023). In the era of LLMs, recent efforts are focused on training a binary classifier using model-generated texts to distinguish between LLM-generated and human-written content (Verma et al., 2023;Venkatraman et al., 2023). However, these methods usually result in limited generalization capabilities when exposed to out-of-distribution data (Pu et al., 2023). Zero-shot approaches detect LLM-generated text by comparing the differences in performance metrics after statistical perturbation without training. The typical methods including log-probability curvature (DetectGPT (Mitchell et al., 2023a)) and conditional log-probability curvature (Fast-DetectGPT (Bao et al., 2023)), normalized log-rank perturbation (DetectLLM (Su et al., 2023)), N-gram divergence between multiple completions of a truncated passage (DNA-GPT (Yang et al., 2023a)), and the intrinsic dimensionality of generated text (PHD (Tulchinskii et al., 2024)). Binoculars (Hans et al., 2024) evaluates the log perplexity of the given text by leveraging an \"observer\" LLM, while a \"performer\" LLM generates next-token predictions. The perplexity of these predictions is then calculated based on the observer's assessment. These methods improve the detectors' ability to adapt to new data and source models, and become leading methods in machine-generated text detection.",
            "score": 0.6269661475838922,
            "section_title": "RELATED WORK",
            "char_start_offset": 5938,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 189
                },
                {
                    "start": 192,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1703
                }
            ],
            "ref_mentions": [
                {
                    "start": 939,
                    "end": 963,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1266,
                    "end": 1292,
                    "matchedPaperCorpusId": "259108779"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93359375
        },
        {
            "corpus_id": "262940018",
            "title": "Advancing Beyond Identification: Multi-bit Watermark for Large Language Models",
            "text": "How can we take a step further from merely identifying machine-generated text to proactively tackling misuses of large language models? The emergence of human-like language models and their easily accessible nature via web interface and APIs have garnered unprecedented attention from the public and academia (Hu, 2023). The ability to follow complex instructions has boosted the productivity of various tasks such as programming, creative writing, and more. However, there have been increasing concerns about exploiting such language models to automate malicious activities such as spreading disinformation. This has necessitated the * Corresponding author development of various methods to detect machinegenerated texts through techniques such as zeroshot detection, supervised training, watermarking, and more (Mitchell et al., 2023;Wang et al., 2023b;Kirchenbauer et al., 2023a;Krishna et al., 2023). These endeavors focus on the crucial task of identifying machine-generated content, which serves as a pivotal step in mitigating the potential harm caused by such text. \n\nHowever, when it comes to more pernicious misuses of large language models, such as the dissemination of misinformation and war propaganda on social media platforms for political or financial gains (Badawy et al., 2018;Pierri et al., 2023;Annie, 2023), the stakes are considerably higher, potentially leading to the erosion of social trust (Valenzuela et al., 2022). In such circumstances, merely identifying the machinegenerated text may not suffice for the language model providers. Instead, the ability to trace back to the adversary user responsible for generating the content becomes pivotal in counteracting such misuses. By doing so, the API providers can take a precursory measure to ban these users from their systems and allow media and social platforms, along with API providers, to collaborate with law enforcement authorities and take more decisive actions. All in all, watermarking the user information (or part thereof) can hold the adversary user accountable for potential harms facilitated through language model APIs without having to store user queries (Krishna et al., 2023), which would be prohibitively expensive and concern ordinary users who value privacy. \n\nAll this can be achieved by embedding multibit information.",
            "score": 0.6257994524094315,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1073
                },
                {
                    "start": 1076,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2256
                },
                {
                    "start": 2259,
                    "end": 2318
                }
            ],
            "ref_mentions": [
                {
                    "start": 1274,
                    "end": 1295,
                    "matchedPaperCorpusId": "3607349"
                },
                {
                    "start": 1295,
                    "end": 1315,
                    "matchedPaperCorpusId": "254125512"
                },
                {
                    "start": 1416,
                    "end": 1441,
                    "matchedPaperCorpusId": "236264931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.876953125
        },
        {
            "corpus_id": "258967971",
            "title": "Baselines for Identifying Watermarked Large Language Models",
            "text": "Recent progress in large language models (LLMs) has resulted in a rapid increase in the ability of models to produce convincingly human-like text, sparking worries that LLMs could be used to spread disinformation, enable plagiarism, and maliciously impersonate people. As such, researchers have begun to develop methods to detect AI generated text. These include watermarking algorithms, which subtly modify the outputted text to allow for better detection, given that the detector has sufficient access to watermarking parameters. Differing from previous work, where the focus is on determining if text has been produced by a watermarked model, here we study the problem of if a language model has been watermarked. Critically, our black-box algorithms only require querying the model and do not necessitate any knowledge of underlying watermarking parameters. \n\n1 Department of Computer Science, Harvard University. \n\nCorrespondence to: Leonard Tang <leonardtang@college.harvard.edu>.",
            "score": 0.6257769473984341,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 861
                },
                {
                    "start": 864,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 986
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "257921485",
            "title": "To ChatGPT, or not to ChatGPT: That is the question!",
            "text": "Below, we examine multiple online tools and delineate their brief functionality. However, it mainly works for content in the English language and only allows text with a character count between 250 and 5000 characters. 5) Hugging Face [13]: This tool was released by Hugging Face for detecting text generated by ChatGPT. However, it tends to over-classify text as being ChatGPT-written. 6) Perplexity (PPL) [17]: The perplexity (PPL) is a widely employed metric for assessing the efficacy of Large language models (LLM). It is calculated as the exponential of the negative average log-likelihood of text under the LLM. A lower PPL value implies that the language model is more confident in its predictions. LLMs are trained on vast text corpora, enabling them to learn common language patterns and text structures. Consequently, PPL can be utilized to gauge how effectively a given text conforms to such typical characteristics. 7) Writefull GPT Detector [36]: It is primarily used for detecting plagiarism, this tool can identify if a piece of text is generated by GPT-3 or ChatGPT. However, the tool's percentage-based system for determining whether the text was created by AI has a degree of uncertainty for both samples generated by humans and those generated by ChatGPT. 8) Copyleaks [10]: This tool claims to detect if a text is generated by GPT-3, ChatGPT, humans, or a combination of humans and AI. The tool accepts only text with 150 or more characters. 9) Content at Scale [3]: This is an online tool available to detect text generated by ChatGPT. However, the tool can only analyze samples with 25000 characters or less. to work with GPT-3 and ChatGPT models. However, its limitation restricts the amount of text that can be checked on each experiment to a maximum of 1500 characters. 12) Draft and Goal [12]: This tool is intended for detecting content generated by GPT-3 and ChatGPT models, and it is equipped to perform detection in both English and French. However, it has a requirement that the input text should be at least 600 characters or longer to work effectively.",
            "score": 0.6246722472018953,
            "section_title": "E. Online tools",
            "char_start_offset": 12859,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2086
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61279296875
        },
        {
            "corpus_id": "273163096",
            "title": "Efficiently Identifying Watermarked Segments in Mixed-Source Texts",
            "text": "Large Language Models (LLMs) have revolutionized human activities, enabling applications ranging from chatbots [OpenAI, 2022] to medical diagnostics [Google, 2024] and robotics [Ahn et al., 2024]. Their ease of use, however, presents serious societal challenges. In education [Intelligent, 2024], students can effortlessly generate essays and homework answers, undermining academic integrity. In journalism [Blum, 2024], distinguishing credible news from fabricated content erodes public trust. The potential for malicious uses, such as phishing [Violino, 2023], and the risk of model collapse due to synthetic data [Shumailov et al., 2024], further underscore the urgent need to detect LLM-generated text and promote the responsible use of this powerful technology. \n\nHowever, identifying AI-generated text is becoming increasingly difficult as LLMs reach human-like proficiency in various tasks. One line of research [OpenAI, 2023, Tian, 2023, Mitchell et al., 2023] trains machine learning models as AI detectors by collecting datasets consisting of both human and LLM-generated texts. Unfortunately, these approaches are often fragile [Shi et al., 2024] and errorprone [Liang et al., 2023], ultimately leading OpenAI to terminate its deployed detector [Kelly, 2023]. Watermarking has emerged as a promising solution to this challenge. By embedding identifiable patterns or markers within the generated text, watermarks can signal whether a piece of text originates from an LLM. \n\nExisting watermark detection methods [Aaronson, 2023, Kirchenbauer et al., 2023, Zhao et al., 2023, Kuditipudi et al., 2023, Christ et al., 2023, Hu et al., 2024] are primarily designed for text-level classification, labeling a piece of text as either watermarked or not. However, these methods are insufficient for many real-world scenarios where documents contain mixed-source texts, and only specific sections are LLM-generated. For instance, malicious actors might use LLMs to manipulate certain sections of a news article to spread misinformation.",
            "score": 0.6239192158452982,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 766
                },
                {
                    "start": 769,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1481
                },
                {
                    "start": 1484,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2036
                }
            ],
            "ref_mentions": [
                {
                    "start": 616,
                    "end": 640,
                    "matchedPaperCorpusId": "271448069"
                },
                {
                    "start": 944,
                    "end": 968,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1139,
                    "end": 1157,
                    "matchedPaperCorpusId": "258987266"
                },
                {
                    "start": 1536,
                    "end": 1563,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 1563,
                    "end": 1582,
                    "matchedPaperCorpusId": "259308864"
                },
                {
                    "start": 1628,
                    "end": 1646,
                    "matchedPaperCorpusId": "264172471"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91259765625
        },
        {
            "corpus_id": "273351050",
            "title": "WaterSeeker: Pioneering Efficient Detection of Watermarked Segments in Large Documents",
            "text": "As large language models (LLMs) generate highquality text, they address practical challenges but also raise concerns such as misinformation (Meg\u00edas et al., 2022;Chen and Shu, 2024) and copyright infringement (Rillig et al., 2023). LLM watermarking technology has emerged to tackle these issues by embedding specific information (watermarks) during text generation, allowing for accurate detection through specialized algorithms. Current watermark detection methods (Kirchenbauer et al., 2023;Zhao et al., 2024;Liu et al., 2024b;Aaronson and Kirchner, 2022;Lu et al., 2024;Lee et al., 2023;Hu et al., 2024;Wu et al., 2023) first calculate watermark scores for individual tokens through single token detection, then compute statistics across the entire document for classification. While these full-text detection methods effectively distinguish between fully watermarked and non-watermarked texts, they fail in real-world scenarios where LLMs generate only brief segments within longer documents, due to dilution effects, as shown in Figure 1. To the best of our knowledge, the WinMax algorithm (Kirchenbauer et al., 2024) is the only work addressing this limitation by examining all possible window sizes and selecting the maximum statistical score across all windows as the final detection result, but suffers from high time complexity. \n\nTo address these issues, we propose a novel and general watermark detection method called Wa-terSeeker. WaterSeeker follows a \"first locate, then detect\" approach, as shown in Figure 1. It initially employs a low-complexity anomaly points extraction algorithm to identify suspected watermark regions, narrowing the detection target from a long text to a small segment encompassing the ground truth segment. Next, a local traversal is performed on the localization result, conducting full-text watermark detection within each window and comparing the highest confidence result to a threshold for the final determination. Theoretical analysis suggests that this coarse-to-fine process has the potential to achieve optimal detection performance while maintaining the lowest possible complexity for solving this problem.",
            "score": 0.6232901048650755,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1337
                },
                {
                    "start": 1340,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 161,
                    "matchedPaperCorpusId": "248326491"
                },
                {
                    "start": 161,
                    "end": 180,
                    "matchedPaperCorpusId": "262465354"
                },
                {
                    "start": 208,
                    "end": 229,
                    "matchedPaperCorpusId": "257098877"
                },
                {
                    "start": 465,
                    "end": 492,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 492,
                    "end": 510,
                    "matchedPaperCorpusId": "259308864"
                },
                {
                    "start": 510,
                    "end": 528,
                    "matchedPaperCorpusId": "263830310"
                },
                {
                    "start": 556,
                    "end": 572,
                    "matchedPaperCorpusId": "268536698"
                },
                {
                    "start": 572,
                    "end": 589,
                    "matchedPaperCorpusId": "260315804"
                },
                {
                    "start": 589,
                    "end": 605,
                    "matchedPaperCorpusId": "264172471"
                },
                {
                    "start": 1094,
                    "end": 1121,
                    "matchedPaperCorpusId": "259095643"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "268357168",
            "title": "DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for Identifying Large Language Model Generated Text",
            "text": "Large Language Models (LLMs), such as ChatGPT [1], have recently achieved notable success.The advancements in LLMs can be advantageous across various domains, yet there also lies the potential for inappropriate applications.A prevailing concern regarding publicly accessible LLMs is the challenge in distinguishing between machine-generated and human-written text, a difficulty that persists even in instances of misuse [2].For instance, students might utilize automatically generated texts as their own submissions for assignments, evading conventional \"plagiarism\" detection.The high fidelity of the text generated by LLMs exacerbates the challenge of detection, marking a significant hurdle.Again, there exist advanced text augmentation methods capable of effortlessly modifying any given text [3] [4] [5] [6].Therefore, devising a method to ascertain the origin of text could serve as a valuable approach to curtail similar misapplications of LLMs.\n\nVarious classifiers have been developed to differentiate between LLM-generated text and human-written text [2,7].However, the efficacy of these classifiers remains somewhat constrained at present.Numerous studies have explored the accuracy of these classifiers [8], along with techniques to circumvent classifier detection [9].A reliable source detection mechanism that is challenging to bypass is crucial, given its potential applications in identifying plagiarism and misuse.Therefore, employing text watermarking for text source detection appears to be a prudent approach, as it is both reliable and challenging to circumvent.Text watermarking entails the covert embedding of information (i.e., the watermark) into cover texts, such that the watermark is only discernible by authorized detectors.While watermarking is more conventionally applied to images [10], its application to text can enable the identification of text originating from specific sources, such as an LLM (refer to figure 1 for the proposed source detection mechanism).However, conventional text watermarking techniques often necessitate manual intervention by linguists, exhibit a lack of robustness, and do not possess the blindness property.",
            "score": 0.6231402404129269,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 90,
                    "end": 224
                },
                {
                    "start": 224,
                    "end": 424
                },
                {
                    "start": 424,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 694
                },
                {
                    "start": 694,
                    "end": 813
                },
                {
                    "start": 813,
                    "end": 952
                },
                {
                    "start": 954,
                    "end": 1067
                },
                {
                    "start": 1067,
                    "end": 1150
                },
                {
                    "start": 1150,
                    "end": 1281
                },
                {
                    "start": 1281,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1583
                },
                {
                    "start": 1583,
                    "end": 1753
                },
                {
                    "start": 1753,
                    "end": 1995
                },
                {
                    "start": 1995,
                    "end": 2170
                }
            ],
            "ref_mentions": [
                {
                    "start": 797,
                    "end": 800,
                    "matchedPaperCorpusId": "259691621"
                },
                {
                    "start": 805,
                    "end": 808,
                    "matchedPaperCorpusId": "259558731"
                },
                {
                    "start": 809,
                    "end": 812,
                    "matchedPaperCorpusId": "247428494"
                },
                {
                    "start": 1813,
                    "end": 1817,
                    "matchedPaperCorpusId": "252783019"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93115234375
        },
        {
            "corpus_id": "264426138",
            "title": "A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions",
            "text": "The powerful ability to understand, follow, and generate complex language emerging from large language models (LLMs) makes LLM-generated text flood many areas of our daily lives at an incredible speed and is widely accepted by humans. As LLMs continue to expand, there is an imperative need to develop detectors that can detect LLM-generated text. This is crucial to mitigate potential misuse of LLMs and safeguard realms like artistic expression and social networks from harmful influence of LLM-generated content. The LLM-generated text detection aims to discern if a piece of text was produced by an LLM, which is essentially a binary classification task. The detector techniques have witnessed notable advancements recently, propelled by innovations in watermarking techniques, statistics-based detectors, neural-base detectors, and human-assisted methods. In this survey, we collate recent research breakthroughs in this area and underscore the pressing need to bolster detector research. We also delve into prevalent datasets, elucidating their limitations and developmental requirements. Furthermore, we analyze various LLM-generated text detection paradigms, shedding light on challenges like out-of-distribution problems, potential attacks, real-world data issues and the lack of effective evaluation framework. Conclusively, we highlight interesting directions for future research in LLM-generated text detection to advance the implementation of responsible artificial intelligence (AI). Our aim with this survey is to provide a clear and comprehensive introduction for newcomers while also offering seasoned researchers a valuable update in the field of LLM-generated text detection. The useful resources are publicly available at: https://github.com/NLP2CT/LLM-generated-Text-Detection.",
            "score": 0.6225903406055884,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.931640625
        },
        {
            "corpus_id": "268296923",
            "title": "WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off",
            "text": "The availability of powerful large-language models (LLMs) allows users to produce texts that look like human writings. The risk for misuse of these models is critical, ranging from the impersonation of individuals to the large-scale generation of fake news. Identifying the provenance of a given piece of text is paramount to limit the impact of such 'weaponization' of LLMs. New initiatives or regulations impose technical means for AI traceability [27,9,5]. \n\nForensics passive methods generally leverage a priori knowledge about the statistics of texts generated by a given class of LLMs [33,26]. Despite their versatility, these methods offer low performance. The reported probabilities of errors are only validated empirically on some datasets, and because of this, they are never lower than 10 \u22123 [14]. \n\nIn contrast, active methods like watermarking are only limited by the fact the LLM owner must integrate the watermarking within the generation processes. This is done by embedding an imperceptible signal in the generated text, which can be retrieved by a detector sharing the secret key of the model owner. Current watermarking methods for generative texts [21,1,22,8] provide low and guaranteed false positives rates. Yet, the trade-off between the detectability and the text quality crucially depends on the entropy of the text to be generated, which in turn depends on the prompt and the LLM, as illustrated in Fig. 1. This implies that the distortion-free property [1,22,8] ensures that watermarking does not degrade text quality but inherently limits the detectability. This paper presents WaterMax, a watermarking technique that trades off robustness not for quality, but for complexity. It obeys the regular constraints found in the literature: it can be integrated into any standard LLM without fine-tuning the weights, the detection does not need the original LLM, and it can spot the watermark on a slice of generated text with some guarantee on the false positive rate. Our contributions are the following: \n\n\u2022 WaterMax is based on a new design not relying on the usual mechanisms of the literature; especially, it keeps the next token distribution and sampling (temperature and method) intact.",
            "score": 0.621832295950321,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 459
                },
                {
                    "start": 462,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 808
                },
                {
                    "start": 811,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2028
                },
                {
                    "start": 2031,
                    "end": 2216
                }
            ],
            "ref_mentions": [
                {
                    "start": 591,
                    "end": 595,
                    "matchedPaperCorpusId": "258865367"
                },
                {
                    "start": 1177,
                    "end": 1179,
                    "matchedPaperCorpusId": "259092330"
                },
                {
                    "start": 1486,
                    "end": 1488,
                    "matchedPaperCorpusId": "259092330"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83251953125
        },
        {
            "corpus_id": "266999874",
            "title": "Cross-Attention watermarking of Large Language Models",
            "text": "Linguistic watermarking refers to the insertion of particular unnoticeable information into a text document while preserving its readability, intended meaning, and ability to withstand noise [1]. Interest in such watermarking is growing due to the widespread emergence of AI-generated text, which poses risks in various sectors, including online influence campaigns, AI exploitation of authorship, spamming, harassment, malware facilitation, and social engineering [2,3]. Text watermarking has various applications, including the tagging of AIgenerated text. It can be used to protect intellectual property, detect leaks, and verify the source. In addition, it aids in complying with regulations that mandate identification of AI-generated texts in countries that are implementing such requirements [4]. Lastly, watermarking can assist language model developers in ensuring that model output is distinguishable to prevent model collapse during training on generated text [5]. \n\nAs the output of large language models (LLMs) approaches the quality of human-generated text, it becomes more difficult to distinguish between them. As LLMs improve, increase in number, and become easier to fine-tune, the existing post-hoc detectors for AIgenerated text are becoming more unreliable in correctly identifying such content. The defenses often fail to generalize to real-world scenarios [6] and often fail in adversarial settings [7]. Intentionally modifying models to embed watermarks in the output text is a promising solution to this challenge. Unfortunately, the implementation of a useful watermark often involves substantial modifications to the distribution of text generated by the template, potentially degrading its quality [8]. \n\nOur focus is on blind watermarking, which enables verification based solely on bits extracted from the watermarked text [1]. They can be used to identify information, copyrights, digital signatures, or any other element that can establish the source or integrity of the generated text. Watermarking ensures accurate content attribution, assists in plagiarism detection, and prevents the malicious manipulation of information. With this approach, nouns, verbs, adjectives, and other grammatical elements are modified without changing the text's intended meaning [1].",
            "score": 0.6216682913050665,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1730
                },
                {
                    "start": 1733,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2298
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 194,
                    "matchedPaperCorpusId": "3783650"
                },
                {
                    "start": 799,
                    "end": 802,
                    "matchedPaperCorpusId": "257271887"
                },
                {
                    "start": 1726,
                    "end": 1729,
                    "matchedPaperCorpusId": "259092330"
                },
                {
                    "start": 1853,
                    "end": 1856,
                    "matchedPaperCorpusId": "3783650"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9326171875
        },
        {
            "corpus_id": "260438786",
            "title": "UPB at IberLEF-2023 AuTexTification: Detection of Machine-Generated Text using Transformer Ensembles",
            "text": "While text classification is one of NLP's fundamental and most well-established tasks, detecting computer-generated text is a relatively novel task. This is probably because, until recently, few systems could produce text realistic enough to fool humans. Creating such texts is commonly called natural language generation [7]. \n\nCurrently, there seem to be various ways of addressing this problem, which can be classified into black-box and white-box methods [8]. White-box techniques require access to the target language model, and they can involve concepts such as watermarks which the models could embed into their outputs to make detection easier. As such, black-box methods are more relevant to the previously mentioned task since we only have access to the model's output, but we do not even know which model produced it. \n\nBlack-box methods can involve both classical machine learning classification algorithms, as well as ones based on deep learning [8]. To make predictions, traditional algorithms combine statistical features and linguistic patterns with classifiers such as Support Vector Machines (SVMs) [9]. On the other hand, deep learning methods usually involve fine-tuning pre-trained language models using supervised learning in order to make predictions. These deep learning approaches often obtain state-of-the-art results but are harder to interpret, which means they are also harder to trust, as well.",
            "score": 0.6215243116442323,
            "section_title": "Related Work",
            "char_start_offset": 1875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 326
                },
                {
                    "start": 329,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 828
                },
                {
                    "start": 831,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1424
                }
            ],
            "ref_mentions": [
                {
                    "start": 1117,
                    "end": 1120,
                    "matchedPaperCorpusId": "661123"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84033203125
        },
        {
            "corpus_id": "270703139",
            "title": "Detecting AI-Generated Text: Factors Influencing Detectability with Current Methods",
            "text": "Black box access  The choice of which detection method to use, out of the many that have been presented, will depend on a number of factors. In Section 2.3 we outlined the most common detection scenarios; we now revisit these categories in Figure 5 together with the different approaches presented in this section. \n\nThe ability to use watermark detection as a strategy is dependent, of course, on whether the text has been watermarked in the first place. In this scenario, only access to the watermark extraction function is needed, and not access to the generating model itself; however, it is assumed that the model is known. \n\nIn cases where the model is known but not watermarked, and we have access to the internal parameters, methods based on the probability, perplexity, and entropy of individual words or sequences can be used to help detect text generated by that model. If, on the other hand, the unwatermarked model is known but we have only black-box access, regeneration strategies can be used instead. Another standard approach for the known model, black-box access scenario is supervised learning, either by first extracting linguistic/stylistic features and training a low-dimensional classifier, or fine-tuning a high-dimensional, pre-trained language model based on the raw texts. \n\nIf the generating model is not known, then we must use known models as proxies. This can involve either estimating probability values based on open-source models, or using classifiers (or off-the-shelf tools) trained on other models. In these cases, we are relying on general properties of AIGT that are expected to hold true regardless of which specific model generated the text. However, experimental results suggest that the detection performance of most methods is somewhat lower when applied to text from an unknown model. \n\nIn practice, there are numerous other factors which can affect the performance of the detection methods outlined above. These factors are discussed in detail in Section 5. However, we first summarize the available datasets for AIGT detection.",
            "score": 0.621133607116483,
            "section_title": "White box access",
            "char_start_offset": 63494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 314
                },
                {
                    "start": 317,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 628
                },
                {
                    "start": 631,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1299
                },
                {
                    "start": 1302,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1829
                },
                {
                    "start": 1832,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2074
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8828125
        },
        {
            "corpus_id": "273228395",
            "title": "Signal Watermark on Large Language Models",
            "text": "Our main contributions are: \n\n\u2022 We have identified and empirically validated the hypothesis that Large Language Models maintain consistent token ranking patterns in both text generation and subsequent re-computation phases. \u2022 We developed an innovative technique for embedding a signal watermark into text generated by Large Language Models. \n\n\u2022 We introduced the application of Digital Signal Processing (DSP) techniques to improve the detection of watermarks in text generated by these models. \u2022 Our approach obviates the need for additional model training, streamlining the watermarking process and reducing computational overhead. \u2022 In the watermark attacking section, we have demonstrated that our signal watermark is resilient to remove.",
            "score": 0.6202439432085346,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 8985,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 30,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 341
                },
                {
                    "start": 344,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 743
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "273564837",
            "title": "Impact of Spelling and Editing Correctness on Detection of LLM-Generated Emails",
            "text": "This approach was also used by Fr\u00f6hling and Zubiaga [10], who observed that AI-generated texts exhibit 1) lack of syntactic and lexical diversity, 2) repetitiveness (tendency to overuse frequent words), 3) lack of coherence, and 4) lack of clear purpose or focus. \n\nAnother group of methods uses LLMs themselves to detect LLM-generated texts. The first set of methods applies LLMs as they are, i.e., without any training steps. Shi et al. [11] introduced Proxy-Guided Efficient Re-sampling (POGER). It worked by selecting a subset of unusual keywords, i.e., characterizing low probabilities of appearing in their contexts according to the given LLM. Then, the text is re-sampled, namely, each identified unusual keyword is removed, and the LLM is prompted to fill in the gap. If the resulting text is similar enough, the original text was likely LLM-generated. \n\nMitchell et al. [12] defined a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, called DetectGPT, does not require training a separate classifier or collecting a dataset of real or generated passages. It uses only log probabilities computed by the LLM of interest and random perturbations of the passage from another generic pre-trained language model. To classify a candidate passage, DetectGPT first generates minor perturbations of the passage using a generic pre-trained model such as T5. DetectGPT compares the log probability under p of the original sample with each perturbed corresponding sample. If the average log ratio is high, the sample is likely AI-generated content. The main contribution of this work was to identify a property of the log probability function computed by a wide variety of large language models, showing that a tractable approximation to the trace of the Hessian of the model's log probability function provides a useful signal for detecting model samples. \n\nThere are also separate approaches based on fine-tuning the LLMs to classify whether the text is AI-generated.",
            "score": 0.6180345303150869,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 4633,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 266,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1904
                },
                {
                    "start": 1907,
                    "end": 2017
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 56,
                    "matchedPaperCorpusId": "233563022"
                },
                {
                    "start": 879,
                    "end": 883,
                    "matchedPaperCorpusId": "256274849"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93359375
        },
        {
            "corpus_id": "273811926",
            "title": "B4: A Black-Box Scrubbing Attack on LLM Watermarks",
            "text": "The rapid advancement of large language models (LLMs) has demonstrated their unimaginable potentials across various applications. Systems such as ChatGPT (OpenAI, 2023) are now seamlessly integrated into many aspects of daily life. Despite benefits, the extensive deployment of LLMs has sparked serious concerns regarding potential misuse, such as large-scale disinformation, automated spamming, and social media manipulation, thereby threatening academic integrity and intellectual property rights (Bender et al., 2021;Liu et al., 2023b). Consequently, detecting LLM-generated content has emerged as a crucial focus in the discourse on LLM safety and responsible deployment  ( Mitchell et al., 2023;Pu et al., 2023;Yang et al., 2023). \n\nWatermarking stands out as a prominent technique for detecting LLM-generated text. It injects a hidden pattern invisible to human into generated contents of a specific LLM (Kirchenbauer et al., 2023a). By altering the original distribution of LLMs to a specific watermark distribution during each decoding step (Kuditipudi et al., 2023;Zhao et al., 2023;Hu et al., 2023) , all model generated contents can be statistically distinguished through hypothesis testing between the watermarked and the original distributions. This approach achieves a high detection rate and can be easily deployed, with only a negligible cost in the quality of generated content. \n\nDespite its supreme performance, the robustness of watermarking methods against adversarial attacks remains underexplored. Among which, the scrubbing attack (Jovanovi\u0107 et al., 2024) presents a notable challenge in practical settings: if an adversary can successfully paraphrase LLM-generated contents into another semantically similar but watermark-free form, the effectiveness of the watermark will be heavily compromised. Wu and Chandrasekaran (2024) and Jovanovi\u0107 et al. (2024) explored this question by proposing different watermark scrubbing methods. However, these approaches typically require strong prior knowledge about the specific type of watermark algorithm used in the victim LLM, even the specific hyperparameter of the algorithm (e.g. the context window size in KGW).",
            "score": 0.617606549347578,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1395
                },
                {
                    "start": 1398,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2180
                }
            ],
            "ref_mentions": [
                {
                    "start": 499,
                    "end": 520,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 700,
                    "end": 716,
                    "matchedPaperCorpusId": "263829122"
                },
                {
                    "start": 910,
                    "end": 938,
                    "matchedPaperCorpusId": "256194179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.861328125
        },
        {
            "corpus_id": "264439054",
            "title": "Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey",
            "text": "Another line of work focuses on training a binary classifier using features extracted from a pre-trained language model for detecting machine-generated text (Fagni et al., 2021;Bakhtin et al., 2019;Jawahar et al., 2020;Chen et al., 2023). This approach has a longer history with hallmark studies concerning finetuning classifiers for detecting neural disinformation in Hovy ( 2016); Zellers et al. (2019). In 2019, (Solaiman et al., 2019) achieved a then state-of-art performance by finetuning RoBERTa models for the task of detecting webpages generated by GPT-2. Recently OpenAI's (OpenAI, 2023) work on machine-generated text detection by finetuning a GPT model has followed these development, although OpenAI's detector has now been taken offline, due to its high false-positive rate. Detectors under this category do not require access to model parameters and hence can operate under complete black-box settings. However, unlike the zero-shot setup, supervised training samples are required in the form of human and machine-generated text to train the detector. \n\nDetectors under this category suffer from a few drawbacks: (1) Collecting sufficient data to train the classifier can be challenging, especially in diverse domains where the availability of training samples is a major bottleneck. \n\n(2) With recent advancements, text generated by language models has become increasingly similar to human-generated text, making detection harder (Zhao et al., 2023a). ( 3) False-positive rates for these detectors are hard to establish and depend crucially on the data distribution used to train the detector (Liang et al., 2023). Further, a recent study by Gambini et al. has shown that detection strategies designed for smaller models such as GPT-2 lose their efficacy when applied to larger models such as GPT-3. Wolff & Wolff (2020) has also questioned the robustness of detectors against adversarial attacks.",
            "score": 0.6167299366871861,
            "section_title": "Methods based on Training and Finetuning of Classifiers",
            "char_start_offset": 46721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1297
                },
                {
                    "start": 1300,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1912
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 177,
                    "matchedPaperCorpusId": "220936111"
                },
                {
                    "start": 383,
                    "end": 404,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 1608,
                    "end": 1628,
                    "matchedPaperCorpusId": "257985499"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8955078125
        },
        {
            "corpus_id": "270123284",
            "title": "Large Language Model Watermark Stealing With Mixed Integer Programming",
            "text": "With the significant progress of Large Language Models (LLMs) in recent years [1,19,24,25], there are increasing risks that LLMs could be deployed for malicious purposes, such as misinformation generation [17], automated phishing [4], and academic fraud [10].Consequently, there is a growing need to address the LLM copyright concerns, monitor AI-generated text, and prevent its misuse.Many existing methods involve collecting AI-generated and humangenerated text and then training a classifier to distinguish them [16,27].However, these methods tend to be biased towards the training dataset [8,18,29] and subject to adversarial attacks [5,13,22].\n\nAs such, LLM watermark, which enables the injection of detectable hidden patterns into the AI-generated text, emerges as a promising technique [11,12,15,31].Typically, they inject watermarks during the text generation process, where LLMs sample the next token based on the distribution computed from the logits [2,6,11,12,15,21,31].The watermark scheme initially generates a key and employs it as a seed for a pseudo-random function to randomly partition the vocabulary into a green list and a red list.Subsequently, a perturbation is added to the logits of tokens in the green list.As a result, tokens from the green list are more likely to be sampled during generation compared to those from the red list, leading to a higher frequency of green list tokens in the watermarked text.This can then be utilized to detect the watermark; if the proportion of green tokens exceeds a predetermined threshold, the text is considered as watermarked.This watermark scheme do not require modification of model parameters and can achieve high detection rates while maintaining the quality of the generated text.\n\nOne predominant watermark approach in existing research utilizes multiple keys generated either from the token level (extracted from prefix tokens) [11,12,14] or the sentence level (obtained from sentence embeddings) [6,15,21].",
            "score": 0.6164979679469602,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 259,
                    "end": 386
                },
                {
                    "start": 386,
                    "end": 523
                },
                {
                    "start": 523,
                    "end": 648
                },
                {
                    "start": 650,
                    "end": 807
                },
                {
                    "start": 807,
                    "end": 982
                },
                {
                    "start": 982,
                    "end": 1153
                },
                {
                    "start": 1153,
                    "end": 1233
                },
                {
                    "start": 1233,
                    "end": 1433
                },
                {
                    "start": 1433,
                    "end": 1591
                },
                {
                    "start": 1591,
                    "end": 1750
                },
                {
                    "start": 1752,
                    "end": 1979
                }
            ],
            "ref_mentions": [
                {
                    "start": 78,
                    "end": 81,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 81,
                    "end": 84,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 254,
                    "end": 258,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 515,
                    "end": 519,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 519,
                    "end": 522,
                    "matchedPaperCorpusId": "268708484"
                },
                {
                    "start": 641,
                    "end": 644,
                    "matchedPaperCorpusId": "257687440"
                },
                {
                    "start": 797,
                    "end": 800,
                    "matchedPaperCorpusId": "259095643"
                },
                {
                    "start": 800,
                    "end": 803,
                    "matchedPaperCorpusId": "263830310"
                },
                {
                    "start": 803,
                    "end": 806,
                    "matchedPaperCorpusId": "259308864"
                },
                {
                    "start": 969,
                    "end": 972,
                    "matchedPaperCorpusId": "259095643"
                },
                {
                    "start": 972,
                    "end": 975,
                    "matchedPaperCorpusId": "263830310"
                },
                {
                    "start": 978,
                    "end": 981,
                    "matchedPaperCorpusId": "259308864"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.892578125
        },
        {
            "corpus_id": "258048481",
            "title": "On the Possibilities of AI-Generated Text Detection",
            "text": "The interesting recent literature by (Sadasivan et al., 2023;Krishna et al., 2023) showed the vulnerabilities of watermark-based detection methodologies using vanilla paraphrasing attacks. (Sadasivan et al., 2023) developed a lightweight neural network-based paraphraser and applied it to the output text of the AI-generative model to evade a whole range of detectors, including watermarking schemes, neural network-based detectors, and zero-shot classifiers. (Sadasivan et al., 2023) also introduced a notion of spoofing attacks where they exposed the vulnerability of LLMs protected by watermarking under such attacks. (Krishna et al., 2023) on the other hand, trained a paraphrase generation model capable of paraphrasing paragraphs and showed that paraphrased texts with DIPPER (Krishna et al., 2023) evade several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier with a significant drop in accuracy. Additionally, (Sadasivan et al., 2023) highlighted the impossibility of machine-generated text detection when the total variation (TV) norm between human and machine-generated text distributions is small. \n\nIn this work, we show that there is a hidden possibility of detecting the AI-generated text even if the TV norm between human and machine-generated text distributions is small. This result is in support of the recent detection possibility claims by Krishna et al. (2023). \n\n3 Proposed Approach: Methodology and Analysis",
            "score": 0.6160173187238469,
            "section_title": "Background on AI-Generated Text Detectors and Related Works",
            "char_start_offset": 8229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1144
                },
                {
                    "start": 1147,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1418
                },
                {
                    "start": 1421,
                    "end": 1466
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82421875
        },
        {
            "corpus_id": "262465111",
            "title": "From Text to Source: Results in Detecting Large Language Model-Generated Content",
            "text": "The widespread use of Large Language Models (LLMs), celebrated for their ability to generate human-like text, has raised concerns about misinformation and ethical implications. Addressing these concerns necessitates the development of robust methods to detect and attribute text generated by LLMs. This paper investigates \u201cCross-Model Detection,\u201d by evaluating whether a classifier trained to distinguish between source LLM-generated and human-written text can also detect text from a target LLM without further training. The study comprehensively explores various LLM sizes and families and assesses the impact of conversational fine-tuning techniques, quantization, and watermarking on classifier generalization. The research also explores Model Attribution, encompassing source model identification, model family, and model size classification, in addition to quantization and watermarking detection. Our results reveal several key findings: a clear inverse relationship between classifier effectiveness and model size, with larger LLMs being more challenging to detect, especially when the classifier is trained on data from smaller models. Training on data from similarly sized LLMs can improve detection performance from larger models but may lead to decreased performance when dealing with smaller models. Additionally, model attribution experiments show promising results in identifying source models and model families, highlighting detectable signatures in LLM-generated text, with particularly remarkable outcomes in watermarking detection, while no detectable signatures of quantization were observed. Overall, our study contributes valuable insights into the interplay of model size, family, and training data in LLM detection and attribution.",
            "score": 0.6158215601340693,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.978515625
        },
        {
            "corpus_id": "270738035",
            "title": "Detecting Machine-Generated Texts: Not Just \"AI vs Humans\" and Explainability is Complicated",
            "text": "Zero-shot detection.This approach leverages some unique statistical properties distinguishing MGTs from HGTs.Past studies have employed various linguistic model-derived characteristics, such as entropy (He et al., 2023), average log-probability scores (Solaiman et al., 2019) and perplexity (Wu et al., 2023), as useful statistical properties for detection.With the evolution of LLMs that generate increasingly sophisticated texts, more recent zeroshot detection strategies (Gehrmann et al., 2019;Mitchell et al., 2023;Su et al., 2023;Wu and Xiang, 2023;Bao et al., 2024;Kumari et al., 2024) have adapted to discern high-order features of advanced text generators.Notably, the Binoculars model (Hans et al., 2024) leverages LLMs to perform next-token predictions at each text position, utilizing the log perplexity ratio compared to the baseline text as a distinguishing statistic.\n\nModel-based detection.This approach involves adapting existing models to learn from specific datasets for MGT detection (OpenAI, 2021;He et al., 2023).It often includes sentence-level detection and analyses different LLM outputs (Wang et al., 2023;Bhattacharjee et al., 2023;Antoun et al., 2023).However, these methods can suffer from overfitting and generally exhibit limited effectiveness in detecting texts across various domains.\n\nOther approaches.There are also other approaches based on watermarking, adversarial learning based training, and human assistance (Wu et al., 2024).These approaches are more complicated and are often a mixture of different approaches, so in this paper we consider two basic approaches only to make our work more focused.",
            "score": 0.6153025867177482,
            "section_title": "Open-Source Detectors",
            "char_start_offset": 4613,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 20,
                    "end": 109
                },
                {
                    "start": 109,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 664
                },
                {
                    "start": 664,
                    "end": 881
                },
                {
                    "start": 883,
                    "end": 905
                },
                {
                    "start": 905,
                    "end": 1034
                },
                {
                    "start": 1034,
                    "end": 1179
                },
                {
                    "start": 1179,
                    "end": 1316
                },
                {
                    "start": 1318,
                    "end": 1335
                },
                {
                    "start": 1335,
                    "end": 1466
                },
                {
                    "start": 1466,
                    "end": 1638
                }
            ],
            "ref_mentions": [
                {
                    "start": 497,
                    "end": 519,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 554,
                    "end": 571,
                    "matchedPaperCorpusId": "263831345"
                },
                {
                    "start": 1112,
                    "end": 1131,
                    "matchedPaperCorpusId": "264128397"
                },
                {
                    "start": 1131,
                    "end": 1158,
                    "matchedPaperCorpusId": "261660497"
                },
                {
                    "start": 1158,
                    "end": 1178,
                    "matchedPaperCorpusId": "262465111"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87548828125
        },
        {
            "corpus_id": "257687440",
            "title": "Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense",
            "text": "Effective watermarks are difficult to remove and have little effect on the quality of generated text. Prior work has watermarked natural language using syntax tree manipulations [Topkara et al., 2005, Meral et al., 2009], and this area has received renewed interest with the advent of LLMs [Abdelnabi andFritz, 2021, Grinbaum andAdomaitis, 2022]. Most recently, Kirchenbauer et al. [2023a] proposed a simple algorithm that watermarks LLMs by slightly perturbing its probability distribution while generating text. \n\nStatistical outlier detection methods detect AI-generated text based on its artifacts [See et al., 2019, Holtzman et al., 2020] instead of modifying the generative algorithm. Early methods detect statistical irregularities in entropy [Lavergne et al., 2008] and perplexity [Beresneva, 2016], while Gehrmann et al. [2019] introduced the GLTR visualizer to assist humans in detecting AI-generated text. The release of ChatGPT prompted the development of two new tools: closed-source GPTZero [Tian, 2023] and open-source DetectGPT [Mitchell et al., 2023]. The latter uses the observation that AI-generated text tends to have significantly higher LLM likelihood than meaningful perturbations of it. \n\nClassifier methods train models to distinguish human-written text from AI-generated text. Early efforts used classifiers to detect fake reviews [Hovy, 2016] and news [Zellers et al., 2019], while others examined classifier performance across domains [Bakhtin et al., 2019] and decoding strategies [Ippolito et al., 2020]. Most recently, OpenAI fine-tuned a GPT model to perform this task and released it as a web interface [OpenAI, 2023a]. Their model uses generations from 34 LLMs, with the human-written text from Wikipedia, WebText, and their internal human demonstration data. \n\nComparison to Sadasivan et al. (2023): In recent concurrent work, Sadasivan et al. [2023] also demonstrate the utility of paraphrasing attacks against AI-generated text detectors.",
            "score": 0.6145119016501934,
            "section_title": "ChatGPT / GPT3",
            "char_start_offset": 6862,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1793
                },
                {
                    "start": 1796,
                    "end": 1975
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 199,
                    "matchedPaperCorpusId": "13415776"
                },
                {
                    "start": 199,
                    "end": 220,
                    "matchedPaperCorpusId": "1192689"
                },
                {
                    "start": 290,
                    "end": 304,
                    "matchedPaperCorpusId": "221516138"
                },
                {
                    "start": 602,
                    "end": 619,
                    "matchedPaperCorpusId": "202734604"
                },
                {
                    "start": 619,
                    "end": 643,
                    "matchedPaperCorpusId": "127986954"
                },
                {
                    "start": 750,
                    "end": 773,
                    "matchedPaperCorpusId": "12098535"
                },
                {
                    "start": 789,
                    "end": 806,
                    "matchedPaperCorpusId": "1175726"
                },
                {
                    "start": 814,
                    "end": 836,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 1357,
                    "end": 1369,
                    "matchedPaperCorpusId": "5315990"
                },
                {
                    "start": 1379,
                    "end": 1401,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 1510,
                    "end": 1533,
                    "matchedPaperCorpusId": "218560609"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79296875
        },
        {
            "corpus_id": "261660497",
            "title": "ConDA: Contrastive Domain Adaptation for AI-generated Text Detection",
            "text": "Generated Text Detection The burgeoning progress in the generation capabilities of large language models has led to a corresponding increase in research and development efforts in the field of detection. Several recent efforts look at methods, varying from simple feature-based classifiers to fine-tuned language model-based detectors, in order to classify whether a piece of input text is human-written or AI-generated (Ippolito et al., 2019;Gehrmann et al., 2019;Mitchell et al., 2023), along with methods that specifically focus on AIgenerated news (Zellers et al., 2019;Bogaert et al., 2022). A related direction of work is that of authorship attribution (AA). While older AA methods focused on human authors, more recent efforts (Uchendu et al., 2020;Munir et al., 2021) build models to identify the generator for a particular input text. Recent work also shows how AI-generated text can deceive state-of-the-art AA models (Jones et al., 2022), thus making the task of detecting such text even more important.",
            "score": 0.6137121822168683,
            "section_title": "Related Work",
            "char_start_offset": 3926,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1014
                }
            ],
            "ref_mentions": [
                {
                    "start": 443,
                    "end": 465,
                    "matchedPaperCorpusId": "233296145"
                },
                {
                    "start": 552,
                    "end": 574,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 756,
                    "end": 775,
                    "matchedPaperCorpusId": "233189559"
                },
                {
                    "start": 928,
                    "end": 948,
                    "matchedPaperCorpusId": "247595180"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95458984375
        },
        {
            "corpus_id": "267406364",
            "title": "LLM-Detector: Improving AI-Generated Chinese Text Detection with Open-Source LLM Instruction Tuning",
            "text": "To fully test the effectiveness of our proposed method, we compared it with AI-generated text detection methods based on statistical information and supervised learning methods. Additionally, we selected two advanced LLMs that have shown excellent performance in English. \n\n\u2022 Fast-DetectGPT (Bao et al., 2023) is a method for zero-shot detection of AI-generated text. This method uses conditional probability curvature as an indicator and detects whether text is machine-generated by sampling and evaluating the differences in word selection probabilities. Compared to DetectGPT (Mitchell et al., 2023), this method has increased detection speed by two orders of magnitude, while accuracy has improved by approximately 75%. \n\n\u2022 GLTR (Gehrmann et al., 2019) studied three types of features of an input text. Their major assumption is that to generate fluent and natural-looking text, most decoding strategies sample high probabilities tokens from the head of the distribution. \n\n\u2022 Perplexity (PPL) (Guo et al., 2023b) is a metric for evaluating the performance of language models. It measures the exponent of the negative average logarithmic likelihood of a given text under the language model. A lower PPL indicates that the language model is more confident in its predictions and is thus considered a better model. We use GPT-2 to calculate the PPL of human-and AI-generated content to distinguish who generated the text. \n\n\u2022 MPU (TEXTS) proposes a multi-scale positive-unlabeled AI text detection method, which models AI text detection as a partial positive-unlabeled problem, utilizes length-based multi-scale PU loss, and introduces a text multi-scaling module. MPU significantly improves the detection performance of short texts and enhances long text detection. It has been implemented based on two methods: BERT and RoBERTa, referred to as BERT-MPU and RoBERTa-MPU, respectively. \n\n\u2022 LLaMA-2 (Touvron et al., 2023) is a language model trained on approximately 2T tokens. It has demonstrated exceptional performance across multiple benchmark tests and has been widely used in LLM research. We adopt LLaMA-2-7B and LLaMA-2-13B as the base model for instruction tuning.",
            "score": 0.613641112891361,
            "section_title": "B.2. Details of Baselines",
            "char_start_offset": 25864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 271
                },
                {
                    "start": 274,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1422
                },
                {
                    "start": 1425,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1886
                },
                {
                    "start": 1889,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2095
                },
                {
                    "start": 2096,
                    "end": 2173
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91162109375
        },
        {
            "corpus_id": "273228395",
            "title": "Signal Watermark on Large Language Models",
            "text": "Paraphrase: We employ the 'T5ForConditionalGeneration' [15] method with the 't5-large' from the Hugging Face library for our task. This model is designed to handle English sentences, taking them as inputs and generating a collection of paraphrased versions. Paraphrased tokens span is set to 10. We also incrementally increase the proportion of paraphrased part within watermark text. By doing so, we aim to analyze and understand how such variations in paraphrasing affect the detection of watermarks in the text. The result is shown in Figure 7b. \n\nCross Watermarking and Detecting Model: We explore a cross-model approach using three different models: OPT-1.3b, OPT-6.7b, and text-davinci-003. This approach involves using each of the three models in both capacities -as a text generation model to embed watermarks (Watermarking) and as a Token Probability Re-computation model (Detecting). By interchanging these models, we aim to assess the effectiveness of watermark detection across different model architectures and training paradigms. Figure 8 visualizes an experiment focused on cross-model watermark detection using three advanced language models: OPT-1.3b, OPT-6.7b, and text-davinci-003.",
            "score": 0.6114792426909186,
            "section_title": "C. RQ 3: Attacking Signal Watermark",
            "char_start_offset": 35945,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 548
                },
                {
                    "start": 551,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1200
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72998046875
        },
        {
            "corpus_id": "268723905",
            "title": "Is The Watermarking Of LLM-Generated Code Robust?",
            "text": "LLM-generated text detection One approach to AI-generated text detection involves looking for features or statistical outliers that distinguish AI-generated text from human text. These features include entropy, perplexity, n-gram frequencies, rank, and, in the case of DetectGPT (Mitchell et al., 2023), the observation that minor perturbations of a LLM-generated text have lower log probability under the LLM on average than the original text. However, these zero-shot statistical detectors often require white-box access to model parameters, fail to detect texts generated by advanced LLMs, and rely on many text perturbations generated by another LLM, which is computationally expensive. \n\nAnother common approach is to train a binary classifier to distinguish between human and LLMgenerated text. This approach assumes that LLM-generated text has distinguishing features that the trained model can identify. The fundamental problem with this is that generative models are designed with the intent of producing realistic output that is extremely hard to distinguish from that generated by humans. Specifically, recent advancements, including GPT-4 and other state-of-theart models, are rapidly narrowing the gap between AI-generated and human-written text. As these generative models become more and more realistic, any black-box text distinguishers would incur large Type 1 and Type 2 errors. Distinguishers such as GPTZero (Tian & Cui, 2023), Sniffer (Li et al., 2023), and LMDNet (Wu et al., 2023) have no guarantee of correctness and are susceptible to issues such as out-of-distribution problems, adversarial attacks, and poisoning. \n\nLLM Watermarking Schemes Recently, Kirchenbauer et al. (2023a) gave the first LLM watermarking scheme with formal guarantees. Their watermark divides the vocabulary into a red list and a green list based on a hash of the previous tokens and biases sampling the next token from the green list during the decoding stage. Then, a detector can count the number of green list tokens and check whether this number is statistically significant to determine whether the model output is generated without knowledge of the red-green rule.",
            "score": 0.6109307996824782,
            "section_title": "RELATED WORK",
            "char_start_offset": 7845,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1640
                },
                {
                    "start": 1643,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1961
                },
                {
                    "start": 1962,
                    "end": 2171
                }
            ],
            "ref_mentions": [
                {
                    "start": 1678,
                    "end": 1705,
                    "matchedPaperCorpusId": "256194179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8310546875
        },
        {
            "corpus_id": "260315804",
            "title": "Robust Distortion-free Watermarks for Language Models",
            "text": "The ability of language models to mass produce human-like text creates an acute, renewed emphasis on the importance of provenance of generated content. For example, the website StackOverflow has banned users from posting answers using OpenAI's ChatGPT model to mitigate the spread of misinformation on the platform [26]. A reliable forensic tool for attributing text to a particular language model would empower individuals-such as platform moderators and teachers-to enact and enforce policies on language model usage; it would also better enable model providers to track the (mis)use of their models, e.g., to scrub synthetic text from the training data of future language models.\n\nTo achieve provenance, a watermark is a signal embedded within some generated contentin our case, text from a language model-that encodes the source of the content. We consider a setting where a (untrusted) third party user queries a language model (LM) by sending prompts to a trusted provider (Figure 1): the LM provider generates text from their language model with a watermark so that a detector may later identify the source of the text if the user publishes it. The ideal watermark should satisfy at least the following three desiderata:\n\n1. distortion-free-the watermark should preserve the original text distribution;\n\n2. agnostic-it should be detectable without the language model and/or prompt;\n\n3. robust-it should withstand perturbations of the watermarked text. Figure 1: We define the following watermarking protocol between three parties: the LM provider, the user, the detector. The LM provider and the detector coordinate via a shared key, while the user is an untrusted third party. The protocol consists of four steps: 1) the user sends a prompt to the LM provider; 2) the LM provider generates watermarked text to the user; 3) the user edits the watermarked text (to avoid detection) and publishes the edited text; 4) the detector verifies which of the published text came from the LM provider.\n\nExisting watermarks either distort the model's sampling distribution, thus altering the API functionality [14,1], or are not robust to editing or cropping the text [6",
            "score": 0.6109003789640995,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91064453125
        },
        {
            "corpus_id": "259129463",
            "title": "DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text",
            "text": "The detection of machine-generated text is commonly formulated as a classification task (Jawahar et al., 2020;Fagni et al., 2021;Bakhtin et al., 2019;Sadasivan et al., 2023;Wang et al., 2023). One way of solving it is to use supervised learning, where a classification model is trained on a dataset containing both machine-generated and human-written texts. For example, GPT-2 Detector (Solaiman et al., 2019) fine-tunes RoBERTa (Liu et al., 2019) on the output of GPT-2, while the ChatGPT Detector (Guo et al., 2023) fine-tunes RoBERTa on the HC3 (Guo et al., 2023) dataset. However, models trained explicitly to detect machine-generated texts may overfit their training distribution of the domains (Bakhtin et al., 2019;Uchendu et al., 2020). \n\nAnother stream of work attempts to distinguish machine-generated from human-written texts based on statistical irregularities in the entropy (Lavergne et al., 2008), perplexity (Beresneva, 2016) or in the n-gram frequencies (Badaskar et al., 2008). Gehrmann et al. (2019) introduced hand-crafted statistical features to assist humans in detectingmachine generated texts. Moreover, (Solaiman et al., 2019)  methods for detecting machine-generated text by evaluating the per-token log probability of texts and using thresholding. Mitchell et al. (2023) observed that machine-generated texts tend to lie in the local curvature of the log probability and proposed De-tectGPT, whose prominent performance can only be guaranteed by the large size of the perturbation function and by a large number of perturbations, and thus costs more computational resources. Other work explored watermarking, which imprints specific patterns of the LLM-output text that can be detected by an algorithm while being imperceptible to humans.",
            "score": 0.6106841597590583,
            "section_title": "Related Work",
            "char_start_offset": 4771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 744
                },
                {
                    "start": 747,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1765
                }
            ],
            "ref_mentions": [
                {
                    "start": 110,
                    "end": 129,
                    "matchedPaperCorpusId": "220936111"
                },
                {
                    "start": 888,
                    "end": 911,
                    "matchedPaperCorpusId": "12098535"
                },
                {
                    "start": 924,
                    "end": 941,
                    "matchedPaperCorpusId": "1175726"
                },
                {
                    "start": 971,
                    "end": 994,
                    "matchedPaperCorpusId": "4324753"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8125
        },
        {
            "corpus_id": "258865787",
            "title": "Ghostbuster: Detecting Text Ghostwritten by Large Language Models",
            "text": "We introduce Ghostbuster, a state-of-the-art system for detecting AI-generated text.Our method works by passing documents through a series of weaker language models, running a structured search over possible combinations of their features, and then training a classifier on the selected features to predict whether documents are AI-generated.Crucially, Ghostbuster does not require access to token probabilities from the target model, making it useful for detecting text generated by black-box or unknown models.In conjunction with our model, we release three new datasets of human- and AI-generated text as detection benchmarks in the domains of student essays, creative writing, and news articles. We compare Ghostbuster to several existing detectors, including DetectGPT and GPTZero, as well as a new RoBERTa baseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is 5.9 F1 higher than the best preexisting model. It also outperforms all previous approaches in generalization across writing domains (+7.5 F1), prompting strategies (+2.1 F1), and language models (+4.4 F1). We also analyze our system\u2019s robustness to a variety of perturbations and paraphrasing attacks, and evaluate its performance on documents by non-native English speakers.",
            "score": 0.6100916446386753,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "269588087",
            "title": "Investigating Wit, Creativity, and Detectability of Large Language Models in Domain-Specific Writing Style Adaptation of Reddit\u2019s Showerthoughts",
            "text": "Ippolito et al. (2020) use a finetuned BERT-based binary classifier to label texts as human-written or AI-generated.However, their model lacks generalizability -when trained on topk samples and evaluated on non-truncated random samples, the model only achieves 43.8% accuracy.The sharp increase in discussions about misuse and plagiarism using tools such as ChatGPT has shifted researchers' focus on this area, e.g., Mitchell et al. (2023) proposed DetectGPT, a zero-shot model for detecting AI-generated text, and Deng et al. (2023) proposed a Bayesian Surrogate Model, claiming to outperform DetectGPT.Tang et al. (2024) provide an overview of further detection techniques.",
            "score": 0.6098912118464546,
            "section_title": "Authorship Identification",
            "char_start_offset": 8353,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 116,
                    "end": 276
                },
                {
                    "start": 276,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 675
                }
            ],
            "ref_mentions": [
                {
                    "start": 604,
                    "end": 622,
                    "matchedPaperCorpusId": "257496757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8310546875
        },
        {
            "corpus_id": "272524281",
            "title": "Provably Robust Multi-bit Watermarking for AI-generated Text",
            "text": "Large Language Models (LLMs) have demonstrated remarkable capabilities of generating texts resembling human language. However, they can be misused by criminals to create deceptive content, such as fake news and phishing emails, which raises ethical concerns. Watermarking is a key technique to address these concerns, which embeds a message (e.g., a bit string) into a text generated by an LLM. By embedding the user ID (represented as a bit string) into generated texts, we can trace generated texts to the user, known as content source tracing. The major limitation of existing watermarking techniques is that they achieve sub-optimal performance for content source tracing in real-world scenarios. The reason is that they cannot accurately or efficiently extract a long message from a generated text. We aim to address the limitations. In this work, we introduce a new watermarking method for LLM-generated text grounded in pseudo-random segment assignment. We also propose multiple techniques to further enhance the robustness of our watermarking algorithm. We conduct extensive experiments to evaluate our method. Our experimental results show that our method substantially outperforms existing baselines in both accuracy and robustness on benchmark datasets. For instance, when embedding a message of length 20 into a 200-token generated text, our method achieves a match rate of $97.6\\%$, while the state-of-the-art work Yoo et al. only achieves $49.2\\%$. Additionally, we prove that our watermark can tolerate edits within an edit distance of 17 on average for each paragraph under the same setting.",
            "score": 0.6093655909324152,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.939453125
        },
        {
            "corpus_id": "273228395",
            "title": "Signal Watermark on Large Language Models",
            "text": "In this study, we introduced an innovative watermarking method -Signal Watermarking -tailored for use with Large Language Models (LLMs). Our approach is grounded in a critical hypothesis about LLMs and employs signal sampling patterns as a benchmark for embedding signals into text. Throughout the process of embedding and detecting watermarks in LLM-generated text, we integrated and applied concepts from signal processing. The inherent properties of the signals ensured the strength and sensitivity of the watermarks, enabling them to remain effective across various attack scenarios and be detectable within very short spans of text. Our method demonstrated high accuracy in watermark detection, surpassing most existing open-source and commercial detectors in identifying model-generated text. This approach is versatile and adaptable across different types of LLMs, enabling the generation of watermarked text even with access limited to model APIs. The success of our technique illustrates the potential of combining signal processing with text generation, paving the way for new applications in safeguarding digital content authenticity and integrity in the AI era. This appendix presents a detailed analysis of the Perplexity (PPL) scores obtained from texts generated by the text-davinci-003 model under varying temperature settings and watermarking conditions. The data includes comparisons between non-watermarked (NW) and watermarked (W) texts, as well as human-written text (Real completion), derived from the C4 dataset. The example comprises four different text, each generated at specific temperature settings (0 and 0.8) with and without watermarking. The PPL scores for these texts are calculated using the text-davinci-003 model. The data is summarized as Figure 9.",
            "score": 0.6088778818581484,
            "section_title": "VIII. CONCLUSION",
            "char_start_offset": 41041,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1785
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9189453125
        },
        {
            "corpus_id": "258108316",
            "title": "Detection of Fake Generated Scientific Abstracts",
            "text": "Additionally, the authors of [18] proposed a tool, called DetectGPT, that detects machine-generated text from LLMs. DetectGPT is a more effective approach for detecting machine-generated text than existing zero-shot methods. The authors found that DetectGPT significantly improved the detection of fake news articles generated by 20B parameter GPT-NeoX, achieving an AUROC of 0.95 compared to the strongest zero-shot baseline's AUROC of 0.81. Additionally, the authors found that DetectGPT does not require training a separate classifier or collecting a dataset of a real or generated corpus, making it a more efficient and practical approach for detecting machine-generated text [18]. \n\nThe [16] has focused on the involvement of AI writing in scientific writing. The authors analyze the similarities and differences between the two types of content, including writing style, consistency, coherence, language redundancy, and factual errors. The results suggest that while AI has the potential to generate accurate scientific content, there is still a gap in terms of depth and overall quality. The study utilizes the logistic regression model as a detector along with OpenAI's RoBERTa detector. The results suggest that there are significant differences in the distribution of text between humanwritten and AI-generated scientific work and that AI-generated abstracts tend to be more generic than human-written ones. Additionally, the logistic regression model achieved a higher F1-score for detecting AI-generated text [16]. \n\nFinally, as the extensive use of GPT-3 and ChatGPT continues to grow, OpenAI published on January 31, 2023, an update of the aforementioned model. The revised version of the AI text classifier fine-tuned pre-trained models in order to distinguish the AI-generated text. The model used data from three origins of the human-written text, a dataset from Wikipedia, the 2019 WebText dataset, and a group of human examples gathered during the instruction of InstructGPT. Although, the model needs 1, 000 tokens and more in order to produce reliable results. The revised edition of the model has shown better results than the previous version, which was based on GPT-2.",
            "score": 0.6077721913762663,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 5711,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1526
                },
                {
                    "start": 1529,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2192
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82421875
        },
        {
            "corpus_id": "257687440",
            "title": "Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense",
            "text": "The rise in malicious usage of large language models, such as fake content creation and academic plagiarism, has motivated the development of approaches that identify AI-generated text, including those based on watermarking or outlier detection. However, the robustness of these detection algorithms to paraphrases of AI-generated text remains unclear. To stress test these detectors, we build a 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, condition on surrounding context, and control lexical diversity and content reordering. Using DIPPER to paraphrase text generated by three large language models (including GPT3.5-davinci-003) successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), without appreciably modifying the input semantics. To increase the robustness of AI-generated text detection to paraphrase attacks, we introduce a simple defense that relies on retrieving semantically-similar generations and must be maintained by a language model API provider. Given a candidate text, our algorithm searches a database of sequences previously generated by the API, looking for sequences that match the candidate text within a certain threshold. We empirically verify our defense using a database of 15M generations from a fine-tuned T5-XXL model and find that it can detect 80% to 97% of paraphrased generations across different settings while only classifying 1% of human-written sequences as AI-generated. We open-source our models, code and data.",
            "score": 0.6077248340936905,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9326171875
        },
        {
            "corpus_id": "276742379",
            "title": "Decoupling Content and Expression: Two-Dimensional Detection of AI-Generated Text",
            "text": "The first is supervised classifiers (Solaiman et al., 2019;Ippolito et al., 2020;Fagni et al., 2021;Hu et al., 2023;Yan et al., 2023;Li et al., 2024;Verma et al., 2024), which train a binary classifier based on a large collection of machinegenerated and human-written text. The second is zero-shot classifiers, including white-box methods (Gehrmann et al., 2019;Su et al., 2023;Bao et al., 2024;Xu et al., 2024;Hans et al., 2024) and black-box methods (Mitchell et al., 2023;Yang et al., 2023a;Bhattacharjee and Liu, 2024;Bao et al., 2025). These technologies usually use pretrained language models to extract detection metrics. The third is text watermarking technology (Kirchenbauer et al., 2023;Zhao et al., 2023a;Christ et al., 2024;Zhao et al., 2024b,a), which identifies machine-generated text by embedding easy-to-detect markers or patterns. \n\nThese techniques are effective in detecting purely machine-generated texts, but may not be robust to various attacks (Gao et al., 2018;Dyrmishi et al., 2023;Krishna et al., 2024;He et al., 2024;Dugan et al., 2024;Wu et al., 2024;Wang et al., 2024c). At the same time, various commercial AI systems are published to serve 'humanizing' ability, bypass existing detectors. To address these challenges, we propose the 2D detection framework as an effective candidate to defend against attacks. \n\nDecoupling of Content and Expression. The idea of decoupling content and expression is related to existing studies on the disentanglement of semantics and syntax.",
            "score": 0.6076361432222642,
            "section_title": "Related Work",
            "char_start_offset": 6680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 848
                },
                {
                    "start": 851,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1340
                },
                {
                    "start": 1343,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1505
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 81,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 81,
                    "end": 100,
                    "matchedPaperCorpusId": "220936111"
                },
                {
                    "start": 100,
                    "end": 116,
                    "matchedPaperCorpusId": "259501842"
                },
                {
                    "start": 116,
                    "end": 133,
                    "matchedPaperCorpusId": "271290619"
                },
                {
                    "start": 133,
                    "end": 149,
                    "matchedPaperCorpusId": "269922444"
                },
                {
                    "start": 149,
                    "end": 168,
                    "matchedPaperCorpusId": "258865787"
                },
                {
                    "start": 339,
                    "end": 362,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 378,
                    "end": 395,
                    "matchedPaperCorpusId": "263831345"
                },
                {
                    "start": 395,
                    "end": 411,
                    "matchedPaperCorpusId": "270845828"
                },
                {
                    "start": 411,
                    "end": 429,
                    "matchedPaperCorpusId": "267068867"
                },
                {
                    "start": 452,
                    "end": 475,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 475,
                    "end": 494,
                    "matchedPaperCorpusId": "258960101"
                },
                {
                    "start": 494,
                    "end": 522,
                    "matchedPaperCorpusId": "260379022"
                },
                {
                    "start": 522,
                    "end": 539,
                    "matchedPaperCorpusId": "274776514"
                },
                {
                    "start": 671,
                    "end": 698,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 698,
                    "end": 717,
                    "matchedPaperCorpusId": "256627372"
                },
                {
                    "start": 717,
                    "end": 737,
                    "matchedPaperCorpusId": "259092330"
                },
                {
                    "start": 968,
                    "end": 986,
                    "matchedPaperCorpusId": "4858173"
                },
                {
                    "start": 986,
                    "end": 1008,
                    "matchedPaperCorpusId": "258888064"
                },
                {
                    "start": 1008,
                    "end": 1029,
                    "matchedPaperCorpusId": "257687440"
                },
                {
                    "start": 1029,
                    "end": 1045,
                    "matchedPaperCorpusId": "257766697"
                },
                {
                    "start": 1064,
                    "end": 1080,
                    "matchedPaperCorpusId": "273706951"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8388671875
        },
        {
            "corpus_id": "270765507",
            "title": "Groningen Team F at SemEval-2024 Task 8: Detecting Machine-Generated Text using Feature-Based Machine Learning Models",
            "text": "Recent large language models (LLMs), such as ChatGPT, have shown remarkable capability of creating fluent responses to a wide variety of user queries.This, in combination with increased accessibility to these models, has lead to an increase of machine-generated content over various channels.However, these LLMs come with concerns regarding the potential misuse of such tasks, like spreading misinformation and misuse within the education system (Wang et al., 2023).Therefore, detecting whether a text is human-written or machinegenerated is extremely important.\n\nUnfortunately, humans perform only slightly better than chance at this task, as found by Gehrmann et al. (2019).This introduces the need for developing automatic systems that can identify machinegenerated texts, in order to mitigate their potential misuse (Wang et al., 2023).\n\nWhile previous work has been done on identifying machine-generated texts, they either focused on only one or two particular languages, or focused on detecting machine-generated text for a specific LLM or a specific domain (Wang et al., 2023).\n\nFor instance, Macko et al. (2023) note that most of the research on machine-generated text detection uses systems that were trained on English datasets and that prior works show that detectors fine-tuned on English data fail to generalize to other languages.This can be seen in the results from Mitchell et al. (2023), where a decrease is seen from 0.946 AUC ROC to 0.537 when working with German data.In addition, Macko et al. (2023) get similar results.\n\nMeanwhile, Sarvazyan et al. (2023) address the issue of detection systems not generalizing across different generation models and domains.They mention that most previous works often overlook that detection systems would be applied to a broad variety of domains, writing styles, and generation models.\n\nThus, the goal of Task 8 of SemEval 2024 (Wang et al., 2024) is to take a first step into creating a mono-or multilingual system that is able to detect machine-generated text created by different LLMs in different domains.The Task is divided into three subtasks, of which we participate in two.",
            "score": 0.6074679262925035,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 150,
                    "end": 292
                },
                {
                    "start": 292,
                    "end": 466
                },
                {
                    "start": 466,
                    "end": 562
                },
                {
                    "start": 564,
                    "end": 676
                },
                {
                    "start": 676,
                    "end": 840
                },
                {
                    "start": 842,
                    "end": 1084
                },
                {
                    "start": 1086,
                    "end": 1344
                },
                {
                    "start": 1344,
                    "end": 1488
                },
                {
                    "start": 1488,
                    "end": 1541
                },
                {
                    "start": 1543,
                    "end": 1681
                },
                {
                    "start": 1681,
                    "end": 1843
                },
                {
                    "start": 1845,
                    "end": 2067
                },
                {
                    "start": 2067,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 653,
                    "end": 675,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 1100,
                    "end": 1119,
                    "matchedPaperCorpusId": "264300609"
                },
                {
                    "start": 1501,
                    "end": 1520,
                    "matchedPaperCorpusId": "264300609"
                },
                {
                    "start": 1886,
                    "end": 1905,
                    "matchedPaperCorpusId": "269292978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8046875
        },
        {
            "corpus_id": "271270718",
            "title": "SMLT-MUGC: Small, Medium, and Large Texts - Machine versus User-Generated Content Detection and Comparison",
            "text": "Modern large language models are becoming increasingly powerful and capable of generating realistic and convincing content.These models can also target users with highly personalized recommendations.Consequently, they have the potential to create and propagate harmful or misleading content, such as fake news or hate speech, whether intentionally or unintentionally [12,8,33].The proliferation of content generated and distributed by these models has amplified the threat and impact of such harmful activities more than ever before [32,41,8,33,10].As AI-generated texts increasingly blend seamlessly with human-written content, the demand for more effective methods to detect misleading information produced by AI grows.Therefore, there is a growing interest in investigating the impact and detection mechanisms for machine-generated content [34,19,40,16,36,35].Accurately detecting machine-generated content is essential to understand their full capabilities while minimizing the potential for any serious consequences.In prior detection efforts, researchers predominantly utilized pre-trained models like RoBERTa, GPT-2, GROVER, and GLTR [20], and more recently, the effectiveness of conventional methods such as logistic regression, SVM, and others is also being explored [38,19].Overall, supervised detection, zero-shot detection, retrieval-based detection, and watermarking-based detection are being used to distinguish machine-generated text from human text [39,2].Despite growing interest and several published studies, a consolidated comparison across different domains, text lengths, and characteristics is still missing.This paper aims to contribute towards that goal by presenting three detailed analyses on datasets comprising varying text lengths-small, medium, and large.First, it compares the performance of different machine learning algorithms on four datasets: small (tweets from Election, FIFA, and Game of Thrones), medium (Wikipedia introductions and PubMed abstracts), and large (OpenAI web text dataset).Second, it compares the characteristics of human and machine-generated writing across multiple dimensions, including linguistics, personality, sentiment, bias, and morality.Third, it compares the algorithm performances on rephrased texts.Our analysis revealed several key findings: Machinegenerated texts are generally more accessible and require a lower level of education to understand compared to humangenerated texts.",
            "score": 0.6074647795099138,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 123,
                    "end": 199
                },
                {
                    "start": 199,
                    "end": 377
                },
                {
                    "start": 377,
                    "end": 549
                },
                {
                    "start": 549,
                    "end": 721
                },
                {
                    "start": 721,
                    "end": 863
                },
                {
                    "start": 863,
                    "end": 1021
                },
                {
                    "start": 1021,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1472
                },
                {
                    "start": 1472,
                    "end": 1631
                },
                {
                    "start": 1631,
                    "end": 1786
                },
                {
                    "start": 1786,
                    "end": 2028
                },
                {
                    "start": 2028,
                    "end": 2201
                },
                {
                    "start": 2201,
                    "end": 2266
                },
                {
                    "start": 2266,
                    "end": 2449
                }
            ],
            "ref_mentions": [
                {
                    "start": 371,
                    "end": 373,
                    "matchedPaperCorpusId": "258060002"
                },
                {
                    "start": 373,
                    "end": 376,
                    "matchedPaperCorpusId": "265066590"
                },
                {
                    "start": 540,
                    "end": 542,
                    "matchedPaperCorpusId": "258060002"
                },
                {
                    "start": 542,
                    "end": 545,
                    "matchedPaperCorpusId": "265066590"
                },
                {
                    "start": 545,
                    "end": 548,
                    "matchedPaperCorpusId": "255773668"
                },
                {
                    "start": 856,
                    "end": 859,
                    "matchedPaperCorpusId": "266390558"
                },
                {
                    "start": 859,
                    "end": 862,
                    "matchedPaperCorpusId": "257496757"
                },
                {
                    "start": 1141,
                    "end": 1145,
                    "matchedPaperCorpusId": "226237099"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.869140625
        },
        {
            "corpus_id": "269614196",
            "title": "Who Wrote This? The Key to Zero-Shot LLM-Generated Text Detection Is GECScore",
            "text": "Black-box LLM-generated text detection mainly identifies LLM-generated text by training neuralbased classifiers. Fine-tuning PLM-based classifiers is the most widely used black-box method and typically requires a large number of human-written and LLM-generated training samples. Pre-trained language models, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and XLNet (Yang et al., 2019), demonstrate strong semantic understanding ability and effectively enhance the performance of NLP tasks (Qiu et al., 2020), especially text classification. Previous research has demonstrated that the fine-tuned classifiers based on RoBERTa are highly effective at detecting LLMgenerated text (Guo et al., 2023;Liu et al., 2023a,b;Chen et al., 2023). Notably, these fine-tuned classifiers have been shown to outperform other methodologies, such as zero-shot approaches and watermark technologies, while also demonstrating commendable adversarial robustness. \n\nRecently, Zhu et al. (2023) demonstrated a novel black-box zero-shot method for detecting LLMgenerated text. This method employs BARTScore (Yuan et al., 2021a), a metric for calculating the semantic similarity between ChatGPT-revised text and the original text. The foundational hypothesis is that ChatGPT tends to introduce fewer modifications to LLM-generated text compared to text written by humans. Next, the similarity score s i between xi and x i is calculated using a similarity metric Sim. Finally, if s i meets or exceeds the threshold \u03b5, the text is more likely to be generated by LLM. \n\n3 Methodology: GECSCORE",
            "score": 0.6072564651898533,
            "section_title": "Black-box LLM-generated Text Detection",
            "char_start_offset": 5006,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1556
                },
                {
                    "start": 1559,
                    "end": 1582
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 342,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 382,
                    "end": 401,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 971,
                    "end": 988,
                    "matchedPaperCorpusId": "266164139"
                },
                {
                    "start": 1100,
                    "end": 1120,
                    "matchedPaperCorpusId": "235593404"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68603515625
        },
        {
            "corpus_id": "273185892",
            "title": "Detecting Machine-Generated Long-Form Content with Latent-Space Variables",
            "text": "In today's digital world, large language models (LLMs) such as GPT-4 have transformed various daily tasks with their human-like text generation capability, such as drafting emails and essays. However, their potential misuse poses substantial risks including impersonation, misinformation, and academic dishonesty (Tang et al., 2024). This highlights the need for effective detection mechanisms. Existing AI content detectors can be categorized into 1) a priori methods such as watermarking (Kirchenbauer et al., 2023), 2) parameterized meth-Figure 1: (a) Existing zero-shot detectors that rely on the token distributions (observation space statistics) are not robust to various real-world scenarios such as high decoding temperature, complex prompts, and adversarial attacks. (b) Our detector with latent features (e.g., discourse tags) are more robust to these changes. \n\nand 3) zero-shot methods that rely on certain statistical differences (Vasilatos et al., 2023;Mitchell et al., 2023;Bao et al., 2024). This paper focuses on the last due to its general pertinence in practice: end users may still choose non-watermarked LLMs outside the distribution of the fine-tuned classifiers (Yang et al., 2023;Ghosal et al., 2023). \n\nExisting zero-shot methods to distinguish machine-generated texts (MGTs) from humanwritten texts (HWTs) typically assume an unchanging relationship between machine and human outputs, ignoring potential distribution shifts resulted from changes in generation setups (Zellers et al., 2019) or adversarial attacks (Shi et al., 2024;Wang et al., 2024). As is shown in Figure 1(a), prior zeroshot detection methods usually assume that MGTs consistently exhibit higher log-likelihood (or conditional curvature) than HWTs. However, certain changes-including increased decoding temperature, paraphrasing, and word substitution-can alter the distribution of MGTs (Hans et al., 2024), making MGTs and HWTs inseparable by loglikelihood after domain shift.",
            "score": 0.6068072267081441,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 870
                },
                {
                    "start": 873,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1972
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 332,
                    "matchedPaperCorpusId": "257496757"
                },
                {
                    "start": 490,
                    "end": 517,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 967,
                    "end": 989,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 989,
                    "end": 1006,
                    "matchedPaperCorpusId": "263831345"
                },
                {
                    "start": 1204,
                    "end": 1224,
                    "matchedPaperCorpusId": "266982110"
                },
                {
                    "start": 1493,
                    "end": 1515,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 1539,
                    "end": 1557,
                    "matchedPaperCorpusId": "258987266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "269614196",
            "title": "Who Wrote This? The Key to Zero-Shot LLM-Generated Text Detection Is GECScore",
            "text": "The white-box LLM-generated text detection methods require access to the source model. Current white-box approaches typically employ zero-shot techniques, where specific metrics are derived from the logits of the LLM output. These metrics are then compared against a statistically derived threshold, which serves as the criterion for identifying LLM-generated text. The most commonly used metric is Log-Likelihood (Solaiman et al., 2019), which evaluates whether a given text was generated by an LLM by measuring the average tokenwise log probability of each token. Similarly, Rank (Gehrmann et al., 2019) calculates the average of the absolute rank and entropy values for each token. Log-Rank (Gehrmann et al., 2019) further improves on Rank by applying a logarithmic transformation to the rank values of each token, achieving better performance. A notable method is LRR (Su et al., 2023), which combines Log-Likelihood and Log-Rank. By taking the ratio of these metrics, LRR comprehensively incorporates their strengths for improved performance. \n\nCertain white-box methods perform LLMgenerated text detection by quantifying met-ric variations following perturbations. Detect-GPT (Mitchell et al., 2023) distinguishes LLMgenerated text by introducing perturbations to the original text via a T5 model (Raffel et al., 2020) and measuring the resultant alterations in the log probability of the source LLM. This method capitalizes on the observation that the average decrease in Log-Likelihood of perturbed LLM-generated text consistently exceeds that of perturbed humanwritten text. Another similar and concurrent work by Su et al. (2023) performs the detection task by measuring Log-Rank. Although perturbed-based curve statistical methods exhibit superior capabilities compared to conventional zero-shot methods, the time overhead of perturbation greatly reduces the usability of these methods. In response to this limitation, Fast-DetectGPT (Bao et al., 2023) replaced the perturbation step of DetectGPT with a more efficient sampling step to achieve superior efficacy and efficiency in LLM-generated-text detection performance.",
            "score": 0.6063098769153848,
            "section_title": "White-box LLM-generated Text Detection",
            "char_start_offset": 2831,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1047
                },
                {
                    "start": 1050,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2132
                }
            ],
            "ref_mentions": [
                {
                    "start": 582,
                    "end": 605,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 694,
                    "end": 717,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 872,
                    "end": 889,
                    "matchedPaperCorpusId": "259129463"
                },
                {
                    "start": 1182,
                    "end": 1205,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1303,
                    "end": 1324,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1623,
                    "end": 1639,
                    "matchedPaperCorpusId": "259129463"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7431640625
        },
        {
            "corpus_id": "265157663",
            "title": "AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising",
            "text": "Large language models (LLMs) have opened up enormous opportunities while simultaneously posing ethical dilemmas. One of the major concerns is their ability to create text that closely mimics human writing, which can lead to potential misuse, such as academic misconduct, disinformation, and fraud. To address this problem, we present AuthentiGPT, an efficient classifier that distinguishes between machine-generated and human-written texts. Under the assumption that human-written text resides outside the distribution of machine-generated text, AuthentiGPT leverages a black-box LLM to denoise input text with artificially added noise, and then semantically compares the denoised text with the original to determine if the content is machine-generated. With only one trainable parameter, AuthentiGPT eliminates the need for a large training dataset, watermarking the LLM's output, or computing the log-likelihood. Importantly, the detection capability of AuthentiGPT can be easily adapted to any generative language model. With a 0.918 AUROC score on a domain-specific dataset, AuthentiGPT demonstrates its effectiveness over other commercial algorithms, highlighting its potential for detecting machine-generated text in academic settings.",
            "score": 0.6050454069353515,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9443359375
        },
        {
            "corpus_id": "266977496",
            "title": "Few-Shot Detection of Machine-Generated Text using Style Representations",
            "text": "Specifically, our evaluations assess the ability to detect writing samples produced by LLM unseen at training time, and in some cases, drawn from new domains and dealing with new topics. Our approach outperforms prominent few-shot learning methods as well as standard zero-shot baselines and differs significantly from prior work in that we do not require access to the predictive distribution of the unseen LLM, like Mitchell et al. (2023), or a large number of samples from it, like Zellers et al. (2019), to effectively detect text generated by these models. \n\nWe also explore factors leading to effective style representations for this task, finding that contrastive training on large amounts of human-authored text is sufficient to obtain useful representations across a variety of few-shot settings. Finally, we release the datasets we generated as part of this work, which include documents generated by a variety of language models. Perhaps the most widely-applied machine-text detector is OpenAI's AI Detector, which is intended to predict whether a given document was human-or machine-generated (Solaiman et al., 2019). The model was trained using documents generated by GPT-2 comprising one class and documents drawn from the corpus used to train GPT-2 comprising the other class. OpenAI released a similar classifier for detecting ChatGPT in January 2023. As of this writing, OpenAI has withdrawn the AI Detector, citing its low rate of accuracy. Indeed, it is well-known that supervised detectors may overfit various properties of their training data, such as specific decoding strategies (Ippolito et al., 2020). \n\nBy perturbing the output logits of an LLM and thereby its decoded text, a recent proposal known as watermarking, one may effectively detect text generated by LLM in some settings. For example, Kirchenbauer et al. (2023) encode a watermark in generated text by splitting a model's vocabulary into so-called red and green lists, encouraging tokens from the green list to be sampled during decoding more frequently than tokens from the red list.",
            "score": 0.6050113659175542,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4106,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 561
                },
                {
                    "start": 564,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1626
                },
                {
                    "start": 1629,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 2071
                }
            ],
            "ref_mentions": [
                {
                    "start": 418,
                    "end": 440,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 485,
                    "end": 506,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 1602,
                    "end": 1625,
                    "matchedPaperCorpusId": "218560609"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9208984375
        },
        {
            "corpus_id": "259308864",
            "title": "Provable Robust Watermarking for AI-Generated Text",
            "text": "However, as language models improve, AI-generated text is becoming increasingly similar to human-generated text, making it more challenging to detect. Gambini et al. [2022] find that existing detection strategies designed for GPT-2 struggle with GPT-3. Moreover, known detectors are found to be fragile to adversarial attacks [Wolff, 2020] and biased towards non-native English writers [Liang et al., 2023]. \n\nImpossibility results? Sadasivan et al. [2023] poses the question of whether detecting machine-generated text is possible and argue that as the human distribution and LLM distribution of texts get closer, any classifier will have to either have a large Type I error or a large Type II error. The authors also argue that (in Corollary 2) if the watermarking scheme can be learned then paraphrasing attacks either evade the detector or also classify humans with a similar distribution as false positives. This does not invalidate our results as we made no theoretical claim about paraphrasing. We do claim that in Theorem 3.1 that the watermarked LM M and original LM M is statistically close -in fact, indistinguishable in the \"differential privacy\" sense. But the indistinguishability is for each token. As the number of tokens gets larger, they will eventually become distinguishable, that is why our Theorem C.4 and Theorem C.13 are not contradicting Theorem 3.1. This argument was initially pointed out by Chakraborty et al. [2023], showing that detection is possible. \n\nLanguage model watermarks with provable guarantees. Concurrent to our work, Christ et al. [2023] consider the problem of formally defining watermarking language models and propose a construction with provable guarantees. The main differences between their work and ours are:",
            "score": 0.6049151018883717,
            "section_title": "A More on related work",
            "char_start_offset": 32149,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 407
                },
                {
                    "start": 410,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1481
                },
                {
                    "start": 1484,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1758
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 172,
                    "matchedPaperCorpusId": "249993639"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81884765625
        },
        {
            "corpus_id": "265213008",
            "title": "A Robust Semantics-based Watermark for Large Language Model against Paraphrasing",
            "text": "Large language models (LLMs) have shown their great ability in various natural language processing (NLP) tasks like Question Answering (QA) (Lu et al., 2022), reasoning tasks (Wei et al., 2022;Creswell et al., 2022) and code development (Xu et al., 2022). However, tremendous concerns have been raised that LLMs are possible to be used improperly and illegally. For example, indistinguishable fake news are easy to be fabricated (Kreps et al., 2022;Zellers et al., 2019) by language models, which, when disseminated, could instigate widespread panic. Similarly, in the commercial sphere, convincingly generated reviews can manipulate consumer perceptions, leading to unethical business competition (Salminen et al., 2022). Therefore, detecting LLM-generated text has become crucial in the real-world applications of LLMs (Wu et al., 2023;Sadasivan et al., 2023;Xu et al., 2023). \n\nAmong diverse methods to detect LLMgenerated texts, the watermark strategies have demonstrated outstanding precision (Liu et al., 2023b;Tang et al., 2023;Ren et al., 2024). It is proposed to encode a secret watermark into the generated texts, such that we can tell whether a text is generated by detecting this watermark. One representative strategy (Kirchenbauer et al., 2023a;Yoo et al., 2023) is to encode the watermark based on the \"partition of vocabulary\". In detail, given a language model, these methods devise a mapping from precedent tokens to a particular partition of the vocabulary by a partition function for the consequent token. The partition function leverages the hashes of the input as the seed of a random generator to split the vocabulary to a green list and a red list. \n\nDuring the text generation phase, the consequent token has an increased probability to be sampled from the green list. In this way, the watermark is encoded through the matching between the precedent tokens and the vocabulary partition for the consequent token.",
            "score": 0.6038716560804471,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 878
                },
                {
                    "start": 881,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1672
                },
                {
                    "start": 1675,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 157,
                    "matchedPaperCorpusId": "252383606"
                },
                {
                    "start": 175,
                    "end": 193,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 237,
                    "end": 254,
                    "matchedPaperCorpusId": "247158549"
                },
                {
                    "start": 429,
                    "end": 449,
                    "matchedPaperCorpusId": "219800227"
                },
                {
                    "start": 449,
                    "end": 470,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 698,
                    "end": 721,
                    "matchedPaperCorpusId": "240535549"
                },
                {
                    "start": 1259,
                    "end": 1276,
                    "matchedPaperCorpusId": "259129912"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93505859375
        },
        {
            "corpus_id": "265552122",
            "title": "MARKMyWORDS: Analyzing and Evaluating Language Model Watermarks",
            "text": "Watermarking can refer to either watermarking models, or watermarking model outputs. Watermarking models [9,19,20,79] defends against model extraction attacks and is out of scope of our work; we focus on watermarking model outputs in order to detect AI-generated text. Watermarking of textual data has been extensively studied [26,32,58,63]. It can be viewed as a form of steganography [10,47,81] with a one-bit message. The message can be embedded post-generation (rule-based and neural-based watermarking), or during generation [64]. We consider the latter. Watermarks could conceivably also be used to prove that text was indeed machine-generated, for instance, to guarantee the provenance of a generation, similar to copyrights. That setting is out of scope for this paper. \n\nAnother approach to detecting AI-generated text is to train a classifier on LLM outputs [4,11,16,28,50,51,53,56,67,76]. This approach avoids the need to modify how text is generated by the LLM, but must be updated whenever the LLM changes. For proprietary LLMs, an alternative is for the vendor to keep a copy of all generated outputs from their LLM and provide an API to look up whether a text was previously produced with their LLM [36]. \n\nLLM benchmarks. New LLMs are accompanied by corresponding benchmarks designed to quantify their advancements over predecessors. While some benchmarks focus on assessing LLMs across a range of tasks-such as MMLU [22], BIG-Bench [75], and HELM [41]-others are tailored to evaluate specific capabilities like programming [3] or multi-turn conversation [80]. In contrast to these existing benchmarks, MARKMYWORDS is the first to evaluate watermarks in LLM outputs across multiple dimensions: quality, efficacy, and tamper resistance. WaterBench [66], a concurrent benchmark, evaluates one class of watermarks [33], and does not evaluate tamper resistance. Furthermore, MarkMyWords better captures the ability of watermarking schemes to preserve quality.",
            "score": 0.6036330886994442,
            "section_title": "A. Related work",
            "char_start_offset": 4332,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 777
                },
                {
                    "start": 780,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1219
                },
                {
                    "start": 1222,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1971
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 108,
                    "matchedPaperCorpusId": "246294509"
                },
                {
                    "start": 108,
                    "end": 111,
                    "matchedPaperCorpusId": "244909149"
                },
                {
                    "start": 111,
                    "end": 114,
                    "matchedPaperCorpusId": "252367820"
                },
                {
                    "start": 114,
                    "end": 117,
                    "matchedPaperCorpusId": "256627372"
                },
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "1926411"
                },
                {
                    "start": 331,
                    "end": 334,
                    "matchedPaperCorpusId": "3783650"
                },
                {
                    "start": 334,
                    "end": 337,
                    "matchedPaperCorpusId": "31847435"
                },
                {
                    "start": 337,
                    "end": 340,
                    "matchedPaperCorpusId": "33130131"
                },
                {
                    "start": 393,
                    "end": 396,
                    "matchedPaperCorpusId": "202537031"
                },
                {
                    "start": 871,
                    "end": 874,
                    "matchedPaperCorpusId": "220936111"
                },
                {
                    "start": 874,
                    "end": 877,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 880,
                    "end": 883,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 889,
                    "end": 892,
                    "matchedPaperCorpusId": "244488758"
                },
                {
                    "start": 892,
                    "end": 895,
                    "matchedPaperCorpusId": "221835708"
                },
                {
                    "start": 895,
                    "end": 898,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 1449,
                    "end": 1453,
                    "matchedPaperCorpusId": "235899097"
                },
                {
                    "start": 1827,
                    "end": 1831,
                    "matchedPaperCorpusId": "256194179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.908203125
        },
        {
            "corpus_id": "267627184",
            "title": "ACW: Enhancing Traceability of AI-Generated Codes Based on Watermarking",
            "text": "In the following, we briefly review existing approaches to the problem and explain why they fail to solve the problem satisfactorily. \n\nThe first category of approaches involves passive detectors that build binary classifiers to distinguish AI-generated and human-written content. Note that there is little work specifically focusing on AI-generated code detection, instead, they aim to identify any generated content regardless of text or codes. Mitchell et al. [29] demonstrate that LLMgenerated text tends to occupy the negative curvature region of the log probability function of an AI model, and propose a curvature-based method DetectGPT to detect AI-generated text. The GPTZero team [43] develops an advanced Web application, GPTZero, for the detection of ChatGPT-generated content. Although existing passive detectors perform satisfactorily on text, Wang et al. [40] report that they cannot be directly applied to codes unless extensive additional fine-tuning is performed. That is, they cannot be generalized for AI-generated code detection. Moreover, similar to task such as image forgery detection [25], passive detectors are considered less effective than watermarking-based approaches, in which the embedded patterns are more distinct as well as stable than those general features. \n\nThe second category is the active watermarking approach. Similarly, most existing efforts focus on watermarking AI-generated text [1,5,16]. As the state-of-the-art, Kirchenbauer et al. [16] propose a method named WLLM (short for W atermarking LLMs) for AI-generated text detection. WLLM works by splitting a vocabulary into green tokens and red tokens, and softly promoting green tokens during text generation. WLLM reports promising results in detecting AI-generated text. However, it has limited effectiveness (see empirical evidence in Section 4) when it comes to AIgenerated codes. In general, less flexibility and encoding space are involved in codes compared to text, which makes watermarking codes more challenging than watermarking text. \n\nTo the best of our knowledge, only Lee et al. [19] attempt to detect LLM-generated codes based on watermarking.",
            "score": 0.602791823575373,
            "section_title": "Existing Approaches Fall Short",
            "char_start_offset": 9411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 136,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1294
                },
                {
                    "start": 1297,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2042
                },
                {
                    "start": 2045,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 463,
                    "end": 467,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1427,
                    "end": 1430,
                    "matchedPaperCorpusId": "221516138"
                },
                {
                    "start": 1430,
                    "end": 1432,
                    "matchedPaperCorpusId": "259092330"
                },
                {
                    "start": 1432,
                    "end": 1435,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 1482,
                    "end": 1486,
                    "matchedPaperCorpusId": "256194179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.748046875
        },
        {
            "corpus_id": "264439054",
            "title": "Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey",
            "text": "In the light of ownership and practical usability, AI-generated text detectors can be categorized into two distinct groups: \"Prepared\" and \"Post-hoc\" detectors. Prepared detection scheme primarily include watermarking and retrieval-based detectors, which involves proactive involvement of the model proprietor during the text generation process. In contrast, Post-hoc detectors encompass zero-shot detection techniques or fine-tuned classifiers which can be used by external parties.",
            "score": 0.6027879731705583,
            "section_title": "Towards the Possibilities of AI-generated Text Detection",
            "char_start_offset": 16521,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 483
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7412109375
        },
        {
            "corpus_id": "259308864",
            "title": "Provable Robust Watermarking for AI-Generated Text",
            "text": "Watermarking natural languages. The concept of watermarking, which involves hiding identifying information within data, has a long history. However, watermarking digital text has been challenging due to its discrete nature [Stefan et al., 2000]. Early approaches relied on techniques such as synonym substitution [Topkara et al., 2006], syntactic structure restructuring [Atallah et al., 2001], or paraphrasing [Atallah et al., 2002]. Later, advancements in modern neural language models led to improved methods that move away from rule-based approaches. Different approaches have been proposed, such as encoding messages by context-aware lexical substitution [Yang et al., 2022] or using mask-infilling models for editing text [Ueoka et al., 2021]. \n\nRecent studies [Zhao et al., 2023, Kirchenbauer et al., 2023] explore modifying the logits of language models during token generation and embedding invisible watermarks in the decoding process. Our objective is to develop a robust watermarking technique for natural language models that maintain high text quality while effectively concealing identifying information. \n\nPost-hoc detection. Rather than watermarking, an alternative approach involves developing detection models for post-hoc analysis of machine-generated text. Some detection methods use statistical outlier detection techniques without requiring additional training. For example, GLTR [Gehrmann et al., 2019] assesses the expected probability of individual tokens and applies thresholding to identify AI-generated content. DetectGPT [Mitchell et al., 2023] suggests that AI-generated passages tend to reside in the negative curvature of the log probability of texts. Another set of methods relies on classifiers that are fine-tuned to distinguish between human-written and machine-generated text. Initial efforts in this domain focus on detecting fake reviews [Hovy, 2016] and fake news [Zellers et al., 2019]. More recently, OpenAI releases a web interface that uses a finetuned GPT model for this discrimination task [OpenAI, 2023b]. However, as language models improve, AI-generated text is becoming increasingly similar to human-generated text, making it more challenging to detect.",
            "score": 0.6027136270209278,
            "section_title": "A More on related work",
            "char_start_offset": 30095,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 749
                },
                {
                    "start": 752,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1119
                },
                {
                    "start": 1122,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 335,
                    "matchedPaperCorpusId": "3061822"
                },
                {
                    "start": 411,
                    "end": 433,
                    "matchedPaperCorpusId": "8903750"
                },
                {
                    "start": 660,
                    "end": 679,
                    "matchedPaperCorpusId": "245144237"
                },
                {
                    "start": 728,
                    "end": 748,
                    "matchedPaperCorpusId": "233307237"
                },
                {
                    "start": 1403,
                    "end": 1426,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 1878,
                    "end": 1890,
                    "matchedPaperCorpusId": "5315990"
                },
                {
                    "start": 1905,
                    "end": 1927,
                    "matchedPaperCorpusId": "168169824"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "260334887",
            "title": "Towards Codable Watermarking for Injecting Multi-Bits Information to LLMs",
            "text": "Recently, with the explosive development of Large Language Models (LLMs) (OpenAI, 2022;Touvron et al., 2023), there has been growing concern in the community about the potential negative effects of the AI-generated content (AIGC). For instance, LLMs could be exploited to produce fake news, encyclopedia entries, or academic papers. Hence, there is an urgent need to reliably distinguish between human-written and AI-generated texts. \n\nText watermarking (Jalil & Mirza, 2009;Venugopal et al., 2011) aims to inject hidden patterns into the generated text, and detect the specific patterns to determine the source of text. The most representative line of LLM watermarking methods (Kirchen. et al., 2023a;Lee et al., 2023;Zhao et al., 2023) injects the watermark by controlling the available part of the vocabulary during LLM's decoding process and can detect whether a text contains a watermark with high accuracy and low false positive rate. However, we argue that these existing LLM watermarking methods encode too Table 1: Taxonomy of LLM watermarking technologies, with a representative work of each direction attached. It can be found that existing LLM watermarking methods either do not make full use of the generation ability of LLMs, or lack customized watermarking information. Our work simultaneously addresses both of these issues, filling the gap in this line of academic research.",
            "score": 0.6018385784565836,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 433
                },
                {
                    "start": 436,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1391
                }
            ],
            "ref_mentions": [
                {
                    "start": 454,
                    "end": 475,
                    "matchedPaperCorpusId": "13902076"
                },
                {
                    "start": 475,
                    "end": 498,
                    "matchedPaperCorpusId": "15210695"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93359375
        },
        {
            "corpus_id": "265506006",
            "title": "I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text",
            "text": "A number of researchers focused on developing machine learning models to identify generated texts. For instance, Fagni et al. (2021) report that transformer based classifiers to be the best discriminators of fake tweets. Guo et al. (2023) collect a dataset of ChatGPT's and experts' responses to questions in various topics such as finance and medicine. and train classifiers to predict if a given passage is computer generated. A similar approach is also followed by the creators of ChatGPT with under-whelming results2 . In our work, we propose a watermarking method to detect generated texts. We discuss watermarking methods in the literature in the following section. Abdelnabi and Fritz (2020) introduce Adversarial Watermarking Transformer (AWT) model, which encodes binary messages in text to trace its origin and prevent malicious use, using a jointly trained encoder-decoder and adversarial training, ensuring the watermark is discreet while maintaining the text's original meaning. Ueoka et al. (2021) proposes using a masked language model, which has a high payload capacity and is less susceptible to automatic detection than generation-based methods.",
            "score": 0.6018015406421038,
            "section_title": "Gehrmann",
            "char_start_offset": 5789,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1163
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 132,
                    "matchedPaperCorpusId": "220936111"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7236328125
        },
        {
            "corpus_id": "273228395",
            "title": "Signal Watermark on Large Language Models",
            "text": "As Large Language Models (LLMs) become increasingly sophisticated, they raise significant security concerns, including the creation of fake news and academic misuse. Most detectors for identifying model-generated text are limited by their reliance on variance in perplexity and burstiness, and they require substantial computational resources. In this paper, we proposed a watermarking method embedding a specific watermark into the text during its generation by LLMs, based on a pre-defined signal pattern. This technique not only ensures the watermark's invisibility to humans but also maintains the quality and grammatical integrity of model-generated text. We utilize LLMs and Fast Fourier Transform (FFT) for token probability computation and detection of the signal watermark. The unique application of signal processing principles within the realm of text generation by LLMs allows for subtle yet effective embedding of watermarks, which do not compromise the quality or coherence of the generated text. Our method has been empirically validated across multiple LLMs, consistently maintaining high detection accuracy, even with variations in temperature settings during text generation. In the experiment of distinguishing between human-written and watermarked text, our method achieved an AUROC score of 0.97, significantly outperforming existing methods like GPTZero, which scored 0.64. The watermark's resilience to various attacking scenarios further confirms its robustness, addressing significant challenges in model-generated text authentication.",
            "score": 0.6013451598477597,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9560546875
        },
        {
            "corpus_id": "270765610",
            "title": "L3i++ at SemEval-2024 Task 8: Can Fine-tuned Large Language Model Detect Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text?",
            "text": "The rise of large language models (LLMs) has led to a significant step forward in producing remarkably controllable, fluent, and grammatical text, triggering a surge in machine-generated content across diverse platforms such as news, social media, question-answering forums, educational, and even academic contexts.Notably, recent LLMs like ChatGPT 1 and GPT-4 (OpenAI, 2023) exhibit a remarkable ability to generate coherent and contextually appropriate responses to a wide array of user queries.\n\nUnfortunately, use and abuse come hand in hand.Although the fluency of these generated texts positions LLMs as potential candidates for replacing human labor in numerous applications, this has 1 https://chat.openai.com/also raised concerns about their potential for misuse, particularly in spreading misinformation and causing disruptions within the education system.Given that humans struggle to distinguish between machine-generated and human-written text, it becomes imperative to develop automated systems capable of identifying machine-generated text to curb the risks associated with its misuse.\n\nIn this paper, as the participants in SemEval-2024 Task 8: Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection (Wang et al., 2024), we investigate the feasibility of training a classifier that can reliably differentiate between text generated by humans and text that appears human-like but is generated by machines in two paradigms:\n\n\u2022 Subtask A: Given a full text, determine whether it is human-written or machinegenerated in monolingual (only English sources) and multilingual versions.\n\n\u2022 Subtask B: Given a full text, determine who generated it (human-written or generated by a specific language model).\n\nTo address these problems, we explore the performance of diverse methodologies, which can be divided into three categories, including:\n\n\u2022 Five different metric-based methods: Log-Likelihood, Rank, Log-Rank, Entropy, and DetectGPT (He et al., 2023).\n\n\u2022 Two traditional sequence-labeling language models: monolingual RoBERTa large 2 (Liu et al., 2019) and multilingual XLM-R large 3 (Conneau et al., 2020).",
            "score": 0.601038750156918,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 315
                },
                {
                    "start": 315,
                    "end": 497
                },
                {
                    "start": 499,
                    "end": 546
                },
                {
                    "start": 546,
                    "end": 718
                },
                {
                    "start": 718,
                    "end": 866
                },
                {
                    "start": 866,
                    "end": 1100
                },
                {
                    "start": 1102,
                    "end": 1471
                },
                {
                    "start": 1473,
                    "end": 1627
                },
                {
                    "start": 1629,
                    "end": 1746
                },
                {
                    "start": 1748,
                    "end": 1882
                },
                {
                    "start": 1884,
                    "end": 1996
                },
                {
                    "start": 1998,
                    "end": 2152
                }
            ],
            "ref_mentions": [
                {
                    "start": 1250,
                    "end": 1269,
                    "matchedPaperCorpusId": "269292978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92529296875
        },
        {
            "corpus_id": "271161969",
            "title": "Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text",
            "text": "The remarkable boost in human-like text generation performances achieved by Large Language Models (LLMs) in recent years has determined a rising challenge in detecting whether and to what extent texts have been generated by humans or machines [21,49,39]. In this context, the \"watermarking\" paradigm rapidly gained attention [25,53,28,50], as it allows embedding specific signals into generated texts that remain invisible to humans but are algorithmically detectable. Statistical learning methods also offer advanced solutions for detecting the authorship of texts. These include probabilistic models [31,1,47,14], log rank information [38], perplexity [44], discourse motifs [24], and other statistical approaches [16,40,45] More recently, we have witnessed the emergence of deep learning to detect or attribute AI-generated content, which stands as a promising body of research. Researchers have been exploiting LLMs to detect generated text [18,46], using ChatGPT itself as a detector [2], or combining LLMs with topological aspects [43]. \n\nA very recent trend involves leveraging contrastive learning to handle textual information. Indeed, despite its origins in the computer vision domain, contrastive representation learning has been proven particularly effective in NLP contexts to improve research on semantic similarity related problems, such as text classification [33,10], spotting hate-speech [23], unveiling intents [55], and eventually detecting AI-generated text through domain adaptation [3] or domain adversarial training [4]. \n\nDespite the advancements in research on detection of AI text generation, each of the above mentioned approaches faces significant challenges. Watermarking approaches are conditioned by the con-crete possibility of watermarking a given text, leaving the detection of non-watermarked texts an open issue. Statistical learning methods typically require access to the models' internals or to information that might be unavailable, limiting their applicability. Yet, more importantly for the sake of comparison with our proposed approach, existing contrastive-learning-based methods require or are better developed when learning a separate model for each AI generator. \n\nContributions.",
            "score": 0.6007203267280924,
            "section_title": "Introduction",
            "char_start_offset": 2188,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1042
                },
                {
                    "start": 1045,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1544
                },
                {
                    "start": 1547,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2210
                },
                {
                    "start": 2213,
                    "end": 2227
                }
            ],
            "ref_mentions": [
                {
                    "start": 325,
                    "end": 329,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 329,
                    "end": 332,
                    "matchedPaperCorpusId": "259129912"
                },
                {
                    "start": 602,
                    "end": 606,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 716,
                    "end": 720,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 720,
                    "end": 723,
                    "matchedPaperCorpusId": "259108779"
                },
                {
                    "start": 989,
                    "end": 992,
                    "matchedPaperCorpusId": "260379022"
                },
                {
                    "start": 1376,
                    "end": 1380,
                    "matchedPaperCorpusId": "236155217"
                },
                {
                    "start": 1380,
                    "end": 1383,
                    "matchedPaperCorpusId": "249840771"
                },
                {
                    "start": 1406,
                    "end": 1410,
                    "matchedPaperCorpusId": "252818980"
                },
                {
                    "start": 1505,
                    "end": 1508,
                    "matchedPaperCorpusId": "261660497"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96337890625
        },
        {
            "corpus_id": "270380163",
            "title": "Enhancing Text Authenticity: A Novel Hybrid Approach for AI-Generated Text Detection",
            "text": "The rapid advancement of Large Language Models (LLMs) has ushered in an era where AI-generated text is increasingly indistinguishable from human-generated content. Detecting AI-generated text has become imperative to combat misinformation, ensure content authenticity, and safeguard against malicious uses of AI. We introduce an innovative mixed methodology that integrates conventional TF-IDF strategies with sophisticated machine learning algorithms, including Bayesian classifiers, Stochastic Gradient Descent (SGD), Categorical Gradient Boosting (CatBoost), and 12 instances of Deberta-v3-large models. Our method tackles the difficulties of identifying AI-produced text by combining the advantages of conventional feature extraction techniques with the latest advancements in deep learning models. Through extensive experiments on a comprehensive dataset, we demonstrate the effectiveness of our proposed method in accurately distinguishing between human and AI-generated text. Our approach achieves superior performance compared to existing methods. This research contributes to the advancement of AI-generated text detection techniques and lays the foundation for developing robust solutions to mitigate the challenges posed by AI-generated content.",
            "score": 0.600711109059142,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90869140625
        },
        {
            "corpus_id": "269292978",
            "title": "SemEval-2024 Task 8: Multidomain, Multimodel and Multilingual Machine-Generated Text Detection",
            "text": "Detecting machine-generated text is primarily formulated as a binary classification task (Zellers et al., 2019;Gehrmann et al., 2019a;Solaiman et al., 2019;Ippolito et al., 2019), naively distinguishing between human-written and machine-generated text.In general, there are two main approaches: the supervised methods (Wang et al., 2024a,b;Uchendu et al., 2021;Zellers et al., 2019;Zhong et al., 2020;Liu et al., 2022) and the unsupervised ones, such as zero-shot methods (Solaiman et al., 2019;Ippolito et al., 2019;Mitchell et al., 2023;Su et al., 2023;Hans et al., 2024).While supervised approaches yield relatively better results, they are susceptible to overfitting (Mitchell et al., 2023;Su et al., 2023).Meanwhile, unsupervised methods may require unrealistic white-box access to the generator.In the following, we provide background information on each subtask, respectively.\n\nSubtask A: Mono-lingual and Multi-lingual Binary Classification Given the prevalence of the binary classification task, various benchmarks assess model performance in both mono-lingual and multi-lingual settings.HC3 (Guo et al., 2023) compares ChatGPT-generated text with humanwritten text in English and Chinese, utilizing logistic regression models trained on GLTR Test-2 features (Gehrmann et al., 2019a) and RoBERTa (Liu et al., 2019)-based classifiers for detection.Benchmark results by Wang et al. (2024b) include evaluations of several supervised detectors, such as RoBERTa (Liu et al., 2019), XLM-R (Conneau et al., 2019), logistic regression classifier with GLTR features (Gehrmann et al., 2019b), and stylistic features (e.g., stylometry (Li et al., 2014), NELA (Horne et al., 2019) features).",
            "score": 0.6002298621965771,
            "section_title": "Background",
            "char_start_offset": 3589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 252,
                    "end": 574
                },
                {
                    "start": 574,
                    "end": 711
                },
                {
                    "start": 711,
                    "end": 801
                },
                {
                    "start": 801,
                    "end": 883
                },
                {
                    "start": 885,
                    "end": 1097
                },
                {
                    "start": 1097,
                    "end": 1356
                },
                {
                    "start": 1356,
                    "end": 1688
                }
            ],
            "ref_mentions": [
                {
                    "start": 89,
                    "end": 111,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 111,
                    "end": 134,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 361,
                    "end": 382,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 1268,
                    "end": 1292,
                    "matchedPaperCorpusId": "182952848"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59130859375
        },
        {
            "corpus_id": "252907813",
            "title": "Machine-Generated Text: A Comprehensive Survey of Threat Models and Detection Methods",
            "text": "A baseline approach to detection of machine generated text is performing text classification using generative models themselves, such as GPT-2 or Grover [143,169,197]. Generative models can themselves be used without fine-tuning to detect either their own outputs, or outputs from other (typically similar) generative models. Autoregressive generative models such as GPT-2, GPT-3, and Grover are uni-directional, with each token having an embedding that is dependent on the embeddings of preceding tokens. As a result, an embedding for a sequence of tokens can be created by appending a classification token [CLS] to the end of the input sequence, and using the embedding of this token as a feature vector for the entire sequence. Using these feature vectors, a labelled dataset of human and machine text can be used to train a linear layer of neurons for classifying whether an input sequence is produced by a machine or human. \n\nIt has been observed in multiple studies that smaller NLG models can be used to detect text generated by larger NLG models [41,169,197]. While the ability of a model to detect larger models does diminish as the difference in scale grows, the predictive ability of smaller architectures may be useful as recreating large multi-billion parameter Transformer architectures is highly compute-intensive. \n\nGrover, a model trained for generation and detection of \"neural fake news\", demonstrates strong zero-shot detection performance specifically within the news domain it was trained on [197], but shows limited performance on out-of-domain text [169,182]. While it was initially suggested by Grover's authors that the best detection method for generative models may be generative models themselves [197], further investigation has shown that the increased representational power of bi-directional Transformer models appears to hold an advantage for machine generated text detection [169]. \n\nSimilar to the weakness of Grover outside of the news domain, it has been found that the zero-shot approach generally underperforms a simple TF-IDF baseline when trying to detect output from a generative model that has been fine-tuned on a different domain [169].",
            "score": 0.5997359006721691,
            "section_title": "Zero-shot Approach.",
            "char_start_offset": 73872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1329
                },
                {
                    "start": 1332,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1916
                },
                {
                    "start": 1919,
                    "end": 2182
                }
            ],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 1062,
                    "end": 1066,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 1514,
                    "end": 1519,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 1578,
                    "end": 1582,
                    "matchedPaperCorpusId": "221835708"
                },
                {
                    "start": 1726,
                    "end": 1731,
                    "matchedPaperCorpusId": "168169824"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85498046875
        },
        {
            "corpus_id": "269157398",
            "title": "Detecting AI Generated Text Based on NLP and Machine Learning Approaches",
            "text": "By developing an AI-detecting framework that can differentiate between artificially generated and human-crafted literature.The detection technique not only resolves existing moral and legal issues, but it also lays the foundation for promoting reasonable and approachable behavior in the expanding field of AI-generated interactions.Recently, AI language models have shown remarkable ability to generate text that mimics human writing.These models use massive data and advanced algorithms to generate coherent and contextually appropriate writing across many themes and styles.This has led to many applications in content generation, virtual assistants, and automated customer support, but it has also raised concerns about the misuse of AI-generated text for malicious purposes like misinformation, public opinion manipulation, and fraud.Thus, there is a rising need to identify and limit AI-generated material, especially on online platforms where it might be difficult to discern between human and AIgenerated writing.Language's intricacy and small differences between human and machine-generated text make spotting AI-generated material difficult.AI language models excel in mimicking human language's syntactic and semantic structures, but they frequently show evidence of their non-human origin.AI-generated writing may be incoherent, inconsistent, or implausible, deviating from human language conventions.AIgenerated writing may also expose training data biases or preferences, making it harder to discern between human and AI-authored material.NLP and machine learning methods for AI-generated text detection have been developed to overcome these issues.These methods analyze text's linguistic, statistical, or behavioral aspects to find abnormalities or departures from human-generated material.Stylometric analysis, anomaly detection, and adversarial testing are common methods.Stylometric analysis examines vocabulary usage, sentence structure, and writing style.Anomaly detection identifies textual anomalies compared to a baseline of human-authored content.",
            "score": 0.5993717790312357,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2446,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 123,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 435
                },
                {
                    "start": 435,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 839
                },
                {
                    "start": 839,
                    "end": 1021
                },
                {
                    "start": 1021,
                    "end": 1151
                },
                {
                    "start": 1151,
                    "end": 1301
                },
                {
                    "start": 1301,
                    "end": 1413
                },
                {
                    "start": 1413,
                    "end": 1553
                },
                {
                    "start": 1553,
                    "end": 1663
                },
                {
                    "start": 1663,
                    "end": 1805
                },
                {
                    "start": 1805,
                    "end": 1889
                },
                {
                    "start": 1889,
                    "end": 1975
                },
                {
                    "start": 1975,
                    "end": 2071
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87841796875
        },
        {
            "corpus_id": "264129505",
            "title": "Using AI-based detectors to control AI-assisted plagiarism in ESL writing: \u201cThe Terminator Versus the Machines\u201d",
            "text": "Zero-shot classifiers employ a pre-trained generative model (e.g., GPT-2) to identify texts generated by itself or comparable models without additional supervised training (Jawahar et al., 2020). An example of a zero-shot classifier is Gehrmann et al. ( 2019)'s Giant Language Model Test Room (GLTR), which utilizes BERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019) models to discriminate between machine-generated and human-written texts based on the probability of word distribution in these models (i.e., the likelihood that the distribution of words in a text was generated by a given model). \n\nThe researchers tested GLTR empirically with machine-generated and human-written articles from different sources. The analysis revealed that GLTR offered frequency and ranking information that improved human raters' accuracy in detecting machine-generated texts. Similarly, Solaiman et al. ( 2019) used a zero-shot detector based on GPT-2 model to assess text origin based on total log probability (i.e., GPT-2 estimation of the likelihood that word distribution in a text is similar to its own generated text). However, they found that it performed poorly compared to a simple classifier. Similarly, Mitchell et al. (2023) developed a zero-shot classifier based on GPT-3, DetectGPT, that detects GPT-generated texts based on their log probability. The researchers conducted several experiments comparing the performance of DetectGPT to previous zero-shot and two fine-tuned classifiers. The experiments revealed that DetectGPT outperformed previous zero-shot classifiers, but underperformed fine-tuned classifiers on general domain topics. These studies indicate that zero-shot classifiers perform poorly in AI text detection but can offer data that aid human-rating of AI-texts. 2019) developed a linear classifier with the GROVER model to detect its own generated fake news articles. The researchers compared GROVER fine-tuned classifier to several fine-tuned classifiers: BERT, GPT-2, and FastText.",
            "score": 0.5992346010426055,
            "section_title": "Zero-shot classifiers",
            "char_start_offset": 23093,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2011
                }
            ],
            "ref_mentions": [
                {
                    "start": 353,
                    "end": 375,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9365234375
        },
        {
            "corpus_id": "251492973",
            "title": "A Comprehensive Survey of Natural Language Generation Advances from the Perspective of Digital Deception",
            "text": "further study has found, perhaps unsurprisingly, that there appears to be little overlap between the cues typically associated with human-crafted deception, and the cues of deception that denote a machine-generated text.\n\nGiven this, recent efforts have been made to develop bespoke systems dedicated to the task of distinguishing between human-created and machine-generated text [216]. This has been of particular concern as studies indicate that current stateof-the-art NLG systems are often able (context depending) to avoid detection by humans, with human detectors identifying generated texts at just a \"chance\" level in some circumstances [216]. Given this, it is becoming increasingly important that detection systems capable of effectively identifying generated texts are developed. In turn, there are a variety of overarching paradigms that have been proposed for developing NLG detectors [216]:\n\nSupervised Systems: Using similar approaches to those adopted by human-written deception detection systems, this typically involves the training of classical ML algorithms (e.g., logistic regression, support vector machines, decision trees) to detect machine-generated text. Rather than using specificallydefined features (such as the linguistic and sentiment features discussed above), these methods more commonly leverage a bag-of-words approach, typically using basic n-gram frequencies or term frequency-inverse document frequencies (TF-IDF). Whilst reasonable performances are achievable, these detectors are typically limited to the specific domain (e.g., Amazon reviews) in which they are trained, showing reduced capabilities towards detecting generated texts even when applied to related domains. Moreover, these approaches have been found to suffer in performance considerably when used to detect generated texts produced by the larger, state-of-the-art LMs typically used in more recent NLG systems.\n\nZero-Shot Classifiers: A more recent approach is the use of existing pre-trained LMs as zero-shot classifiers to detect generated texts produced by the same, or similar pre-trained LMs. To do this, the overall likelihood of the input text (to be classified as machine-generated, or not) being generated, according to the LM detector, is compared to the likelihoods of both machine-generated and human-written reference texts. Whilst the zero-shot nature of these approaches would have distinct advantages in adaptability and",
            "score": 0.5992051563329845,
            "section_title": "B. Detecting Deceptive Text",
            "char_start_offset": 217064,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9111328125
        },
        {
            "corpus_id": "276287904",
            "title": "Modification and Generated-Text Detection: Achieving Dual Detection Capabilities for the Outputs of LLM by Watermark",
            "text": "The powerful generative capabilities of LLMs have greatly enhanced the human capabilities to create text. Whether in literary creation, news writing, or technical documentation, people can complete work more efficiently and quickly with the help of LLMs. However, this technological advance has also raised concerns about the abuse of LLMs. LLMs can generate creations that are difficult to distinguish from human works and potentially create misleading statements or false information [1,2]. Therefore, implementing practical detection tools to determine whether a text is generated by AI becomes particularly important [3,4]. \n\nWatermark is a promising method to reduce the risks of LLM abuse [5,6,7,8,9,10]. Previous watermarking methods [11,12,13,14,15,16,17] often identify machine-generated text based on statistics, which counts the number arXiv:2502.08332v2 [cs.CR] 1 Mar 2025 of tokens with watermark and compares it with the threshold to obtain the detection result. These methods can achieve strong robustness, as it is difficult for attackers to reverse the results of statistical detection by modifying a few tokens. \n\nFigure 1: The framework of dual detection capabilities for LLM-generated text by watermark. We analyze the discarded token(s) caused by modification, which fails to function as evidence for watermark detection. If the number of these tokens is larger than the threshold, it confirms the existence of modification. Meanwhile, we achieve generated-text detection by remaining tokens with watermark in the text. \n\nThe former watermarking methods do solve the problem of detecting machine-generated text. However, their detection strategy is insensitive to tiny modifications, and this advantage triggers another potential abuse. For instance, the text is vulnerable to tampering, leading to ambiguity or a direct distortion of the intended meaning. As shown in TABLE 1, watermarked texts can still have the watermark even after modification and are detected as machine-generated according to \"unpublished\" [18]. Modified watermarked text may be spread through the internet and then used as evidence, which could falsely attribute harmful content to the LLMs mentioned by Gloaguen in \"unpublished\" [19], making the company a scapegoat.",
            "score": 0.5983917142595262,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1540
                },
                {
                    "start": 1543,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2263
                }
            ],
            "ref_mentions": [
                {
                    "start": 486,
                    "end": 489,
                    "matchedPaperCorpusId": "258840876"
                },
                {
                    "start": 621,
                    "end": 624,
                    "matchedPaperCorpusId": "272330209"
                },
                {
                    "start": 624,
                    "end": 626,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 702,
                    "end": 704,
                    "matchedPaperCorpusId": "3783650"
                },
                {
                    "start": 704,
                    "end": 706,
                    "matchedPaperCorpusId": "262940018"
                },
                {
                    "start": 706,
                    "end": 709,
                    "matchedPaperCorpusId": "245144237"
                },
                {
                    "start": 741,
                    "end": 745,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 745,
                    "end": 748,
                    "matchedPaperCorpusId": "259308864"
                },
                {
                    "start": 748,
                    "end": 751,
                    "matchedPaperCorpusId": "258865409"
                },
                {
                    "start": 751,
                    "end": 754,
                    "matchedPaperCorpusId": "263834753"
                },
                {
                    "start": 754,
                    "end": 757,
                    "matchedPaperCorpusId": "260315804"
                },
                {
                    "start": 757,
                    "end": 760,
                    "matchedPaperCorpusId": "259092330"
                },
                {
                    "start": 760,
                    "end": 763,
                    "matchedPaperCorpusId": "264172471"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67236328125
        },
        {
            "corpus_id": "270766459",
            "title": "USTC-BUPT at SemEval-2024 Task 8: Enhancing Machine-Generated Text Detection via Domain Adversarial Neural Networks and LLM Embeddings",
            "text": "The detection of machine-generated text is often expressed as a classification task.One way to solve this problem is to use supervised learning to train classification models on datasets that contain both machine-generated and human-written text.For example, GPTZero (Tian, 2023) collects human-written text from a variety of domains, including student-written articles, news articles, and question-and-answer datasets across multiple disciplines.G3Detector (Zhan et al., 2023) claims to be a general-purpose gpt generated text detector implemented by fine-tuning RoBERTa-large (Liu et al., 2019), however, the effect of text detection generated by multiple generators will be poor.T5sentinel (Chen et al., 2023) trains RoBERTa and T5 (Raffel et al., 2023) classifiers on the OpenG-PTText dataset they built, and then uses the T5 model's ability to predict the conditional probability of the next word to classify multiple text sources.SeqXGPT (Wang et al., 2023a) introduces the sentence-level detection challenge by synthesizing a dataset containing documents that have been polished with a Large Language Model.Se-qXGPT uses sequence annotation methods to train its model and selects the most frequent class as sentence class, which provides a scheme for subtask C.However, a model explicitly trained to detect machine-generated text may overfit the training distribution of its domain (Bakhtin et al., 2019), resulting in poor generalization.\n\nIn addition, (Solaiman et al., 2019) notes the surprising power of a simple zero-shot method for machine-generated text detection, which thresholds candidate paragraphs based on their average log-probability under a generative model, a powerful baseline for many zero-shot learning machine-generated text detection tasks.Detect-GPT (Mitchell et al., 2023) demonstrates that text sampled from LLMs tends to occupy regions of negative curvature of the model's log-probability function.Building upon this observation, a new curvature-based criterion is defined to determine whether a paragraph is generated by a given LLM.",
            "score": 0.5978652757091485,
            "section_title": "Detecting LLM-Generated Text",
            "char_start_offset": 4496,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 84,
                    "end": 246
                },
                {
                    "start": 246,
                    "end": 447
                },
                {
                    "start": 447,
                    "end": 682
                },
                {
                    "start": 682,
                    "end": 936
                },
                {
                    "start": 936,
                    "end": 1114
                },
                {
                    "start": 1114,
                    "end": 1268
                },
                {
                    "start": 1268,
                    "end": 1446
                },
                {
                    "start": 1448,
                    "end": 1769
                },
                {
                    "start": 1769,
                    "end": 1931
                },
                {
                    "start": 1931,
                    "end": 2067
                }
            ],
            "ref_mentions": [
                {
                    "start": 1780,
                    "end": 1803,
                    "matchedPaperCorpusId": "256274849"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7890625
        },
        {
            "corpus_id": "274581321",
            "title": "A Practical Examination of AI-Generated Text Detectors for Large Language Models",
            "text": "There is a variety of related work that discusses text detectors. These works cover different aspects, such as the text detectors themselves, their types, evaluation, and red-teaming of detectors. \n\nText Detectors. Machine-generated text detectors can be divided into trained classifiers, zeroshot classifiers, and watermark methods (Yang et al., 2023b;Hans et al., 2024;Ghosal et al., 2023;Jawahar et al., 2020). (1) Trained detectors use classification models to determine if the text is machine-generated or human-written (Zellers et al., 2019b;Hovy, 2016;Hu et al., 2023;Tian and Cui, 2023;Verma et al., 2024). However, the increasing prevalence of machine-generated content (European-Union, 2022) makes it difficult to label human-generated work for training, as even humans find it hard to distinguish between the two (Darda et al., 2023). ( 2) Zero-shot detectors leverage intrinsic statistical differences between machine-generated and human-generated text (Gehrmann et al., 2019;Mitchell et al., 2023;Bao et al., 2024;Yang et al., 2023a;Venkatraman et al., 2024). Proposed methods include using entropy (Lavergne et al., 2008), log probability (Solaiman et al., 2019), and more recently, intrinsic dimensionality (Tulchinskii et al., 2023). ( 3) Watermark-based detection, introduced by Kirchenbauer et al. ( 2023), involves embedding a hidden but detectable pattern in the generated output. Various enhancements to this method have been suggested (e.g., Zhao et al. (2023); Lee et al. (2023)). This paper focuses on the black-box setting, which closely resembles real-world detection scenarios. \n\nWatermarking is not tested due to its guaranteed detectability and low false positive rates (e.g., (Zhao et al., 2023)).",
            "score": 0.597358006994824,
            "section_title": "Related Work and Background",
            "char_start_offset": 3510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 196
                },
                {
                    "start": 199,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1604
                },
                {
                    "start": 1607,
                    "end": 1727
                }
            ],
            "ref_mentions": [
                {
                    "start": 333,
                    "end": 353,
                    "matchedPaperCorpusId": "264439179"
                },
                {
                    "start": 353,
                    "end": 371,
                    "matchedPaperCorpusId": "267068867"
                },
                {
                    "start": 371,
                    "end": 391,
                    "matchedPaperCorpusId": "264439054"
                },
                {
                    "start": 391,
                    "end": 412,
                    "matchedPaperCorpusId": "226237099"
                },
                {
                    "start": 525,
                    "end": 548,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 548,
                    "end": 559,
                    "matchedPaperCorpusId": "5315990"
                },
                {
                    "start": 559,
                    "end": 575,
                    "matchedPaperCorpusId": "259501842"
                },
                {
                    "start": 594,
                    "end": 613,
                    "matchedPaperCorpusId": "258865787"
                },
                {
                    "start": 824,
                    "end": 844,
                    "matchedPaperCorpusId": "247070685"
                },
                {
                    "start": 965,
                    "end": 988,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 988,
                    "end": 1010,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1027,
                    "end": 1046,
                    "matchedPaperCorpusId": "258960101"
                },
                {
                    "start": 1046,
                    "end": 1071,
                    "matchedPaperCorpusId": "263830440"
                },
                {
                    "start": 1112,
                    "end": 1135,
                    "matchedPaperCorpusId": "12098535"
                },
                {
                    "start": 1222,
                    "end": 1248,
                    "matchedPaperCorpusId": "259108779"
                },
                {
                    "start": 1484,
                    "end": 1501,
                    "matchedPaperCorpusId": "258865409"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9228515625
        },
        {
            "corpus_id": "261822277",
            "title": "Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text",
            "text": "This is the probability distribution over the vocabulary of a model for the next token to be generated. Numerous successful detection methods evaluate the average log probability per token combined with a threshold in a zero-shot setting (Gehrmann et al., 2019;Ippolito et al., 2020;Mitchell et al., 2023). This method is model agnostic and generally performs quite well. At the time of writing, users of ChatGPT do not have access to these probabilities. Without this access or knowledge about model internals, detection methods are limited to using just the generated text in a binary classification setting, with the options being human or machine. These methods use simple classifiers trained on n-grams (Solaiman et al., 2019;Ippolito et al., 2020) or fine-tuned pre-trained language models (Uchendu et al., 2020;Ippolito et al., 2020;Zellers et al., 2020). \n\nAnother group of detection tools we want to mention are the human-machine collaboration systems, as Jawahar et al. (2020) labels them. These tools do not necessarily classify a passage directly but assist a human in making that decision. The previously mentioned work by Gehrmann et al. (2019) visualizes tokens in different colors, depending on where a given token ends up in the top-k most probable tokens from the model. This can also assist a human judge in spotting which part of a larger text might be machine-generated, such as possibly rephrased or copied sections for example. As mentioned, this method requires access to output probabilities, so it is not usable for ChatGPT. Another tool to help humans in the detection task is to outline the linguistic properties and characteristics of machine-generated text. This was one of the main goals of the Real or Fake Text? (RoFT) game created by Dugan et al. (2020Dugan et al. ( , 2023)). This game asked players to decide if a machine partially wrote a piece of text, and if yes, where the transition point from human to machine is in the text. This resulted in a considerable dataset of annotations and indicators humans look for in detecting machine-generated text.",
            "score": 0.59600006340342,
            "section_title": "Related Work on Detecting Machine-Generated Text",
            "char_start_offset": 4436,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2090
                }
            ],
            "ref_mentions": [
                {
                    "start": 238,
                    "end": 261,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 261,
                    "end": 283,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 731,
                    "end": 753,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 796,
                    "end": 818,
                    "matchedPaperCorpusId": "221835708"
                },
                {
                    "start": 818,
                    "end": 840,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 1136,
                    "end": 1158,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 1768,
                    "end": 1786,
                    "matchedPaperCorpusId": "221854248"
                },
                {
                    "start": 1786,
                    "end": 1809,
                    "matchedPaperCorpusId": "255125274"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7802734375
        },
        {
            "corpus_id": "270559095",
            "title": "Benchmarking of LLM Detection: Comparing Two Competing Approaches",
            "text": "LLM-generated texts play an increasingly significant role in a variety of domains.The emergence of LLMs such as ChatGPT has ensured that LLM-generated textual content can be easily generated and found on a massive scale on social networks, websites, in academic or creative writing.This can lead to problems, especially when AI is used for political misinformation, impersonation and forgery (Pr\u00f6hl et al. 2024).Detecting such texts plays a crucial role in reducing the potential damage caused by AI-generated texts.Therefore, several detectors of such texts have been implemented using many different approaches.This leads to the first research question (RQ 1): What are the current approaches for the detection of LLMgenerated texts?Biderman and Raff (2022) investigate the origin of the texts, while Zellers et al. (2019) deal with the investigation of spam, fake, and disinformation campaigns.Furthermore, the study of training data is important to find indicators that signal whether a text was written by a human or generated by an LLM (Bender et al. 2021;Crothers et al. 2022;Mirsky et al. 2023).\n\nText generated by earlier language models is relatively easy to identify, as these texts do not yet sound too natural.The development of transformer-based language models has greatly improved the generation of human-sounding text (Radford et al. 2019).This renders the original detection methods useless.\n\nCurrent approaches can be divided into white box and black box methods.White box methods use knowledge of how the model creates sequences to generate a signal.Krishna et al. (2023) follow the recording approach and Kirchenbauer et al. ( 2023) show possibilities of watermarking during generation.However, both approaches require full control over the generative language model and therefore cannot be applied to ChatGPT texts from OpenAI, for example.\n\nBlack box methods focus on post-hoc detection of LLM-generated texts.This has the advantage that no collaboration with the original developers of the models is necessary, as is the case with watermarking, for example.\n\nBlack box detection methods differ in their underlying approach but can be divided into two categories.The first category involves trained models for detection.",
            "score": 0.5956444987667666,
            "section_title": "Detection of LLMs",
            "char_start_offset": 2069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 82,
                    "end": 282
                },
                {
                    "start": 282,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 516
                },
                {
                    "start": 516,
                    "end": 613
                },
                {
                    "start": 613,
                    "end": 735
                },
                {
                    "start": 735,
                    "end": 897
                },
                {
                    "start": 897,
                    "end": 1103
                },
                {
                    "start": 1105,
                    "end": 1223
                },
                {
                    "start": 1223,
                    "end": 1357
                },
                {
                    "start": 1357,
                    "end": 1409
                },
                {
                    "start": 1411,
                    "end": 1482
                },
                {
                    "start": 1482,
                    "end": 1570
                },
                {
                    "start": 1570,
                    "end": 1707
                },
                {
                    "start": 1707,
                    "end": 1862
                },
                {
                    "start": 1864,
                    "end": 1933
                },
                {
                    "start": 1933,
                    "end": 2081
                },
                {
                    "start": 2083,
                    "end": 2186
                },
                {
                    "start": 2186,
                    "end": 2243
                }
            ],
            "ref_mentions": [
                {
                    "start": 392,
                    "end": 410,
                    "matchedPaperCorpusId": "267774373"
                },
                {
                    "start": 735,
                    "end": 759,
                    "matchedPaperCorpusId": "252110940"
                },
                {
                    "start": 803,
                    "end": 824,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 1042,
                    "end": 1062,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1083,
                    "end": 1102,
                    "matchedPaperCorpusId": "235683109"
                },
                {
                    "start": 1570,
                    "end": 1591,
                    "matchedPaperCorpusId": "257687440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83544921875
        },
        {
            "corpus_id": "273564837",
            "title": "Impact of Spelling and Editing Correctness on Detection of LLM-Generated Emails",
            "text": "Human communication is increasingly flooded by AIgenerated texts. LLMs suggest words and paragraphs or produce entire essays across chat, email, and social media. Therefore, there is a huge need for an effective method of detecting LLM-generated texts (LLMGT). \n\nSeveral approaches for LLMGT detection have been suggested and explored. Some researchers proposed watermarking or registering AI-generated content. The main idea of these approaches is that any organization developing a foundation model intended for public use must demonstrate a reliable detection mechanism for the content it generates as a condition of its public release. Knott et al. [5] proposed using watermarking as a solution for detecting LLMGT. The authors claim that searching for watermarks can be very effective. Another approach [6] relies on retrieving semantically- similar generations from a huge database with language model historical outputs. \n\nMany researchers use feature-based detection, which seems to be the most straightforward. It consists of extracting various text characteristics in hopes of finding differences between human-written and generated texts. Then, those characteristics are used to train a machine learning (ML) model that will be able to analyze them and produce classification results. Many possible characteristics can be measured; those include stylometry features like word frequency and sentence structure, LLMspecific features like perplexity [7] (measure representing how \"surprised\" the specific LLM model is when seeing the given text; human-written texts are usually characterized with higher perplexity scores than LLM generated ones) and burstiness [8] (metric based on words' distribution and the variance of sentence length in the text; humans tend to write with a less consistent style than an LLM might). Their effectiveness was demonstrated by Cingillioglu [9], who used linguistic, semantic, and stylistic features to train a support vector machine (SVM) classifier and got over 92% accuracy when detecting generated essays. This approach was also used by Fr\u00f6hling and Zubiaga [10], who observed that AI-generated texts exhibit 1) lack of syntactic and lexical diversity, 2) repetitiveness (tendency to overuse frequent words), 3) lack of coherence, and 4) lack of clear purpose or focus.",
            "score": 0.5955688675054529,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 2581,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 260
                },
                {
                    "start": 263,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 927
                },
                {
                    "start": 930,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2315
                }
            ],
            "ref_mentions": [
                {
                    "start": 653,
                    "end": 656,
                    "matchedPaperCorpusId": "264418045"
                },
                {
                    "start": 808,
                    "end": 811,
                    "matchedPaperCorpusId": "257687440"
                },
                {
                    "start": 1458,
                    "end": 1461,
                    "matchedPaperCorpusId": "121680873"
                },
                {
                    "start": 1670,
                    "end": 1673,
                    "matchedPaperCorpusId": "263828868"
                },
                {
                    "start": 1883,
                    "end": 1886,
                    "matchedPaperCorpusId": "258411796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9091796875
        },
        {
            "corpus_id": "273162917",
            "title": "RAFT: Realistic Attacks to Fool Text Detectors",
            "text": "To defend against these malicious use cases, various methods have been developed to successfully detect machine-generated text such as Ope-nAI's GPT-2 supervised detector (Solaiman et al., 2019), watermarking (Kirchenbauer et al., 2023), and likelihood-based zero-shot detectors such as DetectGPT (Mitchell et al., 2023). In response, redteaming methods for attacking machine-generated text detectors were created to identify vulnerabilities. Red-teaming methods are primarily based on paraphrasing or word substitution. Paraphrasingbased attacks such as DIPPER fine-tune a generative language model on a large set of manually collected paraphrase pairs (Krishna et al., 2023;Sadasivan et al., 2023). Word substitution-based attacks have leveraged masked language models or auxiliary LLMs to generate replacement candidates (Krishna et al., 2023;Shi et al., 2024). Despite their effectiveness in subverting various detectors, word substitution-based attacks often contain numerous grammatical errors and semantic inconsistencies that are readily discernible upon human evaluation. \n\nIn this work, we explore whether machinegenerated text can subvert detection with realistic perturbations that remain inconspicuous to human readers. A perturbation is considered realistic if it maintains part-of-speech (POS) consistency, minimally increases perplexity, and is indistinguishable from human-written text in manual evaluations. \n\nWe present RAFT, a zero-shot black-box attack framework to subvert machine-generated text detectors. RAFT leverages an auxiliary LLM embedding to optimally select words in machine-generated text for substitution by performing a proxy task. It then employs a black-box LLM to generate replacement candidates, greedily selecting the one that most effectively subverts the target detector. RAFT only requires access to an LLM's embedding layer, making it easily deployable and adaptable with the numerous powerful open-source LLMs available (Hugging Face Inc., 2022). \n\nFigure 1: RAFT can attack a sample text generated by GPT-3.5-turbo",
            "score": 0.5953975242671471,
            "section_title": "* These authors contributed equally to the work",
            "char_start_offset": 623,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1080
                },
                {
                    "start": 1083,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1425
                },
                {
                    "start": 1428,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1992
                },
                {
                    "start": 1995,
                    "end": 2061
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 236,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 297,
                    "end": 320,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 654,
                    "end": 676,
                    "matchedPaperCorpusId": "257687440"
                },
                {
                    "start": 824,
                    "end": 846,
                    "matchedPaperCorpusId": "257687440"
                },
                {
                    "start": 846,
                    "end": 863,
                    "matchedPaperCorpusId": "258987266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90625
        },
        {
            "corpus_id": "261822277",
            "title": "Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text",
            "text": "LLMs have become a driving force in many language processing-related benchmarks and tasks (Radford et al., 2018;Brown et al., 2020;Chowdhery et al., 2022). LLMs can solve complex NLP tasks and generate convincing and trustworthylooking text. However, they are also prone to generating false and misleading information, generally referred to as hallucinating (Lin et al., 2022). Additionally, misuse of these models can pose significant risks in academia, journalism, and many other areas. Currently, human judges are decent at spotting machine-generated text from older LLMs such as GPT-2 (Ippolito et al., 2020;Dugan et al., 2020Dugan et al., , 2023)). Still, the increasing abilities of LLMs give rise to the need for more sophisticated detection tools and models. A recent survey by Crothers et al. (2023) provides a thorough overview of risks, approaches, and detection methods. They discuss interesting aspects such as the effect of domains on the detection task, adversarial attacks, and societal impacts of generated texts. Work done by Jawahar et al. (2020) inspects the field of machine-generated text detection. It outlines three main detection methods: a classifier trained from scratch, zero-shot detection by a language model, and a fine-tuned language model as a classifier. Recently, detection methods from computer vision have also been tried on language models, such as watermarking (Kirchenbauer et al., 2023a,b) or trying to find model-identifying artifacts in generated content (Tay et al., 2020). To use and evaluate these methods, fine-grained access to the source model is required in training and inference time. Both these preconditions are not the case with ChatGPT at the time of writing. \n\nWhen discussing detection methods, an essential factor to consider is access to the log probability output of a model. This is the probability distribution over the vocabulary of a model for the next token to be generated.",
            "score": 0.5953873254236084,
            "section_title": "Related Work on Detecting Machine-Generated Text",
            "char_start_offset": 2599,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1715
                },
                {
                    "start": 1718,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1940
                }
            ],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 376,
                    "matchedPaperCorpusId": "237532606"
                },
                {
                    "start": 589,
                    "end": 612,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 612,
                    "end": 630,
                    "matchedPaperCorpusId": "221854248"
                },
                {
                    "start": 630,
                    "end": 652,
                    "matchedPaperCorpusId": "255125274"
                },
                {
                    "start": 1044,
                    "end": 1065,
                    "matchedPaperCorpusId": "226237099"
                },
                {
                    "start": 1498,
                    "end": 1516,
                    "matchedPaperCorpusId": "215754413"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80029296875
        },
        {
            "corpus_id": "264172471",
            "title": "Unbiased Watermark for Large Language Models",
            "text": "In recent years, large language models (LLMs) [21,42,43] have become an indispensable tool for a wide range of tasks, including text generation [29,11], translation [7,5], summarization [39], etc. With the escalating misuse of LLMs, such as plagiarism, tracking the usage of text generated by machines has become increasingly important. One viable method to monitor the usage of LLMs is watermarking [22,34,63], which embeds imperceptible information within the generated text, thereby allowing for efficient detection and tracking of the model's potential abuse. \n\nWatermarking techniques can serve multiple purposes, such as embedding ownership information within the generated text to protect the intellectual property rights of the model. It can also help mitigate potential harm caused by LLMs by monitoring where the model is being used and whether it is being misused or abused. \n\nA good watermarking method should not adversely affect the normal usage of the language model or degrade the quality of the generated text. However, a prevailing belief holds that there is an inevitable trade-off between the strength of the watermark and the quality of the output text. For instance, recent work by Kirchenbauer et al. [34] introduced a method that augmented the logits of a randomly selected set of \"green\" tokens. By tuning the \"magnitude of logits adjustment\", they demonstrated a trade-off between watermark strength and text quality. \n\nOur primary contribution is to challenge this conventional wisdom. We show that with the right implementation, watermarking can be accomplished without affecting the output quality. We refer to this particular type of watermark as an unbiased watermark. We approach the problem of output quality degradation from the perspective of watermark detection. We posit that if the watermark causes a decline in output quality, there should be a method to guess the presence of the watermark based on the quality. Conversely, if the watermark cannot be detected, it implies that the output quality remains unaffected. Specifically, we provide a proof that with a suitable implementation, watermarking does not affect the output probability distribution.",
            "score": 0.5953854245998953,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 885
                },
                {
                    "start": 888,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1443
                },
                {
                    "start": 1446,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2191
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 168,
                    "matchedPaperCorpusId": "28232901"
                },
                {
                    "start": 168,
                    "end": 170,
                    "matchedPaperCorpusId": "201741133"
                },
                {
                    "start": 186,
                    "end": 190,
                    "matchedPaperCorpusId": "201304248"
                },
                {
                    "start": 407,
                    "end": 410,
                    "matchedPaperCorpusId": "15210695"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92724609375
        },
        {
            "corpus_id": "276575227",
            "title": "Detecting Benchmark Contamination Through Watermarking",
            "text": "Overview. Recent advancements in watermarking techniques for decoder-only large language models (LLMs) involve altering either the probability distribution (Kirchenbauer et al., 2023a) or the method used for sampling the subsequent token (Aaronson and Kirchner, 2023;Kuditipudi et al., 2023). Detection of these watermarks is influenced by the entropy of the generated text (Christ et al., 2023;Huang et al., 2023), so further investigations propose watermarking only sections with high entropy, especially in code (Lee et al., 2023), while other studies explore \"semantic\" watermarks that rely on the semantic representation of the entire preceding text (Liu et al., 2023;Liu and Bu, 2024;Fu et al., 2024). \n\nGreen-list/Red-list watermark. This work focuses on the watermarking scheme proposed by Kirchenbauer et al. (2023b), which modifies the logit vector during token generation based on a context window of k previous tokens and a private key s. Both are hashed to serve as the seed for a random number generator (RNG) to create a \"greenlist\" of \u03b3|V| tokens. Logits of green tokens are incremented by \u03b4 to increase their sampling probability. Detection involves repeating the greenlist computation for each token of a text, incrementing a score by 1 if the token is in the greenlist, and performing a statistical test on the cumulative score. Under the null hypothesis H 0 , which corresponds to \"the text is not watermarked with that scheme\", this score follows a binomial distribution (Fernandez et al., 2023). \n\nRadioactivity of LLM watermarks. Sander et al. (2024) show that fine-tuning language models on LLM-generated watermarked question-answer pairs can be detected with high confidence, as the model retains traces of the watermark bias. The authors adapt the original watermark detection tests to detect this watermark \"radioactivity\", depending on the access to the suspect model and data.",
            "score": 0.5948363565988095,
            "section_title": "Decoding-based watermarking & Radioactivity",
            "char_start_offset": 7357,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 707
                },
                {
                    "start": 710,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1517
                },
                {
                    "start": 1520,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1905
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 395,
                    "matchedPaperCorpusId": "259092330"
                },
                {
                    "start": 673,
                    "end": 690,
                    "matchedPaperCorpusId": "247315260"
                },
                {
                    "start": 690,
                    "end": 706,
                    "matchedPaperCorpusId": "260164516"
                },
                {
                    "start": 1492,
                    "end": 1516,
                    "matchedPaperCorpusId": "260351507"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8857421875
        },
        {
            "corpus_id": "266369103",
            "title": "A Survey on the Detection and Impacts of Deepfakes in Visual, Audio, and Textual Formats",
            "text": "These are straightforward machine learning models trained to distinguish between two classes: human-generated and AI-generated text. Examples include logistic regression, decision trees, and Support Vector Machines (SVMs). These models often use various features of the text, such as word frequencies, sentence lengths, or more complex linguistic features, for classifications. Nguyen-Son et al. [236] devised an approach that takes advantage of the distinct statistical features inherent in computer-generated text to distinguish it from human-authored content. By focusing on specific language patterns, structure distribution, and frequency, the model reveals notable disparities in the statistical properties of human and machine-generated texts. An SVM classifier is utilised to categorise the text based on these unique features. The model was tested using a corpus of 100 English and Finnish books, with the former serving as human-generated examples and the latter translated to English via Google Translate as instances of machine-generated text. The model achieved an accuracy of 89.0% in detecting machinegenerated text. However, this approach has not been evaluated using the LLM. \n\nSolaiman et al. [237] presented a baseline approach using logistic regression trained on TF-IDF unigram and bigram features to detect AI-generated text. Their method achieved an accuracy between 74% and 88% in detecting outputs from models ranging from 124 million to 1.5 billion parameters, respectively. It was evaluated using GPT-2 [48] with an accuracy ranging from 93% to 97%. However, researchers found that shorter outputs of text were more challenging to detect than longer ones, and they anticipated that advanced generation strategies, such as nucleus sampling, might pose further difficulties for detection. Gall\u00e9 et al. [238] proposed an unsupervised and distributional method to identify machine-generated text within documents, discovering a subtle signal in higher-order n-grams, which tends to surface more in machine-generated text than in humanwritten content. This signal underpins a self-training setup in which documents with pseudo-labels are used to train an ensemble of classifiers. The method was shown to be effective in accurately ranking suspect documents in their experiments, with precision rates exceeding 90% for the largest GPT-2 model examined.",
            "score": 0.5939055398270929,
            "section_title": "2) METHODS BASED ON SIMPLE CLASSIFIERS",
            "char_start_offset": 84598,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1192
                },
                {
                    "start": 1195,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2201
                },
                {
                    "start": 2202,
                    "end": 2373
                }
            ],
            "ref_mentions": [
                {
                    "start": 396,
                    "end": 401,
                    "matchedPaperCorpusId": "36455310"
                },
                {
                    "start": 1530,
                    "end": 1534,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9013671875
        },
        {
            "corpus_id": "273163096",
            "title": "Efficiently Identifying Watermarked Segments in Mixed-Source Texts",
            "text": "Text watermarks in large language models (LLMs) are increasingly used to detect synthetic text, mitigating misuse cases like fake news and academic dishonesty. While existing watermarking detection techniques primarily focus on classifying entire documents as watermarked or not, they often neglect the common scenario of identifying individual watermark segments within longer, mixed-source documents. Drawing inspiration from plagiarism detection systems, we propose two novel methods for partial watermark detection. First, we develop a geometry cover detection framework aimed at determining whether there is a watermark segment in long text. Second, we introduce an adaptive online learning algorithm to pinpoint the precise location of watermark segments within the text. Evaluated on three popular watermarking techniques (KGW-Watermark, Unigram-Watermark, and Gumbel-Watermark), our approach achieves high accuracy, significantly outperforming baseline methods. Moreover, our framework is adaptable to other watermarking techniques, offering new insights for precise watermark detection.",
            "score": 0.5939048847005381,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9052734375
        },
        {
            "corpus_id": "263831345",
            "title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
            "text": "Our objective is the zero-shot detection of machine-generated text, treating the challenge as a binary classification problem (detailed in Appendix A). Given a passage x, which may be human-authored or produced by a source model, the goal is to discern whether it is machine-generated. \n\nIn the white-box setting, we have the privilege of accessing the possible source model that a passage is either written by a human or generated by this source model. We use the source model to aid in scoring the candidate passage to inform the classification decision in the setting. Conversely, in the black-box setting, we operate without access to the source model. Instead, we rely on surrogate models to score the passage. Underpinning this approach is the assumption that language models, due to their training on vast human-authored corpora, inherently share characteristic features.",
            "score": 0.5938684103142063,
            "section_title": "TASK AND SETTINGS",
            "char_start_offset": 5738,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 285
                },
                {
                    "start": 288,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 878
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9013671875
        },
        {
            "corpus_id": "273351050",
            "title": "WaterSeeker: Pioneering Efficient Detection of Watermarked Segments in Large Documents",
            "text": "Watermarking algorithms for large language models (LLMs) have attained high accuracy in detecting LLM-generated text. However, existing methods primarily focus on distinguishing fully watermarked text from non-watermarked text, overlooking real-world scenarios where LLMs generate only small sections within large documents. In this scenario, balancing time complexity and detection performance poses significant challenges. This paper presents WaterSeeker, a novel approach to efficiently detect and locate watermarked segments amid extensive natural text. It first applies an efficient anomaly extraction method to preliminarily locate suspicious watermarked regions. Following this, it conducts a local traversal and performs full-text detection for more precise verification. Theoretical analysis and experimental results demonstrate that WaterSeeker achieves a superior balance between detection accuracy and computational efficiency. Moreover, its localization capability lays the foundation for building interpretable AI detection systems. Our code is available at https://github.com/THU-BPM/WaterSeeker.",
            "score": 0.5928091501550059,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8984375
        },
        {
            "corpus_id": "265658859",
            "title": "New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking",
            "text": "With the increasing use of large-language models (LLMs) like ChatGPT, watermarking has emerged as a promising approach for tracing machine-generated content. However, research on LLM watermarking often relies on simple perplexity or diversity-based measures to assess the quality of watermarked text, which can mask important limitations in watermarking. Here we introduce two new easy-to-use methods for evaluating watermarking algorithms for LLMs: 1) evaluation by LLM-judger with specific guidelines; and 2) binary classification on text embeddings to distinguish between watermarked and unwatermarked text. We apply these methods to characterize the effectiveness of current watermarking techniques. Our experiments, conducted across various datasets, reveal that current watermarking methods are detectable by even simple classifiers, challenging the notion of watermarking subtlety. We also found, through the LLM judger, that watermarking impacts text quality, especially in degrading the coherence and depth of the response. Our findings underscore the trade-off between watermark robustness and text quality and highlight the importance of having more informative metrics to assess watermarking quality.",
            "score": 0.5921644148822731,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8955078125
        },
        {
            "corpus_id": "270062554",
            "title": "ReMoDetect: Reward Models Recognize Aligned LLM's Generations",
            "text": "Large Language Model (LLM) generated text detection.There are several approaches to detecting text generated by LLMs, mainly categorized in two: (i) training supervised detectors and (ii) zero-shot detection methods.The first category aims to train a binary classifier (or detector) that classifies LLM-generated texts (LGTs) and human-written texts.While effective, these methods can suffer from overfitting bias, where the detector performs well on the training data but fails to generalize detection on other LGTs [10].It is worth noting that such overfitting issues are also raised in other detection fields, such as out-of-distribution (OOD) detection [32,36].To address this, zero-shot detection methods have emerged as an alternative.These methods define a detection score on a pre-trained LLM, eliminating the need for fine-tuning and thus avoiding overfitting.For instance, using log-likelihood or entropy of the output prediction of the pre-trained LLMs to detect LGTs [19].More recently, several works have employed input text perturbation to measure prediction consistency, significantly improving the detection performance, e.g., DetectGPT [10], log-rank perturbation (NPR) [20], and Fast-DetectGPT [12].While effective, however, prior works have primarily focused on detecting non-aligned LLMs, while recent LLMs are designed to be aligned with human preferences for practical use.In this paper, we demonstrate that the reward model [26] can effectively distinguish between LLM-generated text and human-written text in a zero-shot setting.Based on this, we additionally consider supervised detector training of the reward model while mitigating overfitting biases through the replay technique [30].\n\nCharacteristics of aligned LLMs.[15,32].Moreover, there have been multiple works that utilized data augmentations such as mixup [46], i.e., linear interpolation of inputs and labels, to generate samples that behave like a near-decision boundary sample to improve the calibration [16,17].",
            "score": 0.5913152771853618,
            "section_title": "Related Work",
            "char_start_offset": 4490,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 52,
                    "end": 216
                },
                {
                    "start": 216,
                    "end": 350
                },
                {
                    "start": 350,
                    "end": 522
                },
                {
                    "start": 522,
                    "end": 665
                },
                {
                    "start": 665,
                    "end": 741
                },
                {
                    "start": 741,
                    "end": 869
                },
                {
                    "start": 869,
                    "end": 984
                },
                {
                    "start": 984,
                    "end": 1217
                },
                {
                    "start": 1217,
                    "end": 1395
                },
                {
                    "start": 1395,
                    "end": 1553
                },
                {
                    "start": 1553,
                    "end": 1712
                },
                {
                    "start": 1714,
                    "end": 1746
                },
                {
                    "start": 1746,
                    "end": 1754
                },
                {
                    "start": 1754,
                    "end": 2001
                }
            ],
            "ref_mentions": [
                {
                    "start": 517,
                    "end": 521,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 657,
                    "end": 661,
                    "matchedPaperCorpusId": "174802612"
                },
                {
                    "start": 661,
                    "end": 664,
                    "matchedPaperCorpusId": "220546269"
                },
                {
                    "start": 1153,
                    "end": 1157,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1212,
                    "end": 1216,
                    "matchedPaperCorpusId": "263831345"
                },
                {
                    "start": 1447,
                    "end": 1451,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1707,
                    "end": 1711,
                    "matchedPaperCorpusId": "53860287"
                },
                {
                    "start": 1746,
                    "end": 1750,
                    "matchedPaperCorpusId": "54558282"
                },
                {
                    "start": 1750,
                    "end": 1753,
                    "matchedPaperCorpusId": "174802612"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9404296875
        },
        {
            "corpus_id": "273638693",
            "title": "Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors",
            "text": "Large language models (LLMs) such as ChatGPT (OpenAI, 2023), Llama (Touvron et al., 2023a;b;Meta, 2024) and Mixtral (Jiang et al., 2024), have significantly influenced both the industrial and academic landscapes, with vast applications in news reporting, story writing, and academic research. However, there are growing concerns surrounding the misuse of these models, including the fabrication of fake news (Sun et al., 2024), the emergency of malicious content on website (Radivojevic et al., 2024), and the arise of plagiarism (Khalil & Er, 2023). Concerns regarding misinformation, plagiarism and copyright (Gao et al., 2022;Else, 2023) have prompted some scientific institutions to take a stance on the use of AI-generated content in research papers. In response to these challenges, there is an increasing emphasis on developing robust and reliable detection methods (Sadasivan et al., 2023;Lu et al., 2023;Valiaiev, 2024) for machine-generated texts. \n\nThe methods for detecting AI-generated text ranging from watermarking (Zhao et al., 2023;Kirchenbauer et al., 2023a;Singh & Zou, 2023), training-based methods for binary classifiers (Chen et al., 2023b;Guo et al., 2023;Yu et al., 2023;Li et al., 2023) to zero-shot methods (Bakhtin et al., 2019;Solaiman et al., 2019;Uchendu et al., 2020;Bao et al., 2023;Mitchell et al., 2023a;Yang et al., 2023a). While these detectors may provide temporary reassurance, their reliability and robustness for detecting machine-generated text remain uncertain.",
            "score": 0.5904035442120994,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1503
                }
            ],
            "ref_mentions": [
                {
                    "start": 530,
                    "end": 549,
                    "matchedPaperCorpusId": "256697594"
                },
                {
                    "start": 611,
                    "end": 629,
                    "matchedPaperCorpusId": "255187118"
                },
                {
                    "start": 1049,
                    "end": 1076,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 1076,
                    "end": 1094,
                    "matchedPaperCorpusId": "265658859"
                },
                {
                    "start": 1277,
                    "end": 1298,
                    "matchedPaperCorpusId": "221835708"
                },
                {
                    "start": 1315,
                    "end": 1338,
                    "matchedPaperCorpusId": "256274849"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8115234375
        },
        {
            "corpus_id": "265547756",
            "title": "Authorship Verification in software engineering education",
            "text": "The launch of ChatGPT received a rapid and controversial response early in 2023. Due to its ability to generate largely accurate, well-structured, original text, it is extremely easy for students to outsource their essays to ChatGPT and submit it as their own (Susnjak, 2022;OpenAI, 2023a). Confounding this is the fact that it is difficult to distinguish AI-generated text from human-written using traditional methods (Kasneci et al., 2023). This is further evidenced by the fact that OpenAI quietly removed its AI text classifier due to it's low rate of accuracy (OpenAI, 2023b). There have been several approaches in recent years to distinguish AI-generated text from human-authored text. Most notably, OpenAI released their own RoBERTa-based GPT-2 detector, which can be fine-tuned over newer Large Language Models (LLMs) (Kirchner et al., 2023). Some approaches involve zero-shot detection which avoids the overhead of training models (Ippolito et al., 2019;Mitchell et al., 2023). Another avenue to AI text generation is \"watermarking\" the text to ease detection by incorporating hidden patterns in the text that are unnoticeable and do not affect text quality, but algorithmically identify the text as synthetic (Kirchenbauer et al., 2023). Kirchenbauer et al. (2023) proposed a method of soft watermarking by creating a pseudo-random list of red and green tokens, and generating text whose tokens have a high probability of being from the green list. Non-watermarked text will, in contrast, have an equal probability of generating text with either red or green tokens. proposed a method of soft watermarking by creating a pseudo-random list of red and green tokens, and generating text whose tokens have a high probability of being from the green list. Non-watermarked text will, in contrast, have an equal probability of generating text with either red or green tokens. \n\nThese state-of-the-art methods are unreliable in practice.",
            "score": 0.5897408547287528,
            "section_title": "Background literature Detecting AI-authored text",
            "char_start_offset": 2620,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1878
                },
                {
                    "start": 1881,
                    "end": 1939
                }
            ],
            "ref_mentions": [
                {
                    "start": 419,
                    "end": 441,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 826,
                    "end": 849,
                    "matchedPaperCorpusId": "2545890"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81201171875
        },
        {
            "corpus_id": "267110558",
            "title": "Design and Implementation Plagiarism Checker Application with DetectGPT using Scheduler Algorithm",
            "text": "DetectGPT is a plagiarism detection tool developed to identify and distinguish text generated by generative AI (Mitchell et al. 2023). This tool uses a probability curve from a model probability function to determine whether a text was generated by generative AI or not (Mitchell et al. 2023). DetectGPT does not require separate classification training, collection of original or generated text datasets, or watermarking of generated text (Mitchell et al. 2023). The tool simply uses the log of probabilities (Yang et al. 2023), calculated by the model of interest and random noise of text from another generic pre-trained language model (Mitchell et al. 2023). \n\nBecause",
            "score": 0.5892068900110852,
            "section_title": "DetectGPT",
            "char_start_offset": 2236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 662
                },
                {
                    "start": 665,
                    "end": 672
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7412109375
        },
        {
            "corpus_id": "271270482",
            "title": "Survey on Plagiarism Detection in Large Language Models: The Impact of ChatGPT and Gemini on Academic Integrity",
            "text": "Zero-short approaches are those in which pretrained neural network models can predict unseen classes [107].While typical text classification models predict whether an observation falls into a specific class or not after training the model on a huge set of labelled datasets, the zero-shot learning approach utilizes the ability of pretrained language models to generalize unseen text observations or even new datasets [108].\n\nGiant Language Model Test Room (GLTR) is a pioneering work in AIGC detection based on zero-shot learning.Gehrmann et al. [109] developed GLTR that can detect whether a text is generated by a model or not with a proper visual footprint of generated text through the tool that also proves the prediction.Mireshghallah et al. [110] experimented to test whether other language models can be used to detect machine-generated text from one language model and found out that smaller language models like OPT-125M are better AIGC detectors than large language models like GPTJ-6B.DNA-GPT was introduced in the AIGC detection space, which claimed to surpass the result of OpenAI text detector [22] on four English and German datasets [111].The main idea of DNA-GPT is to compare the probability divergence between the actual tokens and generated tokens [111].\n\nDetect-GPT was another milestone in AIGC detection because it raised the state-of-the-art zero-shot detection of AIGC detection from the previous highest 0.81 AUROC to 0.95 AUROC [112].Detect-GPT was built on the principle that machine-generated text mostly occupies negative curvature regions of log probability generated from the model [112].\n\nRecently, Fast-DetectGPT increased the AUROC to 0.98 and obtained a speed 340 times faster compared to DetectGPT [113].Fast-DetectGPT introduced the notion of conditional probability curvature to clarify disparities in vocabulary selections observed between machine-generated text and human-written text [113].A summary of the research papers on AIGC detection based on zero-shot learning, along with their authors, published year and evaluation metrics, is represented in Table 7.  [116].",
            "score": 0.5891748591639676,
            "section_title": "Zero-shot Based Approaches",
            "char_start_offset": 26908,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 424
                },
                {
                    "start": 426,
                    "end": 531
                },
                {
                    "start": 531,
                    "end": 728
                },
                {
                    "start": 728,
                    "end": 998
                },
                {
                    "start": 998,
                    "end": 1157
                },
                {
                    "start": 1157,
                    "end": 1276
                },
                {
                    "start": 1278,
                    "end": 1463
                },
                {
                    "start": 1463,
                    "end": 1622
                },
                {
                    "start": 1624,
                    "end": 1743
                },
                {
                    "start": 1743,
                    "end": 1934
                },
                {
                    "start": 1934,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 1457,
                    "end": 1462,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 1616,
                    "end": 1621,
                    "matchedPaperCorpusId": "256274849"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9267578125
        },
        {
            "corpus_id": "268819898",
            "title": "AISPACE at SemEval-2024 task 8: A Class-balanced Soft-voting System for Detecting Multi-generator Machine-generated Text",
            "text": "Large Language Models (LLMs) have emerged as a foundational element for artificial intelligence (AI) applications.Their text generation capabilities are impressive and have almost reached the humanlevel performance.However, their widespread use also poses risks.The use of LLM-generated text can lead to the spread of inaccurate information, academic dishonesty, and privacy breaches.Additionally, the machine-generated text may become trapped in a loop during the process of LLMs' own development, gradually replacing human-written training data and reducing the quality and diversity of subsequent models (Wu et al., 2023).To prevent the misuse of LLMs and improve the iterative refinement of AI tools, it is crucial to distinguish between machine-generated and humanwritten text.SemEval-2024 Task 8: Multigener-ator, Multidomain and MultiLingual Black-Box Machine-Generated Text Detection (Wang et al., 2024) introduces the task of detecting machinegenerated text across various generators, domains and languages.Our system focuses on Subtask B, which is a multi-class classification task.It involves detecting text from multi-generators over multi-domains in English only.Given a text, the system tells whether the text is written by a human or generated by a particular LLM.It emphasizes not only the accuracy of detecting the in-domain texts, but also the generalization to identify other out-of-domain text sources.\n\nCurrent research on LLMs text detection primarily focuses on ChatGPT or a specific model in a limited domain.Gao et al., 2023 compares scientific writing between humans and ChatGPT exclusively.Wang et al., 2023b detects AI-generated news by ChatGPT.However, there are many other emerging LLMs that generate various domain texts that needed to be distinguished from those written by humans.Wang et al., 2023a presents a large-scale corpus generated by popular LLMs, including Chat-GPT, Cohere, Davinci, Bloomz, and Dolly, across various domains such as Wikihow, Wikipedia, Arxiv, PeerRead, and Reddit.",
            "score": 0.5887512955575485,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 114,
                    "end": 215
                },
                {
                    "start": 215,
                    "end": 262
                },
                {
                    "start": 262,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 625
                },
                {
                    "start": 625,
                    "end": 782
                },
                {
                    "start": 782,
                    "end": 1016
                },
                {
                    "start": 1016,
                    "end": 1092
                },
                {
                    "start": 1092,
                    "end": 1176
                },
                {
                    "start": 1176,
                    "end": 1279
                },
                {
                    "start": 1279,
                    "end": 1422
                },
                {
                    "start": 1424,
                    "end": 1533
                },
                {
                    "start": 1533,
                    "end": 1617
                },
                {
                    "start": 1617,
                    "end": 1673
                },
                {
                    "start": 1673,
                    "end": 1813
                },
                {
                    "start": 1813,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 892,
                    "end": 911,
                    "matchedPaperCorpusId": "270765847"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75634765625
        },
        {
            "corpus_id": "271051010",
            "title": "On Evaluating The Performance of Watermarked Machine-Generated Texts Under Adversarial Attacks",
            "text": "Watermark of machine-generated text has been extensively studied [27,61,67]. It can be viewed as a variant of steganography [10,47] with one-bit or multi-bit messages. \n\nThe text watermark has a strong requirement that the watermarked text and unwatermarked text are indistinguishable. These watermark schemes can be broadly divided into two categories: pre-text watermarks and post-text watermarks. The pre-text watermarks inject the watermark into texts before its generation which can occur during the training time [36,37,56,68], modify the logits [11,12,25,29,30,35,40,60,79] and modify the token sampling strategy [1,9,34] during the inference time. The Post-text watermarks inject the watermark into the generated texts, focusing on format-level [58,64] or lexical-level [77,78] modifications. \n\nIn this work, we mainly focus on watermarks against generated texts. Note that recent works also develop watermarks against model weights [3,36,63,69], which focus on protecting the intellectual property of the model. Cong et al. [69] develop SSLGuard, which is a watermarking scheme against self-supervised encoders. Li et al. [36] propose a watermarking scheme called weight quantization that modifies the model weight based on fine-tuning. This method embeds the watermark in the gap between full-precision weights and quantized weights. \n\nAnother line of works focuses on detecting whether the text is real or generated by models by training a classifier or setting threshold for pre-defined metrics [15,20,23,49,49,66]. Mitchell et al. [49] develop DetectGPT, which leverages multiple random perturbations to the text and measures the change of the model's log-likelihood. He et al. [23] propose MGTBench, which is a benchmarking framework including different types of detection methods.",
            "score": 0.588650817290183,
            "section_title": "Machine-Generated Text Watermark",
            "char_start_offset": 43306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 167
                },
                {
                    "start": 170,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 800
                },
                {
                    "start": 803,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1343
                },
                {
                    "start": 1346,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1795
                }
            ],
            "ref_mentions": [
                {
                    "start": 65,
                    "end": 69,
                    "matchedPaperCorpusId": "3783650"
                },
                {
                    "start": 72,
                    "end": 75,
                    "matchedPaperCorpusId": "33130131"
                },
                {
                    "start": 124,
                    "end": 128,
                    "matchedPaperCorpusId": "9554249"
                },
                {
                    "start": 523,
                    "end": 526,
                    "matchedPaperCorpusId": "265352433"
                },
                {
                    "start": 526,
                    "end": 529,
                    "matchedPaperCorpusId": "258741304"
                },
                {
                    "start": 753,
                    "end": 757,
                    "matchedPaperCorpusId": "17690312"
                },
                {
                    "start": 941,
                    "end": 944,
                    "matchedPaperCorpusId": "3322503"
                },
                {
                    "start": 950,
                    "end": 953,
                    "matchedPaperCorpusId": "246294509"
                },
                {
                    "start": 1033,
                    "end": 1037,
                    "matchedPaperCorpusId": "246294509"
                },
                {
                    "start": 1507,
                    "end": 1511,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 1514,
                    "end": 1517,
                    "matchedPaperCorpusId": "257766697"
                },
                {
                    "start": 1691,
                    "end": 1695,
                    "matchedPaperCorpusId": "257766697"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87939453125
        },
        {
            "corpus_id": "267681760",
            "title": "DE-COP: Detecting Copyrighted Content in Language Models Training Data",
            "text": "How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give approximately 4% accuracy. The code and datasets are available at https://github.com/LeiLiLab/DE-COP.",
            "score": 0.5885662838861269,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60986328125
        },
        {
            "corpus_id": "246607919",
            "title": "A Benchmark Corpus for the Detection of Automatically Generated Text in Academic Publications",
            "text": "The introduction of the statistical model GLTR (Giant Language model Test Room) (Gehrmann et al., 2019) can be considered as a major milestone for the detection of automatically generated text. The authors consider the stylometric details of texts by incorporating three types of tests: the probability of the word, the absolute rank of a word and the entropy of the predicted distribution. Afterwards, they compute per-token likelihoods and visualize histograms over them to support humans in detection of automatically generated content. A recent research (Al-Kadhimi and L\u00f6wenstr\u00f6m, 2020) has extended the work done in GLTR by feeding the output of GLTR to a Convolutional Neural Network, which automatically classifies whether the input reviews are human written or machine generated. Another turning point is the establishment of the GROVER (Zellers et al., 2019), in which it's architecture is a combination of a generation model and detection model. News are generated using a transformer based model which has an architecture similar to GPT-2 (Radford et al., 2019). But (Radford et al., 2019) have used conditional generation (on article meta data) and nucleus sampling. Afterwards, a zero shot detection is performed using a simple linear classifier on top of the pre-trained GROVER model. The authors have experimented detection with existing other models as well( fastText (Bojanowski et al., 2017) and BERT (Devlin et al., 2018)) reporting the highest accuracy for their own GROVER model and claiming that the best models in forming fake content are also the best models in detection. Contrary to that, Uchendu shows, however, that GROVER cannot correctly detect texts generated by language models other than GROVER itself. Lots of research have leveraged the RoBERTa (Liu et al., 2019), a masked and non-generative language model to detect automatically generated text. (Solaiman et al., 2019) has proved that the discriminative model of RoBERTa outperforms generative models such as GPT-2 in detection task. Such findings contra-dict with GROVER authors' claim that the generative model is better in detecting text generated by itself.",
            "score": 0.588373005322151,
            "section_title": "Automatic Detection of Machine Generated Text",
            "char_start_offset": 3931,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2150
                }
            ],
            "ref_mentions": [
                {
                    "start": 1385,
                    "end": 1410,
                    "matchedPaperCorpusId": "207556454"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93505859375
        },
        {
            "corpus_id": "258740888",
            "title": "Smaller Language Models are Better Black-box Machine-Generated Text Detectors",
            "text": "With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generator were trained on the same data is not critically important to the detection success. For instance the OPT-125M model has an AUC of 0.81 in detecting ChatGPT generations, whereas a larger model from the GPT family, GPTJ-6B, has AUC of 0.45.",
            "score": 0.5878007145726609,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9189453125
        },
        {
            "corpus_id": "265552122",
            "title": "MARKMyWORDS: Analyzing and Evaluating Language Model Watermarks",
            "text": "We now present MARKMYWORDS, our benchmark for evaluating watermarking schemes. MARKMYWORDS focuses on natural language and relies on three text generation tasks, each comprised of about 100 examples. They were chosen to generate long text, in order to ensure enough tokens to watermark most outputs and obtain a good size estimate (the number of tokens needed to detect a watermark, as defined in Section IV-B). They represent scenarios in which LLM could be abused and thus in which watermarking would be useful. \n\n(1) Book reports. Generate a report of a well-known book. \n\n(100 tasks) (2) Story generation. Generate a short story, with a specific tone (e.g., funny, sad, suspenseful) and topic (e.g., \"three strangers that win a getaway vacation\"). (96 tasks) (3) Fake news. Generate a news story about two political figures meeting at an event. (100 tasks) Our benchmark generates a total of 296 outputs from the language model, with a maximum of 1024 tokens per generation. We watermark the outputs of each task and measure quality and watermark size. We then perturb the generations to measure tamper resistance. We only attack the first third of each task to keep the benchmark runtime reasonable. On an A5000 GPU, the benchmark completes within 40 minutes for one combination of a watermarking scheme and parameter setting. Full task prompts are given in Section A. Validation tasks. In addition to the three main tasks, we validate watermarks on eight additional tasks for a more holistic evaluation. (V1) Domain-specific tasks. Generate RFC 3",
            "score": 0.5870653363908029,
            "section_title": "III. MARKMYWORDS",
            "char_start_offset": 10453,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1552
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6416015625
        },
        {
            "corpus_id": "278636542",
            "title": "From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models",
            "text": "With the rapid development of large language models (LLMs) and their widespread applications, incorporating watermarks into LLM-generated content facilitates traceability, thereby significantly enhancing transparency and accountability. Building on previous research, this paper seeks to achieve a balance among the detectability, text quality, security, and robustness of watermarks. We aspire for the framework proposed in this paper to offer novel insights into watermarking methodologies and to be further utilized in safeguarding intellectual property, curbing misinformation, and mitigating AIGC misuse, including academic fraud, thereby fostering public trust in AI technologies. All experiments were conducted on two NVIDIA A100 GPUs. Table 3 presents the average time required by several representative watermarking methods to generate and detect watermark texts of 200 tokens using OPT-6.7B. Our symbiotic watermarking strategy achieves nearly the same efficiency as existing methods in watermark detection. Although our hybrid watermarking method incurs additional computation time for token and semantic entropy during watermark text generation, this overhead remains acceptable in practical applications and contributes to enhanced robustness, security, and text quality. Furthermore, this overhead could be mitigated if entropy calculation were integrated into the Hugging Face 1 tool library in the future. 2023), we evaluated our method using the TOEFL dataset, comprising nonnative English writing samples, as shown in Figure 7. The experimental results show that our approach reliably identifies text with watermarks while nonnative English writing samples are susceptible to misclassification by existing AIGT (AI-generated text) detection methods. These findings highlight the practicality and reliability of our watermarking method, which achieves a near-zero FPR.",
            "score": 0.5869147925222,
            "section_title": "Ethical Statement",
            "char_start_offset": 22530,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1885
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8681640625
        },
        {
            "corpus_id": "260063149",
            "title": "Distinguishing Fact from Fiction: A Benchmark Dataset for Identifying Machine-Generated Scientific Papers in the LLM Era.",
            "text": "The ability to automatically generate convincing content has motivated researchers to work on its automatic detection, especially given its potential implications for various domains.\n\nSeveral approaches to detecting machinegenerated text have emerged, employing a range of techniques. Some studies have focused on utilizing hand-crafted features (Gehrmann et al., 2019), bagof-words features (Fagni et al., 2021), or neural features in combination with supervised models to distinguish between human and machine-generated content (Bakhtin et al., 2019;Ippolito et al., 2019;Fagni et al., 2021).\n\nAlternative approaches explore using the probability curvature of the generative model itself (Mitchell et al., 2023) or watermarking machinegenerated text to facilitate detection (Kirchenbauer et al., 2023).",
            "score": 0.5862099174027828,
            "section_title": "Detection of Machine-Generated Text",
            "char_start_offset": 5461,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87353515625
        },
        {
            "corpus_id": "271088409",
            "title": "ConvNLP: Image-based AI Text Detection",
            "text": "Recent advancements in Generative AI, particularly in Large Language Models (LLMs), have escalated concerns regarding the authenticity of digital content.The body of research focusing on differentiating AI-generated text from human-authored content has grown significantly.Alghamdi et al. (2022) provides a comprehensive comparison between machine learning and deep learning techniques for detecting fake news.Their study emphasizes the effectiveness of deep learning models in identifying nuanced patterns indicative of misinformation, a technique that can be paralleled in the detection of AI-generated texts, considering both domains require discerning subtle inconsistencies in content.Abdali et al. (2024) provides a detailed survey of existing methods for AI generated text detection and categorizes them as supervised (Bakhtin et al., 2019;Li et al., 2023;Quidwai et al., 2023), zero-shot (Su et al., 2023;Mitchell et al., 2023), retrieval-based (Krishna et al., 2023;Liang et al., 2023), watermarking (Kirchenbauer et al., 2023;Yang et al., 2023a), and feature (Yang et al., 2023b;Yu et al., 2023) detection based.\n\nExisting techniques for AI-generated text detection are fragile and susceptible to adversarial attacks involving paraphrasing (Krishna et al., 2023) and spoofing (Pang et al., 2024).With careful prompt engineering, LLM-generated text can be made increasingly harder to detect (Lu et al., 2023).\n\nSeveral tools are available for AI-generated text detection, including GPTZero (GPTZero, 2023), DetectGPT (DetectGPT), Turnitin (Turnitin), Copy-Leaks AI Content Detector (Copyleaks), and Plagium Originality AI Detector (Plagium).Due to the continual introduction of new LLMs, existing tools, trained on data generated using older language models, fail to generalize and have poor performance in detecting text generated using newer LLMs.",
            "score": 0.5860520010929519,
            "section_title": "Related Work",
            "char_start_offset": 4279,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 154,
                    "end": 273
                },
                {
                    "start": 273,
                    "end": 410
                },
                {
                    "start": 410,
                    "end": 690
                },
                {
                    "start": 690,
                    "end": 1122
                },
                {
                    "start": 1124,
                    "end": 1306
                },
                {
                    "start": 1306,
                    "end": 1418
                },
                {
                    "start": 1420,
                    "end": 1650
                },
                {
                    "start": 1650,
                    "end": 1858
                }
            ],
            "ref_mentions": [
                {
                    "start": 863,
                    "end": 884,
                    "matchedPaperCorpusId": "259165572"
                },
                {
                    "start": 913,
                    "end": 935,
                    "matchedPaperCorpusId": "256274849"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83984375
        },
        {
            "corpus_id": "274454618",
            "title": "MAGECODE: Machine-Generated Code Detection Method Using Large Language Models",
            "text": "Recently, the problem of machine-generated text identification has drawn a lot of attention and research efforts. Various methods have been proposed to differentiate text produced by LLMs from text written by humans. These methods can be broadly categorized into metric-based and model-based methods [3]. \n\nIn general, metric-based methods rely on pre-trained LLMs to analyze the text and extract statistical features from it, for example, the token-wise log probability or rank of each word in a document given its previous context [3]. This paper assesses six metric-based detection methods, including Log-Likelihood [13], Rank [14], Log-Rank [4], Entropy [14], GLTR [14], and LRR [15]. These metrics have been demonstrated in prior studies [3] to be rather effective in identifying machine-generated content from six LLMs, including ChatGLM [16], Dolly [17], ChatGPT-turbo [7], \n\n190188 VOLUME 12, 2024 Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply. \n\nGPT4All [18], StableLM [19], and Claude [20], across three distinct datasets. \n\nIn terms of model-based approaches, a classification model is developed by fine-tuning pre-trained language models with datasets that comprise both machine-generated and human-written texts [3], [13]. After the fine-tuning process, the developed classification model should be able to distinguish machine-generated content in the provided dataset. For example, OpenAIDetector [13] fine-tuned a RoBERTa model with texts produced by the largest GPT2 model. Similarly, ChatGPT Detector [21] was developed to detect ChatGPT-generated content by fine-tuning a RoBERTa model with the HC3 dataset as input. These methods have been demonstrated to achieve high accuracy in classifying text origins, even in challenging situations such as maintaining a false positive rate (FPR) below 0.1%. However, recent studies indicate that LLM-generated text detection methods are less effective when performed on source code data [22].",
            "score": 0.5852974200963202,
            "section_title": "C. RELATED WORKS 1) MACHINE-GENERATED TEXT DETECTION METHODS",
            "char_start_offset": 11745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 304
                },
                {
                    "start": 307,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1017
                },
                {
                    "start": 1020,
                    "end": 1097
                },
                {
                    "start": 1100,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2016
                }
            ],
            "ref_mentions": [
                {
                    "start": 645,
                    "end": 648,
                    "matchedPaperCorpusId": "256274849"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.916015625
        },
        {
            "corpus_id": "258714683",
            "title": "Watermarking Text Generated by Black-Box Language Models",
            "text": "LLMs now exhibit human-like skills in various fields, leading to worries about misuse. Thus, detecting generated text is crucial. However, passive detection methods are stuck in domain specificity and limited adversarial robustness. To achieve reliable detection, a watermark-based method was proposed for white-box LLMs, allowing them to embed watermarks during text generation. The method involves randomly dividing the model vocabulary to obtain a special list and adjusting the probability distribution to promote the selection of words in the list. A detection algorithm aware of the list can identify the watermarked text. However, this method is not applicable in many real-world scenarios where only black-box language models are available. For instance, third-parties that develop API-based vertical applications cannot watermark text themselves because API providers only supply generated text and withhold probability distributions to shield their commercial interests. To allow third-parties to autonomously inject watermarks into generated text, we develop a watermarking framework for black-box language model usage scenarios. Specifically, we first define a binary encoding function to compute a random binary encoding corresponding to a word. The encodings computed for non-watermarked text conform to a Bernoulli distribution, wherein the probability of a word representing bit-1 being approximately 0.5. To inject a watermark, we alter the distribution by selectively replacing words representing bit-0 with context-based synonyms that represent bit-1. A statistical test is then used to identify the watermark. Experiments demonstrate the effectiveness of our method on both Chinese and English datasets. Furthermore, results under re-translation, polishing, word deletion, and synonym substitution attacks reveal that it is arduous to remove the watermark without compromising the original semantics.",
            "score": 0.585118600672696,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87841796875
        },
        {
            "corpus_id": "270560542",
            "title": "From Intentions to Techniques: A Comprehensive Taxonomy and Challenges in Text Watermarking for Large Language Models",
            "text": "Large Language Models (LLMs) like Google's Gemini (Team et al., 2023), Meta's LLaMA 3 (Touvron et al., 2023), andOpenAI's GPT 4 (OpenAI, 2023) can mimic human-like comprehension and text generation (Zheng et al., 2024).Consequently, it is challenging to judge whether a text is authored by a human or generated by an LLM.This issue is highlighted by the recent lawsuit initiated by The New York Times against OpenAI and Microsoft, concerning the use of their articles as training data for AI models, emphasizing the urgent need for effective methods to identify and safeguard digital content ownership (New York Times Company, 2023).\n\nText Watermarking provides crucial solutions to protect intellectual property rights, iden-Email: f20212665@goa.bits-pilani.ac.in, {aza6352, dul13}@psu.edu,rajsanjayshah@gatech.edu tify ownership, and keep track of digital content.These techniques embed imperceptible signals or identifiers within digital text documents, which are then used to track the document's origins (Jalil and Mirza, 2009;Kamaruddin et al., 2018).In particular, they aid in tracking the different production sources of text, both human-written and LLM-generated, helping prevent their unauthorized without the owner's consent.Recently, many papers have been published in this direction, reflecting the growing research interest in the field.\n\nGiven this increasing research focus on watermarking techniques, it is important to review various methods, their applications, strengths and limitations.This includes the systematic categorization of current research literature and highlighting key open challenges.The following contributions of our work distinguish it from previous surveys:\n\n\u2022 Taxonomy Construction: We seek to help future researchers in navigating the field of textwatermarking by categorizing various techniques and methods.For this task, we focus on application-driven intentions, evaluation data sources, and watermark addition methods.We also enlist potential adversarial attacks against these methods to caution readers.\u2022 Open Challenge Identification: Next, we describe open challenges and gaps in current research efforts.",
            "score": 0.5847743327143073,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 219,
                    "end": 321
                },
                {
                    "start": 321,
                    "end": 633
                },
                {
                    "start": 635,
                    "end": 791
                },
                {
                    "start": 791,
                    "end": 866
                },
                {
                    "start": 866,
                    "end": 1057
                },
                {
                    "start": 1057,
                    "end": 1236
                },
                {
                    "start": 1236,
                    "end": 1351
                },
                {
                    "start": 1353,
                    "end": 1507
                },
                {
                    "start": 1507,
                    "end": 1619
                },
                {
                    "start": 1619,
                    "end": 1696
                },
                {
                    "start": 1698,
                    "end": 1849
                },
                {
                    "start": 1849,
                    "end": 1963
                },
                {
                    "start": 1963,
                    "end": 2049
                },
                {
                    "start": 2049,
                    "end": 2153
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 218,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 1009,
                    "end": 1032,
                    "matchedPaperCorpusId": "13902076"
                },
                {
                    "start": 1032,
                    "end": 1056,
                    "matchedPaperCorpusId": "3783650"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8466796875
        },
        {
            "corpus_id": "267897651",
            "title": "RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts",
            "text": "whether the text is AI-generated or human-written. The idea of watermarking initially emerged from the field of computer vision and has since been applied to NLP (Wu et al., 2023). This method involves embedding a hidden \"watermark\" during the text generation process with the objective of identifying text generated by a specific language model. In the context of black-box language models, (Yang et al., 2023) utilize this watermarking method to detect and identify text generated by such models. \n\nIn this work, we have used the dataset provided by the M4 article, which is a multilingual dataset. The human-written texts in this dataset were collected from diverse sources spanning different domains. These sources include Wikipedia (March 2022 version), WikiHow, Reddit (ELI5), arXiv, and PeerRead for English. For Chinese, the texts were sourced from Baike and Web question answering (QA). Additionally, texts from news sources were included for Urdu and Indonesian, while for Russian, texts were obtained from RuATD. For Arabic, the texts were collected from Arabic Wikipedia. For the monolingual section, we have used the English corpora. In this dataset, AI-generated texts leverage multilingual LLMs such as ChatGPT, textdavinci-003, LLaMa, FlanT5, Cohere, Dolly-v2, and BLOOMz. These models undertake diverse tasks, including creating Wikipedia articles from titles and abstracts (from arXiv), generating peer reviews from titles and abstracts (Peer-Read), answering questions from platforms like Reddit and Baike/Web QA, and composing news briefs based on the title. This dataset contains 122k human-machine parallel data in total, with 101k for English, 9k for Chinese, 9k for Russian, 9k for Urdu, 9k for Indonesian, and 9k for Arabic, respectively (Wang et al., 2023). \n\nFor our experiment, we used the English corpora of this dataset in the monolingual track.",
            "score": 0.5845151022307769,
            "section_title": "Background",
            "char_start_offset": 4202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 498
                },
                {
                    "start": 501,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1783
                },
                {
                    "start": 1786,
                    "end": 1875
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7197265625
        },
        {
            "corpus_id": "252907813",
            "title": "Machine-Generated Text: A Comprehensive Survey of Threat Models and Detection Methods",
            "text": "Machine generated text often differs from human text in ways that be identified using statistical techniques [41,61,133]. Feature-based approaches to machine generated text detection apply natural language processing to create feature vectors from input sequences, and classify these feature vectors using a downstream classification algorithm, such as a support-vector machines (SVM), random forest (RF), or neural network (NN) [61,133]. We provide a summary of the categories of features that have been used in prior art, with references for further reading on specific categories of features. \n\nAn important consideration in detection of machine generated text using feature-based approaches is that different language model sampling methods (e.g., top- versus top- sampling in Transformer language models, as discussed in \u00a72.3.3) may lead to different artifacts in the generated text [61,78]. As a result, performance of feature-based detection can be diminished when detecting text generated using a different sampling approach than that used to train the detection model [61]. A feature-based detector trained on output from a smaller model can be used to detect output from models of larger size [61,197], though it is more effective to use a detector trained on a larger model to detect outputs from smaller generative models [61]. \n\nWe now proceed with our summary of major feature categories in feature-based detection approaches.",
            "score": 0.5838273517634787,
            "section_title": "Feature-Based Approaches",
            "char_start_offset": 67225,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1339
                },
                {
                    "start": 1342,
                    "end": 1440
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 116,
                    "matchedPaperCorpusId": "233563022"
                },
                {
                    "start": 429,
                    "end": 433,
                    "matchedPaperCorpusId": "233563022"
                },
                {
                    "start": 888,
                    "end": 892,
                    "matchedPaperCorpusId": "233563022"
                },
                {
                    "start": 1077,
                    "end": 1081,
                    "matchedPaperCorpusId": "233563022"
                },
                {
                    "start": 1203,
                    "end": 1207,
                    "matchedPaperCorpusId": "233563022"
                },
                {
                    "start": 1207,
                    "end": 1211,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 1334,
                    "end": 1338,
                    "matchedPaperCorpusId": "233563022"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84619140625
        },
        {
            "corpus_id": "269605585",
            "title": "Vietnamese AI Generated Text Detection",
            "text": "In recent years, the natural language processing (NLP) community has witnessed a paradigm shift with the introduction of Large Language Models (LLMs) [1], [2].The formidable capabilities of LLMs have raised concerns about distinguishing between their generated texts and human-written content.These concerns stem from two main issues.Firstly, LLMs are prone to producing fabricated or outdated information, potentially leading to the spread of misinformation and plagiarism.Secondly, there is a risk of malicious use, including disinformation dissemination, online fraud, social media spam, and academic dishonesty by students using LLMs for essay writing.To address these concerns, it is essential to implement measures that enhance the accountability and accuracy of LLMs, ensuring responsible use across various domains.This detection task involves determining whether a given piece of text is generated by an LLM, essentially constituting a binary classification challenge.Recent advancements in detector techniques, driven by innovations in watermarking, zero-shot methods, finetuning language models, adversarial learning, LLMs as detectors, and human-assisted methods, have significantly enhanced the capabilities of LLM-generated text detection.\n\nIn this study, we introduce a dataset called ViDetect, a pioneering publicly available benchmark Detect AI Generated Text dataset for Vietnamese essay.Our main contributions are described as follow:\n\n1. Constructing ViDetect dataset: a first benchmark Detect AI Generated Text dataset on Vietnamese.ViDetect refer to the essays.This dataset consists of nearly 6.800 samples that have undergone rigorous quality control measures, ensuring the highest quality of the dataset.2. Conducting various experiments employing several state-of-the-art language models including ViT5, BARTpho, mDerbertav3, PhoBERT, and Bert-multilingual on the Vi-Detect AI Generated Text dataset.These models have been fine-tuned and evaluated to investigate their effectiveness for the task.3. Undertaking a comprehensive analysis of the limitations and challenges encountered during the development of the ViDetect dataset, providing valuable insights to guide future research endeavors.\n\nWriting topic Essay .......",
            "score": 0.5831978404962499,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 159,
                    "end": 293
                },
                {
                    "start": 293,
                    "end": 334
                },
                {
                    "start": 334,
                    "end": 474
                },
                {
                    "start": 474,
                    "end": 656
                },
                {
                    "start": 656,
                    "end": 823
                },
                {
                    "start": 823,
                    "end": 977
                },
                {
                    "start": 977,
                    "end": 1253
                },
                {
                    "start": 1255,
                    "end": 1406
                },
                {
                    "start": 1406,
                    "end": 1453
                },
                {
                    "start": 1455,
                    "end": 1554
                },
                {
                    "start": 1554,
                    "end": 1583
                },
                {
                    "start": 1583,
                    "end": 1728
                },
                {
                    "start": 1728,
                    "end": 1925
                },
                {
                    "start": 1925,
                    "end": 2021
                },
                {
                    "start": 2021,
                    "end": 2218
                },
                {
                    "start": 2220,
                    "end": 2247
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 153,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 155,
                    "end": 158,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8466796875
        },
        {
            "corpus_id": "265149688",
            "title": "WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models",
            "text": "To detect LLM-generated text, previous works (Tu et al., 2023;Guo et al., 2023;Mitchell et al., 2023) mainly explored the classifiers that distinguishes human and LLM-generated texts based on features. However, as LLMs are becoming more and more alike human, some classifiers may mistakenly recognize human as LLMs (Sadasivan et al., 2023). \n\nIn addition to black-box classifiers, recent approaches have also introduced white-box detection methods that inject watermarks into LLM generated texts (Tang et al., 2023;Yang et al., 2023;Liu et al., 2024). The inference-time watermarks (Pan et al., 2024) randomly split the vocabulary and ad-",
            "score": 0.5831558127212907,
            "section_title": "Related Work",
            "char_start_offset": 4922,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 638
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8701171875
        },
        {
            "corpus_id": "264439179",
            "title": "A Survey on Detection of LLMs-Generated Content",
            "text": "Text watermarking injects algorithmically detectable patterns into the generated text while ideally preserving the quality and diversity of language model outputs. Although the concept of watermarking is well-established in vision, its application to digital text poses unique challenges due to the text's discrete and semantic-sensitive nature (Kutter et al., 2000). Early works are edit-based methods that modify a pre-existing text. The earliest work can be dated back to Atallah et al. ( 2001), which designs a scheme for watermarking natural language text by embedding small portions of the watermark bit string in the syntactic structure of the text, followed by paraphrasing (Atallah et al., 2003), syntax tree manipulations (Topkara et al., 2005;Meral et al., 2009) and synonym substitution (Topkara et al., 2006). Besides, text watermarking has also been used for steganography and secret communication (Fang et al., 2017;Ziegler et al., 2019;Abdelnabi and Fritz, 2021), and intellectual property protection (He et al., 2022a,b;Zhao et al., 2022Zhao et al., , 2023b)), but this is out the scope of this work. In light of growing ethical considerations, text watermarking has been increasingly used to ascertain the origin of textual content and detect AI-generated content (Grinbaum and Adomaitis, 2022). The primary focus of this paper is on the use of text watermarking to detect AI-generated text. \n\nIn general, watermarking for text detection can also be classified into white-box and black-box watermarking. Watermarking is designed to determine whether the text is coming from a specific language model rather than universally detecting text generated by any potential model. As such, knowledge of the model source is always required in text watermarking for detection.",
            "score": 0.5825173330535727,
            "section_title": "Watermarking",
            "char_start_offset": 21255,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1409
                },
                {
                    "start": 1412,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1784
                }
            ],
            "ref_mentions": [
                {
                    "start": 682,
                    "end": 704,
                    "matchedPaperCorpusId": "8903750"
                },
                {
                    "start": 732,
                    "end": 754,
                    "matchedPaperCorpusId": "13415776"
                },
                {
                    "start": 754,
                    "end": 773,
                    "matchedPaperCorpusId": "1192689"
                },
                {
                    "start": 799,
                    "end": 821,
                    "matchedPaperCorpusId": "3061822"
                },
                {
                    "start": 912,
                    "end": 931,
                    "matchedPaperCorpusId": "10494183"
                },
                {
                    "start": 931,
                    "end": 952,
                    "matchedPaperCorpusId": "202537031"
                },
                {
                    "start": 952,
                    "end": 978,
                    "matchedPaperCorpusId": "221516138"
                },
                {
                    "start": 1037,
                    "end": 1054,
                    "matchedPaperCorpusId": "252762324"
                },
                {
                    "start": 1054,
                    "end": 1076,
                    "matchedPaperCorpusId": "256627372"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74169921875
        },
        {
            "corpus_id": "277322659",
            "title": "TempTest: Local Normalization Distortion and the Detection of Machine-generated Text",
            "text": "There has been a surge of interest in statistical methods for the detection of machine-generated text since the paradigm shift in language modeling triggered by the transformer architecture (Vaswani et al., 2017). \n\nThis version is the Author Accepted Manuscript. The final publication will appear at the 28th International Conference on Artificial Intelligence and Statistics (AISTATS 2025). \n\nThis task has growing societal importance, for example, in combating fake news (Ahmed et al., 2023) or ensuring academic integrity (Elkhatat, 2023). \n\nApproaches based on supervised learning typically struggle with out-of-domain generalization (Bao et al., 2024), leading research to focus on the zero-shot regime. To date, state-of-the-art zero-shot techniques are typically based on three fundamental statistical quantities: log-likelihood, log-rank, and entropy (Gehrmann et al., 2019). Although state-of-the-art methods such as DetectGPT (Mitchell et al., 2023), Fast-DetectGPT (Bao et al., 2024), DNA-GPT (Yang et al., 2024), and Detect-LLM (Su et al., 2023) have made remarkable progress using these measures, our work demonstrates that a wider suite of basic statistical quantities is beneficial. \n\nWe introduce a new zero-shot detection method, TempTest, based on a novel statistical quantity, Temp-Norm. This statistic is derived from the distortion of conditional probability measures caused by the way that temperature sampling renormalizes probability mass. Temperature sampling is a ubiquitous decoding strategy that allows fine control over the diversity or creativity of generated text. Similar techniques extend to top-k and nucleus (top-p) sampling, although the signal for top-p is very small; see Appendices 9.2 and 9.5. \n\nBy explicitly tying detection to artifacts of the text generation process, we are agnostic to the large language model used for generation. This may be important for modern large language models such as Llama 3.1 (Dubey et al., 2024), which in Section 5 we demonstrate cause challenges to existing detectors but not TempTest.",
            "score": 0.5824115728653696,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 216,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 392
                },
                {
                    "start": 395,
                    "end": 543
                },
                {
                    "start": 546,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1734
                },
                {
                    "start": 1737,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2062
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 212,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 526,
                    "end": 542,
                    "matchedPaperCorpusId": "260336295"
                },
                {
                    "start": 639,
                    "end": 657,
                    "matchedPaperCorpusId": "263831345"
                },
                {
                    "start": 860,
                    "end": 883,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 937,
                    "end": 960,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 977,
                    "end": 995,
                    "matchedPaperCorpusId": "263831345"
                },
                {
                    "start": 1005,
                    "end": 1024,
                    "matchedPaperCorpusId": "258960101"
                },
                {
                    "start": 1041,
                    "end": 1058,
                    "matchedPaperCorpusId": "259129463"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91162109375
        },
        {
            "corpus_id": "267897345",
            "title": "Watermarking Makes Language Models Radioactive",
            "text": "We investigate the radioactivity of text generated by large language models (LLM), i.e. whether it is possible to detect that such synthetic input was used to train a subsequent LLM. Current methods like membership inference or active IP protection either work only in settings where the suspected text is known or do not provide reliable statistical guarantees. We discover that, on the contrary, it is possible to reliably determine if a language model was trained on synthetic data if that data is output by a watermarked LLM. Our new methods, specialized for radioactivity, detects with a provable confidence weak residuals of the watermark signal in the fine-tuned LLM. We link the radioactivity contamination level to the following properties: the watermark robustness, its proportion in the training set, and the fine-tuning process. For instance, if the suspect model is open-weight, we demonstrate that training on watermarked instructions can be detected with high confidence ($p$-value $<10^{-5}$) even when as little as $5\\%$ of training text is watermarked.",
            "score": 0.5821426364750429,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83935546875
        },
        {
            "corpus_id": "273403782",
            "title": "De-mark: Watermark Removal in Large Language Models",
            "text": "As language models rapidly evolve and their usage becomes more widespread (Wang et al., 2024a;Thirunavukarasu et al., 2023;Rubenstein et al., 2023;Wang et al., 2024b;Kasneci et al., 2023), concerns among researchers and regulators about their potential for misuse, particularly in generating harmful content, have intensified. The challenge of distinguishing between human-written and AIgenerated text has emerged as a key focus of research. Statistical watermarking (Aaronson, 2022;Kirchenbauer et al., 2023a) presents a promising approach to identifying text produced by sequential LLMs. This method embeds a statistical pattern into the generated text via a pseudo-random generator, which can later be detected through a statistical hypothesis test. \n\nHowever, recent research (Jovanovi\u0107 et al., 2024) highlights that the robustness of existing n-gram watermarking schemes remains insufficiently understood and, in many cases, significantly overestimated. One critical vulnerability lies in water-* Equal contributions mark stealing & removal, a method that seeks to reverse-engineer the watermarking process. With this method, adversaries can effectively strip or alter the embedded watermark, undermining its intended protection. This poses a serious challenge to the security and integrity of watermarked content, as it exposes a weakness in the watermarking scheme's ability to withstand tampering. \n\nIn this paper, we delve deeper into watermark removal techniques and introduce DE-MARK, a comprehensive framework for stealing and removing n-gram-based watermarks. Our approach utilizes a querying strategy on the language model to reconstruct the red and green lists and estimate the watermark strength. Unlike prior work (Jovanovi\u0107 et al., 2024), which has certain limitations -namely, a) requiring knowledge of the underlying hash function of the n-gram and b) requiring paraphrase tool for watermark removal, which cannot preserve the original LM distribution -our method is more general and provides a theoretical guarantee between the post-removal LM distribution and the original ones.",
            "score": 0.5819062778214299,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1405
                },
                {
                    "start": 1408,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 2100
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 94,
                    "matchedPaperCorpusId": "261064713"
                },
                {
                    "start": 94,
                    "end": 123,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 147,
                    "end": 166,
                    "matchedPaperCorpusId": "258762579"
                },
                {
                    "start": 166,
                    "end": 187,
                    "matchedPaperCorpusId": "257445349"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88623046875
        },
        {
            "corpus_id": "270440174",
            "title": "Towards Reliable Detection of LLM-Generated Texts: A Comprehensive Evaluation Framework with CUDRT",
            "text": "Due to the increasingly human-like level of text generated by LLMs, relying on manual differentiation of AI-generated text is not only costly, but also generally low in accuracy.As a variety of LLMs continue to emerge, researchers have increasingly focused on developing targeted AI-generated text detection algorithms to prevent the misuse of these powerful models (Sarzaeim, Doshi, & Mahmoud, 2023).Currently, detectors are primarily categorized based on their working principles and internal architectures, into metric-based and model-based methods (X.Liu et al., 2023;Rouf et al., 2024).Metric-based methods evaluate and identify whether text is generated by artificial intelligence by utilizing a series of quantitative metrics, such as textual consistency, complexity, word rank, and entropy (Rouf et al., 2024).Gehrmann et al. (Gehrmann et al., 2019) proposed a tool GLTR for detecting and visualizing machinegenerated text.This method uses statistical approaches to examine the generation probability of each word, its rank in model predictions, and the entropy of the prediction distribution, helping users determine whether a text was generated by an automated language model.Model-based methods involve initially training machine learning or deep learning models on corpora of human-generated and machine-generated texts, and then using these trained models to classify the two types of texts.Guo et al. (Guo et al., 2023) proposed the ChatGPT Detector, a model that uses the RoBERTa architecture as its backbone.The model is fine-tuned on texts generated under various scenarios to enable its multifunctional detection capabilities.Wang et al. (P. Wang et al., 2023) proposed a method called SeqXGPT for detecting AIgenerated text at the sentence level.This model uses a list of log probabilities of tokens extracted from a white-box language model as features.It processes these features through convolutional networks and self-attention networks to achieve fine-grained classification of the text.Liu et al. (X. Liu et al., 2023) proposed a machine-generated text detection method COCO, enhancing text coherence under low-resource conditions using contrastive learning.",
            "score": 0.5817744788560434,
            "section_title": "AI-generated Text Detection Method",
            "char_start_offset": 9489,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 178,
                    "end": 401
                },
                {
                    "start": 401,
                    "end": 555
                },
                {
                    "start": 555,
                    "end": 591
                },
                {
                    "start": 591,
                    "end": 818
                },
                {
                    "start": 818,
                    "end": 931
                },
                {
                    "start": 931,
                    "end": 1186
                },
                {
                    "start": 1186,
                    "end": 1404
                },
                {
                    "start": 1404,
                    "end": 1524
                },
                {
                    "start": 1524,
                    "end": 1644
                },
                {
                    "start": 1644,
                    "end": 1765
                },
                {
                    "start": 1765,
                    "end": 1873
                },
                {
                    "start": 1873,
                    "end": 2011
                },
                {
                    "start": 2011,
                    "end": 2183
                }
            ],
            "ref_mentions": [
                {
                    "start": 366,
                    "end": 400,
                    "matchedPaperCorpusId": "263170980"
                },
                {
                    "start": 555,
                    "end": 572,
                    "matchedPaperCorpusId": "264406273"
                },
                {
                    "start": 572,
                    "end": 590,
                    "matchedPaperCorpusId": "269627110"
                },
                {
                    "start": 798,
                    "end": 817,
                    "matchedPaperCorpusId": "269627110"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84765625
        },
        {
            "corpus_id": "267759853",
            "title": "GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick",
            "text": "As advanced language models increasingly demonstrate remarkable capabilities, concerns regarding their misuse have escalated. Consequently, the development of effective methods for detecting machine-generated text has become crucial. The GM watermark has emerged as a highly effective technique for differentiating between machinegenerated and natural text. Nevertheless, the GM watermark is limited by issues of diversity, which may hinder its practical application. The Gumbel-Soft watermark represents a straightforward yet effective strategy to address this limitation. This approach maintains the watermark's detectability while significantly enhancing its generative diversity. We believe that our method will facilitate the broader implementation of the GM watermark in various applications.",
            "score": 0.5817547494824789,
            "section_title": "Ethical Considerations",
            "char_start_offset": 21037,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 798
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89697265625
        },
        {
            "corpus_id": "267406364",
            "title": "LLM-Detector: Improving AI-Generated Chinese Text Detection with Open-Source LLM Instruction Tuning",
            "text": "For black-box detection, classifiers are restricted to APIlevel access to LLMs (only available for the text). et al., 2019) features and an end-to-end RoBerta (Guo et al., 2023b) classifier, to detect whether a certain text (English and Chinese) is generated by ChatGPT or humans across several domains. However, the study conducted demonstrates that a limitation of supervised models is the potential occurrence of overfitting within the domain, resulting in poor detection performance OOD (OOD) (Chakraborty et al., 2023). To address the limitations of supervised classifiers, zero-shot classifiers, using a pre-trained language model directly without fine-tuning, are immune to domain-specific degradation. Zero-shot classifiers such as GPT-Zero1 , DetectGPT (Mitchell et al., 2023) and Fast-DetectGPT (Bao et al., 2023) have been developed. These methods utilize checks on perplexity and burstiness in the text to determine whether it is artificially generated or authored by a human. The current zero-shot classifiers require input documents of considerable length (exceeding 100 tokens) for the classifier to effectively capture contextual features of the text. In terms of classifying short sentences, their performance is relatively poor.",
            "score": 0.581289109613353,
            "section_title": "Black-Box Detection",
            "char_start_offset": 5319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1246
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8388671875
        },
        {
            "corpus_id": "258108316",
            "title": "Detection of Fake Generated Scientific Abstracts",
            "text": "Although the concept of AI text recognition based on Large Language Models (LLMs) is relatively new, there is already related research in the field. A tool called Giant Language Model Test Room (GLTR) [7] is implemented to detect whether a text has been generated by a machine or written by a human. The work is based on several statistical methods to detect AI-generated text from GPT-2 and BERT, including computing the model density of generated output and comparing it to human-generated text, as well as using the probability of a word and the distribution over four buckets of absolute ranks of predictions. On the same page, in order to discriminate machine-generated text, the study [27] proposes the FActual Structure of Text (FAST) method, a graph-based model that utilizes the factual structure of a document for deep fake detection of text. The graph nodes are extracted by Named Entity Recognition (NER). Furthermore, the sentence representations were constructed via document-level aggregation for the final prediction, where the consistency and coherence of continuous sentences are sequentially modeled. For the evaluation of FAST, the authors used AI-generated text data generated from GROVER and GPT-2. The approach outperformed transformer-based models [27]. Based on the text generated by GPT-2 model, OpenAI created a detector of whether a text is artificially generated. The detector has been based on a fine-tuning version of the RoBERTa base. The model has been trained using 510 tokens and using 5, 000 text samples from the WebText dataset 5, 000 texts generated by a GPT-2 model. The model achieved 95% of accuracy [23]. Furthermore, in [14] is presented a coherence-based contrastive learning model (CoCo) for the detection of text generated from machines. To achieve that, the authors modeled the text coherence with entity consistency and sentence interaction. The CoCo model outperformed in terms of accuracy and F1 score similar models such as GPT-2, RoBERTa, and XLNet but also the GROVER and FAST models. Additionally, the authors of [18] proposed a tool, called DetectGPT, that detects machine-generated text from LLMs.",
            "score": 0.5811109722421142,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 3672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2154
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.931640625
        },
        {
            "corpus_id": "268230672",
            "title": "A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization",
            "text": "Recent years have seen a surge in interest in developing AI text detection techniques, leading to a broad array of approaches that fall into two main categories: watermark-based and post-hoc detection.Watermarking involves embedding a detectable pattern into AI-generated text during training or decoding to later identify the text as originating from a specific LLM (Ren et al., 2023;Liu et al., 2024).While effective, the application of watermarking is limited by the requirement of cooperation from the organization or the developer creating or hosting the LLM, a constraint not always met, especially with maliciously deployed LLMs.Consequently, post-hoc detection methods have gained prominence in AI-generated text forensics.Therefore, in the scope of our survey, we focus on post-hoc detection, further dividing it into supervised and zero-shot detection based on the training methodology employed.",
            "score": 0.5805793869530771,
            "section_title": "Watermarking vs. Post-hoc Detection",
            "char_start_offset": 4569,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 201,
                    "end": 403
                },
                {
                    "start": 403,
                    "end": 636
                },
                {
                    "start": 636,
                    "end": 731
                },
                {
                    "start": 731,
                    "end": 905
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.888671875
        },
        {
            "corpus_id": "264439054",
            "title": "Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey",
            "text": "For zero-shot text detection, no access to machine-generated or human-written text samples is required. The core idea is that generic text sequences generated by a language model contain some form of detectable information that can be picked up and flagged by a detector. Detection under this category may be performed using a pre-trained language model which may not be similar to the source model, or through a fully separate statistical approach. \n\nGiven a text passage, common approaches for zero-shot text detection include: (1) statistical outlier detection based on entropy (Lavergne et al., 2008), perplexity (Beresneva, 2016;Tian, 2023), or n-gram frequencies (Badaskar et al., 2008), and (2) calculating average per-token log probability of the given sequence and then thresholding (Solaiman et al., 2019;Mitchell et al., 2023).",
            "score": 0.5799151245096121,
            "section_title": "Zero-shotDetection",
            "char_start_offset": 39200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 449
                },
                {
                    "start": 452,
                    "end": 838
                }
            ],
            "ref_mentions": [
                {
                    "start": 581,
                    "end": 604,
                    "matchedPaperCorpusId": "12098535"
                },
                {
                    "start": 617,
                    "end": 634,
                    "matchedPaperCorpusId": "1175726"
                },
                {
                    "start": 669,
                    "end": 692,
                    "matchedPaperCorpusId": "4324753"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8662109375
        },
        {
            "corpus_id": "269757699",
            "title": "RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors",
            "text": "As the generation capabilities of language models have continued to increase, accurately and automatically detecting machine-generated text has become an important priority.Detection efforts have even surpassed the bounds of natural language processing research, spurring discussions by social media companies and governments on possibly mandating labels for machine-generated content.Despite the protective intentions of these mandates, our work shows that such regulations would be difficult to enforce even if they were implemented.Detectors are not yet robust enough for widespread deployment or high-stakes use: many detectors we tested are nearly inoperable at low false positive rates, fail to generalize to alternative decoding strategies or repetition penalties, show clear bias towards certain models and domains, and quickly degrade with simple black-box adversarial attacks.\n\nThe bulk of our findings may sound bleak, but we did uncover promising signs of improvement.Binoculars, for example, performed impressively well across models even at extremely low false positive rates, Originality achieved high precision in some constrained scenarios, and GPTZero was unusually robust to adversarial attacks.We believe that openly evaluating detectors on large, diverse, shared resources is critical to accelerating progress-and trust-in detection.Evaluating robustness is particularly important for detection, and it only increases in importance and the scale of public deployment grows.\n\nWe also need to remember that detection is just one tool for a larger, even more valuable motivation: preventing harm by the mass distribution of text.Detecting machine-generated text was a useful proxy for identifying harmful text for a long time, but language models have improved to the point that generated text is frequently legitimate and not harmful (Schuster et al., 2020).Therefore, detecting specific harmful elements-like misinformation, hate speech, and abuse-should take precedence over whether or not the text was authored by a machine.Knowing if a text was machinegenerated, however, does still offer insights on the types of errors we can expect or the recency of the facts cited within.We hope that our analyses and the RAID dataset are a step toward a future in which AI detection tools are safely integrated into society as a multi-pronged approach to reducing harm.We encourage future work to build on this by including more models, languages, and generation settings in future shared resources.",
            "score": 0.5795574319084954,
            "section_title": "Conclusion",
            "char_start_offset": 20592,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 173,
                    "end": 385
                },
                {
                    "start": 385,
                    "end": 535
                },
                {
                    "start": 535,
                    "end": 886
                },
                {
                    "start": 888,
                    "end": 980
                },
                {
                    "start": 980,
                    "end": 1214
                },
                {
                    "start": 1214,
                    "end": 1354
                },
                {
                    "start": 1354,
                    "end": 1494
                },
                {
                    "start": 1496,
                    "end": 1647
                },
                {
                    "start": 1647,
                    "end": 1877
                },
                {
                    "start": 1877,
                    "end": 2046
                },
                {
                    "start": 2046,
                    "end": 2199
                },
                {
                    "start": 2199,
                    "end": 2381
                },
                {
                    "start": 2381,
                    "end": 2511
                }
            ],
            "ref_mentions": [
                {
                    "start": 1853,
                    "end": 1876,
                    "matchedPaperCorpusId": "211204954"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86376953125
        },
        {
            "corpus_id": "259501760",
            "title": "ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey",
            "text": "LLMs like ChatGPT are capable of generating grammatically flawless and seeminglyhuman replies to different types of questions on various domains [122]. Educators are integrating such technology into classroom instruction, while GAI tools are increasingly becoming publicly available, leading to greater exposure to synthetic content during web browsing. The academic community is also actively exploring methods to discern whether a given text was produced by a machine or a human, leveraging the fact that there exist systematic differences between human-generated and machinegenerated text. Expectedly some tools have quickly emerged to address this problem. Examples of existing tools for such detection include GPTZero [123], GPTRadar [124], Turnitin [125], and Originally.AI [126], However, the problem is far from being resolved [127,128].\n\nOne objective of research in AI-generated content is to enhance the quality of machine-generated text so that it closely approximates human-produced text. In recent research [129] watermarking techniques are proposed that can be incorporated into LLMs used to generate AI-generated text, although these methods are not infallible. During the generation process, LLMs predict the next likely word in a sentence by comparing various alternatives. By designating certain word patterns as off-limits for the AI text generator, a watermark may enable the detection of text produced by a human when the watermark rules are violated multiple times during text scanning.\n\nIn the recent study [130], the classification results clearly show that the disambiguation between ChatGPT-generated and human-generated reviews is more challenging for the ML model when the reviews have been rephrased from existing humangenerated texts and are not generated by custom queries. ChatGPT can now generate text that is difficult to distinguish from that written by a human to the point [128]. The study suggested that existing techniques for the detection of AI-generated text are not reliable in practice. This paradigm shift challenges our conventional notions of fluency in the language, prompting the need for novel techniques to distinguish between human and machine-generated content. While new tools may become necessary to discern synthetic media in the future, the advice against writing like a robot remains relevant.",
            "score": 0.5787343720931365,
            "section_title": "Detection of AI-Generated Content",
            "char_start_offset": 70661,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 150,
                    "matchedPaperCorpusId": "256976087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79736328125
        },
        {
            "corpus_id": "258048481",
            "title": "On the Possibilities of AI-Generated Text Detection",
            "text": "Remark 1: Insights for watermark design. From a practical perspective, even though Theorem 1 shows that detection is always possible by collecting more samples, it might be costly as well if the number n needed is extremely high. However, one could mitigate this trade-off by developing efficient watermarking techniques as discussed in (Kirchenbauer et al., 2023;Aaronson, 2022), which essentially increases the Chernoff information, or in other words, increases the \u03b4, eventually reducing the required number of samples. Nevertheless, empirical demonstrations in (Sadasivan et al., 2023;Krishna et al., 2023) exposed the vulnerability of the watermark-based detectors with paraphrasingbased attacks, raising a genuine concern in the community about the detection of AI-generated texts. \n\nTo address this concern, more recently, interesting work by (Krishna et al., 2023) proposed a novel defense mechanism based on information retrieval principles to combat prior attacks and demonstrated its effectiveness even with a corpus size of 15M generations. This result also supports our theory, indicating that it is always possible to detect AI-generated text depending on the detection method. In addition, there are some recent open-sourced text detection tools (AIT, a,b) whose performances are also worth considering and validate the fact that detection is indeed possible under certain settings. We believe that with the new insights from this work, one can design more efficient and robust watermarks spanning a larger corpus of text, which will be hard to remove via vanilla paraphrasers. \n\nRemark 2: Insights for detector design. This work demonstrates that detecting AI-generated text should be almost always possible but one would need to collect more samples depending on the hardness of the problem (controlled by the closeness of human and machine distributions). The recent study by (Liang et al., 2023) raises an important concern regarding the bias in some of the existing detectors. The authors in Liang et al. (2023) revealed that a significant proportion of the current detectors inaccurately classify non-native English writing samples as AI-generated, potentially leading to unjust consequences in various contexts.",
            "score": 0.5780642893992451,
            "section_title": "Appendix A Additional Insights and Remarks",
            "char_start_offset": 30550,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1592
                },
                {
                    "start": 1595,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2233
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77294921875
        },
        {
            "corpus_id": "277322659",
            "title": "TempTest: Local Normalization Distortion and the Detection of Machine-generated Text",
            "text": "Across a wide range of scenarios, we have demonstrated that TempTest is an effective new approach for detecting machine-generated text that is comparable or superior to the current state-of-the-art, with this effect most pronounced for short passage lengths and on modern language models. This test is based on a novel statistical quantity, TempNorm, which quantifies the local normalization distortion of conditional probability measures. Importantly, this provides an extra tool for researchers and practitioners to use when building future detection algorithms. \n\nHowever, there are drawbacks to our method that present opportunities for future work. First, we consider only English and these empirical investigations should be expanded to include other languages. \n\nSecond, the distortion we detect is only present in temperature-sampled text and thus is not applicable to text generated through different generation strategies such as top-p or top-k. That said, an analogous AUROCs averaged across all 3 datasets and 5 models used in Table 1 at input text sizes ranging from 25 to 100 tokens. method may be derived for top-k, see Appendix 9.2. Moreover, we hope that future work will look at ways of ensembling or unifying different zero-shot methods into a single approach, creating methods that are broadly applicable and exploiting the advantages of each technique. Ultimately, the more diverse our statistical toolbox is for analyzing text, the better we will be able to detect the traces machines leave behind. \n\nTable 3: Detecting GPT-3.5 And GPT-4 Generated Text With Scoring Model GPT-Neo 2.7b. In this experiment the generation temperature was 0.8 and the text length was 50 tokens. TempTest was evaluated with scoring temperatures, denoted in parentheses, between 0.6 and 0.9.",
            "score": 0.5778128367746405,
            "section_title": "CONCLUSIONS",
            "char_start_offset": 23728,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 564
                },
                {
                    "start": 567,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 767
                },
                {
                    "start": 770,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1791
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87158203125
        },
        {
            "corpus_id": "267413235",
            "title": "Detection of Machine-Generated Text: Literature Survey",
            "text": "The machine-generated text has the potential to improve many elements of human interaction with textual information. Grammar correction, machine translation, writing support for both textual and programming material, and even poetry production are all examples of useful uses. However, when dealing with bad content, such as misinformation, unwanted messages, and misleading methods, a slew of issues develop. As a result, the capacity to distinguish between human-authored and machine-generated content is critical. \n\nIn general, simpler and more readily understood material is less likely to be seen as synthetic. The key dangers are authorship loss, authenticity, and general divergence from the truth. The industry need a simple, feature-based classifier that can distinguish between human and produced content. Grover, for example, can not only produce but also identify generated text. RoBERTa outperforms GPT-2 in detecting tasks, especially with short text snippets like as tweets, but problems with spelling mistakes and homoglyphs. \n\nThe development of methods that examine linguistic traits, statistical traits, and the substance of the generated text has been the main goal of research into the detection of fake text. \n\nPopular techniques for detection include: \n\n\u2022 Feature-based strategies These methods use linguistic analysis to distinguish between writing that was produced by humans and text that was generated artificially. \n\n\u2022 Statistical techniques In these methods, deviations from natural language are found by analyzing statistical aspects of the text, such as word embeddings or n-gram frequencies. \u2022 Methods based on machine learning These techniques use machine learning algorithms, either supervised or unsupervised, to categorize synthetic text based on properties retrieved from the text. \n\nThe essence of the problem is that current detectors beat supervised spam classifiers, but they struggle to keep up with natural language model developments. Because of lower processing costs and the availability of free data, these models have flourished. As a result, future research should concentrate on constructing customized sub-classifiers for each dataset and use an ensemble technique for output. Many existing interim solutions rely on generator knowledge, which provides difficulty. \n\nFalse positive findings are especially troubling because they may lead to prejudice against actual human perspectives. Furthermore, detection approaches are sometimes difficult to transfer across several generators. As a result, no final answer exists.",
            "score": 0.5775619410447389,
            "section_title": "CONCLUSIONS",
            "char_start_offset": 34832,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1230
                },
                {
                    "start": 1233,
                    "end": 1274
                },
                {
                    "start": 1277,
                    "end": 1442
                },
                {
                    "start": 1445,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1818
                },
                {
                    "start": 1821,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2227
                },
                {
                    "start": 2228,
                    "end": 2315
                },
                {
                    "start": 2318,
                    "end": 2436
                },
                {
                    "start": 2437,
                    "end": 2533
                },
                {
                    "start": 2534,
                    "end": 2570
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82958984375
        },
        {
            "corpus_id": "271902835",
            "title": "Authorship Attribution in the Era of LLMs: Problems, Methodologies, and Challenges",
            "text": "Various zero-shot detection methods provide distinct strategies to enhance both detection accuracy and efficiency. Gall\u00e9 et al. (2021) introduced an unsupervised method that identifies over-appearance of repeated higher-order n-grams, distinguishing them from humangenerated text. DetectGPT (Mitchell et al., 2023) relies on the observation that LLM-generated passages often fall into regions of negative curvature in log probability. (Bao et al., 2023) enhanced this approach and proposed Fast-DetectGPT, which increases efficiency by using conditional probability curvature. Additionally, some methods leverage LLMs themselves for text classification (Solaiman et al., 2019, Zellers et al., 2019). Different decoding methods are often applied to generate more diverse and less repetitive text, though these can also lead to hallucinations and less verifiable content (Guo et al., 2023, Shakeel andJain, 2021). Fact-checking methods can mitigate these issues (Schuster et al., 2020, Zhong et al., 2020). Additionally, Krishna et al. (2024a) developed a detector that uses information retrieval to store LLM outputs in a database and search for semantically similar content to identify LLM-generated text, though this method raises privacy concerns regarding the storage of user conversations. \n\nWatermarking. Watermarking involves embedding specific patterns within text generated by LLMs, making them imperceptible to humans but detectable through specialized methods. (Kirchenbauer et al., 2023, Meral et al., 2009, Topkara et al., 2005, Zhao et al., 2023). By imprinting distinct patterns, watermarking enables the identification of LLM-generated text. Various methods include parsed syntactic tree structures (Atallah et al., 2001, Topkara et al., 2005), synonym tables (Jalil and Mirza, 2009), adversarial watermarking (Abdelnabi and Fritz, 2021), and context-aware lexical substitution (Yang et al., 2022).",
            "score": 0.5770939914384758,
            "section_title": "Methodologies",
            "char_start_offset": 23686,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1293
                },
                {
                    "start": 1296,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1913
                }
            ],
            "ref_mentions": [
                {
                    "start": 675,
                    "end": 698,
                    "matchedPaperCorpusId": "168169824"
                },
                {
                    "start": 960,
                    "end": 982,
                    "matchedPaperCorpusId": "211204954"
                },
                {
                    "start": 982,
                    "end": 1003,
                    "matchedPaperCorpusId": "222377949"
                },
                {
                    "start": 1019,
                    "end": 1041,
                    "matchedPaperCorpusId": "257687440"
                },
                {
                    "start": 1471,
                    "end": 1497,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 1497,
                    "end": 1517,
                    "matchedPaperCorpusId": "1192689"
                },
                {
                    "start": 1517,
                    "end": 1539,
                    "matchedPaperCorpusId": "13415776"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66015625
        },
        {
            "corpus_id": "249712169",
            "title": "DIALOG-22 RuATD Generated Text Detection",
            "text": "As the language neural nets got better at generating texts, it's getting harder and harder to distinguish the human-written text from generated one. So manual detection of these texts got impossible. For that reason, it's desirable to build a system that can automatically detect generated text. \n\nThe proposed system will use various features extracted from the text such as length, punctuation, word choice etc. To determine whether the text is human-generated or not. The accuracy of this system can be improved by using machine learning algorithms which will learn how humans generate texts and then use those features for detection purpose. \n\nThere are many ways to build such a system, but probably the most reliable one is based on machine learning algorithms. These algorithms can be trained on a large number of examples -both humangenerated and computer-generated texts. After being trained, they should be able to identify which texts are computer-generated with high accuracy. \n\nThis approach already works in other areas, such as spam detection. Some early experiments have shown promising results and indicate that this approach works well for the detection of the generated text. \n\nThe DIALOG-22 shared task of RuATD 2022 had 2 tracks for binary classification and multiclass classification. In this report, we will describe the data, how we handled it, the models we used, and the ensembling technique. For the binary classification task   , we frame the generated text detection problem as follows: given a piece of text , label it as either human-written or machine-generated   = {,  }. \n\n  :  \u2192   For the multiclass classification task   , we set up the problem as follows: given a piece of text , label it as one of the 14 classes that represent deep neural models   = { M2M-100, Human, OPUS-MT, M-BART50, ruGPT3-Medium, ruGPT3-Small, mT5-Large, ruGPT3-Large, ruT5-Base-Multitask, mT5-Small, ruT5-Base, ruGPT2-Large, M-BART, ruT5-Large }   :  \u2192  ",
            "score": 0.5767661247087659,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 295
                },
                {
                    "start": 298,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 988
                },
                {
                    "start": 991,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1194
                },
                {
                    "start": 1197,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1604
                },
                {
                    "start": 1607,
                    "end": 1966
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74658203125
        },
        {
            "corpus_id": "259360677",
            "title": "Evade ChatGPT Detectors via A Single Space",
            "text": "ChatGPT brings revolutionary social value but also raises concerns about the misuse of AI-generated text. Consequently, an important question is how to detect whether texts are generated by ChatGPT or by human. Existing detectors are built upon the assumption that there are distributional gaps between human-generated and AI-generated text. These gaps are typically identified using statistical information or classifiers. Our research challenges the distributional gap assumption in detectors. We find that detectors do not effectively discriminate the semantic and stylistic gaps between human-generated and AI-generated text. Instead, the\"subtle differences\", such as an extra space, become crucial for detection. Based on this discovery, we propose the SpaceInfi strategy to evade detection. Experiments demonstrate the effectiveness of this strategy across multiple benchmarks and detectors. We also provide a theoretical explanation for why SpaceInfi is successful in evading perplexity-based detection. And we empirically show that a phenomenon called token mutation causes the evasion for language model-based detectors. Our findings offer new insights and challenges for understanding and constructing more applicable ChatGPT detectors.",
            "score": 0.5766859952478824,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67919921875
        },
        {
            "corpus_id": "270703139",
            "title": "Detecting AI-Generated Text: Factors Influencing Detectability with Current Methods",
            "text": "As described in the previous section, there are three main methods for the detection of AI-generated text (AIGT): watermarking, statistical and stylistic analysis, and the use of pre-trained language models (LMs). The detection of a watermark requires knowledge of the watermark extraction algorithm; beyond that, no extra data are needed. However, the other two broad detection methods require data in order to learn the patterns that distinguish AIGT from human-written text -ideally, a dataset where the positive examples were generated by the AI model we wish to detect, and in a setting as close as possible to how we expect to encounter these texts in the real world. Previous work has indicated that the most effective detectors are trained on data from the same domain (news articles, social media posts, academic essays, etc.), language (English, Chinese, French, etc.) and model settings (decoding algorithm, prompt, length of output, etc.) as the test data. At the same time, the research also shows that for maximal generalizability and robustness, it is essential to train a detector on a wide variety of data so that it is not overfit to one very narrow range of data samples. Therefore, for any given application, it is important to select appropriate data to first train the detector, and also to test the detector for its accuracy: \"The effectiveness of black-box detection models is heavily dependent on the quality and diversity of the acquired data\" (Tang et al., 2024). \n\nTable 3 lists some of the most frequently used datasets that include human-written and AI-generated texts. We observe that the vast majority of available datasets are in English, with another sizeable chunk being multilingual (including English as well as other languages). There is a well-known bias in the field of NLP that much of the research focuses on English, to the detriment of other languages. This bias is no doubt exacerbated by the fact that many of the large language models (LLMs) used to generate these datasets were, at least initially, only available in English.",
            "score": 0.5766219431200634,
            "section_title": "AIGT Datasets",
            "char_start_offset": 65586,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1490
                },
                {
                    "start": 1493,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2073
                }
            ],
            "ref_mentions": [
                {
                    "start": 1470,
                    "end": 1489,
                    "matchedPaperCorpusId": "257496757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94384765625
        },
        {
            "corpus_id": "267095281",
            "title": "Raidar: geneRative AI Detection viA Rewriting",
            "text": "Ghostbuster (Verma et al., 2023) operates under the assumption that the scoring and target model are different, but it still requires access to generated documents from the target model. In addition, the output from the above deep scoring models can contain nuisances and spurious features, and can also be manipulated by adversarial attacks (Jin et al., 2019;Zou et al., 2023), making detection not robust. Another line of work aims to watermark the AI-generated text to enable detection (Kirchenbauer et al., 2023). We show the similarity score under a transformation and its reverse transformation; the machinegenerated text is more equivariant under transformation. (c) We show the uncertainty of text produced by humans and GPT. GPT input is more stable than human input. The samples are run on the Yelp Review dataset with 4000 samples. The discrepancies in invariance, equivariance, and output uncertainty allow us to detect machine-generated text. \n\nBypassing Machine Text Detection. Krishna et al. (2023) showed rephrase can remove watermark. Krishna et al. (2023);Sadasivan et al. (2023) show that paraharase can efficiently evade detection, including DetectGPT (Mitchell et al., 2023), GLTR (Gehrmann et al., 2019), OpenAI's generated text detectors, and other zero-shot methods Ippolito et al. (2019); Solaiman et al. (2019). There is a line of work that watermarks the generated text to enable future detection. However, they are shown to be easily broken by rephrasing, too. Our detection can be robust to rephrasing. \n\nPrompt Engineering. Prompting is the most effective and popular strategy to adapt and instruct LLM to perform tasks Li & Liang (2021); Zhou et al. (2022); Wei et al. (2022); Kojima et al. (2022). Zero-shot GPT prompts the GPT model by asking \"is the input generated by GPT\" to predict if this is GPT generated (Verma et al., 2023).",
            "score": 0.5766020820430231,
            "section_title": "RELATED WORK",
            "char_start_offset": 6284,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1531
                },
                {
                    "start": 1534,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1865
                }
            ],
            "ref_mentions": [
                {
                    "start": 1689,
                    "end": 1706,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1708,
                    "end": 1728,
                    "matchedPaperCorpusId": "249017743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66943359375
        },
        {
            "corpus_id": "264439054",
            "title": "Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey",
            "text": "In contrast to the development of detection frameworks, interestingly, recent studies (Sadasivan et al., 2023;Krishna et al., 2023) have shown that state-of-art detection frameworks can be vulnerable and fragile to paraphrasing-based attacks. Further, Sadasivan et al. (2023) have also provided interesting theoretical insights about the impossibility of AI-generated text detection, outlining the fundamental impossibility of detecting a theoretically optimal language model. This analysis is followed by work elucidating the possibilities of AI-generated text detection in Chakraborty et al. (2023), which highlights that it should always be possible to detect AI-generated text via increasing the text length unless the distributions of human and machine-generated texts are the same over the entire support, i.e. the language model is theoretically optimal. This possibility result is also supported in subsequent work in Kirchenbauer et al. (2023b), showing that for the example of watermarking, empirically, attacks such as paraphrasing only dilute the signal that the text is AI-generated and that observing a sufficiently large amount of text makes detection possible again. However, how much text is available from a single source is application-specific. In this review, we provide a taxonomy of recent studies of this nature, discuss their interplay, detail their approaches to detection, outline their inherent limits, and identify the applications where specific techniques show promise.",
            "score": 0.5765122683102822,
            "section_title": "Introduction",
            "char_start_offset": 3516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1500
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60888671875
        },
        {
            "corpus_id": "267095281",
            "title": "Raidar: geneRative AI Detection viA Rewriting",
            "text": "data, even they are generated from a different model. We also evaluate our detection efficiency on the Claude (Anthropic, 2023) generated text on student essay (Verma et al., 2023), where we achieve an F1 score of 57.80. \n\nIn the out-of-distribution experiment, we train the detector on data from two language models, assuming it is unaware that the test text will be generated from the third model. Despite a performance drop on detecting the out-of-distribution test data generated from the third model, our method remains effective in detecting content from this unseen model, underscoring our approach's robustness and adaptability, with up to 91 points on F1 score. The number shows the number of data, reflecting by the size of the dot. \n\nType of Detection Model. Mireshghallah et al. (2023) showed that model size affects performance in perturbation-based detection methods. Given the same input text generated from GPT-3.5, We explore our approach's efficacy with alternative rewriting models with different size. In addition to using the costly GPT-3.5 to rewrite, we incorporate two smaller models, Ada and Text-Davinci-002, and evaluate their detection performance when they are used to rewrite. In Table 5, while all models achieve significant detection performance, our results indicate that a larger rewriting language model enhances detection performance in our method. \n\nImpact of Different Prompts. Figure 6 displays the detection F1 score for various prompts across three datasets. While Mitchell et al. (2023) employs up to 100 perturbations to query LLM and compute curvature from loss, our approach achieves high detection performance using just a single rewriting prompt. \n\nImpact of Content Length. We assess our detection method's performance across varying input lengths using the Yelp Review dataset in Figure 5. Longer inputs, in general, achieve higher detection performance. Notably, while many algorithms fail with shorter inputs (Tian, 2023;Verma et al., 2023), our method can achieve 74 points of detection F1 score even with inputs as brief as ten words, highlighting the effectiveness of our approach.",
            "score": 0.5763264235984993,
            "section_title": "LLM for",
            "char_start_offset": 29564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 220
                },
                {
                    "start": 223,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 742
                },
                {
                    "start": 745,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1384
                },
                {
                    "start": 1387,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1693
                },
                {
                    "start": 1696,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2135
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.611328125
        },
        {
            "corpus_id": "277272207",
            "title": "Understanding the Effects of RLHF on the Quality and Detectability of LLM-Generated Texts",
            "text": "Improvements in LLMs have increased the need for effective LLM-generated text detection methods. These methods are critical in various domains, including misinformation detection, academic integrity, and cybersecurity (Mubarak et al., 2023). Existing detection approaches can be broadly classified into training-based methods (Li, Li, Cui, Bi, Wang, Yang, Shi and Zhang, 2023b), zero-shot methods (Mitchell, Lee, Khazatsky, Manning and Finn, 2023), and watermarking techniques (Kirchenbauer, Geiping, Wen, Katz, Miers and Goldstein, 2023). Training-based methods, such as GPTZero, rely on pre-trained models that learn to identify patterns specific to LLM-generated text through supervised learning. On the other hand, zero-shot methods, such as Fast-DetectGPT (Bao, Zhao, Teng, Yang and Zhang, 2023), leverage inherent statistical differences between human-and machine-generated texts without requiring extensive labeled data for training. Watermarking is a technique that adds algorithmically detectable patterns to the generated text as an indicator. However, as LLMs become more adept at mimicking human writing, the detection task may become increasingly complex. For instance, the recent study on the HUMPA (Humanized Proxy Attack) strategy demonstrates that by leveraging a reinforcement learning fine-tuned small language model, attackers can manipulate LLMs to generate text that closely mimics human writing, thereby effectively deceiving state-of-the-art detectors (Wang, Chen, Liu, Chen, Chen, Zhang and Cheng, 2025); DIPPER (Krishna, Song, Karpinska, Wieting and Iyyer, 2023) exposed the vulnerability of current AI-generated text detectors when confronted with paraphrasing attacks. This research specifically examines the interaction between RLHF and detection methods, assessing whether RLHF-induced texts open up new challenges or opportunities for detection. The findings of this study are expected to contribute to the development of more robust detection techniques capable of keeping pace with the rapid evolution of LLMs.",
            "score": 0.5762926428013555,
            "section_title": "LLM-Generated Text Detection",
            "char_start_offset": 7137,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2043
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73974609375
        },
        {
            "corpus_id": "258865367",
            "title": "LLMDet: A Third Party Large Language Models Generated Text Detection Tool",
            "text": "White-box detection methods require full access to LLMs, thereby enabling control over the generation behavior of the model or embedding watermark within the generated text (Abdelnabi and Fritz, 2021;Ueoka et al., 2021;Dai et al., 2022). This enables the tracking and detection of machinegenerated text within white-box settings. \n\nThe current state-of-the-art approach, as proposed by (Kirchenbauer et al., 2023), partitions the model's vocabulary into whitelist and blacklist tokens when predicting the next token given a prompt. During text generation, the goal is to produce whitelist tokens as much as possible, effectively creating a strong watermark. Third parties can determine if the text is machine-generated by analyzing the frequency of whitelist tokens within the text. While watermarking methods offer robustness and interpretability, they can compromise the quality of the generated text and may not be highly practical in certain scenarios (Sadasivan et al., 2023).",
            "score": 0.5759719276181958,
            "section_title": "White-box Detection",
            "char_start_offset": 6255,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 329
                },
                {
                    "start": 332,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 981
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 200,
                    "matchedPaperCorpusId": "221516138"
                },
                {
                    "start": 200,
                    "end": 219,
                    "matchedPaperCorpusId": "233307237"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93505859375
        },
        {
            "corpus_id": "264129505",
            "title": "Using AI-based detectors to control AI-assisted plagiarism in ESL writing: \u201cThe Terminator Versus the Machines\u201d",
            "text": "Simple classifiers are basic machine-learning algorithms trained from scratch to discriminate between human-written and machine-generated texts (Solaiman et al., 2019). A basic form of this classifier is the \"bag-of-words, \" which is typically a logistic regression model trained on large sets of machine-generated and human-written texts to identify subtle stylistic or linguistic differences between the two categories (Jawahar et al., 2020). Solaiman et al. (2019) experimented with training a simple classifier using logistic regression to discriminate between GPT-2 output and internet articles across different model sizes and sampling methods. The researchers found that detection accuracy varied depending on the generative model's size, as texts generated by larger models were more challenging to detect. Another form of simple classifiers is machine-configuration detectors, which distinguish machine-generated texts by identifying distinct traces of the modeling choices used to generate a text (e.g., model size, decoding method, or model architecture) and use these traces to determine the origin of a text (Jawahar et al., 2020). For instance, Tay et al. (2020) empirically investigated the potential use of textual artifacts that appear in machine-generated texts due to modeling and sampling choices to identify their generative language model. Using texts generated by GROVER (Zeller et al., 2019) language model in different modeling conditions (e.g., model size) and the CNN/Dailymail news corpus (as human-written texts), the researchers trained and tested a classifier model to distinguish between texts based on modeling choices' artifacts. The experiments revealed that modeling choices left artifacts in the generated texts that could be used to predict modeling choices based on the generated texts alone and could be the basis of a new approach to machine configuration detection. These studies suggest that large language models challenge simple classifiers trained from scratch. Still, they can be used to identify traces of a language model in a generated text, which can contribute to future innovations in AI-text detection.",
            "score": 0.5759133054186021,
            "section_title": "Simple classifiers",
            "char_start_offset": 20911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2156
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "273654899",
            "title": "DeTeCtive: Detecting AI-generated Text via Multi-Level Contrastive Learning",
            "text": "Recently, the field of large language models (LLMs) [6,12,60,69] has witnessed swift advancements, bringing great convenience to both professional settings and daily life. However, the widespread use of AI-generated text also poses threats to global information security, manifesting in the propagation of disinformation, misinformation, and content that can incite harmful or destructive behaviors [16]. Hence, the detection of AI-generated text has ascended as a task of vital importance. \n\nOn the other hand, with the advancement of LLMs, the task of AI-generated text detection has elevated into an escalating challenge. Early methods, such as watermarking methods [23,32] and statisticalbased methods [63,47] encountered performance bottlenecks due to their reliance on manually hand-crafted forms. Moreover, the inherent inability to swiftly adapt to newly emerged LLMs further restricts their effectiveness. In stark contrast, recent training-based methods [11,27,24] have showcased notable improvements in performance. However, they remain constrained by the necessity of precisely paired training data and exhibit unsatisfactory generalization in out-of-distribution (OOD) detection scenarios due to the fixed binary classification formulation. \n\nIn this paper, to overcome these challenges, we revisit AI-generated text detection and approach the problem from a fresh perspective. Individual authors invariably exhibit unique writing styles, collectively constituting a vast feature space of writing styles. Our key insight is that an LLM can be viewed as a specific author, with the text it generates conforming consistently to its unique style. In line with this key observation, we propose to reformulate AI-generated text detection as a task of distinguishing diverse writing styles within the feature space, rather than merely treating it as a binary classification problem between human-written and AI-generated. This reformulation presents a fresh perspective from which to approach the detection of AI-generated text. \n\nWhile distinguishing writing styles within a vast feature space may seem more challenging than binary classification, we can take advantage of mature techniques within the field of Natural Language Processing (NLP). Specifically, contrastive learning [9,25,21] employs a self-supervised approach to identify similarities and differences between positive and negative samples, thereby acquiring discriminative feature representations.",
            "score": 0.5754803562503713,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 490
                },
                {
                    "start": 493,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1253
                },
                {
                    "start": 1256,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2035
                },
                {
                    "start": 2038,
                    "end": 2253
                },
                {
                    "start": 2254,
                    "end": 2471
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 55,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 55,
                    "end": 58,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 61,
                    "end": 64,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 673,
                    "end": 676,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 710,
                    "end": 713,
                    "matchedPaperCorpusId": "256274849"
                },
                {
                    "start": 968,
                    "end": 971,
                    "matchedPaperCorpusId": "259501842"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92236328125
        },
        {
            "corpus_id": "258832418",
            "title": "G3Detector: General GPT-Generated Text Detector",
            "text": "Synthetic Test Data from GPT3.5-turbo For WMT, the machine-generated data is derived from translation of German sentences into English using the GPT3.5/4 model. The synthetic data related to Law and Medical fields is obtained through paraphrasing humanwritten text using GPT3.5/4. The Awesome data comprises a collection of prompt examples intended for practical applications. \n\n2023). Beyond disseminating distorted news, the potential misappropriation of LLMs may engender a myriad of social and ethical dilemmas, including academic malfeasance and election manipulation. This incident underscores the escalating urgency within scholarly communities to devise strategies for the detection and thorough scrutiny of machinegenerated text. \n\nIn the realm of scholarly research, there are two primary schools for discerning machine-generated text. The first approach defines the detection of machine-generated text versus human-generated text as a classification problem (Solaiman et al., 2019;Ippolito et al., 2020;Munir et al., 2021;Rodriguez et al., 2022). The second technique is to use watermarking algorithms to embed arXiv:2305.12680v1 [cs.CL] 22 May 2023 unique self-identifiable information to the generative model (Venugopal et al., 2011;Kirchenbauer et al., 2023). This identifier can subsequently be utilized to differentiate between text authored by humans and text generated by machines. In this work, we mainly focus on the classification approach, which can be easily adapted for identifying plagiarism, detecting less trustworthy matchinegenerated content in social media, etc. \n\nDespite emerging endeavors on synthetic text detection, as demonstrated in Table 1, prior detection systems (e.g., GPTZero2 and the OpenAI detector (OpenAI, 2023)) are often inadequate in identifying synthetic data produced by state-of-theart LLMs, such as GPT3.5-turbo (also known as ChatGPT) and GPT4.",
            "score": 0.5752611488637672,
            "section_title": "Detectors",
            "char_start_offset": 771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 376
                },
                {
                    "start": 379,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1592
                },
                {
                    "start": 1595,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1898
                }
            ],
            "ref_mentions": [
                {
                    "start": 992,
                    "end": 1014,
                    "matchedPaperCorpusId": "218560609"
                },
                {
                    "start": 1014,
                    "end": 1033,
                    "matchedPaperCorpusId": "233189559"
                },
                {
                    "start": 1033,
                    "end": 1056,
                    "matchedPaperCorpusId": "250390908"
                },
                {
                    "start": 1222,
                    "end": 1246,
                    "matchedPaperCorpusId": "15210695"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62158203125
        },
        {
            "corpus_id": "273098310",
            "title": "A Watermark for Black-Box Language Models",
            "text": "It can be critical to understand whether a piece of text is generated by a large language model (LLM). For instance, one often wants to know how trustworthy a piece of text is, and those written by an LLM may be deemed untrustworthy as these models can hallucinate. The goal of watermarking is to cleverly bias the LLM so that detecting its generations becomes easier. Most proposed techniques do not modify the underlying LLM's model weights or its training procedure but rather inject the watermark during autoregressive decoding at inference time. They require access to the next-token logits and inject the watermark every step of the sampling loop. This required access prevents third-party users of an LLM from applying their own watermark as proprietary APIs currently do not support this option. Supporting this functionality presents a security risk in addition to significant engineering considerations. Concretely, Carlini et al. (2024) showed that parts of a production language model can be stolen from API access that exposes logits. In this work, we propose a watermarking scheme that gives power back to the people -third-party users can watermark a language model given nothing more than the ability to sample sequences from it. Our scheme is faithful to the underlying language model and it can outperform existing white-box schemes.",
            "score": 0.5751139077039326,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1351
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92041015625
        },
        {
            "corpus_id": "267897651",
            "title": "RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts",
            "text": "Today, due to the remarkable advancements in Natural Language Processing (NLP), models like ChatGPT, GPT-3, Gemini (formerly known as Bard), and others have reached a point where they can generate texts that closely resemble human writing. Consequently, the task of identifying texts generated by AI has become increasingly important. This task holds significant value across various domains, including content moderation, plagiarism detection, and ensuring transparency in AIgenerated content. The approaches for this task can be categorized into three categories: (1) Deep Learning-based Detection, (2) Statistical Discrepancy Detection, and (3) Watermark-based Detection. Deep learning-based models can be formulated as a classification task where the input is a text that can be generated by either a human or an AI. The model is trained with labeled data, where each text is assigned a label indicating whether it was generated by AI or by a human. This allows the model to learn patterns and features that can accurately classify texts based on their origin. These methods are susceptible to adversarial attacks, which can manipulate the input text to deceive the model's classification. However, deep learning-based models generally demonstrate good performance on the training data distribution (Guo et al., 2023). Statistical Discrepancy Detection methods first learn the patterns of AI-generated and human-written texts separately. Then, they identify statistical discrepancies between these patterns to distinguish between the two. By analyzing various linguistic features, such as word frequencies, sentence structures, or syntactic patterns, these methods can detect differences that arise from the distinct nature of AI-generated and human-written texts. Some tools like GPTZero (Mitchell et al., 2023) use perplexity (how well a language model predicts the next word based on the previous ones) and burstiness (variations in sentence length) to assess Figure 1: The input text is divided into meaningful units, and the probability of each segment based on their logits is assessed using a fine-tuned XLM-RoBERTa model; the combined evidence leads to a definitive classification. \n\nwhether the text is AI-generated or human-written.",
            "score": 0.5748669139555911,
            "section_title": "Background",
            "char_start_offset": 2006,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 2193
                },
                {
                    "start": 2196,
                    "end": 2246
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.939453125
        },
        {
            "corpus_id": "268793677",
            "title": "The Impact of Prompts on Zero-Shot Detection of AI-Generated Text",
            "text": "In recent years, there have been significant advancements in the development of Large Language Models (LLMs). While their practical applications are now widespread, their potential for misuse, such as generating fake news and committing plagiarism, has posed significant concerns. To address this issue, detectors have been developed to evaluate whether a given text is human-generated or AI-generated. Among others, zero-shot detectors stand out as effective approaches that do not require additional training data and are often likelihood-based. In chat-based applications, users commonly input prompts and utilize the AI-generated texts. However, zero-shot detectors typically analyze these texts in isolation, neglecting the impact of the original prompts. It is conceivable that this approach may lead to a discrepancy in likelihood assessments between the text generation phase and the detection phase. So far, there remains an unverified gap concerning how the presence or absence of prompts impacts detection accuracy for zero-shot detectors. In this paper, we introduce an evaluative framework to empirically analyze the impact of prompts on the detection accuracy of AI-generated text. We assess various zero-shot detectors using both white-box detection, which leverages the prompt, and black-box detection, which operates without prompt information. Our experiments reveal the significant influence of prompts on detection accuracy. Remarkably, compared with black-box detection without prompts, the white-box methods using prompts demonstrate an increase in AUC of at least $0.1$ across all zero-shot detectors tested. Code is available: \\url{https://github.com/kaito25atugich/Detector}.",
            "score": 0.5746105308051994,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94775390625
        },
        {
            "corpus_id": "276116307",
            "title": "SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models",
            "text": "The rapid proliferation of large language models (LLMs) has created an urgent need for reliable methods to detect whether a text is generated by such models. In this paper, we propose SimMark, a posthoc watermarking algorithm that makes LLMs' outputs traceable without requiring access to the model's internal logits, enabling compatibility with a wide range of LLMs, including API-only models. By leveraging the similarity of semantic sentence embeddings and rejection sampling to impose detectable statistical patterns imperceptible to humans, and employing a soft counting mechanism, SimMark achieves robustness against paraphrasing attacks. Experimental results demonstrate that SimMark sets a new benchmark for robust watermarking of LLM-generated content, surpassing prior sentence-level watermarking techniques in robustness, sampling efficiency, and applicability across diverse domains, all while preserving the text quality.",
            "score": 0.5738127812110826,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98388671875
        },
        {
            "corpus_id": "257921485",
            "title": "To ChatGPT, or not to ChatGPT: That is the question!",
            "text": "OpenAI [33] has also developed a GPT-2 detector using a 1.5 billion parameter GPT-2 model that can identify the top 40 generated outputs with an accuracy of 83% to 85%. However, when the model was fine-tuned to the Amazon reviews dataset, the accuracy dropped to 76%. In a different study [25], the authors explored the Zero-shot detection of AI-generated text and deployed an online detection tool (DetectGPT) to distinguish GPT-2 generated text from the human-generated text. They used the generative model's log probabilities to achieve this. The authors experimented and demonstrated that AI-generated text occupies the negative curvature regions of the model's log probability function. However, it should be noted that the authors assumed that one could evaluate the log probabilities of the model(s) under consideration, which may not always be possible. Moreover, as mentioned by the authors, this approach is only practical for GPT-2 prompts. Zellers et al. [39] utilized a transformer identical to the one used for GPT-2, except that they used nucleus sampling instead of top-k sampling to select the next word during text generation. The model they developed, known as Grover, can generate text such as fake news and detect its own generated text. It is also available online. Authors used Grover, GPT-2 (124M or 355M ), BERT (BERT-Base or BERT-Large), and FastText verification tools to classify the news articles generated by Grover. They proved that Grover is the best among the previously mentioned detector to verify its selfgenerated fake news. However, it's not visible that it will work for the text generated by the GPT models. Also, it has been shown that the bi-directional transformer model RoBERTa outperforms Grover models with equivalent parameter size in detecting GPT-2 texts [33].",
            "score": 0.5736194312581697,
            "section_title": "B. Zero-shot detection techniques",
            "char_start_offset": 7624,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1809
                }
            ],
            "ref_mentions": [
                {
                    "start": 967,
                    "end": 971,
                    "matchedPaperCorpusId": "168169824"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79296875
        },
        {
            "corpus_id": "268819799",
            "title": "A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules",
            "text": "Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of incorrectly classifying LLM-generated text as human-written). Our framework further reduces the problem of determining the optimal detection rule to solving a minimax optimization program. We apply this framework to two representative watermarks -- one of which has been internally implemented at OpenAI -- and obtain several findings that can be instrumental in guiding the practice of implementing watermarks. In particular, we derive optimal detection rules for these watermarks under our framework. These theoretically derived detection rules are demonstrated to be competitive and sometimes enjoy a higher power than existing detection approaches through numerical experiments.",
            "score": 0.5735401811517837,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96044921875
        },
        {
            "corpus_id": "268732682",
            "title": "WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models",
            "text": "Large Language Models (LLMs) have progressed tremendously and are capable of generating highquality texts for a diverse range of tasks.While these systems enhance automation, concerns arise about potential misuse, such as students using chat assistants for assignments or malicious users generating fake news articles.To counter this, current work has introduced the idea of LLM watermarking (Kirchenbauer et al., 2023a), where imperceptible patterns are injected into the generated text, enabling the statistical identification of whether text was generated by an LLM or not.However, most proposed watermarking schemes restrict the output generation space, which may lead to a trade-off Figure 1: High-level overview of the WaterJudge Framework: Given a system, watermarking parameters, and set of inputs, watermarked outputs are assessed in terms of quality and detectability, leading to a curve over all operating points.between quality and watermarking detection performance.Although there has been great effort into improving watermarking schemes for LLMs (Yoo et al., 2023;Kuditipudi et al., 2023;Kirchenbauer et al., 2023b), less work has analyzed the resulting quality degradation.It is common for watermarking schemes to measure quality by reporting the perplexity from a larger pre-trained LLM (Kirchenbauer et al., 2023a;Takezawa et al., 2023;Wang et al., 2023;Zhao et al., 2023;Ren et al., 2023;Liu et al., 2023), or to report similarity metrics such as BLEU or ROUGE (Fu et al., 2023;Takezawa et al., 2023;Li et al., 2023;Kirchenbauer et al., 2023b), however, these metrics are simplistic heuristics and may not truly capture actual output text quality, as discussed by Zhong et al. (2022); Wang et al. (2022); Zheng et al. (2023).\n\nThis work proposes WaterJudge, a framework for analyzing the trade-off between watermarking detectability and the quality of generated watermarked text.",
            "score": 0.5728131959887863,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 135,
                    "end": 318
                },
                {
                    "start": 318,
                    "end": 576
                },
                {
                    "start": 576,
                    "end": 924
                },
                {
                    "start": 924,
                    "end": 979
                },
                {
                    "start": 979,
                    "end": 1189
                },
                {
                    "start": 1189,
                    "end": 1744
                },
                {
                    "start": 1746,
                    "end": 1898
                }
            ],
            "ref_mentions": [
                {
                    "start": 392,
                    "end": 420,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 1304,
                    "end": 1332,
                    "matchedPaperCorpusId": "256194179"
                },
                {
                    "start": 1519,
                    "end": 1535,
                    "matchedPaperCorpusId": "265220983"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92236328125
        },
        {
            "corpus_id": "273877389",
            "title": "Distinguishing LLM-Generated from Human-Written Code by Contrastive Learning",
            "text": "The recently released work by Nguyen et al. [66], independent and concurrent to our work, proposes a CodeBERT-based classifier named GPTSniffer that is similar to our idea. Our work is different in the following aspects: 1) Nguyen et al. focus on assessing the impact of different preprocessing settings on prediction performance instead of developing the best AI-generated code detector, while our work proposes a novel detection approach that fine-tunes UniXcoder with contrastive learning and achieves optimal prediction performance; 2) The dataset used in Nguyen et al. 's work is limited in scale, consisting of only 1.5K code snippets derived from common programming tasks or exercises of a particular book [52]. In contrast, our dataset consists of 550K pairs of human-written and ChatGPT-generated code sourced from GitHub, which more closely replicates real-world software development scenarios; 3) Apart from evaluation experiments, we also perform a qualitative analysis to discover certain characteristics of ChatGPT-generated code. Different from our training-based method, Yang et al. [104] concentrate on the zero-shot detection of LLM-generated code and introduce DetectGPT4Code, a method utilizing a small proxy model to approximate the logits on the conditional probability curve. Inspired by previous text watermarking techniques [44], Lee et al. [48] propose watermarking for code generation, which involves selectively injecting watermarks to tokens with entropy higher than a predefined threshold, to enhance the detectability and generated code quality.",
            "score": 0.5725510965179565,
            "section_title": "LLM-generated Content Detection",
            "char_start_offset": 62010,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1576
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 48,
                    "matchedPaperCorpusId": "269197535"
                },
                {
                    "start": 1349,
                    "end": 1353,
                    "matchedPaperCorpusId": "256194179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63330078125
        },
        {
            "corpus_id": "267750325",
            "title": "k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text",
            "text": "The proliferation of large language models capable of generating realistic texts has drastically increased the need to detect machine-generated text. By proposing k-SEMSTAMP, we hope that practitioners will use this as a tool for governing modelgenerated texts. Although k-SEMSTAMP shows promising paraphrastic robustness, it is still not perfect for all kinds of attacks and thus should not be solely relied on in all scenarios. Finally, we hope this work motivates future research interests in not only semantic watermarking but also general adversarial-robust methods for AI governance.",
            "score": 0.5719334396450633,
            "section_title": "Ethical Considerations",
            "char_start_offset": 15615,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 589
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86767578125
        },
        {
            "corpus_id": "260706073",
            "title": "STADEE: STAtistics-Based DEEp Detection of Machine Generated Text",
            "text": "This method leverages a PLM to compute statistical features, such as rank, for each token in the text sequence. Subsequently, these tokens are grouped into distinct buckets based on their rank, with the number of tokens in each bucket serving as input. Detection can be performed through human observation or by training a logistic regression classifier [13]. By extracting statistical features, this method exhibits significantly better generalizability than fine-tuning the PLM. Nonetheless, it faces challenges in effectively combining different types of statistical features and may lose the text's sequence information, resulting in a currently low recognition rate. \n\nIn order to address the critical limitations of existing methods in terms of generalizability and recognition rate, we introduce a novel detection approach called STAtistics-based DEEp detection (STADEE). This method merges the superior generalization of statistical features with the robust automatic modeling capabilities of neural networks. In STADEE, a Pretrained Language Model (PLM) extracts various statistical features from the original text, encompassing well-known features such as probability, rank, information entropy, and a newly designed feature called cumulative probability, specifically designed for nucleus sampling [14], a prevalent text generation sampling algorithm. Additionally, we transform the rank using the following equation: r \u2032 \u2190 log 10 r, since the extensive vocabulary of the PLM may lead to a long-tail effect, where numerous low-probability tokens exist at the tail of the distribution, but they exhibit significant differences in rank. These four features are combined into a tensor of shape [sequence_length, 4], preserving the original sequence information of the text. Subsequently, this tensor is input into a 3-layer Transformer-Encoder [15] model to determine whether the text is machine-generated. \n\nGeneralizability is a crucial factor in assessing the effectiveness of a detector for practical applications. The significance of generalizability stems from the unpredictability of how malicious actors generate text, including their choice of pre-trained language models, fine-tuning corpora, prompts during generation, and decoding algorithm hyperparameters. Unfortunately, this field has seen relatively few studies and suffers from a lack of standardized datasets.",
            "score": 0.5712591905238827,
            "section_title": "Introduction",
            "char_start_offset": 2286,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 671
                },
                {
                    "start": 674,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1914
                },
                {
                    "start": 1917,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2277
                },
                {
                    "start": 2278,
                    "end": 2385
                }
            ],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 358,
                    "matchedPaperCorpusId": "182952848"
                },
                {
                    "start": 1309,
                    "end": 1313,
                    "matchedPaperCorpusId": "127986954"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76513671875
        },
        {
            "paperId": "3f358e5bedaae0eb49849dce98edb516f0731df5",
            "corpusId": 258588289,
            "title": "DeepTextMark: Deep Learning based Text Watermarking for Detection of Large Language Model Generated Text",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 57,
            "citationCount": 32,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.05773",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2305.05773?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2305.05773, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2106504469",
                    "name": "Travis J. E. Munyer"
                },
                {
                    "authorId": "2113310369",
                    "name": "Xin Zhong"
                }
            ],
            "abstract": "The capabilities of text generators have grown with the rapid development of Large Language Models (LLM). To prevent potential misuse, the ability to detect whether texts are produced by LLM has become increasingly important. Several related works have attempted to solve this problem using binary classifiers that categorize input text as human-written or LLM-generated. However, these classifiers have been shown to be unreliable. As impactful decisions could be made based on the result of the classification, the text source detection needs to be high-quality. To this end, this paper presents DeepTextMark, a deep learning-based text watermarking method for text source detection. Applying Word2Vec and Sentence Encoding for watermark insertion and a transformer-based classifier for watermark detection, DeepTextMark achieves blindness, robustness, imperceptibility, and reliability simultaneously. As discussed further in the paper, these traits are indispensable for generic text source detection, and the application focus of this paper is on the text generated by LLM. DeepTextMark can be implemented as an \"add-on\" to existing text generation systems. That is, the method does not require access or modification to the text generation technique. Experiments have shown high imperceptibility, high detection accuracy, enhanced robustness, reliability, and fast running speed of DeepTextMark.",
            "corpus_id": "258588289",
            "text": "The capabilities of text generators have grown with the rapid development of Large Language Models (LLM). To prevent potential misuse, the ability to detect whether texts are produced by LLM has become increasingly important. Several related works have attempted to solve this problem using binary classifiers that categorize input text as human-written or LLM-generated. However, these classifiers have been shown to be unreliable. As impactful decisions could be made based on the result of the classification, the text source detection needs to be high-quality. To this end, this paper presents DeepTextMark, a deep learning-based text watermarking method for text source detection. Applying Word2Vec and Sentence Encoding for watermark insertion and a transformer-based classifier for watermark detection, DeepTextMark achieves blindness, robustness, imperceptibility, and reliability simultaneously. As discussed further in the paper, these traits are indispensable for generic text source detection, and the application focus of this paper is on the text generated by LLM. DeepTextMark can be implemented as an \"add-on\" to existing text generation systems. That is, the method does not require access or modification to the text generation technique. Experiments have shown high imperceptibility, high detection accuracy, enhanced robustness, reliability, and fast running speed of DeepTextMark.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.96337890625
        },
        {
            "paperId": "70886685fc76e5f25d41792eb60d9b984e7e316e",
            "corpusId": 272988111,
            "title": "VMAD: Visual-enhanced Multimodal Large Language Model for Zero-Shot Anomaly Detection",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 52,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.20146, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284570103",
                    "name": "Huilin Deng"
                },
                {
                    "authorId": "47029826",
                    "name": "Hongcheng Luo"
                },
                {
                    "authorId": "2000495780",
                    "name": "Wei Zhai"
                },
                {
                    "authorId": "2269706577",
                    "name": "Yang Cao"
                },
                {
                    "authorId": "2116398144",
                    "name": "Yu Kang"
                }
            ],
            "abstract": "Zero-shot anomaly detection (ZSAD) recognizes and localizes anomalies in previously unseen objects by establishing feature mapping between textual prompts and inspection images, demonstrating excellent research value in flexible industrial manufacturing. However, existing ZSAD methods are limited by closed-world settings, struggling to unseen defects with predefined prompts. Recently, adapting Multimodal Large Language Models (MLLMs) for Industrial Anomaly Detection (IAD) presents a viable solution. Unlike fixed-prompt methods, MLLMs exhibit a generative paradigm with open-ended text interpretation, enabling more adaptive anomaly analysis. However, this adaption faces inherent challenges as anomalies often manifest in fine-grained regions and exhibit minimal visual discrepancies from normal samples. To address these challenges, we propose a novel framework VMAD (Visual-enhanced MLLM Anomaly Detection) that enhances MLLM with visual-based IAD knowledge and fine-grained perception, simultaneously providing precise detection and comprehensive analysis of anomalies. Specifically, we design a Defect-Sensitive Structure Learning scheme that transfers patch-similarities cues from visual branch to our MLLM for improved anomaly discrimination. Besides, we introduce a novel visual projector, Locality-enhanced Token Compression, which mines multi-level features in local contexts to enhance fine-grained detection. Furthermore, we introduce the Real Industrial Anomaly Detection (RIAD), a comprehensive IAD dataset with detailed anomaly descriptions and analyses, offering a valuable resource for MLLM-based IAD development. Extensive experiments on zero-shot benchmarks, including MVTec-AD, Visa, WFDD, and RIAD datasets, demonstrate our superior performance over state-of-the-art methods. The code and dataset will be available soon.",
            "corpus_id": "272988111",
            "text": "Zero-shot anomaly detection (ZSAD) recognizes and localizes anomalies in previously unseen objects by establishing feature mapping between textual prompts and inspection images, demonstrating excellent research value in flexible industrial manufacturing. However, existing ZSAD methods are limited by closed-world settings, struggling to unseen defects with predefined prompts. Recently, adapting Multimodal Large Language Models (MLLMs) for Industrial Anomaly Detection (IAD) presents a viable solution. Unlike fixed-prompt methods, MLLMs exhibit a generative paradigm with open-ended text interpretation, enabling more adaptive anomaly analysis. However, this adaption faces inherent challenges as anomalies often manifest in fine-grained regions and exhibit minimal visual discrepancies from normal samples. To address these challenges, we propose a novel framework VMAD (Visual-enhanced MLLM Anomaly Detection) that enhances MLLM with visual-based IAD knowledge and fine-grained perception, simultaneously providing precise detection and comprehensive analysis of anomalies. Specifically, we design a Defect-Sensitive Structure Learning scheme that transfers patch-similarities cues from visual branch to our MLLM for improved anomaly discrimination. Besides, we introduce a novel visual projector, Locality-enhanced Token Compression, which mines multi-level features in local contexts to enhance fine-grained detection. Furthermore, we introduce the Real Industrial Anomaly Detection (RIAD), a comprehensive IAD dataset with detailed anomaly descriptions and analyses, offering a valuable resource for MLLM-based IAD development. Extensive experiments on zero-shot benchmarks, including MVTec-AD, Visa, WFDD, and RIAD datasets, demonstrate our superior performance over state-of-the-art methods. The code and dataset will be available soon.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.452392578125
        },
        {
            "paperId": "e5a363a4b93706b8a49d3aca91c97abe8972f816",
            "corpusId": 273185997,
            "title": "HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "referenceCount": 55,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.04380, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324784971",
                    "name": "Yuto Nishimura"
                },
                {
                    "authorId": "2313430818",
                    "name": "Takumi Hirose"
                },
                {
                    "authorId": "2313431366",
                    "name": "Masanari Ohi"
                },
                {
                    "authorId": "2324783785",
                    "name": "Hideki Nakayama"
                },
                {
                    "authorId": "2262445410",
                    "name": "Nakamasa Inoue"
                }
            ],
            "abstract": "Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) models using residual vector quantization (RVQ). However, long-form speech synthesis remains a significant challenge due to the high frame rate, which increases the length of audio tokens and makes it difficult for autoregressive language models to generate audio tokens for even a minute of speech. To address this challenge, this paper introduces two novel post-training approaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E. MReQ is a framework to reduce the frame rate of pre-trained NAC models. Specifically, it incorporates multi-resolution residual vector quantization (MRVQ) module that hierarchically reorganizes discrete audio tokens through teacher-student distillation. HALL-E is an LLM-based TTS model designed to predict hierarchical tokens of MReQ. Specifically, it incorporates the technique of using MRVQ sub-modules and continues training from a pre-trained LLM-based TTS model. Furthermore, to promote TTS research, we create MinutesSpeech, a new benchmark dataset consisting of 40k hours of filtered speech data for training and evaluating speech synthesis ranging from 3s up to 180s. In experiments, we demonstrated the effectiveness of our approaches by applying our post-training framework to VALL-E. We achieved the frame rate down to as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single inference step. Audio samples, dataset, codes and pre-trained models are available at https://yutonishimura-v2.github.io/HALL-E_DEMO/.",
            "corpus_id": "273185997",
            "text": "Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) models using residual vector quantization (RVQ). However, long-form speech synthesis remains a significant challenge due to the high frame rate, which increases the length of audio tokens and makes it difficult for autoregressive language models to generate audio tokens for even a minute of speech. To address this challenge, this paper introduces two novel post-training approaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E. MReQ is a framework to reduce the frame rate of pre-trained NAC models. Specifically, it incorporates multi-resolution residual vector quantization (MRVQ) module that hierarchically reorganizes discrete audio tokens through teacher-student distillation. HALL-E is an LLM-based TTS model designed to predict hierarchical tokens of MReQ. Specifically, it incorporates the technique of using MRVQ sub-modules and continues training from a pre-trained LLM-based TTS model. Furthermore, to promote TTS research, we create MinutesSpeech, a new benchmark dataset consisting of 40k hours of filtered speech data for training and evaluating speech synthesis ranging from 3s up to 180s. In experiments, we demonstrated the effectiveness of our approaches by applying our post-training framework to VALL-E. We achieved the frame rate down to as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single inference step. Audio samples, dataset, codes and pre-trained models are available at https://yutonishimura-v2.github.io/HALL-E_DEMO/.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.181884765625
        },
        {
            "paperId": "b9c1d0a8d0a0beca3756f511f952e51b06cb5cec",
            "corpusId": 268156717,
            "title": "Sentiment and Interest Detection in Social Media using GPT-based Large Language Models",
            "venue": "MLNLP",
            "year": 2023,
            "referenceCount": 20,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3639479.3639523?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3639479.3639523, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305870882",
                    "name": "Md Abdullah Al Asad"
                },
                {
                    "authorId": "2288273834",
                    "name": "Hasan Md Imran"
                },
                {
                    "authorId": "2253708581",
                    "name": "Md Alamin"
                },
                {
                    "authorId": "2284078026",
                    "name": "Tareque Abu Abdullah"
                },
                {
                    "authorId": "2284080749",
                    "name": "Suriya Islam Chowdhury"
                }
            ],
            "abstract": "In the ever-expanding realm of social media, deciphering sentiments and identifying relevant topics from various textual content is challenging. This paper comprehensively investigates sentiment analysis and interest/topic detection using cutting-edge language models, ChatGPT3.5 and gpt4All. The methodology encompasses data collection, meticulous text pre-processing, innovative prompt design, and the exploration of zero-shot, one-shot, and few-shot learning techniques. We unveil the nuances of model performance in sentiment analysis and interest/topic detection through a detailed comparative analysis. Our findings highlight the power of ChatGPT3.5 in achieving a substantial accuracy enhancement in sentiment analysis compared to gpt4All. Moreover, we delve into the intricacies of interest detection, demonstrating the complexities of linguistic structures and model biases. We offer a holistic view of social entities\u2019 preferences by categorizing topics into distinct domains. A web portal developed using Google\u2019s Flutter SDK facilitates the visualization of user-friendly sentiment and interest outcomes. This research contributes to the understanding of sentiment analysis and interest detection and underscores the evolving capabilities of AI and NLP in navigating the dynamic landscape of social media content.",
            "corpus_id": "268156717",
            "text": "In the ever-expanding realm of social media, deciphering sentiments and identifying relevant topics from various textual content is challenging. This paper comprehensively investigates sentiment analysis and interest/topic detection using cutting-edge language models, ChatGPT3.5 and gpt4All. The methodology encompasses data collection, meticulous text pre-processing, innovative prompt design, and the exploration of zero-shot, one-shot, and few-shot learning techniques. We unveil the nuances of model performance in sentiment analysis and interest/topic detection through a detailed comparative analysis. Our findings highlight the power of ChatGPT3.5 in achieving a substantial accuracy enhancement in sentiment analysis compared to gpt4All. Moreover, we delve into the intricacies of interest detection, demonstrating the complexities of linguistic structures and model biases. We offer a holistic view of social entities\u2019 preferences by categorizing topics into distinct domains. A web portal developed using Google\u2019s Flutter SDK facilitates the visualization of user-friendly sentiment and interest outcomes. This research contributes to the understanding of sentiment analysis and interest detection and underscores the evolving capabilities of AI and NLP in navigating the dynamic landscape of social media content.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.28076171875
        },
        {
            "paperId": "827ee5f812746b2b4851a60e023622cfedfdb964",
            "corpusId": 276920734,
            "title": "Detection avoidance techniques for large language models",
            "venue": "Data & Policy",
            "year": 2025,
            "referenceCount": 61,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1017/dap.2025.6",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.07595, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2147389103",
                    "name": "Sinclair Schneider"
                },
                {
                    "authorId": "1452354993",
                    "name": "Florian Steuber"
                },
                {
                    "authorId": "2261737794",
                    "name": "Jo\u00e3o A. G. Schneider"
                },
                {
                    "authorId": "2349460082",
                    "name": "Gabi Dreo Rodosek"
                }
            ],
            "abstract": "Abstract The increasing popularity of large language models has not only led to widespread use but has also brought various risks, including the potential for systematically spreading fake news. Consequently, the development of classification systems such as DetectGPT has become vital. These detectors are vulnerable to evasion techniques, as demonstrated in an experimental series: Systematic changes of the generative models\u2019 temperature proofed shallow learning\u2014detectors to be the least reliable (Experiment 1). Fine-tuning the generative model via reinforcement learning circumvented BERT-based\u2014detectors (Experiment 2). Finally, rephrasing led to a >90% evasion of zero-shot\u2014detectors like DetectGPT, although texts stayed highly similar to the original (Experiment 3). A comparison with existing work highlights the better performance of the presented methods. Possible implications for society and further research are discussed.",
            "corpus_id": "276920734",
            "text": "Abstract The increasing popularity of large language models has not only led to widespread use but has also brought various risks, including the potential for systematically spreading fake news. Consequently, the development of classification systems such as DetectGPT has become vital. These detectors are vulnerable to evasion techniques, as demonstrated in an experimental series: Systematic changes of the generative models\u2019 temperature proofed shallow learning\u2014detectors to be the least reliable (Experiment 1). Fine-tuning the generative model via reinforcement learning circumvented BERT-based\u2014detectors (Experiment 2). Finally, rephrasing led to a >90% evasion of zero-shot\u2014detectors like DetectGPT, although texts stayed highly similar to the original (Experiment 3). A comparison with existing work highlights the better performance of the presented methods. Possible implications for society and further research are discussed.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.88916015625
        },
        {
            "paperId": "3d181992f7b6b65c889346ee7ea99fdc1570d9b6",
            "corpusId": 264426761,
            "title": "PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "referenceCount": 54,
            "citationCount": 27,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.14192, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2064714291",
                    "name": "Gaurav Sahu"
                },
                {
                    "authorId": "1712417",
                    "name": "Olga Vechtomova"
                },
                {
                    "authorId": "3335364",
                    "name": "Dzmitry Bahdanau"
                },
                {
                    "authorId": "3266173",
                    "name": "I. Laradji"
                }
            ],
            "abstract": "Data augmentation is a widely used technique to address the problem of text classification when there is a limited amount of training data. Recent work often tackles this problem using large language models (LLMs) like GPT3 that can generate new examples given already available ones. In this work, we propose a method to generate more helpful augmented data by utilizing the LLM's abilities to follow instructions and perform few-shot classifications. Our specific PromptMix method consists of two steps: 1) generate challenging text augmentations near class boundaries; however, generating borderline examples increases the risk of false positives in the dataset, so we 2) relabel the text augmentations using a prompting-based LLM classifier to enhance the correctness of labels in the generated data. We evaluate the proposed method in challenging 2-shot and zero-shot settings on four text classification datasets: Banking77, TREC6, Subjectivity (SUBJ), and Twitter Complaints. Our experiments show that generating and, crucially, relabeling borderline examples facilitates the transfer of knowledge of a massive LLM like GPT3.5-turbo into smaller and cheaper classifiers like DistilBERT$_{base}$ and BERT$_{base}$. Furthermore, 2-shot PromptMix outperforms multiple 5-shot data augmentation methods on the four datasets. Our code is available at https://github.com/ServiceNow/PromptMix-EMNLP-2023.",
            "corpus_id": "264426761",
            "text": "Data augmentation is a widely used technique to address the problem of text classification when there is a limited amount of training data. Recent work often tackles this problem using large language models (LLMs) like GPT3 that can generate new examples given already available ones. In this work, we propose a method to generate more helpful augmented data by utilizing the LLM's abilities to follow instructions and perform few-shot classifications. Our specific PromptMix method consists of two steps: 1) generate challenging text augmentations near class boundaries; however, generating borderline examples increases the risk of false positives in the dataset, so we 2) relabel the text augmentations using a prompting-based LLM classifier to enhance the correctness of labels in the generated data. We evaluate the proposed method in challenging 2-shot and zero-shot settings on four text classification datasets: Banking77, TREC6, Subjectivity (SUBJ), and Twitter Complaints. Our experiments show that generating and, crucially, relabeling borderline examples facilitates the transfer of knowledge of a massive LLM like GPT3.5-turbo into smaller and cheaper classifiers like DistilBERT$_{base}$ and BERT$_{base}$. Furthermore, 2-shot PromptMix outperforms multiple 5-shot data augmentation methods on the four datasets. Our code is available at https://github.com/ServiceNow/PromptMix-EMNLP-2023.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.56103515625
        },
        {
            "paperId": "c13a368c710863fada90be1e27157847394d8667",
            "corpusId": 266999641,
            "title": "Harnessing Large Language Models Over Transformer Models for Detecting Bengali Depressive Social Media Text: A Comprehensive Study",
            "venue": "Natural Language Processing Journal",
            "year": 2024,
            "referenceCount": 56,
            "citationCount": 17,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.07310, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279549992",
                    "name": "Ahmadul Karim Chowdhury"
                },
                {
                    "authorId": "2279547872",
                    "name": "Saidur Rahman Sujon"
                },
                {
                    "authorId": "2279548082",
                    "name": "Md. Shirajus Salekin Shafi"
                },
                {
                    "authorId": "2279545470",
                    "name": "Tasin Ahmmad"
                },
                {
                    "authorId": "1387850316",
                    "name": "Sifat Ahmed"
                },
                {
                    "authorId": "68973629",
                    "name": "Khan Md Hasib"
                },
                {
                    "authorId": "2279832386",
                    "name": "Faisal Muhammad Shah"
                }
            ],
            "abstract": "In an era where the silent struggle of underdiagnosed depression pervades globally, our research delves into the crucial link between mental health and social media. This work focuses on early detection of depression, particularly in extroverted social media users, using LLMs such as GPT 3.5, GPT 4 and our proposed GPT 3.5 fine-tuned model DepGPT, as well as advanced Deep learning models(LSTM, Bi-LSTM, GRU, BiGRU) and Transformer models(BERT, BanglaBERT, SahajBERT, BanglaBERT-Base). The study categorized Reddit and X datasets into\"Depressive\"and\"Non-Depressive\"segments, translated into Bengali by native speakers with expertise in mental health, resulting in the creation of the Bengali Social Media Depressive Dataset (BSMDD). Our work provides full architecture details for each model and a methodical way to assess their performance in Bengali depressive text categorization using zero-shot and few-shot learning techniques. Our work demonstrates the superiority of SahajBERT and Bi-LSTM with FastText embeddings in their respective domains also tackles explainability issues with transformer models and emphasizes the effectiveness of LLMs, especially DepGPT, demonstrating flexibility and competence in a range of learning contexts. According to the experiment results, the proposed model, DepGPT, outperformed not only Alpaca Lora 7B in zero-shot and few-shot scenarios but also every other model, achieving a near-perfect accuracy of 0.9796 and an F1-score of 0.9804, high recall, and exceptional precision. Although competitive, GPT-3.5 Turbo and Alpaca Lora 7B show relatively poorer effectiveness in zero-shot and few-shot situations. The work emphasizes the effectiveness and flexibility of LLMs in a variety of linguistic circumstances, providing insightful information about the complex field of depression detection models.",
            "corpus_id": "266999641",
            "text": "In an era where the silent struggle of underdiagnosed depression pervades globally, our research delves into the crucial link between mental health and social media. This work focuses on early detection of depression, particularly in extroverted social media users, using LLMs such as GPT 3.5, GPT 4 and our proposed GPT 3.5 fine-tuned model DepGPT, as well as advanced Deep learning models(LSTM, Bi-LSTM, GRU, BiGRU) and Transformer models(BERT, BanglaBERT, SahajBERT, BanglaBERT-Base). The study categorized Reddit and X datasets into\"Depressive\"and\"Non-Depressive\"segments, translated into Bengali by native speakers with expertise in mental health, resulting in the creation of the Bengali Social Media Depressive Dataset (BSMDD). Our work provides full architecture details for each model and a methodical way to assess their performance in Bengali depressive text categorization using zero-shot and few-shot learning techniques. Our work demonstrates the superiority of SahajBERT and Bi-LSTM with FastText embeddings in their respective domains also tackles explainability issues with transformer models and emphasizes the effectiveness of LLMs, especially DepGPT, demonstrating flexibility and competence in a range of learning contexts. According to the experiment results, the proposed model, DepGPT, outperformed not only Alpaca Lora 7B in zero-shot and few-shot scenarios but also every other model, achieving a near-perfect accuracy of 0.9796 and an F1-score of 0.9804, high recall, and exceptional precision. Although competitive, GPT-3.5 Turbo and Alpaca Lora 7B show relatively poorer effectiveness in zero-shot and few-shot situations. The work emphasizes the effectiveness and flexibility of LLMs in a variety of linguistic circumstances, providing insightful information about the complex field of depression detection models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.2398681640625
        },
        {
            "paperId": "d35192207aa333885a5eba66900d7c9538247580",
            "corpusId": 277321916,
            "title": "CLogLLM: A Large Language Model Enabled Approach to Cybersecurity Log Anomaly Analysis",
            "venue": "2024 4th International Conference on Electronic Information Engineering and Computer Communication (EIECC)",
            "year": 2024,
            "referenceCount": 20,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EIECC64539.2024.10929078?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EIECC64539.2024.10929078, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2352761659",
                    "name": "Hengyi Ren"
                },
                {
                    "authorId": "2352104993",
                    "name": "Kun Lan"
                },
                {
                    "authorId": "2278795507",
                    "name": "Zhi Sun"
                },
                {
                    "authorId": "2345884239",
                    "name": "Shan Liao"
                }
            ],
            "abstract": "Log data is commonly used to record the status of the system and the events that occur, which are often used to assist security staff to determine whether the system is abnormal, diagnose problems, troubleshooting. However, with the increasing complexity of the system, the log data becomes more and more numerous and complex, Therefore the analysis of the log becomes extremely difficult. Not coincidentally, with the emergence of various system software updates, traditional deep learning-based anomaly detection methods, which rely on pre-existing data for training, often struggle to maintain their effectiveness. At the same time, simply detecting anomalies in significant number of complex log data is no longer meet operational needs. Therefore, LLM which is capable of realizing zero-shot learning and text generation has attracted attention. This paper proposed method which is based on the implementation of LLMs for security log detection and analysis. The approach involves fine-tuning the LLMs to improve their adaptability to complex and diverse log data. Moreover, It incorporates two advanced prompt engineering techniques, ToT and Self-Refine, while also exploring the combination of existing techniques such as CoT and In-Context prompting. These three different prompt engineering methods significantly enhance the LLMs' the accuracy in log anomaly detection and the quality in generation of log anomaly analysis.",
            "corpus_id": "277321916",
            "text": "Log data is commonly used to record the status of the system and the events that occur, which are often used to assist security staff to determine whether the system is abnormal, diagnose problems, troubleshooting. However, with the increasing complexity of the system, the log data becomes more and more numerous and complex, Therefore the analysis of the log becomes extremely difficult. Not coincidentally, with the emergence of various system software updates, traditional deep learning-based anomaly detection methods, which rely on pre-existing data for training, often struggle to maintain their effectiveness. At the same time, simply detecting anomalies in significant number of complex log data is no longer meet operational needs. Therefore, LLM which is capable of realizing zero-shot learning and text generation has attracted attention. This paper proposed method which is based on the implementation of LLMs for security log detection and analysis. The approach involves fine-tuning the LLMs to improve their adaptability to complex and diverse log data. Moreover, It incorporates two advanced prompt engineering techniques, ToT and Self-Refine, while also exploring the combination of existing techniques such as CoT and In-Context prompting. These three different prompt engineering methods significantly enhance the LLMs' the accuracy in log anomaly detection and the quality in generation of log anomaly analysis.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.266357421875
        },
        {
            "paperId": "14cfe2588311870325e2770c5159d3100d7031ea",
            "corpusId": 270214464,
            "title": "Envisioning Outlier Exposure by Large Language Models for Out-of-Distribution Detection",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "referenceCount": 62,
            "citationCount": 14,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.00806, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2304614976",
                    "name": "Chentao Cao"
                },
                {
                    "authorId": "2304682665",
                    "name": "Zhun Zhong"
                },
                {
                    "authorId": "1768392566",
                    "name": "Zhanke Zhou"
                },
                {
                    "authorId": "2260822055",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2244770736",
                    "name": "Tongliang Liu"
                },
                {
                    "authorId": "2257285217",
                    "name": "Bo Han"
                }
            ],
            "abstract": "Detecting out-of-distribution (OOD) samples is essential when deploying machine learning models in open-world scenarios. Zero-shot OOD detection, requiring no training on in-distribution (ID) data, has been possible with the advent of vision-language models like CLIP. Existing methods build a text-based classifier with only closed-set labels. However, this largely restricts the inherent capability of CLIP to recognize samples from large and open label space. In this paper, we propose to tackle this constraint by leveraging the expert knowledge and reasoning capability of large language models (LLM) to Envision potential Outlier Exposure, termed EOE, without access to any actual OOD data. Owing to better adaptation to open-world scenarios, EOE can be generalized to different tasks, including far, near, and fine-grained OOD detection. Technically, we design (1) LLM prompts based on visual similarity to generate potential outlier class labels specialized for OOD detection, as well as (2) a new score function based on potential outlier penalty to distinguish hard OOD samples effectively. Empirically, EOE achieves state-of-the-art performance across different OOD tasks and can be effectively scaled to the ImageNet-1K dataset. The code is publicly available at: https://github.com/tmlr-group/EOE.",
            "corpus_id": "270214464",
            "text": "Detecting out-of-distribution (OOD) samples is essential when deploying machine learning models in open-world scenarios. Zero-shot OOD detection, requiring no training on in-distribution (ID) data, has been possible with the advent of vision-language models like CLIP. Existing methods build a text-based classifier with only closed-set labels. However, this largely restricts the inherent capability of CLIP to recognize samples from large and open label space. In this paper, we propose to tackle this constraint by leveraging the expert knowledge and reasoning capability of large language models (LLM) to Envision potential Outlier Exposure, termed EOE, without access to any actual OOD data. Owing to better adaptation to open-world scenarios, EOE can be generalized to different tasks, including far, near, and fine-grained OOD detection. Technically, we design (1) LLM prompts based on visual similarity to generate potential outlier class labels specialized for OOD detection, as well as (2) a new score function based on potential outlier penalty to distinguish hard OOD samples effectively. Empirically, EOE achieves state-of-the-art performance across different OOD tasks and can be effectively scaled to the ImageNet-1K dataset. The code is publicly available at: https://github.com/tmlr-group/EOE.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.285400390625
        },
        {
            "paperId": "d0fd60b71e71e5e4dd69f7fdf0ed5039b727884a",
            "corpusId": 267406414,
            "title": "A Comparative Analysis of Conversational Large Language Models in Knowledge-Based Text Generation",
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 32,
            "citationCount": 7,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.01495, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2186852856",
                    "name": "Phillip Schneider"
                },
                {
                    "authorId": "2277602092",
                    "name": "Manuel Klettner"
                },
                {
                    "authorId": "2274939157",
                    "name": "E. Simperl"
                },
                {
                    "authorId": "2243005966",
                    "name": "Florian Matthes"
                }
            ],
            "abstract": "Generating natural language text from graph-structured data is essential for conversational information seeking. Semantic triples derived from knowledge graphs can serve as a valuable source for grounding responses from conversational agents by providing a factual basis for the information they communicate. This is especially relevant in the context of large language models, which offer great potential for conversational interaction but are prone to hallucinating, omitting, or producing conflicting information. In this study, we conduct an empirical analysis of conversational large language models in generating natural language text from semantic triples. We compare four large language models of varying sizes with different prompting techniques. Through a series of benchmark experiments on the WebNLG dataset, we analyze the models\u2019 performance and identify the most common issues in the generated predictions. Our findings show that the capabilities of large language models in triple verbalization can be significantly improved through few-shot prompting, post-processing, and efficient fine-tuning techniques, particularly for smaller models that exhibit lower zero-shot performance.",
            "corpus_id": "267406414",
            "text": "Generating natural language text from graph-structured data is essential for conversational information seeking. Semantic triples derived from knowledge graphs can serve as a valuable source for grounding responses from conversational agents by providing a factual basis for the information they communicate. This is especially relevant in the context of large language models, which offer great potential for conversational interaction but are prone to hallucinating, omitting, or producing conflicting information. In this study, we conduct an empirical analysis of conversational large language models in generating natural language text from semantic triples. We compare four large language models of varying sizes with different prompting techniques. Through a series of benchmark experiments on the WebNLG dataset, we analyze the models\u2019 performance and identify the most common issues in the generated predictions. Our findings show that the capabilities of large language models in triple verbalization can be significantly improved through few-shot prompting, post-processing, and efficient fine-tuning techniques, particularly for smaller models that exhibit lower zero-shot performance.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.6962890625
        },
        {
            "paperId": "c686ec6f9ccd30611f93fcca3106cf15e69d5cbf",
            "corpusId": 270063275,
            "title": "TEII: Think, Explain, Interact and Iterate with Large Language Models to Solve Cross-lingual Emotion Detection",
            "venue": "Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",
            "year": 2024,
            "referenceCount": 19,
            "citationCount": 5,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.17129, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303427602",
                    "name": "Long Cheng"
                },
                {
                    "authorId": "2303404453",
                    "name": "Qihao Shao"
                },
                {
                    "authorId": "2303433256",
                    "name": "Christine Zhao"
                },
                {
                    "authorId": "2303399350",
                    "name": "Sheng Bi"
                },
                {
                    "authorId": "1733034",
                    "name": "Gina-Anne Levow"
                }
            ],
            "abstract": "Cross-lingual emotion detection allows us to analyze global trends, public opinion, and social phenomena at scale. We participated in the Explainability of Cross-lingual Emotion Detection (EXALT) shared task, achieving an F1-score of 0.6046 on the evaluation set for the emotion detection sub-task. Our system outperformed the baseline by more than 0.16 F1-score absolute, and ranked second amongst competing systems. We conducted experiments using fine-tuning, zero-shot learning, and few-shot learning for Large Language Model (LLM)-based models as well as embedding-based BiLSTM and KNN for non-LLM-based techniques. Additionally, we introduced two novel methods: the Multi-Iteration Agentic Workflow and the Multi-Binary-Classifier Agentic Workflow. We found that LLM-based approaches provided good performance on multilingual emotion detection. Furthermore, ensembles combining all our experimented models yielded higher F1-scores than any single approach alone.",
            "corpus_id": "270063275",
            "text": "Cross-lingual emotion detection allows us to analyze global trends, public opinion, and social phenomena at scale. We participated in the Explainability of Cross-lingual Emotion Detection (EXALT) shared task, achieving an F1-score of 0.6046 on the evaluation set for the emotion detection sub-task. Our system outperformed the baseline by more than 0.16 F1-score absolute, and ranked second amongst competing systems. We conducted experiments using fine-tuning, zero-shot learning, and few-shot learning for Large Language Model (LLM)-based models as well as embedding-based BiLSTM and KNN for non-LLM-based techniques. Additionally, we introduced two novel methods: the Multi-Iteration Agentic Workflow and the Multi-Binary-Classifier Agentic Workflow. We found that LLM-based approaches provided good performance on multilingual emotion detection. Furthermore, ensembles combining all our experimented models yielded higher F1-scores than any single approach alone.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.3017578125
        },
        {
            "paperId": "1ba422f7679c36cdd3df099088152131e81aa3b5",
            "corpusId": 268680354,
            "title": "EDDA: A Encoder-Decoder Data Augmentation Framework for Zero-Shot Stance Detection",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 37,
            "citationCount": 4,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.15715, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2199013050",
                    "name": "Daijun Ding"
                },
                {
                    "authorId": "2278003897",
                    "name": "Li Dong"
                },
                {
                    "authorId": "2158556264",
                    "name": "Zhichao Huang"
                },
                {
                    "authorId": "2216558848",
                    "name": "Guangning Xu"
                },
                {
                    "authorId": "2277650501",
                    "name": "Xu Huang"
                },
                {
                    "authorId": "2293283734",
                    "name": "Bo Liu"
                },
                {
                    "authorId": "2147240380",
                    "name": "Liwen Jing"
                },
                {
                    "authorId": "2208118550",
                    "name": "Bowen Zhang"
                }
            ],
            "abstract": "Stance detection aims to determine the attitude expressed in text towards a given target. Zero-shot stance detection (ZSSD) has emerged to classify stances towards unseen targets during inference. Recent data augmentation techniques for ZSSD increase transferable knowledge between targets through text or target augmentation. However, these methods exhibit limitations. Target augmentation lacks logical connections between generated targets and source text, while text augmentation relies solely on training data, resulting in insufficient generalization. To address these issues, we propose an encoder-decoder data augmentation (EDDA) framework. The encoder leverages large language models and chain-of-thought prompting to summarize texts into target-specific if-then rationales, establishing logical relationships. The decoder generates new samples based on these expressions using a semantic correlation word replacement strategy to increase syntactic diversity. We also analyze the generated expressions to develop a rationale-enhanced network that fully utilizes the augmented data. Experiments on benchmark datasets demonstrate our approach substantially improves over state-of-the-art ZSSD techniques. The proposed EDDA framework increases semantic relevance and syntactic variety in augmented texts while enabling interpretable rationale-based learning.",
            "corpus_id": "268680354",
            "text": "Stance detection aims to determine the attitude expressed in text towards a given target. Zero-shot stance detection (ZSSD) has emerged to classify stances towards unseen targets during inference. Recent data augmentation techniques for ZSSD increase transferable knowledge between targets through text or target augmentation. However, these methods exhibit limitations. Target augmentation lacks logical connections between generated targets and source text, while text augmentation relies solely on training data, resulting in insufficient generalization. To address these issues, we propose an encoder-decoder data augmentation (EDDA) framework. The encoder leverages large language models and chain-of-thought prompting to summarize texts into target-specific if-then rationales, establishing logical relationships. The decoder generates new samples based on these expressions using a semantic correlation word replacement strategy to increase syntactic diversity. We also analyze the generated expressions to develop a rationale-enhanced network that fully utilizes the augmented data. Experiments on benchmark datasets demonstrate our approach substantially improves over state-of-the-art ZSSD techniques. The proposed EDDA framework increases semantic relevance and syntactic variety in augmented texts while enabling interpretable rationale-based learning.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.469970703125
        },
        {
            "paperId": "96899ed54653e381b39d6fee4ed69420c225f46b",
            "corpusId": 268752488,
            "title": "GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation",
            "venue": "European Conference on Information Retrieval",
            "year": 2024,
            "referenceCount": 35,
            "citationCount": 16,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.03746, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "4834571",
                    "name": "Kaustubh D. Dhole"
                },
                {
                    "authorId": "1685296",
                    "name": "Eugene Agichtein"
                }
            ],
            "abstract": "Query Reformulation(QR) is a set of techniques used to transform a user's original search query to a text that better aligns with the user's intent and improves their search experience. Recently, zero-shot QR has been shown to be a promising approach due to its ability to exploit knowledge inherent in large language models. By taking inspiration from the success of ensemble prompting strategies which have benefited many tasks, we investigate if they can help improve query reformulation. In this context, we propose an ensemble based prompting technique, GenQREnsemble which leverages paraphrases of a zero-shot instruction to generate multiple sets of keywords ultimately improving retrieval performance. We further introduce its post-retrieval variant, GenQREnsembleRF to incorporate pseudo relevant feedback. On evaluations over four IR benchmarks, we find that GenQREnsemble generates better reformulations with relative nDCG@10 improvements up to 18% and MAP improvements upto 24% over the previous zero-shot state-of-art. On the MSMarco Passage Ranking task, GenQREnsembleRF shows relative gains of 5% MRR using pseudo-relevance feedback, and 9% nDCG@10 using relevant feedback documents.",
            "corpus_id": "268752488",
            "text": "Query Reformulation(QR) is a set of techniques used to transform a user's original search query to a text that better aligns with the user's intent and improves their search experience. Recently, zero-shot QR has been shown to be a promising approach due to its ability to exploit knowledge inherent in large language models. By taking inspiration from the success of ensemble prompting strategies which have benefited many tasks, we investigate if they can help improve query reformulation. In this context, we propose an ensemble based prompting technique, GenQREnsemble which leverages paraphrases of a zero-shot instruction to generate multiple sets of keywords ultimately improving retrieval performance. We further introduce its post-retrieval variant, GenQREnsembleRF to incorporate pseudo relevant feedback. On evaluations over four IR benchmarks, we find that GenQREnsemble generates better reformulations with relative nDCG@10 improvements up to 18% and MAP improvements upto 24% over the previous zero-shot state-of-art. On the MSMarco Passage Ranking task, GenQREnsembleRF shows relative gains of 5% MRR using pseudo-relevance feedback, and 9% nDCG@10 using relevant feedback documents.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1971435546875
        },
        {
            "paperId": "15a633ae9bbf0d63ad835ad8abf0bdc2dac88e48",
            "corpusId": 275587865,
            "title": "Bias Detection and Mitigation in Zero-Shot Spam Classification using LLMs",
            "venue": "International Conference on Trust, Privacy and Security in Intelligent Systems and Applications",
            "year": 2024,
            "referenceCount": 24,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TPS-ISA62245.2024.00019?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TPS-ISA62245.2024.00019, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2290008386",
                    "name": "Hossein Salemi"
                },
                {
                    "authorId": "2313102091",
                    "name": "Anuridhi Gupta"
                },
                {
                    "authorId": "2323366031",
                    "name": "Hemant Purohit"
                }
            ],
            "abstract": "There is a growing number of scams through various communication mediums, including social media, phone calls and messages, search engine advertising, etc. Often these scams are realized via sending spam texts on any of these communication mediums, and therefore, prior research has investigated the task of spam classification to design information filtering systems. However, existing works have explored supervised machine learning techniques primarily, which suffer from the bottleneck of requiring large labeled datasets. Further, the studies are based on platform-specific data and lack critical analyses of biases in the predictive modeling behavior. In this paper, we propose a zero-shot spam classification task, which does not require any labeled data for model training in an unseen domain. We propose a novel method to leverage state-of-the-art large language models (LLMs) in natural language processing (NLP) for this task. We employ this method to analyze zero-shot performance on spam datasets across two communication platforms (YouTube and phone messages) while mitigating biases in the model behavior. Our experimental results demonstrate the strong performance of a LLM-based zero-shot classifier with a goal-oriented prompting strategy. The resulting classifier is platform-agnostic, shows less bias towards data with certain behavioral attributes (e.g., spam content with sadness emotion), and is efficient in minimizing both false positive and false negative errors. The application of this research can inform effective spam filtering systems to prevent scams prevalent in different communication media ultimately.",
            "corpus_id": "275587865",
            "text": "There is a growing number of scams through various communication mediums, including social media, phone calls and messages, search engine advertising, etc. Often these scams are realized via sending spam texts on any of these communication mediums, and therefore, prior research has investigated the task of spam classification to design information filtering systems. However, existing works have explored supervised machine learning techniques primarily, which suffer from the bottleneck of requiring large labeled datasets. Further, the studies are based on platform-specific data and lack critical analyses of biases in the predictive modeling behavior. In this paper, we propose a zero-shot spam classification task, which does not require any labeled data for model training in an unseen domain. We propose a novel method to leverage state-of-the-art large language models (LLMs) in natural language processing (NLP) for this task. We employ this method to analyze zero-shot performance on spam datasets across two communication platforms (YouTube and phone messages) while mitigating biases in the model behavior. Our experimental results demonstrate the strong performance of a LLM-based zero-shot classifier with a goal-oriented prompting strategy. The resulting classifier is platform-agnostic, shows less bias towards data with certain behavioral attributes (e.g., spam content with sadness emotion), and is efficient in minimizing both false positive and false negative errors. The application of this research can inform effective spam filtering systems to prevent scams prevalent in different communication media ultimately.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.30810546875
        },
        {
            "paperId": "afc5092a4116f27b4c64733c7815cd662bab78f7",
            "corpusId": 258762636,
            "title": "Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 55,
            "citationCount": 95,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.11176",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.11176, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2243292807",
                    "name": "Siyuan Huang"
                },
                {
                    "authorId": "50676465",
                    "name": "Zhengkai Jiang"
                },
                {
                    "authorId": null,
                    "name": "Hao Dong"
                },
                {
                    "authorId": "2059129841",
                    "name": "Y. Qiao"
                },
                {
                    "authorId": "144740494",
                    "name": "Peng Gao"
                },
                {
                    "authorId": "47893312",
                    "name": "Hongsheng Li"
                }
            ],
            "abstract": "Foundation models have made significant strides in various applications, including text-to-image generation, panoptic segmentation, and natural language processing. This paper presents Instruct2Act, a framework that utilizes Large Language Models to map multi-modal instructions to sequential actions for robotic manipulation tasks. Specifically, Instruct2Act employs the LLM model to generate Python programs that constitute a comprehensive perception, planning, and action loop for robotic tasks. In the perception section, pre-defined APIs are used to access multiple foundation models where the Segment Anything Model (SAM) accurately locates candidate objects, and CLIP classifies them. In this way, the framework leverages the expertise of foundation models and robotic abilities to convert complex high-level instructions into precise policy codes. Our approach is adjustable and flexible in accommodating various instruction modalities and input types and catering to specific task demands. We validated the practicality and efficiency of our approach by assessing it on robotic tasks in different scenarios within tabletop manipulation domains. Furthermore, our zero-shot method outperformed many state-of-the-art learning-based policies in several tasks. The code for our proposed approach is available at https://github.com/OpenGVLab/Instruct2Act, serving as a robust benchmark for high-level robotic instruction tasks with assorted modality inputs.",
            "corpus_id": "258762636",
            "text": "Foundation models have made significant strides in various applications, including text-to-image generation, panoptic segmentation, and natural language processing. This paper presents Instruct2Act, a framework that utilizes Large Language Models to map multi-modal instructions to sequential actions for robotic manipulation tasks. Specifically, Instruct2Act employs the LLM model to generate Python programs that constitute a comprehensive perception, planning, and action loop for robotic tasks. In the perception section, pre-defined APIs are used to access multiple foundation models where the Segment Anything Model (SAM) accurately locates candidate objects, and CLIP classifies them. In this way, the framework leverages the expertise of foundation models and robotic abilities to convert complex high-level instructions into precise policy codes. Our approach is adjustable and flexible in accommodating various instruction modalities and input types and catering to specific task demands. We validated the practicality and efficiency of our approach by assessing it on robotic tasks in different scenarios within tabletop manipulation domains. Furthermore, our zero-shot method outperformed many state-of-the-art learning-based policies in several tasks. The code for our proposed approach is available at https://github.com/OpenGVLab/Instruct2Act, serving as a robust benchmark for high-level robotic instruction tasks with assorted modality inputs.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1378173828125
        }
    ],
    "quotes": {
        "cost": 0.32214600000000004,
        "quotes": [
            {
                "idx": 0,
                "key": "[256274849 | Mitchell et al. | 2023 | Citations: 627]",
                "snippets": "In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 166,
                        "end": 1181,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[258048481 | Chakraborty et al. | 2023 | Citations: 127]",
                "snippets": "Traditional approaches. They involve statistical outlier detection methods, which employ statistical metrics such as entropy, perplexity, and n-gram frequency to differentiate between human and machine-generated texts (Lavergne et al., 2008)Gehrmann et al., 2019). However, with the advent of ChatGPT (OpenAI) (OpenAI, 2023), a new innovative statistical detection methodology, DetectGPT (Mitchell et al., 2023), has been developed. It operates on the principle that text generated by the model tends to lie in the negative curvature areas of the model's log probability. DetectGPT (Mitchell et al., 2023) generates and compares multiple perturbations of model-generated text to determine whether the text is machine-generated or not based on the log probability of the original text and the perturbed versions. DetectGPT significantly outperforms the majority of the existing zero-shot methods for model sample detection with very high AUC scores (note that we use the terms AUROC and AUC interchangeably for presentation convenience). \n\nClassifier-based detectors. In contrast to statistical methods, classifier-based detectors are common in natural language detection paradigms, particularly in fake news and misinformation detection (Schildhauer, 2022;Zou & Ling, 2021). OpenAI has recently fine-tuned a GPT model (OpenAI, 2023) using data from Wikipedia, WebText, and internal human demonstration data to create a web interface for a discrimination task using text generated by 34 language models. This approach combines a classifier-based approach with a human evaluation component to determine whether a given text was machine-generated or not. These recent advancements in the field of detecting AI-generated text have significant implications for detecting and preventing the spread of misinformation and fake news, thereby contributing to the betterment of society (Schildhauer, 2022;Zou & Ling, 2021;Kshetri & Voas, 2022). \n\nWatermark-based identification.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Background on AI-Generated Text Detectors and Related Works",
                        "pdf_hash": "",
                        "start": 256,
                        "end": 2223,
                        "sentence_offsets": [
                            {
                                "start": 256,
                                "end": 279
                            },
                            {
                                "start": 280,
                                "end": 520
                            },
                            {
                                "start": 521,
                                "end": 688
                            },
                            {
                                "start": 689,
                                "end": 827
                            },
                            {
                                "start": 828,
                                "end": 1067
                            },
                            {
                                "start": 1068,
                                "end": 1292
                            },
                            {
                                "start": 1295,
                                "end": 1322
                            },
                            {
                                "start": 1323,
                                "end": 1530
                            },
                            {
                                "start": 1531,
                                "end": 1758
                            },
                            {
                                "start": 1759,
                                "end": 1907
                            },
                            {
                                "start": 1908,
                                "end": 2189
                            },
                            {
                                "start": 2192,
                                "end": 2223
                            }
                        ],
                        "ref_mentions": [
                            "12098535"
                        ],
                        "quote": "Traditional approaches. They involve statistical outlier detection methods, which employ statistical metrics such as entropy, perplexity, and n-gram frequency to differentiate between human and machine-generated texts (Lavergne et al., 2008)Gehrmann et al., 2019). However, with the advent of ChatGPT (OpenAI) (OpenAI, 2023), a new innovative statistical detection methodology, DetectGPT (Mitchell et al., 2023), has been developed. It operates on the principle that text generated by the model tends to lie in the negative curvature areas of the model's log probability. DetectGPT (Mitchell et al., 2023) generates and compares multiple perturbations of model-generated text to determine whether the text is machine-generated or not based on the log probability of the original text and the perturbed versions. DetectGPT significantly outperforms the majority of the existing zero-shot methods for model sample detection with very high AUC scores (note that we use the terms AUROC and AUC interchangeably for presentation convenience). \n\nClassifier-based detectors. In contrast to statistical methods, classifier-based detectors are common in natural language detection paradigms, particularly in fake news and misinformation detection (Schildhauer, 2022;Zou & Ling, 2021). OpenAI has recently fine-tuned a GPT model (OpenAI, 2023) using data from Wikipedia, WebText, and internal human demonstration data to create a web interface for a discrimination task using text generated by 34 language models. This approach combines a classifier-based approach with a human evaluation component to determine whether a given text was machine-generated or not. These recent advancements in the field of detecting AI-generated text have significant implications for detecting and preventing the spread of misinformation and fake news, thereby contributing to the betterment of society (Schildhauer, 2022;Zou & Ling, 2021;Kshetri & Voas, 2022). \n\nWatermark-based identification."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[258588289 | Munyer et al. | 2023 | Citations: 32]",
                "snippets": "Several related works have attempted to solve this problem using binary classifiers that categorize input text as human-written or LLM-generated. However, these classifiers have been shown to be unreliable. As impactful decisions could be made based on the result of the classification, the text source detection needs to be high-quality. To this end, this paper presents DeepTextMark, a deep learning-based text watermarking method for text source detection. Applying Word2Vec and Sentence Encoding for watermark insertion and a transformer-based classifier for watermark detection, DeepTextMark achieves blindness, robustness, imperceptibility, and reliability simultaneously.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Several related works have attempted to solve this problem using binary classifiers that categorize input text as human-written or LLM-generated. However, these classifiers have been shown to be unreliable. As impactful decisions could be made based on the result of the classification, the text source detection needs to be high-quality. To this end, this paper presents DeepTextMark, a deep learning-based text watermarking method for text source detection. Applying Word2Vec and Sentence Encoding for watermark insertion and a transformer-based classifier for watermark detection, DeepTextMark achieves blindness, robustness, imperceptibility, and reliability simultaneously.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[258714683 | Yang et al. | 2023 | Citations: 62]",
                "snippets": "Recent advances in large language models (LLMs) have enabled them to reach human-level proficiency across numerous professional and academic tasks [24,(Ouyang et al., 2022)[33]. One of the most impressive examples is OpenAI's ChatGPT [23], which has demonstrated remarkable prowess in answering questions, composing emails, essays, and even generating code. However, this impressive ability to create human-like text with remarkable efficiency has ignited apprehension regarding the potential abuse of LLMs for malicious purposes [6,(2023)(Liebrenz et al., 2023)(Stokel-Walker, 2022), such as phishing, disinformation campaigns, and academic dishonesty...Therefore, detecting and authenticating generated text becomes crucial to ensure the responsible and secure use of LLMs.\n\nA prevalent solution is passive detection [1, 10,18,25,28,40], where a text classifier, usually fine-tuned on a pretrained language model like RoBERTa [16] and GPT-2 [30], is adopted to distinguish between generated and human-written text. However, these learning-based methods perform well only when the input data share a similar distribution with the training data, thereby limiting their applicability to specific domains.\n\nStatistical Discrepancy Detection. Several methods distinguish between generated and human-written text by identifying statistical discrepancies between them, as exemplified by two recent tools: GPTZero [40] and DetectGPT [18]. GPTZero uses perplexity and burstiness to tell apart human-written and generated text, as language models tend to produce more predictable and consistent text based on the patterns they learned from training data, resulting in lower perplexity scores for generated text. DetectGPT exploits the negative curvature regions of a model's log probability function to identify generated text by comparing the log probability of unperturbed and perturbed text variations. However, as language models are constantly improving and becoming more sophisticated, these heuristic features struggle to achieve robustness and generalization. Deep Learning-based Detection. Deep learning-based methods rely on gathering human-written and generated samples to train classifiers. Recently, OpenAI fine-tuned a GPT model for this discrimination task using a dataset comprising paired human and AI-generated texts on identical topics [25]. Similarly, Guo et al. [10] fine-tuned a text classifier based on pre-trained autoencoding LLMs (e.g., RoBERTa) by collecting the Human ChatGPT Comparison Corpus (HC3). Deep learning-based methods exhibit strong performance under the training data distribution, but they are susceptible to adversarial attacks, lack interpretability, and struggle to provide reliable judgments in human-AI collaboration scenarios. Watermark-based Detection. Kirchenbauer et al. [12] proposed the watermarking framework for white-box language models. The watermarking operates by randomly selecting a random set of \"greenlist\" words from the model's vocabulary and softly encouraging the use of these \"greenlist\" words by interfering with the sampling process at each generation step. The watermark can be detected by testing the following null hypothesis,  0 : The text sequence is generated with no knowledge of the selection rule of \"greenlist\" words.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[246426909 | Ouyang et al. | 2022 | Citations: 13203]": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 591,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 158
                            },
                            {
                                "start": 159,
                                "end": 338
                            },
                            {
                                "start": 339,
                                "end": 592
                            }
                        ],
                        "ref_mentions": [
                            "246426909",
                            "256229858",
                            "256655912",
                            "254530623"
                        ],
                        "quote": "Recent advances in large language models (LLMs) have enabled them to reach human-level proficiency across numerous professional and academic tasks [24,(Ouyang et al., 2022)[33]. One of the most impressive examples is OpenAI's ChatGPT [23], which has demonstrated remarkable prowess in answering questions, composing emails, essays, and even generating code. However, this impressive ability to create human-like text with remarkable efficiency has ignited apprehension regarding the potential abuse of LLMs for malicious purposes [6,(2023)(Liebrenz et al., 2023)(Stokel-Walker, 2022), such as phishing, disinformation campaigns, and academic dishonesty"
                    },
                    {
                        "quote": "Therefore, detecting and authenticating generated text becomes crucial to ensure the responsible and secure use of LLMs.\n\nA prevalent solution is passive detection [1, 10,18,25,28,40], where a text classifier, usually fine-tuned on a pretrained language model like RoBERTa [16] and GPT-2 [30], is adopted to distinguish between generated and human-written text. However, these learning-based methods perform well only when the input data share a similar distribution with the training data, thereby limiting their applicability to specific domains.\n\nStatistical Discrepancy Detection. Several methods distinguish between generated and human-written text by identifying statistical discrepancies between them, as exemplified by two recent tools: GPTZero [40] and DetectGPT [18]. GPTZero uses perplexity and burstiness to tell apart human-written and generated text, as language models tend to produce more predictable and consistent text based on the patterns they learned from training data, resulting in lower perplexity scores for generated text. DetectGPT exploits the negative curvature regions of a model's log probability function to identify generated text by comparing the log probability of unperturbed and perturbed text variations. However, as language models are constantly improving and becoming more sophisticated, these heuristic features struggle to achieve robustness and generalization. Deep Learning-based Detection. Deep learning-based methods rely on gathering human-written and generated samples to train classifiers. Recently, OpenAI fine-tuned a GPT model for this discrimination task using a dataset comprising paired human and AI-generated texts on identical topics [25]. Similarly, Guo et al. [10] fine-tuned a text classifier based on pre-trained autoencoding LLMs (e.g., RoBERTa) by collecting the Human ChatGPT Comparison Corpus (HC3). Deep learning-based methods exhibit strong performance under the training data distribution, but they are susceptible to adversarial attacks, lack interpretability, and struggle to provide reliable judgments in human-AI collaboration scenarios. Watermark-based Detection. Kirchenbauer et al. [12] proposed the watermarking framework for white-box language models. The watermarking operates by randomly selecting a random set of \"greenlist\" words from the model's vocabulary and softly encouraging the use of these \"greenlist\" words by interfering with the sampling process at each generation step. The watermark can be detected by testing the following null hypothesis,  0 : The text sequence is generated with no knowledge of the selection rule of \"greenlist\" words.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[258762215 | Lu et al. | 2023 | Citations: 53]",
                "snippets": "In recent years, the research community has developed a wide range of detectors for AI-generated contents. In general, these detectors can be classified into three categories: training-based, statistical, and watermarking methods. Training-based methods treat the detection problem as a binary classification task, where neural networks are trained using AI-generated text and human-written text. Early studies utilized classifiers to identify fake reviews (Hovy, 2016) and fake news (Zellers et al., 2019). More recently, researchers have trained classifiers using text generated by LLMs, such as the GPT-3.5 detector (Guo et al., 2023) and GPT-2 detector (Solaiman et al., 2019). Statistical methods, on the other hand, focus on zero-shot detection without any additional training overhead. These methods seek to distinguish between human-written text and AI-generated text based on the statistical characteristics of text, such as the statistical irregularities in measures like entropy (Lavergne et al., 2008), perplexity (Beresneva, 2016) and token rank (Gehrmann et al., 2019). A recent method, DetectGPT (Mitchell et al., 2023), exploits the phenomenon that AI-generated text tends to lie in the negative curvature regions of log probability of text. The watermarking methods involve modifying the LLM's text generation process to imprint specific patterns on the generated text, such that it can be detected (Abdelnabi et al., 2020)Grinbaum & Adomaitis, 2022;Kirchenbauer et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[168169824 | Zellers et al. | 2019 | Citations: 1030]": "Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. \nModern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. \nDeveloping robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.",
                    "[182952848 | Gehrmann et al. | 2019 | Citations: 546]": "The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by non-experts. In this work, we introduce GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across multiple sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs.",
                    "[221516138 | Abdelnabi et al. | 2020 | Citations: 152]": "Recent advances in natural language generation have introduced powerful language models with high-quality output text. However, this raises concerns about the potential misuse of such models for malicious purposes. In this paper, we study natural language watermarking as a defense to help better mark and trace the provenance of text. We introduce the Adversarial Watermarking Transformer (AWT) with a jointly trained encoder-decoder and adversarial training that, given an input text and a binary message, generates an output text that is unobtrusively encoded with the given message. We further study different training and inference strategies to achieve minimal changes to the semantics and correctness of the input text.AWT is the first end-to-end model to hide data in text by automatically learning -without ground truth- word substitutions along with their locations in order to encode the message. We empirically show that our model is effective in largely preserving text utility and decoding the watermark while hiding its presence against adversaries. Additionally, we demonstrate that our method is robust against a range of attacks."
                },
                "metadata": [
                    {
                        "section_title": "Related works 2.1 AI-generated text detection",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1495,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 106
                            },
                            {
                                "start": 107,
                                "end": 230
                            },
                            {
                                "start": 231,
                                "end": 396
                            },
                            {
                                "start": 397,
                                "end": 507
                            },
                            {
                                "start": 508,
                                "end": 681
                            },
                            {
                                "start": 682,
                                "end": 792
                            },
                            {
                                "start": 793,
                                "end": 1083
                            },
                            {
                                "start": 1084,
                                "end": 1257
                            },
                            {
                                "start": 1258,
                                "end": 1495
                            }
                        ],
                        "ref_mentions": [
                            "168169824",
                            "12098535",
                            "1175726",
                            "182952848",
                            "221516138"
                        ],
                        "quote": "In recent years, the research community has developed a wide range of detectors for AI-generated contents. In general, these detectors can be classified into three categories: training-based, statistical, and watermarking methods. Training-based methods treat the detection problem as a binary classification task, where neural networks are trained using AI-generated text and human-written text. Early studies utilized classifiers to identify fake reviews (Hovy, 2016) and fake news (Zellers et al., 2019). More recently, researchers have trained classifiers using text generated by LLMs, such as the GPT-3.5 detector (Guo et al., 2023) and GPT-2 detector (Solaiman et al., 2019). Statistical methods, on the other hand, focus on zero-shot detection without any additional training overhead. These methods seek to distinguish between human-written text and AI-generated text based on the statistical characteristics of text, such as the statistical irregularities in measures like entropy (Lavergne et al., 2008), perplexity (Beresneva, 2016) and token rank (Gehrmann et al., 2019). A recent method, DetectGPT (Mitchell et al., 2023), exploits the phenomenon that AI-generated text tends to lie in the negative curvature regions of log probability of text. The watermarking methods involve modifying the LLM's text generation process to imprint specific patterns on the generated text, such that it can be detected (Abdelnabi et al., 2020)Grinbaum & Adomaitis, 2022;Kirchenbauer et al., 2023)."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[258865787 | Verma et al. | 2023 | Citations: 60]",
                "snippets": "We introduce Ghostbuster, a state-of-the-art system for detecting AI-generated text.Our method works by passing documents through a series of weaker language models, running a structured search over possible combinations of their features, and then training a classifier on the selected features to predict whether documents are AI-generated.Crucially, Ghostbuster does not require access to token probabilities from the target model, making it useful for detecting text generated by black-box or unknown models.In conjunction with our model, we release three new datasets of human- and AI-generated text as detection benchmarks in the domains of student essays, creative writing, and news articles. We compare Ghostbuster to several existing detectors, including DetectGPT and GPTZero, as well as a new RoBERTa baseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is 5.9 F1 higher than the best preexisting model. It also outperforms all previous approaches in generalization across writing domains (+7.5 F1), prompting strategies (+2.1 F1), and language models (+4.4 F1).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1096,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We introduce Ghostbuster, a state-of-the-art system for detecting AI-generated text.Our method works by passing documents through a series of weaker language models, running a structured search over possible combinations of their features, and then training a classifier on the selected features to predict whether documents are AI-generated.Crucially, Ghostbuster does not require access to token probabilities from the target model, making it useful for detecting text generated by black-box or unknown models.In conjunction with our model, we release three new datasets of human- and AI-generated text as detection benchmarks in the domains of student essays, creative writing, and news articles. We compare Ghostbuster to several existing detectors, including DetectGPT and GPTZero, as well as a new RoBERTa baseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is 5.9 F1 higher than the best preexisting model. It also outperforms all previous approaches in generalization across writing domains (+7.5 F1), prompting strategies (+2.1 F1), and language models (+4.4 F1)."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[258947640 | Deng et al. | 2023 | Citations: 20]",
                "snippets": "The detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires querying the source LLM with hundreds of its perturbations.\n\nSome methods (e.g., OpenAI, 2023b) involve training supervised classifiers, which, yet, suffer from overfitting to the training data and ineffectiveness to generalize to new test data. Zero-shot LLM-generated text detection approaches bypass these issues by leveraging the source LLM to detect its samples (Solaiman et al., 2019;Gehrmann et al., 2019b;Ippolito et al., 2020). They usually proceed by inspecting the average per-token log probability of the candidate text, but the practical detection performance can be unsatisfactory.\n\nDetectGPT (Mitchell et al., 2023) is a recent method that achieves improved zero-shot detection efficacy by exploring the probability curvature of LLMs. It generates multiple perturbations of the candidate text and scores them using the source LLM to define detection statistics. It can detect texts generated by GPT-2 (Radford et al., 2019) and GPT-NeoX-20B (Black et al., 2022). Yet, DetectGPT relies on hundreds of queries to the source LLM to estimate the local probability curvature surrounding one single candidate passage.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires querying the source LLM with hundreds of its perturbations.\n\nSome methods (e.g., OpenAI, 2023b) involve training supervised classifiers, which, yet, suffer from overfitting to the training data and ineffectiveness to generalize to new test data. Zero-shot LLM-generated text detection approaches bypass these issues by leveraging the source LLM to detect its samples (Solaiman et al., 2019;Gehrmann et al., 2019b;Ippolito et al., 2020). They usually proceed by inspecting the average per-token log probability of the candidate text, but the practical detection performance can be unsatisfactory.\n\nDetectGPT (Mitchell et al., 2023) is a recent method that achieves improved zero-shot detection efficacy by exploring the probability curvature of LLMs. It generates multiple perturbations of the candidate text and scores them using the source LLM to define detection statistics. It can detect texts generated by GPT-2 (Radford et al., 2019) and GPT-NeoX-20B (Black et al., 2022). Yet, DetectGPT relies on hundreds of queries to the source LLM to estimate the local probability curvature surrounding one single candidate passage.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[258967971 | Tang et al. | 2023 | Citations: 3]",
                "snippets": "Recent methods such as DetectGPT and GPTZero distinguish between machine-generated and human-written text by analyzing their statistical discrepancies (Tian, 2023;Mitchell et al., 2023). DetectGPT compares the log probability computed by a model on unperturbed text and perturbed variations, leveraging the observation that text sampled from a LLM generally occupy negative curvature regions of the model's log probability function. GPTZero instead uses perplexity and burstiness to distinguish human from machine text, with lower perplexity and burstiness indicating a greater likelihood of machine-generated text. However, these heuristics do not generalize and are often fallible.\n\nDetection by Learning Classifiers Several papers have proposed to train classifiers to distinguish between AI and human generated text. During the initial GPT-2 release, OpenAI trained a RoBERTa classifier to detect GPT-2 generated text with 95% accuracy (Solaiman et al., 2019). More recently, OpenAI fine-tuned a GPT model on a dataset of machine-generated and human texts focusing on the same topic, with a true positive identification rate of 26% (Ope-nAI, 2023). Similarly, Guo et al. (2023) collected the Human ChatGPT Comparison Corpuse (HC3) and fine-tuned RoBERTa for the detection task.\n\nNotably, the capabilities of such classifiers decrease as machine-generated text becomes increasingly human-like. Sadasivan et al. (2023) show theoretically that for sufficiently advanced language models, machine-generated text detectors offer only a marginal improvement over random classifiers. Moreover, such methods are prone to adversarial attacks and are not robust to out-of-distribution text.\n\nWatermarking Large Language Models An alternative to detecting of machine-generated text using statistical discrepancies and learned classifiers is the concept of watermarks. Watermarks are hidden patterns in machinegenerated text that are imperceptible to humans, but algorithmically identifiable as synthetic. Natural language watermarks long predate the development of LLMs, relying on methods such as synonym substitution, as well as syntactic and semantic transformations (Topkara et al., 2005).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work Generated Text Detection Via Statistical Discrepancies",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2185,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 186
                            },
                            {
                                "start": 187,
                                "end": 432
                            },
                            {
                                "start": 433,
                                "end": 615
                            },
                            {
                                "start": 616,
                                "end": 683
                            },
                            {
                                "start": 686,
                                "end": 821
                            },
                            {
                                "start": 822,
                                "end": 965
                            },
                            {
                                "start": 966,
                                "end": 1153
                            },
                            {
                                "start": 1154,
                                "end": 1282
                            },
                            {
                                "start": 1285,
                                "end": 1398
                            },
                            {
                                "start": 1399,
                                "end": 1581
                            },
                            {
                                "start": 1582,
                                "end": 1685
                            },
                            {
                                "start": 1688,
                                "end": 1862
                            },
                            {
                                "start": 1863,
                                "end": 1999
                            },
                            {
                                "start": 2000,
                                "end": 2188
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recent methods such as DetectGPT and GPTZero distinguish between machine-generated and human-written text by analyzing their statistical discrepancies (Tian, 2023;Mitchell et al., 2023). DetectGPT compares the log probability computed by a model on unperturbed text and perturbed variations, leveraging the observation that text sampled from a LLM generally occupy negative curvature regions of the model's log probability function. GPTZero instead uses perplexity and burstiness to distinguish human from machine text, with lower perplexity and burstiness indicating a greater likelihood of machine-generated text. However, these heuristics do not generalize and are often fallible.\n\nDetection by Learning Classifiers Several papers have proposed to train classifiers to distinguish between AI and human generated text. During the initial GPT-2 release, OpenAI trained a RoBERTa classifier to detect GPT-2 generated text with 95% accuracy (Solaiman et al., 2019). More recently, OpenAI fine-tuned a GPT model on a dataset of machine-generated and human texts focusing on the same topic, with a true positive identification rate of 26% (Ope-nAI, 2023). Similarly, Guo et al. (2023) collected the Human ChatGPT Comparison Corpuse (HC3) and fine-tuned RoBERTa for the detection task.\n\nNotably, the capabilities of such classifiers decrease as machine-generated text becomes increasingly human-like. Sadasivan et al. (2023) show theoretically that for sufficiently advanced language models, machine-generated text detectors offer only a marginal improvement over random classifiers. Moreover, such methods are prone to adversarial attacks and are not robust to out-of-distribution text.\n\nWatermarking Large Language Models An alternative to detecting of machine-generated text using statistical discrepancies and learned classifiers is the concept of watermarks. Watermarks are hidden patterns in machinegenerated text that are imperceptible to humans, but algorithmically identifiable as synthetic. Natural language watermarks long predate the development of LLMs, relying on methods such as synonym substitution, as well as syntactic and semantic transformations (Topkara et al., 2005)."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[258987266 | Shi et al. | 2023 | Citations: 56]",
                "snippets": "Recent detectors for AI-generated text mostly fall into three categories. First, classifier-based detectors are trained with labeled data to distinguish human-written text and AI-generated text. For example, the AI Text Classifier developed by OpenAI (OpenAI, 2023a) is a fine-tuned language model. Second, watermarking methods introduce distinct patterns into AI-generated text, allowing for its identification. Among them, (Kirchenbauer et al., 2023) randomly partition the vocabulary into a greenlist and a redlist during the generation, where the division is based on the hash of the previously generated tokens. The language model only uses words in the greenlists, and thereby the generated text has a different pattern compared to human-written text which does not consider such greenlists and redlists. Third, De-tectGPT (Mitchell et al., 2023) uses the likelihood of the generated text for the detection, as they find that text generated by language models tends to reside in the negative curvature region of the log probability function. Consequently, they define a curvature-based criterion for the detection.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[256194179 | Kirchenbauer et al. | 2023 | Citations: 507]": "Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of\"green\"tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 33,
                        "end": 1152,
                        "sentence_offsets": [
                            {
                                "start": 33,
                                "end": 106
                            },
                            {
                                "start": 107,
                                "end": 227
                            },
                            {
                                "start": 228,
                                "end": 331
                            },
                            {
                                "start": 332,
                                "end": 445
                            },
                            {
                                "start": 446,
                                "end": 648
                            },
                            {
                                "start": 649,
                                "end": 842
                            },
                            {
                                "start": 843,
                                "end": 1079
                            },
                            {
                                "start": 1080,
                                "end": 1152
                            }
                        ],
                        "ref_mentions": [
                            "256194179"
                        ],
                        "quote": "Recent detectors for AI-generated text mostly fall into three categories. First, classifier-based detectors are trained with labeled data to distinguish human-written text and AI-generated text. For example, the AI Text Classifier developed by OpenAI (OpenAI, 2023a) is a fine-tuned language model. Second, watermarking methods introduce distinct patterns into AI-generated text, allowing for its identification. Among them, (Kirchenbauer et al., 2023) randomly partition the vocabulary into a greenlist and a redlist during the generation, where the division is based on the hash of the previously generated tokens. The language model only uses words in the greenlists, and thereby the generated text has a different pattern compared to human-written text which does not consider such greenlists and redlists. Third, De-tectGPT (Mitchell et al., 2023) uses the likelihood of the generated text for the detection, as they find that text generated by language models tends to reside in the negative curvature region of the log probability function. Consequently, they define a curvature-based criterion for the detection."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[259129463 | Su et al. | 2023 | Citations: 141]",
                "snippets": "Previous work has studied several zero-shot methods, which require no training data. These methods achieve good performance, but there is still a lot of room for improvement. In this paper, we introduce two novel zero-shot methods for detecting machine-generated text by leveraging the log rank information. One is called DetectLLM-LRR, which is fast and efficient, and the other is called DetectLLM-NPR, which is more accurate, but slower due to the need for perturbations. Our experiments on three datasets and seven language models show that our proposed methods improve over the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover, DetectLLM-NPR needs fewer perturbations than previous work to achieve the same level of performance, which makes it more practical for real-world use.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 405,
                        "end": 1204,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Previous work has studied several zero-shot methods, which require no training data. These methods achieve good performance, but there is still a lot of room for improvement. In this paper, we introduce two novel zero-shot methods for detecting machine-generated text by leveraging the log rank information. One is called DetectLLM-LRR, which is fast and efficient, and the other is called DetectLLM-NPR, which is more accurate, but slower due to the need for perturbations. Our experiments on three datasets and seven language models show that our proposed methods improve over the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover, DetectLLM-NPR needs fewer perturbations than previous work to achieve the same level of performance, which makes it more practical for real-world use."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[260164516 | Fu et al. | 2023 | Citations: 37]",
                "snippets": "Automatic Detection The detection of AI-generated text, particularly in the context of large language models (LLMs), has recently attracted significant research interest (Bakhtin et al. 2019;Schuster et al. 2020;Fr\u00f6hling and Zubiaga 2021;Sadasivan et al. 2023;Mitchell et al. 2023). Previous approaches have primarily focused on leveraging the perplexi-ties of generated texts for detection. For example, Solaiman et al. (2019) utilized a classifier to evaluate the total log probability of the text, using it as a means to determine whether the content originated from a machine. Building on this premise, Mitchell et al. (2023) further validated that the log probability of machine-generated text diminishes upon perturbation, while the log probability of human-written text remains unpredictable when perturbed.\n\nWatermarking There has been a recent emergence of watermarking specific patterns into language models for AI detection. Zhao, Wang, and Li (2023)",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[182952342 | Bakhtin et al. | 2019 | Citations: 173]": "Energy-based models (EBMs), a.k.a. un-normalized models, have had recent successes in continuous spaces. However, they have not been successfully applied to model text sequences. While decreasing the energy at training samples is straightforward, mining (negative) samples where the energy should be increased is difficult. In part, this is because standard gradient-based methods are not readily applicable when the input is high-dimensional and discrete. Here, we side-step this issue by generating negatives using pre-trained auto-regressive language models. The EBM then works in the residual of the language model; and is trained to discriminate real text from text generated by the auto-regressive models. We investigate the generalization ability of residual EBMs, a pre-requisite for using them in other applications. We extensively analyze generalization for the task of classifying whether an input is machine or human generated, a natural task given the training loss and how we mine negatives. Overall, we observe that EBMs can generalize remarkably well to changes in the architecture of the generators producing negatives. However, EBMs exhibit more sensitivity to the training set used by such generators.",
                    "[211204954 | Schuster et al. | 2019 | Citations: 126]": "Recent developments in neural language models (LMs) have raised concerns about their potential misuse for automatically spreading misinformation. In light of these concerns, several studies have proposed to detect machine-generated fake news by capturing their stylistic differences from human-written text. These approaches, broadly termed stylometry, have found success in source attribution and misinformation detection in human-written texts. However, in this work, we show that stylometry is limited against machine-generated misinformation. Whereas humans speak differently when trying to deceive, LMs generate stylistically consistent text, regardless of underlying motive. Thus, though stylometry can successfully prevent impersonation by identifying text provenance, it fails to distinguish legitimate LM applications from those that introduce false information. We create two benchmarks demonstrating the stylistic similarity between malicious and legitimate uses of LMs, utilized in auto-completion and editing-assistance settings.1 Our findings highlight the need for non-stylometry approaches in detecting machine-generated misinformation, and open up the discussion on the desired evaluation benchmarks.",
                    "[233563022 | Frohling et al. | 2021 | Citations: 110]": "The recent improvements of language models have drawn much attention to potential cases of use and abuse of automatically generated text. Great effort is put into the development of methods to detect machine generations among human-written text in order to avoid scenarios in which the large-scale generation of text with minimal cost and effort undermines the trust in human interaction and factual information online. While most of the current approaches rely on the availability of expensive language models, we propose a simple feature-based classifier for the detection problem, using carefully crafted features that attempt to model intrinsic differences between human and machine text. Our research contributes to the field in producing a detection method that achieves performance competitive with far more expensive methods, offering an accessible \u201cfirst line-of-defense\u201d against the abuse of language models. Furthermore, our experiments show that different sampling methods lead to different types of flaws in generated text."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 961,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 282
                            },
                            {
                                "start": 283,
                                "end": 391
                            },
                            {
                                "start": 392,
                                "end": 580
                            },
                            {
                                "start": 581,
                                "end": 814
                            },
                            {
                                "start": 817,
                                "end": 936
                            },
                            {
                                "start": 937,
                                "end": 962
                            }
                        ],
                        "ref_mentions": [
                            "182952342",
                            "211204954",
                            "233563022"
                        ],
                        "quote": "Automatic Detection The detection of AI-generated text, particularly in the context of large language models (LLMs), has recently attracted significant research interest (Bakhtin et al. 2019;Schuster et al. 2020;Fr\u00f6hling and Zubiaga 2021;Sadasivan et al. 2023;Mitchell et al. 2023). Previous approaches have primarily focused on leveraging the perplexi-ties of generated texts for detection. For example, Solaiman et al. (2019) utilized a classifier to evaluate the total log probability of the text, using it as a means to determine whether the content originated from a machine. Building on this premise, Mitchell et al. (2023) further validated that the log probability of machine-generated text diminishes upon perturbation, while the log probability of human-written text remains unpredictable when perturbed.\n\nWatermarking There has been a recent emergence of watermarking specific patterns into language models for AI detection. Zhao, Wang, and Li (2023)"
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[260351507 | Fernandez et al. | 2023 | Citations: 55]",
                "snippets": "Discerning between generated and natural texts is increasingly challenging. In this context, watermarking emerges as a promising technique for ascribing text to a specific generative model. It alters the sampling generation process to leave an invisible trace in the output, facilitating later detection. This research consolidates watermarks for large language models based on three theoretical and empirical considerations. First, we introduce new statistical tests that offer robust theoretical guarantees which remain valid even at low false-positive rates (less than 10\u22126). Second, we compare the effectiveness of watermarks using classical benchmarks in the field of natural language processing, gaining insights into their real-world applicability. Third, we develop advanced detection schemes for scenarios where access to the LLM is available, as well as multi-bit watermarking.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 887,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Discerning between generated and natural texts is increasingly challenging. In this context, watermarking emerges as a promising technique for ascribing text to a specific generative model. It alters the sampling generation process to leave an invisible trace in the output, facilitating later detection. This research consolidates watermarks for large language models based on three theoretical and empirical considerations. First, we introduce new statistical tests that offer robust theoretical guarantees which remain valid even at low false-positive rates (less than 10\u22126). Second, we compare the effectiveness of watermarks using classical benchmarks in the field of natural language processing, gaining insights into their real-world applicability. Third, we develop advanced detection schemes for scenarios where access to the LLM is available, as well as multi-bit watermarking."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[261660497 | Bhattacharjee et al. | 2023 | Citations: 47]",
                "snippets": "Several recent efforts look at methods, varying from simple feature-based classifiers to fine-tuned language model-based detectors, in order to classify whether a piece of input text is human-written or AI-generated (Ippolito et al., 2019;(Ghosh et al., 2021)Mitchell et al., 2023), along with methods that specifically focus on AIgenerated news (Zellers et al., 2019)Bogaert et al., 2022). A related direction of work is that of authorship attribution (AA). While older AA methods focused on human authors, more recent efforts (Uchendu et al., 2020;(Munir et al., 2021) build models to identify the generator for a particular input text. Recent work also shows how AI-generated text can deceive state-of-the-art AA models (Jones et al., 2022), thus making the task of detecting such text even more important.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[168169824 | Zellers et al. | 2019 | Citations: 1030]": "Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. \nModern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. \nDeveloping robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.",
                    "[233189559 | Munir et al. | 2021 | Citations: 26]": "Given the potential misuse of recent advances in synthetic text generation by language models (LMs), it is important to have the capacity to attribute authorship of synthetic text. While stylometric organic (i.e., human written) authorship attribution has been quite successful, it is unclear whether similar approaches can be used to attribute a synthetic text to its source LM. We address this question with the key insight that synthetic texts carry subtle distinguishing marks inherited from their source LM and that these marks can be leveraged by machine learning (ML) algorithms for attribution. We propose and test several ML-based attribution methods. Our best attributor built using a fine-tuned version of XLNet (XLNet-FT) consistently achieves excellent accuracy scores (91% to near perfect 98%) in terms of attributing the parent pre-trained LM behind a synthetic text. Our experiments show promising results across a range of experiments where the synthetic text may be generated using pre-trained LMs, fine-tuned LMs, or by varying text generation parameters.",
                    "[233296145 | Ghosh et al. | 2021 | Citations: 59]": "Deep neural network-based classifiers trained with the categorical cross-entropy (CCE) loss are sensitive to label noise in the training data. One common type of method that can mitigate the impact of label noise can be viewed as supervised robust methods; one can simply replace the CCE loss with a loss that is robust to label noise, or re-weight training samples and down-weight those with higher loss values. Recently, another type of method using semi-supervised learning (SSL) has been proposed, which augments these supervised robust methods to exploit (possibly) noisy samples more effectively. Although supervised robust methods perform well across different data types, they have been shown to be inferior to the SSL methods on image classification tasks under label noise. Therefore, it remains to be seen that whether these supervised robust methods can also perform well if they can utilize the unlabeled samples more effectively. In this paper, we show that by initializing supervised robust methods using representations learned through contrastive learning leads to significantly improved performance under label noise. Surprisingly, even the simplest method (training a classifier with the CCE loss) can outperform the state-of-the-art SSL method by more than 50% under high label noise when initialized with contrastive learning. Our implementation will be publicly available at https://github.com/arghosh/noisy_label_pretrain.",
                    "[247595180 | Jones et al. | 2022 | Citations: 19]": "Recently, there has been a rise in the development of powerful pre-trained natural language models, including GPT-2, Grover, and XLM. These models have shown state-of-the-art capabilities towards a variety of different NLP tasks, including question answering, content summarisation, and text generation. Alongside this, there have been many studies focused on online authorship attribution (AA). That is, the use of trained models to identify the authors of online texts. Given the power of natural language models in generating convincing texts, this paper examines the degree to which these language models can generate texts capable of deceiving online AA models. Experimenting with both blog and Twitter data, we utilise GPT-2 language models to generate texts using the existing posts of online users. We then examine whether GPT-2-based text generators are capable of mimicking authorial style to such a degree that they can deceive typical AA models. From this, we find that current AI-based text generators are able to successfully mimic authorship, showing capabilities towards this on both datasets. Our findings, in turn, highlight the current capacity of powerful natural language models to generate original online posts capable of mimicking authorial style sufficiently to deceive popular AA methods. This is a key finding given the proposed role of AA in real-world applications such as spam-detection and the investigation of criminal activity online -- where deceptive texts could be automatically generated to mimic authorship in order to mislead these critical AA systems."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 204,
                        "end": 1014,
                        "sentence_offsets": [
                            {
                                "start": 204,
                                "end": 596
                            },
                            {
                                "start": 597,
                                "end": 664
                            },
                            {
                                "start": 665,
                                "end": 843
                            },
                            {
                                "start": 844,
                                "end": 1014
                            }
                        ],
                        "ref_mentions": [
                            "233296145",
                            "168169824",
                            "233189559",
                            "247595180"
                        ],
                        "quote": "Several recent efforts look at methods, varying from simple feature-based classifiers to fine-tuned language model-based detectors, in order to classify whether a piece of input text is human-written or AI-generated (Ippolito et al., 2019;(Ghosh et al., 2021)Mitchell et al., 2023), along with methods that specifically focus on AIgenerated news (Zellers et al., 2019)Bogaert et al., 2022). A related direction of work is that of authorship attribution (AA). While older AA methods focused on human authors, more recent efforts (Uchendu et al., 2020;(Munir et al., 2021) build models to identify the generator for a particular input text. Recent work also shows how AI-generated text can deceive state-of-the-art AA models (Jones et al., 2022), thus making the task of detecting such text even more important."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[261697402 | Anand | 2023 | Citations: 0]",
                "snippets": "Watermarking in text generation is a technique that introduces hidden signals into generated text, which can be used to detect whether the text is artificial. In this approach, the language model generates a probability distribution for the next word based on a given sequence of words. The next word is then randomly selected from this distribution. The watermarking technique operates by pseudo-randomly sampling words from the distribution, thereby enabling the detection of text generated by the language model.\n\nIn a recent study conducted by Kirchenbauer et al. [33], the effectiveness of the watermarking technique was evaluated on a multi-billion parameter model from the Open Pre-trained Transformer (OPT) family. The proposed method involved the selection of randomized sets of green tokens (allowed tokens) and red tokens (restricted tokens) during the generation process. The use of green tokens was promoted while sampling, while red tokens were discouraged. This approach facilitated the detection of artificial text by counting the occurrences of red tokens. In a similar direction, Ope-nAI has also expressed intentions to incorporate watermarking into its language models.\n\nAs outlined in a blog post by Scott Aaronson 6 , OpenAI plans to use cryptographic pseudo-random functions to sample words and generate detectors using cryptographic keys. This approach aims to embed watermarks into the language model, allowing for the identification of artificial text.\n\nIn practice, evading watermarking detection in language models can be achieved through various techniques, such as inserting or deleting words randomly, rearranging sentence order, or paraphrasing text. These methods pose challenges to the effectiveness of watermarking as a detection mechanism. Additionally, the responsibility of embedding watermarking lies with the developer of the language model, and only they have the ability to build the corresponding detection models. This limitation means that watermarking can only mitigate the potential harms of language models to a certain extent, as it relies on the actions and choices of the developer.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Watermarking in Language Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2133,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Watermarking in text generation is a technique that introduces hidden signals into generated text, which can be used to detect whether the text is artificial. In this approach, the language model generates a probability distribution for the next word based on a given sequence of words. The next word is then randomly selected from this distribution. The watermarking technique operates by pseudo-randomly sampling words from the distribution, thereby enabling the detection of text generated by the language model.\n\nIn a recent study conducted by Kirchenbauer et al. [33], the effectiveness of the watermarking technique was evaluated on a multi-billion parameter model from the Open Pre-trained Transformer (OPT) family. The proposed method involved the selection of randomized sets of green tokens (allowed tokens) and red tokens (restricted tokens) during the generation process. The use of green tokens was promoted while sampling, while red tokens were discouraged. This approach facilitated the detection of artificial text by counting the occurrences of red tokens. In a similar direction, Ope-nAI has also expressed intentions to incorporate watermarking into its language models.\n\nAs outlined in a blog post by Scott Aaronson 6 , OpenAI plans to use cryptographic pseudo-random functions to sample words and generate detectors using cryptographic keys. This approach aims to embed watermarks into the language model, allowing for the identification of artificial text.\n\nIn practice, evading watermarking detection in language models can be achieved through various techniques, such as inserting or deleting words randomly, rearranging sentence order, or paraphrasing text. These methods pose challenges to the effectiveness of watermarking as a detection mechanism. Additionally, the responsibility of embedding watermarking lies with the developer of the language model, and only they have the ability to build the corresponding detection models. This limitation means that watermarking can only mitigate the potential harms of language models to a certain extent, as it relies on the actions and choices of the developer."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[263831345 | Bao et al. | 2023 | Citations: 153]",
                "snippets": "Our research centers on zero-shot detection of machine-generated text, under the premise that our model has not undergone training using any machine-generated text. This distinguishes our approach from conventional supervised methods, which commonly employ discriminative training strategies to acquire specific syntactic or semantic attributes customized for machine-generated text. In contrast, our zero-shot methodology capitalizes on the inherent capabilities of large language models to identify anomalies that function as markers of machine-generated content.\n\nThe White-box Setting. Conventional zero-shot methodologies often operate under the assumption that the source model responsible for generating machine-generated text is accessible. We refer to this context as the white-box setting, where the primary goal is to distinguish machine-generated texts produced by the source model from those generated by humans. In this white-box setting, our detection decisions are dependent on the source model, but it is not mandatory to possess detailed knowledge of the source model's architecture and parameters. For instance, within the white-box framework, a system like DetectGPT utilizes the OpenAI API to identify text generated by GPT-3, without requiring extensive knowledge of the inner workings of GPT-3.\n\nThe Black-box Setting. In real-world situations, there could be instances where we lack knowledge about the specific source models employed for content generation. This necessitates the development of a versatile detector capable of identifying texts generated by a variety of automated systems. We term this scenario the black-box setting, where the objective is to differentiate between machinegenerated texts produced by diverse, unidentified models and those composed by humans. In this context, the term \"black box\" signifies that we lack access to information about the source model or any details pertaining to it.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A ZERO-SHOT DETECTION TASK AND SETTINGS",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1940,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 164
                            },
                            {
                                "start": 165,
                                "end": 383
                            },
                            {
                                "start": 384,
                                "end": 565
                            },
                            {
                                "start": 568,
                                "end": 590
                            },
                            {
                                "start": 591,
                                "end": 749
                            },
                            {
                                "start": 750,
                                "end": 926
                            },
                            {
                                "start": 927,
                                "end": 1117
                            },
                            {
                                "start": 1118,
                                "end": 1318
                            },
                            {
                                "start": 1321,
                                "end": 1343
                            },
                            {
                                "start": 1344,
                                "end": 1484
                            },
                            {
                                "start": 1485,
                                "end": 1616
                            },
                            {
                                "start": 1617,
                                "end": 1803
                            },
                            {
                                "start": 1804,
                                "end": 1942
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our research centers on zero-shot detection of machine-generated text, under the premise that our model has not undergone training using any machine-generated text. This distinguishes our approach from conventional supervised methods, which commonly employ discriminative training strategies to acquire specific syntactic or semantic attributes customized for machine-generated text. In contrast, our zero-shot methodology capitalizes on the inherent capabilities of large language models to identify anomalies that function as markers of machine-generated content.\n\nThe White-box Setting. Conventional zero-shot methodologies often operate under the assumption that the source model responsible for generating machine-generated text is accessible. We refer to this context as the white-box setting, where the primary goal is to distinguish machine-generated texts produced by the source model from those generated by humans. In this white-box setting, our detection decisions are dependent on the source model, but it is not mandatory to possess detailed knowledge of the source model's architecture and parameters. For instance, within the white-box framework, a system like DetectGPT utilizes the OpenAI API to identify text generated by GPT-3, without requiring extensive knowledge of the inner workings of GPT-3.\n\nThe Black-box Setting. In real-world situations, there could be instances where we lack knowledge about the specific source models employed for content generation. This necessitates the development of a versatile detector capable of identifying texts generated by a variety of automated systems. We term this scenario the black-box setting, where the objective is to differentiate between machinegenerated texts produced by diverse, unidentified models and those composed by humans. In this context, the term \"black box\" signifies that we lack access to information about the source model or any details pertaining to it."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[264426339 | Wang et al. | 2023 | Citations: 7]",
                "snippets": "Recent research has shown promising results in the development of detection methods. The existing detectors are built on the assumption that there is a distributional difference between human-generated texts and AI-generated texts. These differences are typically identified by training classifiers or using statistical information. \n\nClassifier-based detectors. Classifier-based detectors are commonly used in natural language processing detection paradigms, especially in fake news and misinformation detection [3]. Guo et al. [4] proposed the ChatGPT Detector, where they initially constructed a dataset consisting of ChatGPT conversations with human questions and answers, and trained a text detection classifier based on this dataset. The use of these methods requires substantial data collection and incurs the cost of training these classifier models. \n\nStatistical-based detectors. Statistical-based detectors utilize statistical metrics such as entropy, perplexity, and n-gram frequency to differentiate between human-generated and AI-generated texts [1]. \n\nSome other work is based on watermark-based detectors. In previous research, watermarks have been applied in the field of image processing and computer vision to protect copyrighted content and prevent intellectual property theft (Langelaar et al., 2000). \n\nRecently, with the emergence of ChatGPT, the work by Kirchenbauer et al. [7] demonstrated how to incorporate a watermark using only the logarithmic credentials of each step to mark AI-generated texts. While watermark-based detectors are an intriguing area of research, adding watermarks may affect the readability of the texts, and the removal of watermarks is also a challenge we need to address. \n\nHowever, with the emergence of ChatGPT, an innovative statistical detection method called DetectGPT [5] has been developed. Its principle is that text generated by the model typically resides in the negative curvature region of the model's log probability. DetectGPT [5] generates and compares multiple variants of model-generated texts to determine whether the texts are machine-generated based on the log probabilities of the original texts and these variants. \n\nDetectGPT [5] outperforms the vast majority of existing zero-shot methods in terms of model sample detection, achieving very high AUC. It is based on this concept that DetectGPT-SC was proposed.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[62101493 | Langelaar et al. | 2000 | Citations: 878]": "The authors begin by discussing the need for watermarking and the requirements. They go on to discuss digital watermarking techniques based on correlation and techniques that are not based on correlation."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2363,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 84
                            },
                            {
                                "start": 85,
                                "end": 231
                            },
                            {
                                "start": 232,
                                "end": 332
                            },
                            {
                                "start": 335,
                                "end": 362
                            },
                            {
                                "start": 363,
                                "end": 517
                            },
                            {
                                "start": 518,
                                "end": 739
                            },
                            {
                                "start": 740,
                                "end": 858
                            },
                            {
                                "start": 861,
                                "end": 889
                            },
                            {
                                "start": 890,
                                "end": 1064
                            },
                            {
                                "start": 1067,
                                "end": 1121
                            },
                            {
                                "start": 1122,
                                "end": 1301
                            },
                            {
                                "start": 1304,
                                "end": 1504
                            },
                            {
                                "start": 1505,
                                "end": 1701
                            },
                            {
                                "start": 1704,
                                "end": 1827
                            },
                            {
                                "start": 1828,
                                "end": 1960
                            },
                            {
                                "start": 1961,
                                "end": 2166
                            },
                            {
                                "start": 2169,
                                "end": 2303
                            },
                            {
                                "start": 2304,
                                "end": 2363
                            }
                        ],
                        "ref_mentions": [
                            "62101493"
                        ],
                        "quote": "Recent research has shown promising results in the development of detection methods. The existing detectors are built on the assumption that there is a distributional difference between human-generated texts and AI-generated texts. These differences are typically identified by training classifiers or using statistical information. \n\nClassifier-based detectors. Classifier-based detectors are commonly used in natural language processing detection paradigms, especially in fake news and misinformation detection [3]. Guo et al. [4] proposed the ChatGPT Detector, where they initially constructed a dataset consisting of ChatGPT conversations with human questions and answers, and trained a text detection classifier based on this dataset. The use of these methods requires substantial data collection and incurs the cost of training these classifier models. \n\nStatistical-based detectors. Statistical-based detectors utilize statistical metrics such as entropy, perplexity, and n-gram frequency to differentiate between human-generated and AI-generated texts [1]. \n\nSome other work is based on watermark-based detectors. In previous research, watermarks have been applied in the field of image processing and computer vision to protect copyrighted content and prevent intellectual property theft (Langelaar et al., 2000). \n\nRecently, with the emergence of ChatGPT, the work by Kirchenbauer et al. [7] demonstrated how to incorporate a watermark using only the logarithmic credentials of each step to mark AI-generated texts. While watermark-based detectors are an intriguing area of research, adding watermarks may affect the readability of the texts, and the removal of watermarks is also a challenge we need to address. \n\nHowever, with the emergence of ChatGPT, an innovative statistical detection method called DetectGPT [5] has been developed. Its principle is that text generated by the model typically resides in the negative curvature region of the model's log probability. DetectGPT [5] generates and compares multiple variants of model-generated texts to determine whether the texts are machine-generated based on the log probabilities of the original texts and these variants. \n\nDetectGPT [5] outperforms the vast majority of existing zero-shot methods in terms of model sample detection, achieving very high AUC. It is based on this concept that DetectGPT-SC was proposed."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[265157663 | Guo et al. | 2023 | Citations: 11]",
                "snippets": "One of the major concerns is their ability to create text that closely mimics human writing, which can lead to potential misuse, such as academic misconduct, disinformation, and fraud. To address this problem, we present AuthentiGPT, an efficient classifier that distinguishes between machine-generated and human-written texts. Under the assumption that human-written text resides outside the distribution of machine-generated text, AuthentiGPT leverages a black-box LLM to denoise input text with artificially added noise, and then semantically compares the denoised text with the original to determine if the content is machine-generated. With only one trainable parameter, AuthentiGPT eliminates the need for a large training dataset, watermarking the LLM's output, or computing the log-likelihood.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 113,
                        "end": 914,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "One of the major concerns is their ability to create text that closely mimics human writing, which can lead to potential misuse, such as academic misconduct, disinformation, and fraud. To address this problem, we present AuthentiGPT, an efficient classifier that distinguishes between machine-generated and human-written texts. Under the assumption that human-written text resides outside the distribution of machine-generated text, AuthentiGPT leverages a black-box LLM to denoise input text with artificially added noise, and then semantically compares the denoised text with the original to determine if the content is machine-generated. With only one trainable parameter, AuthentiGPT eliminates the need for a large training dataset, watermarking the LLM's output, or computing the log-likelihood."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[266369103 | Mubarak et al. | 2023 | Citations: 36]",
                "snippets": "In the context of the rapid advancement of Large Language Models (LLMs), the ability to produce highly accurate and contextually relevant text is becoming an increasing concern in online security. The sophisticated text generation capabilities of these models can potentially facilitate harmful activities on online platforms, emphasising the importance of detecting AI-generated texts as a mitigation strategy. Several detection methodologies have been developed, each with varying degrees of effectiveness influenced by factors such as the architecture of the language model, the decoding strategy, the length of the text and the availability of raw log probabilities from the LLMs. \n\nSimple classifiers offer a basic, yet sometimes limited, approach, particularly with shorter texts and complex generation strategies. Zero-shot models provide a robust detection strategy by exploiting unique statistical patterns in machine-generated text; however, they are susceptible to adversarial actions or different generation strategies. Finetuned language models represent another approach that provides a more promising route for enhancing the detection accuracy. However, their effectiveness can be influenced by the sampling method and the length of the generated sequences. \n\nWatermarking, a more recent development, has emerged as a unique method for detection. This involves embedding watermarks in the logits of an LLM during text generation, forming a pattern that can potentially identify synthetic text. Despite the promise of watermarking, its vulnerability to spoofing/paraphrasing attacks and evasion techniques presents significant challenges that warrant further research.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "D. DETECTING SYNTHETIC TEXT AND THEIR OVERLAP WITH BOT DETECTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1682,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 196
                            },
                            {
                                "start": 197,
                                "end": 411
                            },
                            {
                                "start": 412,
                                "end": 684
                            },
                            {
                                "start": 687,
                                "end": 820
                            },
                            {
                                "start": 821,
                                "end": 1031
                            },
                            {
                                "start": 1032,
                                "end": 1159
                            },
                            {
                                "start": 1160,
                                "end": 1272
                            },
                            {
                                "start": 1275,
                                "end": 1361
                            },
                            {
                                "start": 1362,
                                "end": 1508
                            },
                            {
                                "start": 1509,
                                "end": 1682
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the context of the rapid advancement of Large Language Models (LLMs), the ability to produce highly accurate and contextually relevant text is becoming an increasing concern in online security. The sophisticated text generation capabilities of these models can potentially facilitate harmful activities on online platforms, emphasising the importance of detecting AI-generated texts as a mitigation strategy. Several detection methodologies have been developed, each with varying degrees of effectiveness influenced by factors such as the architecture of the language model, the decoding strategy, the length of the text and the availability of raw log probabilities from the LLMs. \n\nSimple classifiers offer a basic, yet sometimes limited, approach, particularly with shorter texts and complex generation strategies. Zero-shot models provide a robust detection strategy by exploiting unique statistical patterns in machine-generated text; however, they are susceptible to adversarial actions or different generation strategies. Finetuned language models represent another approach that provides a more promising route for enhancing the detection accuracy. However, their effectiveness can be influenced by the sampling method and the length of the generated sequences. \n\nWatermarking, a more recent development, has emerged as a unique method for detection. This involves embedding watermarks in the logits of an LLM during text generation, forming a pattern that can potentially identify synthetic text. Despite the promise of watermarking, its vulnerability to spoofing/paraphrasing attacks and evasion techniques presents significant challenges that warrant further research."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[266690717 | Caiado et al. | 2023 | Citations: 1]",
                "snippets": "Many tools to detect AI-generated text are now offered. Some popular tools geared toward educators are Copyleaks AI Content Detector, Crossplag, GPTZero, Hugging Face OpenAI Detector, Originality.ai, Turnitin AI Detection and ZeroGPT. The list of detectors and their capability is constantly changing following the fast-paced changes seen in the development of large language models.\n\nMost tools are based on detecting artifacts of the text generation process, including word choice, writing style, sentence length, and many more. A report by Open AI [9] lays out three AI content detection strategies, including a simple classifier learned from scratch, a classifier resulting from fine-tuning an existing language model, or using the probabilities assigned by the model to strings. Many existing tools follow the first two approaches. For example, the Hugging Face Open AI detector is a transformer-based classifier that is fine-tuned to detect GPT-2 text. Self-detection introduced in this paper is most closely related to the third approach. However, it does not require access to the model parameters to assess probabilities. It relies on the model itself to perform the detection.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Detection of AI-generated Text",
                        "pdf_hash": "",
                        "start": 680,
                        "end": 1866,
                        "sentence_offsets": [
                            {
                                "start": 672,
                                "end": 813
                            },
                            {
                                "start": 816,
                                "end": 871
                            },
                            {
                                "start": 872,
                                "end": 1015
                            },
                            {
                                "start": 1016,
                                "end": 1050
                            },
                            {
                                "start": 1051,
                                "end": 1199
                            },
                            {
                                "start": 1202,
                                "end": 1347
                            },
                            {
                                "start": 1348,
                                "end": 1600
                            },
                            {
                                "start": 1601,
                                "end": 1653
                            },
                            {
                                "start": 1654,
                                "end": 1775
                            },
                            {
                                "start": 1776,
                                "end": 1862
                            },
                            {
                                "start": 1863,
                                "end": 1947
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Many tools to detect AI-generated text are now offered. Some popular tools geared toward educators are Copyleaks AI Content Detector, Crossplag, GPTZero, Hugging Face OpenAI Detector, Originality.ai, Turnitin AI Detection and ZeroGPT. The list of detectors and their capability is constantly changing following the fast-paced changes seen in the development of large language models.\n\nMost tools are based on detecting artifacts of the text generation process, including word choice, writing style, sentence length, and many more. A report by Open AI [9] lays out three AI content detection strategies, including a simple classifier learned from scratch, a classifier resulting from fine-tuning an existing language model, or using the probabilities assigned by the model to strings. Many existing tools follow the first two approaches. For example, the Hugging Face Open AI detector is a transformer-based classifier that is fine-tuned to detect GPT-2 text. Self-detection introduced in this paper is most closely related to the third approach. However, it does not require access to the model parameters to assess probabilities. It relies on the model itself to perform the detection."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[267027819 | Bethany et al. | 2024 | Citations: 6]",
                "snippets": "One of the subsets of statistical features leveraged in detecting machine-generated text revolves around term frequencies within text samples. Text authored by humans typically aligns with Zipf's Law, where the most frequent word in humanwritten text has roughly twice the frequency of the second most frequent word, and nearly three times that of the third most frequent word and so on (Nguyen-Son et al., 2018). Another feature-based method for detecting machine-generated text focuses on clarity and coherence. Earlier models like GPT-2 have shown that in longer outputs, there is a higher likelihood that inconsistencies, redundancy, or logical errors will emerge (See et al., 2019). Some works build classifiers on bag-of-words features and TF-IDF (Fagni et al., 2020) to analyze shorter text sequences. Other works such as Fr\u00f6hling et al. (Fr\u00f6hling et al., 2021) combine several features, including lack of syntactic and lexical diversity, repetitiveness, and extract linguistic features such as named entities and part-of-speech distributions to build a neural network classifier on this information. However, a notable limitation of these statistical and feature-based methods is their potential vulnerability to more advanced language models, which continually improve in generating human-like text patterns and linguistic diversity. \n\nZero-shot methods for detecting machine-generated text have garnered attention due to the lack of need to train models on large datasets. Works such as those by Gehrmann et al. (Gehrmann et al., 2019) find that human-written texts exhibit a broader range of word choices, whereas machine-generated texts tend to be more constrained and predictable and calculate entropy on text sequences to help distinguish machine-generated text from human-generated text. Other works used simple logprobability thresholding, where LLM token probabilities are averaged and a threshold is determined [56]. Recent works like DetectGPT [40] take this idea further by introducing the observation that the curvature of a model's log probability function is typically more negative for machine-generated text compared to human-written text.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[182952848 | Gehrmann et al. | 2019 | Citations: 546]": "The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by non-experts. In this work, we introduce GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across multiple sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs.",
                    "[202734604 | See et al. | 2019 | Citations: 168]": "Large neural language models trained on massive amounts of text have emerged as a formidable strategy for Natural Language Understanding tasks. However, the strength of these models as Natural Language Generators is less clear. Though anecdotal evidence suggests that these models generate better quality text, there has been no detailed study characterizing their generation abilities. In this work, we compare the performance of an extensively pretrained model, OpenAI GPT2-117 (Radford et al., 2019), to a state-of-the-art neural story generation model (Fan et al., 2018). By evaluating the generated text across a wide variety of automatic metrics, we characterize the ways in which pretrained models do, and do not, make better storytellers. We find that although GPT2-117 conditions more strongly on context, is more sensitive to ordering of events, and uses more unusual words, it is just as likely to produce repetitive and under-diverse text when using likelihood-maximizing decoding algorithms.",
                    "[220936111 | Fagni et al. | 2020 | Citations: 224]": "The recent advances in language modeling significantly improved the generative capabilities of deep neural models: in 2019 OpenAI released GPT-2, a pre-trained language model that can autonomously generate coherent, non-trivial and human-like text samples. Since then, ever more powerful text generative models have been developed. Adversaries can exploit these tremendous generative capabilities to enhance social bots that will have the ability to write plausible deepfake messages, hoping to contaminate public debate. To prevent this, it is crucial to develop deepfake social media messages detection systems. However, to the best of our knowledge no one has ever addressed the detection of machine-generated texts on social networks like Twitter or Facebook. With the aim of helping the research in this detection field, we collected the first dataset of real deepfake tweets, TweepFake. It is real in the sense that each deepfake tweet was actually posted on Twitter. We collected tweets from a total of 23 bots, imitating 17 human accounts. The bots are based on various generation techniques, i.e., Markov Chains, RNN, RNN+Markov, LSTM, GPT-2. We also randomly selected tweets from the humans imitated by the bots to have an overall balanced dataset of 25,572 tweets (half human and half bots generated). The dataset is publicly available on Kaggle. Lastly, we evaluated 13 deepfake text detection methods (based on various state-of-the-art approaches) to both demonstrate the challenges that Tweepfake poses and create a solid baseline of detection techniques. We hope that TweepFake can offer the opportunity to tackle the deepfake detection on social media messages as well.",
                    "[233563022 | Frohling et al. | 2021 | Citations: 110]": "The recent improvements of language models have drawn much attention to potential cases of use and abuse of automatically generated text. Great effort is put into the development of methods to detect machine generations among human-written text in order to avoid scenarios in which the large-scale generation of text with minimal cost and effort undermines the trust in human interaction and factual information online. While most of the current approaches rely on the availability of expensive language models, we propose a simple feature-based classifier for the detection problem, using carefully crafted features that attempt to model intrinsic differences between human and machine text. Our research contributes to the field in producing a detection method that achieves performance competitive with far more expensive methods, offering an accessible \u201cfirst line-of-defense\u201d against the abuse of language models. Furthermore, our experiments show that different sampling methods lead to different types of flaws in generated text."
                },
                "metadata": [
                    {
                        "section_title": "Background",
                        "pdf_hash": "",
                        "start": 156,
                        "end": 2231,
                        "sentence_offsets": [
                            {
                                "start": 156,
                                "end": 298
                            },
                            {
                                "start": 299,
                                "end": 548
                            },
                            {
                                "start": 549,
                                "end": 648
                            },
                            {
                                "start": 649,
                                "end": 808
                            },
                            {
                                "start": 809,
                                "end": 913
                            },
                            {
                                "start": 914,
                                "end": 1193
                            },
                            {
                                "start": 1194,
                                "end": 1428
                            },
                            {
                                "start": 1431,
                                "end": 1568
                            },
                            {
                                "start": 1569,
                                "end": 1869
                            },
                            {
                                "start": 1870,
                                "end": 2001
                            },
                            {
                                "start": 2002,
                                "end": 2231
                            }
                        ],
                        "ref_mentions": [
                            "36455310",
                            "202734604",
                            "220936111",
                            "233563022",
                            "182952848"
                        ],
                        "quote": "One of the subsets of statistical features leveraged in detecting machine-generated text revolves around term frequencies within text samples. Text authored by humans typically aligns with Zipf's Law, where the most frequent word in humanwritten text has roughly twice the frequency of the second most frequent word, and nearly three times that of the third most frequent word and so on (Nguyen-Son et al., 2018). Another feature-based method for detecting machine-generated text focuses on clarity and coherence. Earlier models like GPT-2 have shown that in longer outputs, there is a higher likelihood that inconsistencies, redundancy, or logical errors will emerge (See et al., 2019). Some works build classifiers on bag-of-words features and TF-IDF (Fagni et al., 2020) to analyze shorter text sequences. Other works such as Fr\u00f6hling et al. (Fr\u00f6hling et al., 2021) combine several features, including lack of syntactic and lexical diversity, repetitiveness, and extract linguistic features such as named entities and part-of-speech distributions to build a neural network classifier on this information. However, a notable limitation of these statistical and feature-based methods is their potential vulnerability to more advanced language models, which continually improve in generating human-like text patterns and linguistic diversity. \n\nZero-shot methods for detecting machine-generated text have garnered attention due to the lack of need to train models on large datasets. Works such as those by Gehrmann et al. (Gehrmann et al., 2019) find that human-written texts exhibit a broader range of word choices, whereas machine-generated texts tend to be more constrained and predictable and calculate entropy on text sequences to help distinguish machine-generated text from human-generated text. Other works used simple logprobability thresholding, where LLM token probabilities are averaged and a threshold is determined [56]. Recent works like DetectGPT [40] take this idea further by introducing the observation that the curvature of a model's log probability function is typically more negative for machine-generated text compared to human-written text."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[267094724 | Xiong et al. | 2024 | Citations: 5]",
                "snippets": "In the realm of distinguishing texts generated by large language models (LLMs) from those authored by humans, researchers have employed a variety of methods and tools. Broadly, these approaches can be categorized into two main types: black-box and white-box detection methods (Tang et al., 2023). Black-box detection relies on API-level access to LLMs, utilizing textual samples from both human and machine sources to train classification models (Dugan et al., 2020). The study by Guo et al. (2023) integrated existing question-and-answer datasets and leveraged fine-tuning of pre-trained models to investigate the characteristics and similarities between human-generated and AI-generated texts (Guo et al., 2023). \n\nIn the context of white-box detection, Kirchenbauer et al. ( 2023) introduced a novel approach involving the embedding of watermarks in the outputs of large language models to facilitate the detection of AI-generated text (Kirchenbauer et al., 2023). Additionally, a variety of tools and methodologies, including XGBoost, decision trees, and transformer-based models, have been evaluated for their efficacy in detecting texts produced by AI (Zaitsu and Jin, 2023). These techniques incorporate multiple stylistic measurement features to differentiate between AI-generated and human-generated texts (Shijaku and Canhasi, 2023). \n\nSpecific tools and techniques in this domain include the GLTR tool developed by Gehrmann et al. (2019), which analyzes the usage of rare words in texts to distinguish between those generated by the GPT-2 model and human writers (Gehrmann et al., 2019). The DetectGPT method posits that minor rewrites of LLM-generated texts tend to reduce the log probability under the model, a hypothesis that has been explored in depth (Mitchell et al., 2023). Furthermore, intrinsic dimension analysis, including methods like the Persistent Homology Dimension estimator (PHD), has been applied to distinguish between authentic texts and those generated artificially (Tulchinskii et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2025,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 167
                            },
                            {
                                "start": 168,
                                "end": 296
                            },
                            {
                                "start": 297,
                                "end": 467
                            },
                            {
                                "start": 468,
                                "end": 714
                            },
                            {
                                "start": 717,
                                "end": 967
                            },
                            {
                                "start": 968,
                                "end": 1181
                            },
                            {
                                "start": 1182,
                                "end": 1343
                            },
                            {
                                "start": 1346,
                                "end": 1598
                            },
                            {
                                "start": 1599,
                                "end": 1791
                            },
                            {
                                "start": 1792,
                                "end": 2025
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the realm of distinguishing texts generated by large language models (LLMs) from those authored by humans, researchers have employed a variety of methods and tools. Broadly, these approaches can be categorized into two main types: black-box and white-box detection methods (Tang et al., 2023). Black-box detection relies on API-level access to LLMs, utilizing textual samples from both human and machine sources to train classification models (Dugan et al., 2020). The study by Guo et al. (2023) integrated existing question-and-answer datasets and leveraged fine-tuning of pre-trained models to investigate the characteristics and similarities between human-generated and AI-generated texts (Guo et al., 2023). \n\nIn the context of white-box detection, Kirchenbauer et al. ( 2023) introduced a novel approach involving the embedding of watermarks in the outputs of large language models to facilitate the detection of AI-generated text (Kirchenbauer et al., 2023). Additionally, a variety of tools and methodologies, including XGBoost, decision trees, and transformer-based models, have been evaluated for their efficacy in detecting texts produced by AI (Zaitsu and Jin, 2023). These techniques incorporate multiple stylistic measurement features to differentiate between AI-generated and human-generated texts (Shijaku and Canhasi, 2023). \n\nSpecific tools and techniques in this domain include the GLTR tool developed by Gehrmann et al. (2019), which analyzes the usage of rare words in texts to distinguish between those generated by the GPT-2 model and human writers (Gehrmann et al., 2019). The DetectGPT method posits that minor rewrites of LLM-generated texts tend to reduce the log probability under the model, a hypothesis that has been explored in depth (Mitchell et al., 2023). Furthermore, intrinsic dimension analysis, including methods like the Persistent Homology Dimension estimator (PHD), has been applied to distinguish between authentic texts and those generated artificially (Tulchinskii et al., 2023)."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[268357168 | Munyer et al. | 2023 | Citations: 15]",
                "snippets": "Recent endeavors have been directed towards developing classifiers aimed at differentiating between LLM-generated text and human-written text.The prevailing approach entails the collection and labeling of LLM-generated and humanwritten texts, followed by the training of a binary classifier through supervised learning.Although the efficacy of these classifiers has yet to be fully established, some preliminary analyses have been reported [8,9].One study [9] elucidated three distinct methods, substantiated with examples, to circumvent the GPTZero [7] classifier detection.Another investigation [8] conducted a direct assessment of GPTZero's accuracy, uncovering inconsistencies in its ability to detect human-written text.Moreover, classifier-based LLM-generated text detectors commonly necessitate a substantial character count to perform detection accurately.For instance, GPTZero [7] required a minimum of 250 characters to initiate detection.Looking ahead, OpenAI is planning a cryptography-based watermarking system for ChatGPTgenerated text detection [15], although no definitive work has been disclosed as of yet.Zero-shot learning-based methods have also demonstrated some advancement.For example, Cer et al. [16] reported an increment in AUROC from 1% to 14% compared to other zero-shot detection strategies across various datasets; however, the accuracy might still fall short in real-world applications concerning text generated by models.\n\nA method has been proposed for detecting LLM-generated texts based on text watermarking (Kirchenbauer et al., 2023), which involves watermarking the text by modifying the LLMs (sensitive tokens are defined and excluded from the output of the LLMs).In contrast, our proposed DeepTextMark does not necessitate access to or modifications of the LLM.Distinct from model-dependent methods, DeepTextMark exhibits a model-independent feature, enabling its application to any text.Moreover, DeepTextMark employs a substantially more compact architecture with about 50 million parameters, whereas the method in [11] necessitates billions of parameters to implement the watermarking process.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[256194179 | Kirchenbauer et al. | 2023 | Citations: 507]": "Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of\"green\"tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security."
                },
                "metadata": [
                    {
                        "section_title": "Text Source Detection for Large Language Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2113,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 142
                            },
                            {
                                "start": 142,
                                "end": 319
                            },
                            {
                                "start": 319,
                                "end": 446
                            },
                            {
                                "start": 446,
                                "end": 575
                            },
                            {
                                "start": 575,
                                "end": 725
                            },
                            {
                                "start": 725,
                                "end": 864
                            },
                            {
                                "start": 864,
                                "end": 949
                            },
                            {
                                "start": 949,
                                "end": 1123
                            },
                            {
                                "start": 1123,
                                "end": 1196
                            },
                            {
                                "start": 1196,
                                "end": 1453
                            },
                            {
                                "start": 1455,
                                "end": 1680
                            },
                            {
                                "start": 1680,
                                "end": 1778
                            },
                            {
                                "start": 1778,
                                "end": 1905
                            },
                            {
                                "start": 1905,
                                "end": 2113
                            }
                        ],
                        "ref_mentions": [
                            "256194179"
                        ],
                        "quote": "Recent endeavors have been directed towards developing classifiers aimed at differentiating between LLM-generated text and human-written text.The prevailing approach entails the collection and labeling of LLM-generated and humanwritten texts, followed by the training of a binary classifier through supervised learning.Although the efficacy of these classifiers has yet to be fully established, some preliminary analyses have been reported [8,9].One study [9] elucidated three distinct methods, substantiated with examples, to circumvent the GPTZero [7] classifier detection.Another investigation [8] conducted a direct assessment of GPTZero's accuracy, uncovering inconsistencies in its ability to detect human-written text.Moreover, classifier-based LLM-generated text detectors commonly necessitate a substantial character count to perform detection accurately.For instance, GPTZero [7] required a minimum of 250 characters to initiate detection.Looking ahead, OpenAI is planning a cryptography-based watermarking system for ChatGPTgenerated text detection [15], although no definitive work has been disclosed as of yet.Zero-shot learning-based methods have also demonstrated some advancement.For example, Cer et al. [16] reported an increment in AUROC from 1% to 14% compared to other zero-shot detection strategies across various datasets; however, the accuracy might still fall short in real-world applications concerning text generated by models.\n\nA method has been proposed for detecting LLM-generated texts based on text watermarking (Kirchenbauer et al., 2023), which involves watermarking the text by modifying the LLMs (sensitive tokens are defined and excluded from the output of the LLMs).In contrast, our proposed DeepTextMark does not necessitate access to or modifications of the LLM.Distinct from model-dependent methods, DeepTextMark exhibits a model-independent feature, enabling its application to any text.Moreover, DeepTextMark employs a substantially more compact architecture with about 50 million parameters, whereas the method in [11] necessitates billions of parameters to implement the watermarking process."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[268793677 | Taguchi et al. | 2024 | Citations: 7]",
                "snippets": "Detectors for AI-generated text can be broadly classified into three categories: a zero-shot detec-tor leveraging statistical properties (Gehrmann et al., 2019)(Mitchell et al., 2023)6,7,8,9,10,11], a detector employing supervised learning [12,13,14,15], and a detector utilizing watermarking (Kirchenbauer et al., 2023)[17].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[256274849 | Mitchell et al. | 2023 | Citations: 627]": "The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.",
                    "[182952848 | Gehrmann et al. | 2019 | Citations: 546]": "The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by non-experts. In this work, we introduce GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across multiple sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs.",
                    "[256194179 | Kirchenbauer et al. | 2023 | Citations: 507]": "Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of\"green\"tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 448,
                        "end": 708,
                        "sentence_offsets": [
                            {
                                "start": 448,
                                "end": 708
                            }
                        ],
                        "ref_mentions": [
                            "182952848",
                            "256274849",
                            "256194179"
                        ],
                        "quote": "Detectors for AI-generated text can be broadly classified into three categories: a zero-shot detec-tor leveraging statistical properties (Gehrmann et al., 2019)(Mitchell et al., 2023)6,7,8,9,10,11], a detector employing supervised learning [12,13,14,15], and a detector utilizing watermarking (Kirchenbauer et al., 2023)[17]."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[268819799 | Li et al. | 2024 | Citations: 22]",
                "snippets": "Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 298,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[270370917 | Zeng et al. | 2024 | Citations: 2]",
                "snippets": "Methods for detecting text generated by Large Language Models are broadly categorized into watermarking (Abdelnabi et al., 2020)[19][20](Christ et al., 2023), training-based classifiers [22][23][24][25][26], and zero-shot detectors. Watermarking methods discreetly embed identifiable markers within the text output, striving to retain the model's linguistic integrity. However, this tactic is implementable solely by the model provider. Trainingbased classifiers, while effective, are costly and often lack the agility to adapt to new domains or model updates. Our emphasis is on zero-shot detectors that exploit the intrinsic differences between text written by machines and humans, offering the advantage of being generally training-free. \n\nMost zero-shot detectors primarily depend on analyzing model output logits for detection. Notably, DetectGPT [27] operates on probability divergence based upon principles of perturbation theory, while DNA-GPT [28] harnesses reprompting-based probability divergence, and Fast-DetectGPT [29] builds on variations in conditional probability distributions. In scenarios requiring the scrutiny of black-box models, these strategies commonly leverage a surrogate model to approximate the behavior of the target model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[221516138 | Abdelnabi et al. | 2020 | Citations: 152]": "Recent advances in natural language generation have introduced powerful language models with high-quality output text. However, this raises concerns about the potential misuse of such models for malicious purposes. In this paper, we study natural language watermarking as a defense to help better mark and trace the provenance of text. We introduce the Adversarial Watermarking Transformer (AWT) with a jointly trained encoder-decoder and adversarial training that, given an input text and a binary message, generates an output text that is unobtrusively encoded with the given message. We further study different training and inference strategies to achieve minimal changes to the semantics and correctness of the input text.AWT is the first end-to-end model to hide data in text by automatically learning -without ground truth- word substitutions along with their locations in order to encode the message. We empirically show that our model is effective in largely preserving text utility and decoding the watermark while hiding its presence against adversaries. Additionally, we demonstrate that our method is robust against a range of attacks.",
                    "[259092330 | Christ et al. | 2023 | Citations: 146]": "Recent advances in the capabilities of large language models such as GPT-4 have spurred increasing concern about our ability to detect AI-generated text. Prior works have suggested methods of embedding watermarks in model outputs, by noticeably altering the output distribution. We ask: Is it possible to introduce a watermark without incurring any detectable change to the output distribution? To this end we introduce a cryptographically-inspired notion of undetectable watermarks for language models. That is, watermarks can be detected only with the knowledge of a secret key; without the secret key, it is computationally intractable to distinguish watermarked outputs from those of the original model. In particular, it is impossible for a user to observe any degradation in the quality of the text. Crucially, watermarks should remain undetectable even when the user is allowed to adaptively query the model with arbitrarily chosen prompts. We construct undetectable watermarks based on the existence of one-way functions, a standard assumption in cryptography."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 743,
                        "end": 1960,
                        "sentence_offsets": [
                            {
                                "start": 743,
                                "end": 938
                            },
                            {
                                "start": 939,
                                "end": 1074
                            },
                            {
                                "start": 1075,
                                "end": 1142
                            },
                            {
                                "start": 1143,
                                "end": 1266
                            },
                            {
                                "start": 1267,
                                "end": 1446
                            },
                            {
                                "start": 1449,
                                "end": 1538
                            },
                            {
                                "start": 1539,
                                "end": 1801
                            },
                            {
                                "start": 1802,
                                "end": 1960
                            }
                        ],
                        "ref_mentions": [
                            "221516138",
                            "259092330"
                        ],
                        "quote": "Methods for detecting text generated by Large Language Models are broadly categorized into watermarking (Abdelnabi et al., 2020)[19][20](Christ et al., 2023), training-based classifiers [22][23][24][25][26], and zero-shot detectors. Watermarking methods discreetly embed identifiable markers within the text output, striving to retain the model's linguistic integrity. However, this tactic is implementable solely by the model provider. Trainingbased classifiers, while effective, are costly and often lack the agility to adapt to new domains or model updates. Our emphasis is on zero-shot detectors that exploit the intrinsic differences between text written by machines and humans, offering the advantage of being generally training-free. \n\nMost zero-shot detectors primarily depend on analyzing model output logits for detection. Notably, DetectGPT [27] operates on probability divergence based upon principles of perturbation theory, while DNA-GPT [28] harnesses reprompting-based probability divergence, and Fast-DetectGPT [29] builds on variations in conditional probability distributions. In scenarios requiring the scrutiny of black-box models, these strategies commonly leverage a surrogate model to approximate the behavior of the target model."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[270703139 | Fraser et al. | 2024 | Citations: 13]",
                "snippets": "In this survey, we summarize stateof-the art approaches to AIGT detection, including watermarking, statistical and stylistic analysis, and machine learning classification. We also provide information about existing datasets for this task.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 566,
                        "end": 804,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In this survey, we summarize stateof-the art approaches to AIGT detection, including watermarking, statistical and stylistic analysis, and machine learning classification. We also provide information about existing datasets for this task."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[271161969 | Cava et al. | 2024 | Citations: 3]",
                "snippets": "The remarkable boost in human-like text generation performances achieved by Large Language Models (LLMs) in recent years has determined a rising challenge in detecting whether and to what extent texts have been generated by humans or machines [21,49,39]. In this context, the \"watermarking\" paradigm rapidly gained attention (Kirchenbauer et al., 2023)(Yoo et al., 2023)28,50], as it allows embedding specific signals into generated texts that remain invisible to humans but are algorithmically detectable. Statistical learning methods also offer advanced solutions for detecting the authorship of texts. These include probabilistic models (Mitchell et al., 2023)1,47,14], log rank information [38], perplexity [44], discourse motifs [24], and other statistical approaches (Gehrmann et al., 2019)(Tulchinskii et al., 2023)[45] More recently, we have witnessed the emergence of deep learning to detect or attribute AI-generated content, which stands as a promising body of research. Researchers have been exploiting LLMs to detect generated text [18,46], using ChatGPT itself as a detector (Bhattacharjee et al., 2023), or combining LLMs with topological aspects [43].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[256274849 | Mitchell et al. | 2023 | Citations: 627]": "The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.",
                    "[182952848 | Gehrmann et al. | 2019 | Citations: 546]": "The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by non-experts. In this work, we introduce GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across multiple sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs.",
                    "[256194179 | Kirchenbauer et al. | 2023 | Citations: 507]": "Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of\"green\"tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.",
                    "[259108779 | Tulchinskii et al. | 2023 | Citations: 89]": "Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over different text domains and varying proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant for human-written texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings for a given text sample. We show that the average intrinsic dimensionality of fluent texts in a natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\\approx 1.5$ lower, with a clear statistical separation between human-generated and AI-generated distributions. This property allows us to build a score-based artificial text detector. The proposed detector's accuracy is stable over text domains, generator models, and human writer proficiency levels, outperforming SOTA detectors in model-agnostic and cross-domain scenarios by a significant margin.",
                    "[259129912 | Yoo et al. | 2023 | Citations: 83]": "Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security measures. This calls for a secure watermarking system to guarantee copyright protection through leakage tracing or ownership identification. To effectively combat piracy and protect copyrights, a multi-bit watermarking framework should be able to embed adequate bits of information and extract the watermarks in a robust manner despite possible corruption. In this work, we explore ways to advance both payload and robustness by following a well-known proposition from image watermarking and identify features in natural language that are invariant to minor corruption. Through a systematic analysis of the possible sources of errors, we further propose a corruption-resistant infill model. Our full method improves upon the previous work on robustness by +16.8% point on average on four datasets, three corruption types, and two corruption ratios",
                    "[260379022 | Bhattacharjee et al. | 2023 | Citations: 62]": "Large language models (LLMs) such as ChatGPT are increasingly being used for various use cases, including text content generation at scale. Although detection methods for such AI-generated text exist already, we investigate ChatGPT's performance as a detector on such AI-generated text, inspired by works that use ChatGPT as a data labeler or annotator. We evaluate the zeroshot performance of ChatGPT in the task of human-written vs. AI-generated text detection, and perform experiments on publicly available datasets. We empirically investigate if ChatGPT is symmetrically effective in detecting AI-generated or human-written text. Our findings provide insight on how ChatGPT and similar LLMs may be leveraged in automated detection pipelines by simply focusing on solving a specific aspect of the problem and deriving the rest from that solution. All code and data is available at https://github.com/AmritaBh/ChatGPT-as-Detector."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1042,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 254
                            },
                            {
                                "start": 255,
                                "end": 468
                            },
                            {
                                "start": 469,
                                "end": 566
                            },
                            {
                                "start": 567,
                                "end": 881
                            },
                            {
                                "start": 882,
                                "end": 1042
                            }
                        ],
                        "ref_mentions": [
                            "256194179",
                            "259129912",
                            "256274849",
                            "182952848",
                            "259108779",
                            "260379022"
                        ],
                        "quote": "The remarkable boost in human-like text generation performances achieved by Large Language Models (LLMs) in recent years has determined a rising challenge in detecting whether and to what extent texts have been generated by humans or machines [21,49,39]. In this context, the \"watermarking\" paradigm rapidly gained attention (Kirchenbauer et al., 2023)(Yoo et al., 2023)28,50], as it allows embedding specific signals into generated texts that remain invisible to humans but are algorithmically detectable. Statistical learning methods also offer advanced solutions for detecting the authorship of texts. These include probabilistic models (Mitchell et al., 2023)1,47,14], log rank information [38], perplexity [44], discourse motifs [24], and other statistical approaches (Gehrmann et al., 2019)(Tulchinskii et al., 2023)[45] More recently, we have witnessed the emergence of deep learning to detect or attribute AI-generated content, which stands as a promising body of research. Researchers have been exploiting LLMs to detect generated text [18,46], using ChatGPT itself as a detector (Bhattacharjee et al., 2023), or combining LLMs with topological aspects [43]."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[272987651 | Feng et al. | 2024 | Citations: 0]",
                "snippets": "AI Generated Text Identification. While large language models are applied to various scenarios with their increasingly powerful text generation capabilities, such as story generation (Yuan et al., 2022), article translation [59] and code assistance [53], the potential risks they bring [7,(Lee et al., 2022)(Yanagi et al., 2020) have made the identi- Direct detection methods often identify AI-generated text by uncovering distinguishing characteristics between AIgenerated and human-written texts or utilizing an extensive dataset of AI-generated text to train a accurate text classifier for AI-generated content. For example, regarding the first kind of method, Hamed and Wu [1] discovered that the bigram similarity of texts generated by ChatGPT is higher than that of human-written texts and developed a corresponding detection method based on this finding. Mitchell et al. found that AIgenerated texts tend to occupy the negative curvature region of the model's log-probability function. Based on this observation they proposed DetectGPT (Mitchell et al., 2023). Building on Mitchell et al.'s approach, Bao et al. [2] improved the perturbation step in their method, significantly enhancing both its accuracy and speed. However, with the rapid advancement in both the size of model and text generation capabilities of LLMs, the gap between AI-generated texts and human-written texts has been narrowing [16,35,38,(Pack et al., 2024). As a result, methods based on text characteristics are becoming increasingly less effective.For the second type of method, Mindner et al. (Mindner et al., 2023) employed multidimensional text feature extraction approaches to construct a classifier, with the best classifier outperforming GPTZero [14] in terms of F1-score. Chen et al. [6] and Liu et al. [32] utilized the advanced language understanding capabilities of pretrained LLMs [33](Raffel et al., 2019), finetuning them as binary classifiers on various text datasets for AI-generated text detection. While these methods perform well on their respective test datasets, their effectiveness may be limited when applied to texts generated by newly emerging models. Likewise, as the capabilities of large language models continue to advance, their effectiveness remains a question. \n\nWatermarking, as an alternative method for AI-generated text identification, is generally more effective, versatile, and interpretable compared to direct detection.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[256274849 | Mitchell et al. | 2023 | Citations: 627]": "The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.",
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
                    "[218781040 | Yanagi et al. | 2020 | Citations: 33]": "Recently, fake news is shared via social networks and makes wrong rumors more diffusible. This problem is serious because the wrong rumor sometimes make social damage by deceived people. Fact-checking is a solution to measure the credibility of news articles. However the process usually takes a long time and it is hard to make it before their diffusion. Automatic detection of fake news is a popular researching topic. It is confirmed that considering not only articles but also social contexts(i.e. likes, retweets, replies, comments) supports to spot fake news correctly. However, the social contexts are naturally unavailable when an article comes out, making early fake news detection by means of the social context useless. We propose a fake news detector with the ability to generate fake social contexts, aiming to detect fake news in the early stage of its diffusion where few social contexts are available. The fake context generation is based on a fake news generator model. This model is trained to generate comments using a dataset which consists of news articles and their social contexts. In addition, we also trained a classify model. This used news articles, real-posted comments, and generated comments. To measure our detector\u2019s effectiveness, we examined the performance of the generated comments for articles with real comments and generated ones by the classifying model. As a result, we conclude that considering a generated comment help detect more fake news than considering real comments only. It suggests that our proposed detector will be effective to spot fake news on social networks.",
                    "[247450984 | Lee et al. | 2022 | Citations: 78]": "Past literature has illustrated that language models (LMs) often memorize parts of training instances and reproduce them in natural language generation (NLG) processes. However, it is unclear to what extent LMs \u201creuse\u201d a training corpus. For instance, models can generate paraphrased sentences that are contextually similar to training samples. In this work, therefore, we study three types of plagiarism (i.e., verbatim, paraphrase, and idea) among GPT-2 generated texts, in comparison to its training data, and further analyze the plagiarism patterns of fine-tuned LMs with domain-specific corpora which are extensively used in practice. Our results suggest that (1) three types of plagiarism widely exist in LMs beyond memorization, (2) both size and decoding methods of LMs are strongly associated with the degrees of plagiarism they exhibit, and (3) fine-tuned LMs\u2019 plagiarism patterns vary based on their corpus similarity and homogeneity. Given that a majority of LMs\u2019 training data is scraped from the Web without informing content owners, their reiteration of words, phrases, and even core ideas from training sets into generated texts has ethical implications. Their patterns are likely to exacerbate as both the size of LMs and their training data increase, raising concerns about indiscriminately pursuing larger models with larger training corpora. Plagiarized content can also contain individuals\u2019 personal and sensitive information. These findings overall cast doubt on the practicality of current LMs in mission-critical writing tasks and urge more discussions around the observed phenomena. Data and source code are available at https://github.com/Brit7777/LM-plagiarism.",
                    "[247585187 | Yuan et al. | 2022 | Citations: 311]": "The latest generation of large neural language models such as GPT-3 have achieved new levels of performance on benchmarks for language understanding and generation. These models have even demonstrated an ability to perform arbitrary tasks without explicit training. In this work, we sought to learn how people might use such models in the process of creative writing. We built Wordcraft, a text editor in which users collaborate with a generative language model to write a story. We evaluated Wordcraft with a user study in which participants wrote short stories with and without the tool. Our results show that large language models enable novel co-writing experiences. For example, the language model is able to engage in open-ended conversation about the story, respond to writers\u2019 custom requests expressed in natural language (such as \u201drewrite this text to be more Dickensian\u201d), and generate suggestions that serve to unblock writers in the creative process. Based on these results, we discuss design implications for future human-AI co-writing systems.",
                    "[260775600 | Mindner et al. | 2023 | Citations: 45]": "Recently, generative AIs like ChatGPT have become available to the wide public. These tools can for instance be used by students to generate essays or whole theses. But how does a teacher know whether a text is written by a student or an AI? In our work, we explore traditional and new features to (1) detect text generated by AI from scratch and (2) text rephrased by AI. Since we found that classification is more difficult when the AI has been instructed to create the text in a way that a human would not recognize that it was generated by an AI, we also investigate this more advanced case. For our experiments, we produced a new text corpus covering 10 school topics. Our best systems to classify basic and advanced human-generated/AI-generated texts have F1-scores of over 96%. Our best systems for classifying basic and advanced human-generated/AI-rephrased texts have F1-scores of more than 78%. The systems use a combination of perplexity, semantic, list lookup, error-based, readability, AI feedback, and text vector features. Our results show that the new features substantially help to improve the performance of many classifiers. Our best basic text rephrasing detection system even outperforms GPTZero by 183.8% relative in F1-score."
                },
                "metadata": [
                    {
                        "section_title": "Related work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2320,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 33
                            },
                            {
                                "start": 34,
                                "end": 566
                            },
                            {
                                "start": 567,
                                "end": 813
                            },
                            {
                                "start": 814,
                                "end": 944
                            },
                            {
                                "start": 945,
                                "end": 1000
                            },
                            {
                                "start": 1001,
                                "end": 1156
                            },
                            {
                                "start": 1157,
                                "end": 1353
                            },
                            {
                                "start": 1354,
                                "end": 1658
                            },
                            {
                                "start": 1659,
                                "end": 1876
                            },
                            {
                                "start": 1877,
                                "end": 2037
                            },
                            {
                                "start": 2038,
                                "end": 2153
                            },
                            {
                                "start": 2156,
                                "end": 2320
                            }
                        ],
                        "ref_mentions": [
                            "247585187",
                            "247450984",
                            "218781040",
                            "256274849",
                            "269738126",
                            "260775600",
                            "204838007"
                        ],
                        "quote": "AI Generated Text Identification. While large language models are applied to various scenarios with their increasingly powerful text generation capabilities, such as story generation (Yuan et al., 2022), article translation [59] and code assistance [53], the potential risks they bring [7,(Lee et al., 2022)(Yanagi et al., 2020) have made the identi- Direct detection methods often identify AI-generated text by uncovering distinguishing characteristics between AIgenerated and human-written texts or utilizing an extensive dataset of AI-generated text to train a accurate text classifier for AI-generated content. For example, regarding the first kind of method, Hamed and Wu [1] discovered that the bigram similarity of texts generated by ChatGPT is higher than that of human-written texts and developed a corresponding detection method based on this finding. Mitchell et al. found that AIgenerated texts tend to occupy the negative curvature region of the model's log-probability function. Based on this observation they proposed DetectGPT (Mitchell et al., 2023). Building on Mitchell et al.'s approach, Bao et al. [2] improved the perturbation step in their method, significantly enhancing both its accuracy and speed. However, with the rapid advancement in both the size of model and text generation capabilities of LLMs, the gap between AI-generated texts and human-written texts has been narrowing [16,35,38,(Pack et al., 2024). As a result, methods based on text characteristics are becoming increasingly less effective.For the second type of method, Mindner et al. (Mindner et al., 2023) employed multidimensional text feature extraction approaches to construct a classifier, with the best classifier outperforming GPTZero [14] in terms of F1-score. Chen et al. [6] and Liu et al. [32] utilized the advanced language understanding capabilities of pretrained LLMs [33](Raffel et al., 2019), finetuning them as binary classifiers on various text datasets for AI-generated text detection. While these methods perform well on their respective test datasets, their effectiveness may be limited when applied to texts generated by newly emerging models. Likewise, as the capabilities of large language models continue to advance, their effectiveness remains a question. \n\nWatermarking, as an alternative method for AI-generated text identification, is generally more effective, versatile, and interpretable compared to direct detection."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[273228395 | Xu et al. | 2024 | Citations: 0]",
                "snippets": "As Large Language Models (LLMs) become increasingly sophisticated, they raise significant security concerns, including the creation of fake news and academic misuse. Most detectors for identifying model-generated text are limited by their reliance on variance in perplexity and burstiness, and they require substantial computational resources.\n\nNumerous open-source and commercial tools have been developed to detect model-generated text, such as GPT2-Detector [29], RoBERTa-QA [31], DetectGPT [30], GPTZero [32], Writer [33], and others. However, there are three significant limitations to these methods. First, their probability predictions cannot provide conclusive certainty and should only be regarded as a reference. Second, most current approaches rely on methods that differentiate human text from modelgenerated text based on variations in perplexity and burstiness. Third, methods like DetectGPT and RoBERTa-QA, which require multiple text-generation process and model retraining, consume substantial computational resources and time when detecting model-generated text. As LLMs advance, the generated text increasingly resembles human writing, making these methods based on variance in perplexity between human and model-generated text less applicable. Therefore, embedding watermarks in text generated by LLMs presents a faster, more stable, and reliable method of verification.\n\nRecent advancements in watermarking techniques for language models have been crucial in ensuring text authenticity and model protection. Abdelnabi and Fritz [20] pioneered the Adversarial Watermarking Transformer (AWT), marking a significant step in embedding binary messages into text and demonstrating resilience against multiple attacks. He et al. [21] developed CATER, a conditional watermarking framework that secures text generation APIs against imitation attacks while preserving word distribution integrity. Zhao, Li, and Wang [22] introduced Distillation-Resistant Watermarking (DRW), a method effective in protecting NLP models from distillation theft and maintaining accuracy. Kirchenbauer et al. [23] proposed a framework for embedding watermarks in language models that are detectable with minimal impact on text quality. In a similar vein, Zhao, Wang, and Li's GINSEW technique [24] innovatively embeds secret signals in the probability vector of decoding steps, enhancing IP infringement detection. Peng et al. [25] developed EmbMarker for EaaS, embedding watermarks in trigger words for effective copyright verification. Yoo et al. [26] explored robust multi-bit watermarking, focusing on error resistance and enhanced payload. Lastly, Li et al. [27] introduced a novel strategy for embedding watermarks in the weight quantization process of large language models, ensuring model security without compromising operational modes.\n\nThe advancement in language models has prompted the development of various detection tools for identifying model-generated text. Gehrmann [28] developed GLTR, a tool to assist non-experts in detecting texts generated by models. It uses statistical methods to identify generation artifacts and has been shown to improve human detection rates of fake text significantly. GPT2-Detector developed by OpenAI [29] is a RoBERTa base fine-tuned with outputs from the 1.5Bparameter GPT-2 model. Its primary function is to predict if text was generated by a GPT-2 model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "As Large Language Models (LLMs) become increasingly sophisticated, they raise significant security concerns, including the creation of fake news and academic misuse. Most detectors for identifying model-generated text are limited by their reliance on variance in perplexity and burstiness, and they require substantial computational resources.\n\nNumerous open-source and commercial tools have been developed to detect model-generated text, such as GPT2-Detector [29], RoBERTa-QA [31], DetectGPT [30], GPTZero [32], Writer [33], and others. However, there are three significant limitations to these methods. First, their probability predictions cannot provide conclusive certainty and should only be regarded as a reference. Second, most current approaches rely on methods that differentiate human text from modelgenerated text based on variations in perplexity and burstiness. Third, methods like DetectGPT and RoBERTa-QA, which require multiple text-generation process and model retraining, consume substantial computational resources and time when detecting model-generated text. As LLMs advance, the generated text increasingly resembles human writing, making these methods based on variance in perplexity between human and model-generated text less applicable. Therefore, embedding watermarks in text generated by LLMs presents a faster, more stable, and reliable method of verification.\n\nRecent advancements in watermarking techniques for language models have been crucial in ensuring text authenticity and model protection. Abdelnabi and Fritz [20] pioneered the Adversarial Watermarking Transformer (AWT), marking a significant step in embedding binary messages into text and demonstrating resilience against multiple attacks. He et al. [21] developed CATER, a conditional watermarking framework that secures text generation APIs against imitation attacks while preserving word distribution integrity. Zhao, Li, and Wang [22] introduced Distillation-Resistant Watermarking (DRW), a method effective in protecting NLP models from distillation theft and maintaining accuracy. Kirchenbauer et al. [23] proposed a framework for embedding watermarks in language models that are detectable with minimal impact on text quality. In a similar vein, Zhao, Wang, and Li's GINSEW technique [24] innovatively embeds secret signals in the probability vector of decoding steps, enhancing IP infringement detection. Peng et al. [25] developed EmbMarker for EaaS, embedding watermarks in trigger words for effective copyright verification. Yoo et al. [26] explored robust multi-bit watermarking, focusing on error resistance and enhanced payload. Lastly, Li et al. [27] introduced a novel strategy for embedding watermarks in the weight quantization process of large language models, ensuring model security without compromising operational modes.\n\nThe advancement in language models has prompted the development of various detection tools for identifying model-generated text. Gehrmann [28] developed GLTR, a tool to assist non-experts in detecting texts generated by models. It uses statistical methods to identify generation artifacts and has been shown to improve human detection rates of fake text significantly. GPT2-Detector developed by OpenAI [29] is a RoBERTa base fine-tuned with outputs from the 1.5Bparameter GPT-2 model. Its primary function is to predict if text was generated by a GPT-2 model.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[273482129 | Tao et al. | 2024 | Citations: 2]",
                "snippets": "Existing detection methods are categorized into metricbased and model-based approaches. Metric-based methods quantify linguistic features, such as stylometric analysis and perplexity scoring, to determine text origin. Gehrmann et al. [22] introduced GLTR, which uses metrics like word probability and entropy for detection, while Solaiman et al. [23] proposed a zero-shot detection method leveraging pre-trained models like GPT-2 or GROVER. However, as LLMs advance, these methods have become less effective. To address these limitations, more advanced and sophisticated detection techniques have emerged. Mitchell et al. (Mitchell et al., 2023) presented DetectGPT, a zero-shot method utilizing probability curvature analysis, and Tian et al. [25] developed the Multiscale Positive-Unlabeled (MPU) framework to enhance detection across varying text lengths. While metricbased approaches rely on linguistic analysis, model-based methods use advanced machine learning to classify text as human or AI-generated. These often involve fine-tuning language models or developing specialized architectures. For instance, Guo et al. [26] fine-tuned a RoBERTa model to detect ChatGPT-generated text, while Wang et al. [27] introduced SeqXGPT, a sentence-level method using logprobability lists and self-attention networks for enhanced detection. Liu et al. (Liu et al., 2023) proposed COCO, which improves detection in low-resource settings by combining an entity coherence graph with contrastive learning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[256274849 | Mitchell et al. | 2023 | Citations: 627]": "The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.",
                    "[264406273 | Liu et al. | 2023 | Citations: 25]": "Machine-Generated Text (MGT) detection, a 001 task that discriminates MGT from Human-002 Written Text (HWT), plays a crucial role in 003 preventing misuse of text generative models, 004 which excel in mimicking human writing style 005 recently. Latest proposed detectors usually take 006 coarse text sequences as input and fine-tune 007 pretrained models with standard cross-entropy 008 loss. However, these methods fail to consider 009 the linguistic structure of texts. Moreover, they 010 lack the ability to handle the low-resource prob-011 lem which could often happen in practice con-012 sidering the enormous amount of textual data 013 online. In this paper, we present a co herence-014 based co ntrastive learning model named C O C O 015 to detect the possible MGT under low-resource 016 scenario. To exploit the linguistic feature, we 017 encode coherence information in form of graph 018 into text representation. To tackle the chal-019 lenges of low data resource, we employ a con-020 trastive learning framework and propose an im-021 proved contrastive loss for preventing perfor-022 mance degradation brought by simple samples. 023 The experiment results on two public datasets 024 and two self-constructed datasets prove our ap-025 proach outperforms the state-of-art methods 026 significantly. 027"
                },
                "metadata": [
                    {
                        "section_title": "AI-generated Content Detection",
                        "pdf_hash": "",
                        "start": 438,
                        "end": 1901,
                        "sentence_offsets": [
                            {
                                "start": 438,
                                "end": 525
                            },
                            {
                                "start": 526,
                                "end": 655
                            },
                            {
                                "start": 656,
                                "end": 878
                            },
                            {
                                "start": 879,
                                "end": 946
                            },
                            {
                                "start": 947,
                                "end": 1043
                            },
                            {
                                "start": 1044,
                                "end": 1277
                            },
                            {
                                "start": 1278,
                                "end": 1428
                            },
                            {
                                "start": 1429,
                                "end": 1517
                            },
                            {
                                "start": 1518,
                                "end": 1754
                            },
                            {
                                "start": 1755,
                                "end": 1901
                            }
                        ],
                        "ref_mentions": [
                            "256274849",
                            "264406273"
                        ],
                        "quote": "Existing detection methods are categorized into metricbased and model-based approaches. Metric-based methods quantify linguistic features, such as stylometric analysis and perplexity scoring, to determine text origin. Gehrmann et al. [22] introduced GLTR, which uses metrics like word probability and entropy for detection, while Solaiman et al. [23] proposed a zero-shot detection method leveraging pre-trained models like GPT-2 or GROVER. However, as LLMs advance, these methods have become less effective. To address these limitations, more advanced and sophisticated detection techniques have emerged. Mitchell et al. (Mitchell et al., 2023) presented DetectGPT, a zero-shot method utilizing probability curvature analysis, and Tian et al. [25] developed the Multiscale Positive-Unlabeled (MPU) framework to enhance detection across varying text lengths. While metricbased approaches rely on linguistic analysis, model-based methods use advanced machine learning to classify text as human or AI-generated. These often involve fine-tuning language models or developing specialized architectures. For instance, Guo et al. [26] fine-tuned a RoBERTa model to detect ChatGPT-generated text, while Wang et al. [27] introduced SeqXGPT, a sentence-level method using logprobability lists and self-attention networks for enhanced detection. Liu et al. (Liu et al., 2023) proposed COCO, which improves detection in low-resource settings by combining an entity coherence graph with contrastive learning."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[273547059 | Dathathri et al. | 2024 | Citations: 74]",
                "snippets": "Multiple strategies have emerged to address this problem. One is a retrieval-based approach, which involves keeping a growing record of all generated texts and checking against it for matches 12. This requires scale and coordination, and raises privacy concerns as it requires accessing and storing all LLM interactions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[235694265 | Clark et al. | 2021 | Citations: 414]": "Human evaluations are typically considered the gold standard in natural language generation, but as models\u2019 fluency improves, how well can evaluators detect and judge machine-generated text? We run a study assessing non-experts\u2019 ability to distinguish between human- and machine-authored text (GPT2 and GPT3) in three domains (stories, news articles, and recipes). We find that, without training, evaluators distinguished between GPT3- and human-authored text at random chance level. We explore three approaches for quickly training evaluators to better identify GPT3-authored text (detailed instructions, annotated examples, and paired examples) and find that while evaluators\u2019 accuracy improved up to 55%, it did not significantly improve across the three domains. Given the inconsistent results across text domains and the often contradictory reasons evaluators gave for their judgments, we examine the role untrained human evaluations play in NLG evaluation and provide recommendations to NLG researchers for improving human evaluations of text generated from state-of-the-art models.",
                    "[249674779 | Jakesch et al. | 2022 | Citations: 188]": "Significance Human communication is now rife with language generated by AI. Every day, across the web, chat, email, and social media, AI systems produce billions of messages that could be perceived as created by humans. In this work, we analyze human judgments of self-presentations written by humans and generated by AI systems. We find that people cannot detect AI-generated self-presentations as their judgment is misguided by intuitive but flawed heuristics for AI-generated language. We demonstrate that AI systems can exploit these heuristics to produce text perceived as \u201cmore human than human.\u201d Our results raise the question of how humanity will adapt to AI-generated text, illustrating the need to reorient the development of AI language systems to ensure that they support rather than undermine human cognition."
                },
                "metadata": [
                    {
                        "section_title": "body",
                        "pdf_hash": "",
                        "start": 1760,
                        "end": 2080,
                        "sentence_offsets": [
                            {
                                "start": 1751,
                                "end": 1917
                            },
                            {
                                "start": 1918,
                                "end": 2138
                            }
                        ],
                        "ref_mentions": [
                            "221562332",
                            "235694265",
                            "249674779"
                        ],
                        "quote": "Multiple strategies have emerged to address this problem. One is a retrieval-based approach, which involves keeping a growing record of all generated texts and checking against it for matches 12. This requires scale and coordination, and raises privacy concerns as it requires accessing and storing all LLM interactions."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[273849981 | Lau et al. | 2024 | Citations: 1]",
                "snippets": "Currently, there are 2 major streams of LLM-generated text detectors: (i) zero-shot classifiers [25,33], which identify LLM-generated text based on the pattern and characteristics of the input, and (ii) watermark detectors [17], which rely on detecting the presence of watermarks which are imprinted into the text during the generation process (Kirchenbauer et al., 2023), and are effective in the cases where the watermarks have been added by the LLM. The detectors then examine the input, classifying it as LLM-generated if the level of watermarking exceeds a set threshold, or as human-generated otherwise.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[256194179 | Kirchenbauer et al. | 2023 | Citations: 507]": "Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of\"green\"tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 914,
                        "end": 1500,
                        "sentence_offsets": [
                            {
                                "start": 914,
                                "end": 1343
                            },
                            {
                                "start": 1344,
                                "end": 1500
                            }
                        ],
                        "ref_mentions": [
                            "256194179"
                        ],
                        "quote": "Currently, there are 2 major streams of LLM-generated text detectors: (i) zero-shot classifiers [25,33], which identify LLM-generated text based on the pattern and characteristics of the input, and (ii) watermark detectors [17], which rely on detecting the presence of watermarks which are imprinted into the text during the generation process (Kirchenbauer et al., 2023), and are effective in the cases where the watermarks have been added by the LLM. The detectors then examine the input, classifying it as LLM-generated if the level of watermarking exceeds a set threshold, or as human-generated otherwise."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[273901392 | Nguyen-Son et al. | 2024 | Citations: 2]",
                "snippets": "Detection of text generated by large language models uses three main techniques. Firstly, supervised learning methods (Solaiman et al., 2019;(Wang et al., 2023)(Hu et al., 2023)(Wu et al., 2023) train classifiers on datasets of original and generated text, though this requires large volumes of training data. The zero-shot approach (Bhattacharjee and Liu, 2023;(Mitchell et al., 2023) eliminates the need for training but is sensitive to out-of-distribution text. Recent research explores watermarking methodologies (Kirchenbauer et al., 2023) to force models to produce predefined words, aiding detection, but this requires modifying the models, which is impractical for proprietary models like ChatGPT.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[256274849 | Mitchell et al. | 2023 | Citations: 627]": "The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.",
                    "[256194179 | Kirchenbauer et al. | 2023 | Citations: 507]": "Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of\"green\"tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.",
                    "[258865367 | Wu et al. | 2023 | Citations: 39]": "Generated texts from large language models (LLMs) are remarkably close to high-quality human-authored text, raising concerns about their potential misuse in spreading false information and academic misconduct. Consequently, there is an urgent need for a highly practical detection tool capable of accurately identifying the source of a given text. However, existing detection tools typically rely on access to LLMs and can only differentiate between machine-generated and human-authored text, failing to meet the requirements of fine-grained tracing, intermediary judgment, and rapid detection. Therefore, we propose LLMDet, a model-specific, secure, efficient, and extendable detection tool, that can source text from specific LLMs, such as GPT-2, OPT, LLaMA, and others. In LLMDet, we record the next-token probabilities of salient n-grams as features to calculate proxy perplexity for each LLM. By jointly analyzing the proxy perplexities of LLMs, we can determine the source of the generated text. Experimental results show that LLMDet yields impressive detection performance while ensuring speed and security, achieving 98.54% precision and x3.5 faster for recognizing human-authored text. Additionally, LLMDet can effortlessly extend its detection capabilities to a new open-source model. We will provide an open-source tool at https://github.com/TrustedLLM/LLMDet.",
                    "[259501842 | Hu et al. | 2023 | Citations: 127]": "Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a robust AI-text detector via adversarial learning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic content to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5-Turbo.",
                    "[264128397 | Wang et al. | 2023 | Citations: 57]": "Widely applied large language models (LLMs) can generate human-like content, raising concerns about the abuse of LLMs. Therefore, it is important to build strong AI-generated text (AIGT) detectors. Current works only consider document-level AIGT detection, therefore, in this paper, we first introduce a sentence-level detection challenge by synthesizing a dataset that contains documents that are polished with LLMs, that is, the documents contain sentences written by humans and sentences modified by LLMs. Then we propose \\textbf{Seq}uence \\textbf{X} (Check) \\textbf{GPT}, a novel method that utilizes log probability lists from white-box LLMs as features for sentence-level AIGT detection. These features are composed like \\textit{waves} in speech processing and cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution and self-attention networks. We test it in both sentence and document-level detection challenges. Experimental results show that previous methods struggle in solving sentence-level AIGT detection, while our method not only significantly surpasses baseline methods in both sentence and document-level detection challenges but also exhibits strong generalization capabilities."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 405,
                        "end": 1106,
                        "sentence_offsets": [
                            {
                                "start": 405,
                                "end": 485
                            },
                            {
                                "start": 486,
                                "end": 711
                            },
                            {
                                "start": 712,
                                "end": 865
                            },
                            {
                                "start": 866,
                                "end": 1106
                            }
                        ],
                        "ref_mentions": [
                            "264128397",
                            "259501842",
                            "258865367",
                            "256274849",
                            "256194179"
                        ],
                        "quote": "Detection of text generated by large language models uses three main techniques. Firstly, supervised learning methods (Solaiman et al., 2019;(Wang et al., 2023)(Hu et al., 2023)(Wu et al., 2023) train classifiers on datasets of original and generated text, though this requires large volumes of training data. The zero-shot approach (Bhattacharjee and Liu, 2023;(Mitchell et al., 2023) eliminates the need for training but is sensitive to out-of-distribution text. Recent research explores watermarking methodologies (Kirchenbauer et al., 2023) to force models to produce predefined words, aiding detection, but this requires modifying the models, which is impractical for proprietary models like ChatGPT."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[273901404 | Guo et al. | 2024 | Citations: 2]",
                "snippets": "Watermarking enables people to determine whether the text is generated by a specific model. It injects a unique signature based on the \"green-red\" list that can be tracked during detection, where the words in green lists are encouraged to be generated. Recent researchers propose to fix the green/red lists or increase the proportion of green tokens to defend against paraphrasing attacks. However, these methods cause degradation of text quality due to semantic disparities between the watermarked text and the unwatermarked text.\n\nCurrent watermark methods mainly inject the signature according to \"green-red list\" (Ren et al., 2023): they divide vocabulary into green/red lists, regard words in green lists as the unique signature, and encourage to generate green tokens, which is easy to be recognized. The methods can be divided into two categories: (1) Token sampling biasing-based watermark forces LLMs to select only green tokens during generation. EXP-Edit (Kuditipudi et al., 2023) intervenes in the sampling process of each token. However, forcing LLMs to sample green tokens restricts the semantic richness of LLM-generated text, thus undermining its text quality. (2) To improve the generation quality, researchers further propose token probability biasing-based watermark, which enriches the semantics of watermarked texts by introducing a bias to the probability distribution to softly encourage generating green tokens instead of restricting to select green tokens. Takezawa et al. (2023) proposed NS-Mark to constrain the frequency of biasing. Wu et al. (2023) introduced DiPMark to approximate the biased probability distribution to the original one. These methods mitigate the impact of biasing on text quality and ensure superior text quality.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Watermarking enables people to determine whether the text is generated by a specific model. It injects a unique signature based on the \"green-red\" list that can be tracked during detection, where the words in green lists are encouraged to be generated. Recent researchers propose to fix the green/red lists or increase the proportion of green tokens to defend against paraphrasing attacks. However, these methods cause degradation of text quality due to semantic disparities between the watermarked text and the unwatermarked text.\n\nCurrent watermark methods mainly inject the signature according to \"green-red list\" (Ren et al., 2023): they divide vocabulary into green/red lists, regard words in green lists as the unique signature, and encourage to generate green tokens, which is easy to be recognized. The methods can be divided into two categories: (1) Token sampling biasing-based watermark forces LLMs to select only green tokens during generation. EXP-Edit (Kuditipudi et al., 2023) intervenes in the sampling process of each token. However, forcing LLMs to sample green tokens restricts the semantic richness of LLM-generated text, thus undermining its text quality. (2) To improve the generation quality, researchers further propose token probability biasing-based watermark, which enriches the semantics of watermarked texts by introducing a bias to the probability distribution to softly encourage generating green tokens instead of restricting to select green tokens. Takezawa et al. (2023) proposed NS-Mark to constrain the frequency of biasing. Wu et al. (2023) introduced DiPMark to approximate the biased probability distribution to the original one. These methods mitigate the impact of biasing on text quality and ensure superior text quality.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[274581321 | Tufts et al. | 2024 | Citations: 2]",
                "snippets": "Many methods have been released recently that claim to have a strong ability to detect the difference between AI-generated and human-generated texts. These detectors primarily fall into three categories: trained detectors, zero-shot detectors, and watermarking techniques (Yang et al., 2023)(Ghosal et al., 2023)Tang et al., 2023). Trained detectors utilize datasets of human and AI-generated texts and train a binary classification model to detect the source of a text (Zellers et al., 2019)(Hovy, 2016)(Hu et al., 2023)Tian and Cui, 2023;(Verma et al., 2023). Zero-shot detection utilizes a language model's inherent traits to identify text it generates, without explicit training for detection tasks other than calibrating a threshold for detection in some cases (Gehrmann et al., 2019;Mitchell et al., 2023;Bao et al., 2024;Yang et al., 2023a;Venkatraman et al., 2024).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258865787 | Verma et al. | 2023 | Citations: 60]": "We introduce Ghostbuster, a state-of-the-art system for detecting AI-generated text.Our method works by passing documents through a series of weaker language models, running a structured search over possible combinations of their features, and then training a classifier on the selected features to predict whether documents are AI-generated.Crucially, Ghostbuster does not require access to token probabilities from the target model, making it useful for detecting text generated by black-box or unknown models.In conjunction with our model, we release three new datasets of human- and AI-generated text as detection benchmarks in the domains of student essays, creative writing, and news articles. We compare Ghostbuster to several existing detectors, including DetectGPT and GPTZero, as well as a new RoBERTa baseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is 5.9 F1 higher than the best preexisting model. It also outperforms all previous approaches in generalization across writing domains (+7.5 F1), prompting strategies (+2.1 F1), and language models (+4.4 F1). We also analyze our system\u2019s robustness to a variety of perturbations and paraphrasing attacks, and evaluate its performance on documents by non-native English speakers.",
                    "[168169824 | Zellers et al. | 2019 | Citations: 1030]": "Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. \nModern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. \nDeveloping robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.",
                    "[259501842 | Hu et al. | 2023 | Citations: 127]": "Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a robust AI-text detector via adversarial learning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic content to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5-Turbo.",
                    "[264439054 | Ghosal et al. | 2023 | Citations: 37]": "Large Language Models (LLMs) have revolutionized the domain of natural language processing (NLP) with remarkable capabilities of generating human-like text responses. However, despite these advancements, several works in the existing literature have raised serious concerns about the potential misuse of LLMs such as spreading misinformation, generating fake news, plagiarism in academia, and contaminating the web. To address these concerns, a consensus among the research community is to develop algorithmic solutions to detect AI-generated text. The basic idea is that whenever we can tell if the given text is either written by a human or an AI, we can utilize this information to address the above-mentioned concerns. To that end, a plethora of detection frameworks have been proposed, highlighting the possibilities of AI-generated text detection. But in parallel to the development of detection frameworks, researchers have also concentrated on designing strategies to elude detection, i.e., focusing on the impossibilities of AI-generated text detection. This is a crucial step in order to make sure the detection frameworks are robust enough and it is not too easy to fool a detector. Despite the huge interest and the flurry of research in this domain, the community currently lacks a comprehensive analysis of recent developments. In this survey, we aim to provide a concise categorization and overview of current work encompassing both the prospects and the limitations of AI-generated text detection. To enrich the collective knowledge, we engage in an exhaustive discussion on critical and challenging open questions related to ongoing research on AI-generated text detection.",
                    "[264439179 | Yang et al. | 2023 | Citations: 55]": "The burgeoning capabilities of advanced large language models (LLMs) such as ChatGPT have led to an increase in synthetic content generation with implications across a variety of sectors, including media, cybersecurity, public discourse, and education. As such, the ability to detect LLMs-generated content has become of paramount importance. We aim to provide a detailed overview of existing detection strategies and benchmarks, scrutinizing their differences and identifying key challenges and prospects in the field, advocating for more adaptable and robust models to enhance detection accuracy. We also posit the necessity for a multi-faceted approach to defend against various attacks to counter the rapidly advancing capabilities of LLMs. To the best of our knowledge, this work is the first comprehensive survey on the detection in the era of LLMs. We hope it will provide a broad understanding of the current landscape of LLMs-generated content detection, offering a guiding reference for researchers and practitioners striving to uphold the integrity of digital information in an era increasingly dominated by synthetic content. The relevant papers are summarized and will be consistently updated at https://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection.git.",
                    "[5315990 | Hovy | 2016 | Citations: 52]": "Online reviews are a growing market, but it is struggling with fake reviews. They undermine both the value of reviews to the user, and their trust in the review sites. However, fake positive reviews can boost a business, and so a small industry producing fake reviews has developed. The two sides are facing an arms race that involves more and more natural language processing (NLP). So far, NLP has been used mostly for detection, and works well on human-generated reviews. But what happens if NLP techniques are used to generate fake reviews as well? We investigate the question in an adversarial setup, by assessing the detectability of different fake-review generation strategies. We use generative models to produce reviews based on meta-information, and evaluate their effectiveness against deceptiondetection models and human judges. We find that meta-information helps detection, but that NLP-generated reviews conditioned on such information are also much harder to detect than conventional ones."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1034,
                        "end": 1905,
                        "sentence_offsets": [
                            {
                                "start": 1034,
                                "end": 1183
                            },
                            {
                                "start": 1184,
                                "end": 1365
                            },
                            {
                                "start": 1366,
                                "end": 1593
                            },
                            {
                                "start": 1594,
                                "end": 1905
                            }
                        ],
                        "ref_mentions": [
                            "264439179",
                            "264439054",
                            "168169824",
                            "5315990",
                            "259501842",
                            "258865787"
                        ],
                        "quote": "Many methods have been released recently that claim to have a strong ability to detect the difference between AI-generated and human-generated texts. These detectors primarily fall into three categories: trained detectors, zero-shot detectors, and watermarking techniques (Yang et al., 2023)(Ghosal et al., 2023)Tang et al., 2023). Trained detectors utilize datasets of human and AI-generated texts and train a binary classification model to detect the source of a text (Zellers et al., 2019)(Hovy, 2016)(Hu et al., 2023)Tian and Cui, 2023;(Verma et al., 2023). Zero-shot detection utilizes a language model's inherent traits to identify text it generates, without explicit training for detection tasks other than calibrating a threshold for detection in some cases (Gehrmann et al., 2019;Mitchell et al., 2023;Bao et al., 2024;Yang et al., 2023a;Venkatraman et al., 2024)."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[275342724 | Miralles-Gonz'alez et al. | 2025 | Citations: 0]",
                "snippets": "There is a large and growing body of work focused on detection methods for AI-generated text. We review some of the main methods that appear in the literature, breaking them into four categories. The first category includes watermarking methods, which add hidden patterns to AI-generated text to make it identifiable. Zero-shot detectors rely on large language models (LLMs) pre-trained for next-token prediction to identify AI-generated content without additional training, using the next-token distributions of the text to generate statistics that separate human-and machine-generated examples. Other approaches involve fine-tuning language models and training them specifically for the detection classification task. Finally, LLMs have also been prompted to detect whether a text is AI-generated, asking them to complete the given prompt with the answer.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Detection Methods",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 857,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 93
                            },
                            {
                                "start": 94,
                                "end": 195
                            },
                            {
                                "start": 196,
                                "end": 317
                            },
                            {
                                "start": 318,
                                "end": 596
                            },
                            {
                                "start": 597,
                                "end": 719
                            },
                            {
                                "start": 720,
                                "end": 857
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "There is a large and growing body of work focused on detection methods for AI-generated text. We review some of the main methods that appear in the literature, breaking them into four categories. The first category includes watermarking methods, which add hidden patterns to AI-generated text to make it identifiable. Zero-shot detectors rely on large language models (LLMs) pre-trained for next-token prediction to identify AI-generated content without additional training, using the next-token distributions of the text to generate statistics that separate human-and machine-generated examples. Other approaches involve fine-tuning language models and training them specifically for the detection classification task. Finally, LLMs have also been prompted to detect whether a text is AI-generated, asking them to complete the given prompt with the answer."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[276116307 | Dabiriaghdam et al. | 2025 | Citations: 1]",
                "snippets": "The rapid proliferation of large language models (LLMs) has created an urgent need for reliable methods to detect whether a text is generated by such models. In this paper, we propose SimMark, a posthoc watermarking algorithm that makes LLMs' outputs traceable without requiring access to the model's internal logits, enabling compatibility with a wide range of LLMs, including API-only models. By leveraging the similarity of semantic sentence embeddings and rejection sampling to impose detectable statistical patterns imperceptible to humans, and employing a soft counting mechanism, SimMark achieves robustness against paraphrasing attacks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 644,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The rapid proliferation of large language models (LLMs) has created an urgent need for reliable methods to detect whether a text is generated by such models. In this paper, we propose SimMark, a posthoc watermarking algorithm that makes LLMs' outputs traceable without requiring access to the model's internal logits, enabling compatibility with a wide range of LLMs, including API-only models. By leveraging the similarity of semantic sentence embeddings and rejection sampling to impose detectable statistical patterns imperceptible to humans, and employing a soft counting mechanism, SimMark achieves robustness against paraphrasing attacks."
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[276574900 | Trivedi et al. | 2025 | Citations: 0]",
                "snippets": "To address these risks, various algorithms have been developed to detect AI-generated text, which include watermarking techniques [3], tools like GPTZero [4], DetectGPT [5], and OpenAI's text classifier [6].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 313,
                        "end": 520,
                        "sentence_offsets": [
                            {
                                "start": 313,
                                "end": 520
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To address these risks, various algorithms have been developed to detect AI-generated text, which include watermarking techniques [3], tools like GPTZero [4], DetectGPT [5], and OpenAI's text classifier [6]."
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[276773353 | Bitton et al. | 2025 | Citations: 0]",
                "snippets": "Classical approaches applied feature extraction on the texts in order to quantify stylistic or syntactic features such as perplexity, burstiness, and word density. These features are then fed into machine learning algorithms such as logistic regression or SVM (which provide an advantage of explainability), or into \"deep\" neural networks such as LSTMs. More modern approaches use pre-trained language models such as BERT, and fine-tune them for the specific classification task. This eliminates the need to manually define a list of features, and overall presents a much higher accuracy compared to feature-based methods, mainly because this approach is able to capture more general elements and nuances of the writing style, and is less dependent on a specific domain or topic. [20](Yadagiri et al., 2024)(Dathathri et al., 2024)(Kwon et al., 2025) Approaches that rely solely on capturing the differences between AI and human texts operate under the assumption that such differences exist; however, in reality, the differences between machine and human text may diminish over time as these systems evolve. A \"watermarking\" approach has recently emerged in an attempt to overcome these shortcomings. \"Watermarks\" can be added to the text during the generation phase (generative watermarking) or after it (edit-based watermarking). (Dathathri et al., 2024) However, this approach has its own limitations, such as lower effectiveness on factual responses (as there are fewer options to control the output), and higher sensitivity to small edits of the watermarked text.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[273547059 | Dathathri et al. | 2024 | Citations: 74]": "Large language models (LLMs) have enabled the generation of high-quality synthetic text, often indistinguishable from human-written content, at a scale that can markedly affect the nature of the information ecosystem1\u20133. Watermarking can help identify synthetic text and limit accidental or deliberate misuse4, but has not been adopted in production systems owing to stringent quality, detectability and computational efficiency requirements. Here we describe SynthID-Text, a production-ready text watermarking scheme that preserves text quality and enables high detection accuracy, with minimal latency overhead. SynthID-Text does not affect LLM training and modifies only the sampling procedure; watermark detection is computationally efficient, without using the underlying LLM. To enable watermarking at scale, we develop an algorithm integrating watermarking with speculative sampling, an efficiency technique frequently used in production systems5. Evaluations across multiple LLMs empirically show that SynthID-Text provides improved detectability over comparable methods, and standard benchmarks and human side-by-side ratings indicate no change in LLM capabilities. To demonstrate the feasibility of watermarking in large-scale-production systems, we conducted a live experiment that assessed feedback from nearly 20\u2009million Gemini6 responses, again confirming the preservation of text quality. We hope that the availability of SynthID-Text7 will facilitate further development of watermarking and responsible use of LLM systems.",
                    "[276142840 | Kwon et al. | 2025 | Citations: 5]": "This paper presents a pioneering and comprehensive analysis of fake text, a pressing issue in the digital age, by categorizing it into two main types: Misinformation and LM-generated texts. It is the first study to systematically dissect and examine the intricate challenges and nuances in distinguishing between genuine and artificial text. Through a meticulous review of various methodologies and technologies in fake text detection, the paper provides an in-depth evaluation of their effectiveness across diverse scenarios. Furthermore, this research delves into the significant societal impacts of both misinformation and LM-generated texts, underlining the urgent need for precise and effective detection mechanisms in our increasingly information-saturated world. This extensive survey not only offers a unique perspective on the current landscape of fake text detection, but also paves the way for future research, highlighting critical areas where further innovation and exploration are essential."
                },
                "metadata": [
                    {
                        "section_title": "AI Forensics",
                        "pdf_hash": "",
                        "start": 688,
                        "end": 2180,
                        "sentence_offsets": [
                            {
                                "start": 688,
                                "end": 851
                            },
                            {
                                "start": 852,
                                "end": 1041
                            },
                            {
                                "start": 1042,
                                "end": 1167
                            },
                            {
                                "start": 1168,
                                "end": 1467
                            },
                            {
                                "start": 1468,
                                "end": 1739
                            },
                            {
                                "start": 1740,
                                "end": 1832
                            },
                            {
                                "start": 1833,
                                "end": 1963
                            },
                            {
                                "start": 1964,
                                "end": 2182
                            }
                        ],
                        "ref_mentions": [
                            "275994170",
                            "273547059",
                            "276142840",
                            "273547059"
                        ],
                        "quote": "Classical approaches applied feature extraction on the texts in order to quantify stylistic or syntactic features such as perplexity, burstiness, and word density. These features are then fed into machine learning algorithms such as logistic regression or SVM (which provide an advantage of explainability), or into \"deep\" neural networks such as LSTMs. More modern approaches use pre-trained language models such as BERT, and fine-tune them for the specific classification task. This eliminates the need to manually define a list of features, and overall presents a much higher accuracy compared to feature-based methods, mainly because this approach is able to capture more general elements and nuances of the writing style, and is less dependent on a specific domain or topic. [20](Yadagiri et al., 2024)(Dathathri et al., 2024)(Kwon et al., 2025) Approaches that rely solely on capturing the differences between AI and human texts operate under the assumption that such differences exist; however, in reality, the differences between machine and human text may diminish over time as these systems evolve. A \"watermarking\" approach has recently emerged in an attempt to overcome these shortcomings. \"Watermarks\" can be added to the text during the generation phase (generative watermarking) or after it (edit-based watermarking). (Dathathri et al., 2024) However, this approach has its own limitations, such as lower effectiveness on factual responses (as there are fewer options to control the output), and higher sensitivity to small edits of the watermarked text."
                    }
                ]
            },
            {
                "idx": 39,
                "key": "[277595847 | Cao | 2025 | Citations: 0]",
                "snippets": "One popular approach leverages an external LLM to classify whether a piece of text is AI-generated or not [12]. With carefully crafted prompts, these zero-shot methods can achieve moderate accuracy quickly. However, sensitivity to prompt design and the adversarial gap between the LLM detector and the text generator are common issues [22].\n\n2) Linguistic and statistical signatures: Traditional stylometric features (e.g., function words, syntax complexity, average phrase length) have long been used in authorship attribution [23], [24]. More modern detection focuses on computing perplexity or log-likelihood using reference language models, observing that LLM-generated text tends to show distinctive probability distributions. Additionally, specialized white-box methods can measure rank ordering of tokens if the generating model is partially known [25].\n\n3) Supervised classification (training-based): Labeled corpora of AI versus human text enable fine-tuning of large pre-trained transformers like RoBERTa or T5 to discriminate synthetic text [26], [27]. Researchers improve robustness with adversarial training sets that contain paraphrased or AIgenerated passages shifted in style. Tools like GPTZero [28] and RADAR [29] exemplify advanced supervised detectors. However, assembling high-quality, representative training data remains a challenge, especially as new generator architectures emerge frequently.\n\n4) Watermarking for AI text: Cooperative watermarking modifies text generation at token selection time, embedding an imperceptible pattern in the distribution of words or punctuation [30], [31]. A verifier can detect such patterns after the fact. While promising for major industrial LLMs that adopt the standard, watermarking fails if malicious or open-source models do not embed it, or if paraphrasing disrupts the signal [32], [33].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[256274849 | Mitchell et al. | 2023 | Citations: 627]": "The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.",
                    "[256194179 | Kirchenbauer et al. | 2023 | Citations: 507]": "Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of\"green\"tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.",
                    "[258960584 | Tian et al. | 2023 | Citations: 47]": "Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may impact the authenticity of texts. Previous works proposed methods to detect these AI-generated texts, including simple ML classifiers, pretrained-model-based zero-shot methods, and finetuned language classification models. However, mainstream detectors always fail on short texts, like SMSes, Tweets, and reviews. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the difficulty of short-text detection without sacrificing long-texts. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase AI text detection as a partial Positive-Unlabeled (PU) problem by regarding these short machine texts as partially ``unlabeled\". Then in this PU context, we propose the length-sensitive Multiscale PU Loss, where a recurrent model in abstraction is used to estimate positive priors of scale-variant corpora. Additionally, we introduce a Text Multiscaling module to enrich training corpora. Experiments show that our MPU method augments detection performance on long AI-generated texts, and significantly improves short-text detection of language model detectors. Language Models trained with MPU could outcompete existing detectors on various short-text and long-text detection benchmarks. The codes are available at https://github.com/mindspore-lab/mindone/tree/master/examples/detect_chatgpt and https://github.com/YuchuanTian/AIGC_text_detector.",
                    "[259501842 | Hu et al. | 2023 | Citations: 127]": "Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a robust AI-text detector via adversarial learning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic content to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5-Turbo.",
                    "[260379022 | Bhattacharjee et al. | 2023 | Citations: 62]": "Large language models (LLMs) such as ChatGPT are increasingly being used for various use cases, including text content generation at scale. Although detection methods for such AI-generated text exist already, we investigate ChatGPT's performance as a detector on such AI-generated text, inspired by works that use ChatGPT as a data labeler or annotator. We evaluate the zeroshot performance of ChatGPT in the task of human-written vs. AI-generated text detection, and perform experiments on publicly available datasets. We empirically investigate if ChatGPT is symmetrically effective in detecting AI-generated or human-written text. Our findings provide insight on how ChatGPT and similar LLMs may be leveraged in automated detection pipelines by simply focusing on solving a specific aspect of the problem and deriving the rest from that solution. All code and data is available at https://github.com/AmritaBh/ChatGPT-as-Detector.",
                    "[263831179 | Hou et al. | 2024 | Citations: 66]": "Existing watermarked generation algorithms employ token-level designs and therefore, are vulnerable to paraphrase attacks. To address this issue, we introduce watermarking on the semantic representation of sentences. We propose SemStamp, a robust sentence-level semantic watermarking algorithm that uses locality-sensitive hashing (LSH) to partition the semantic space of sentences. The algorithm encodes and LSH-hashes a candidate sentence generated by a language model, and conducts rejection sampling until the sampled sentence falls in watermarked partitions in the semantic embedding space. To test the paraphrastic robustness of watermarking algorithms, we propose a \u201cbigram paraphrase\u201d attack that produces paraphrases with small bigram overlap with the original sentence. This attack is shown to be effective against existing token-level watermark algorithms, while posing only minor degradations to SemStamp. Experimental results show that our novel semantic watermark algorithm is not only more robust than the previous state-of-the-art method on various paraphrasers and domains, but also better at preserving the quality of generation.",
                    "[267068867 | Hans et al. | 2024 | Citations: 107]": "Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called Binoculars, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate Binoculars on a number of text sources and in varied situations. Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data."
                },
                "metadata": [
                    {
                        "section_title": "B. Approaches 1) LLM prompting and zero-shot methods:",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1854,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 111
                            },
                            {
                                "start": 112,
                                "end": 206
                            },
                            {
                                "start": 207,
                                "end": 340
                            },
                            {
                                "start": 343,
                                "end": 540
                            },
                            {
                                "start": 541,
                                "end": 732
                            },
                            {
                                "start": 733,
                                "end": 861
                            },
                            {
                                "start": 864,
                                "end": 1065
                            },
                            {
                                "start": 1066,
                                "end": 1194
                            },
                            {
                                "start": 1195,
                                "end": 1274
                            },
                            {
                                "start": 1275,
                                "end": 1419
                            },
                            {
                                "start": 1422,
                                "end": 1616
                            },
                            {
                                "start": 1617,
                                "end": 1668
                            },
                            {
                                "start": 1669,
                                "end": 1857
                            }
                        ],
                        "ref_mentions": [
                            "260379022",
                            "256274849",
                            "267068867",
                            "258960584",
                            "259501842",
                            "256194179",
                            "263831179"
                        ],
                        "quote": "One popular approach leverages an external LLM to classify whether a piece of text is AI-generated or not [12]. With carefully crafted prompts, these zero-shot methods can achieve moderate accuracy quickly. However, sensitivity to prompt design and the adversarial gap between the LLM detector and the text generator are common issues [22].\n\n2) Linguistic and statistical signatures: Traditional stylometric features (e.g., function words, syntax complexity, average phrase length) have long been used in authorship attribution [23], [24]. More modern detection focuses on computing perplexity or log-likelihood using reference language models, observing that LLM-generated text tends to show distinctive probability distributions. Additionally, specialized white-box methods can measure rank ordering of tokens if the generating model is partially known [25].\n\n3) Supervised classification (training-based): Labeled corpora of AI versus human text enable fine-tuning of large pre-trained transformers like RoBERTa or T5 to discriminate synthetic text [26], [27]. Researchers improve robustness with adversarial training sets that contain paraphrased or AIgenerated passages shifted in style. Tools like GPTZero [28] and RADAR [29] exemplify advanced supervised detectors. However, assembling high-quality, representative training data remains a challenge, especially as new generator architectures emerge frequently.\n\n4) Watermarking for AI text: Cooperative watermarking modifies text generation at token selection time, embedding an imperceptible pattern in the distribution of words or punctuation [30], [31]. A verifier can detect such patterns after the fact. While promising for major industrial LLMs that adopt the standard, watermarking fails if malicious or open-source models do not embed it, or if paraphrasing disrupts the signal [32], [33]."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.052185,
        "cot": "I need to create sections that will effectively organize all the quoted content to answer the user's query about recent methods and benchmarks for detecting AI-generated text. The query specifically mentions classifier-based approaches, zero-shot detection, and watermarking techniques, so I'll include these as main dimensions.\n\nLet me plan the dimensions:\n\n1. \"Introduction to AI-Generated Text Detection\" - This will provide background on why detection is important and the general landscape. This should be a synthesis to provide context.\n\n2. \"Zero-Shot Detection Methods\" - This should cover methods that don't require training data, including DetectGPT and other statistical approaches. Given the many quotes about zero-shot methods, this should be a synthesis to explain the concepts thoroughly.\n\n3. \"Classifier-Based Detection Approaches\" - This will cover supervised learning approaches where classifiers are trained on datasets of human and AI-generated text. This should be a synthesis to explain the underlying concepts and implementations.\n\n4. \"Watermarking Techniques\" - This will cover methods that embed signals or patterns into AI-generated text during the generation process. This should be a synthesis to explain the concepts and implementations.\n\n5. \"Notable Detection Tools and Benchmarks\" - This will be a list of specific tools and benchmarks mentioned in the quotes, which would be useful for the user to know about. A list format makes sense here since it provides a clear overview of available resources.\n\n6. \"Limitations and Challenges\" - This will cover the challenges and limitations of current detection methods. This should be a synthesis to provide a nuanced understanding of the current state of the field.\n\nI will now assign each quote to the most relevant dimension and ensure all quotes are covered.",
        "plan": {
            "Introduction to AI-Generated Text Detection (synthesis)": [
                3,
                17,
                25,
                26,
                30,
                37
            ],
            "Zero-Shot Detection Methods (synthesis)": [
                0,
                1,
                4,
                6,
                7,
                9,
                10,
                14,
                15,
                19,
                20,
                24,
                29,
                32
            ],
            "Classifier-Based Detection Approaches (synthesis)": [
                1,
                3,
                4,
                7,
                8,
                12,
                15,
                20,
                21,
                27,
                29,
                32,
                38,
                39
            ],
            "Watermarking Techniques (synthesis)": [
                2,
                3,
                4,
                7,
                8,
                11,
                13,
                23,
                28,
                31,
                33,
                36,
                39
            ],
            "Notable Detection Tools and Benchmarks (list)": [
                5,
                7,
                8,
                16,
                18,
                22,
                27,
                28,
                34,
                35
            ],
            "Limitations and Challenges (synthesis)": [
                7,
                17,
                21,
                28,
                31,
                32,
                38,
                39
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to AI-Generated Text Detection",
                "tldr": "The emergence of sophisticated Large Language Models (LLMs) capable of producing human-like text has created an urgent need for reliable detection methods. Current approaches to AI-generated text detection include classifier-based methods, statistical analysis, zero-shot techniques, and watermarking, each with their own strengths and limitations. (11 sources)",
                "text": "\nThe remarkable advancement of Large Language Models (LLMs) has enabled them to reach human-level proficiency in generating text across numerous professional and academic tasks <Paper corpusId=\"258714683\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"246426909\" paperTitle=\"(Ouyang et al., 2022)\" isShortName></Paper>. While these capabilities bring many benefits, they also raise significant concerns about potential misuse, including phishing, disinformation campaigns, and academic dishonesty, making the detection and authentication of generated text crucial for responsible AI use <Paper corpusId=\"258714683\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper>.\n\nSeveral approaches have emerged to address the challenge of AI-generated text detection. These include passive detection methods that use text classifiers fine-tuned on pretrained language models like RoBERTa and GPT-2 to distinguish between human and machine-written text <Paper corpusId=\"258714683\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper>. Statistical discrepancy detection tools like GPTZero and DetectGPT <Paper corpusId=\"256274849\" paperTitle=\"(Mitchell et al., 2023)\" isShortName></Paper> exploit unique patterns in machine-generated text, with GPTZero using metrics such as perplexity and burstiness, while DetectGPT analyzes the negative curvature regions of a model's log probability function <Paper corpusId=\"258714683\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper>.\n\nDeep learning-based methods rely on training classifiers using datasets of paired human and AI-generated texts on identical topics, showing strong performance within the training data distribution but struggling with adversarial attacks and human-AI collaboration scenarios <Paper corpusId=\"258714683\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper>. More recent approaches include watermarking techniques that embed signals into generated text that remain invisible to humans but are algorithmically detectable <Paper corpusId=\"271161969\" paperTitle=\"(Cava et al., 2024)\" isShortName></Paper> <Paper corpusId=\"256194179\" paperTitle=\"(Kirchenbauer et al., 2023)\" isShortName></Paper>.\n\nThe effectiveness of these detection methodologies varies based on factors such as the architecture of the language model, the decoding strategy, the length of the text, and the availability of raw log probabilities from the LLMs <Paper corpusId=\"266369103\" paperTitle=\"(Mubarak et al., 2023)\" isShortName></Paper>. Simple classifiers may be limited, particularly with shorter texts and complex generation strategies, while zero-shot models offer robust detection by exploiting statistical patterns but remain vulnerable to adversarial actions <Paper corpusId=\"266369103\" paperTitle=\"(Mubarak et al., 2023)\" isShortName></Paper>.\n\nSome researchers have explored retrieval-based approaches that involve maintaining records of all generated texts and checking against them for matches, though this raises concerns about scale, coordination, and privacy <Paper corpusId=\"273547059\" paperTitle=\"(Dathathri et al., 2024)\" isShortName></Paper> <Paper corpusId=\"235694265\" paperTitle=\"(Clark et al., 2021)\" isShortName></Paper> <Paper corpusId=\"249674779\" paperTitle=\"(Jakesch et al., 2022)\" isShortName></Paper>. Other notable detection tools include GPTZero, DetectGPT, and OpenAI's text classifier <Paper corpusId=\"276574900\" paperTitle=\"(Trivedi et al., 2025)\" isShortName></Paper>.\n\nAn interesting discovery in detection research is that the intrinsic dimensionality of embeddings for human-written texts consistently hovers around 9 for alphabet-based languages and 7 for Chinese, while AI-generated texts typically score approximately 1.5 lower, allowing for a statistical separation that outperforms state-of-the-art detectors in model-agnostic and cross-domain scenarios <Paper corpusId=\"259108779\" paperTitle=\"(Tulchinskii et al., 2023)\" isShortName></Paper>. This property demonstrates the ongoing search for invariant features that can robustly separate natural and AI-generated texts regardless of generation model, sampling method, or text domain.",
                "citations": [
                    {
                        "id": "(Yang et al., 2023)",
                        "snippets": [
                            "Recent advances in large language models (LLMs) have enabled them to reach human-level proficiency across numerous professional and academic tasks [24,(Ouyang et al., 2022)[33]. One of the most impressive examples is OpenAI's ChatGPT [23], which has demonstrated remarkable prowess in answering questions, composing emails, essays, and even generating code. However, this impressive ability to create human-like text with remarkable efficiency has ignited apprehension regarding the potential abuse of LLMs for malicious purposes [6,(2023)(Liebrenz et al., 2023)(Stokel-Walker, 2022), such as phishing, disinformation campaigns, and academic dishonesty",
                            "Therefore, detecting and authenticating generated text becomes crucial to ensure the responsible and secure use of LLMs.\n\nA prevalent solution is passive detection [1, 10,18,25,28,40], where a text classifier, usually fine-tuned on a pretrained language model like RoBERTa [16] and GPT-2 [30], is adopted to distinguish between generated and human-written text. However, these learning-based methods perform well only when the input data share a similar distribution with the training data, thereby limiting their applicability to specific domains.\n\nStatistical Discrepancy Detection. Several methods distinguish between generated and human-written text by identifying statistical discrepancies between them, as exemplified by two recent tools: GPTZero [40] and DetectGPT [18]. GPTZero uses perplexity and burstiness to tell apart human-written and generated text, as language models tend to produce more predictable and consistent text based on the patterns they learned from training data, resulting in lower perplexity scores for generated text. DetectGPT exploits the negative curvature regions of a model's log probability function to identify generated text by comparing the log probability of unperturbed and perturbed text variations. However, as language models are constantly improving and becoming more sophisticated, these heuristic features struggle to achieve robustness and generalization. Deep Learning-based Detection. Deep learning-based methods rely on gathering human-written and generated samples to train classifiers. Recently, OpenAI fine-tuned a GPT model for this discrimination task using a dataset comprising paired human and AI-generated texts on identical topics [25]. Similarly, Guo et al. [10] fine-tuned a text classifier based on pre-trained autoencoding LLMs (e.g., RoBERTa) by collecting the Human ChatGPT Comparison Corpus (HC3). Deep learning-based methods exhibit strong performance under the training data distribution, but they are susceptible to adversarial attacks, lack interpretability, and struggle to provide reliable judgments in human-AI collaboration scenarios. Watermark-based Detection. Kirchenbauer et al. [12] proposed the watermarking framework for white-box language models. The watermarking operates by randomly selecting a random set of \"greenlist\" words from the model's vocabulary and softly encouraging the use of these \"greenlist\" words by interfering with the sampling process at each generation step. The watermark can be detected by testing the following null hypothesis,  0 : The text sequence is generated with no knowledge of the selection rule of \"greenlist\" words."
                        ],
                        "paper": {
                            "corpus_id": 258714683,
                            "title": "Watermarking Text Generated by Black-Box Language Models",
                            "authors": [
                                {
                                    "authorId": "2143365293",
                                    "name": "Xi Yang"
                                },
                                {
                                    "authorId": "8780109",
                                    "name": "Kejiang Chen"
                                },
                                {
                                    "authorId": "51027868",
                                    "name": "Weiming Zhang"
                                },
                                {
                                    "authorId": "40997227",
                                    "name": "Chang-rui Liu"
                                },
                                {
                                    "authorId": "2217971466",
                                    "name": "Yuang Qi"
                                },
                                {
                                    "authorId": "2155863060",
                                    "name": "Jie Zhang"
                                },
                                {
                                    "authorId": "12464207",
                                    "name": "Han Fang"
                                },
                                {
                                    "authorId": "2052212945",
                                    "name": "Neng H. Yu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 62
                        },
                        "score": 0.97021484375
                    },
                    {
                        "id": "(Ouyang et al., 2022)",
                        "snippets": [
                            "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."
                        ],
                        "paper": {
                            "corpus_id": 246426909,
                            "title": "Training language models to follow instructions with human feedback",
                            "authors": [
                                {
                                    "authorId": "31793034",
                                    "name": "Long Ouyang"
                                },
                                {
                                    "authorId": "49387725",
                                    "name": "Jeff Wu"
                                },
                                {
                                    "authorId": "2115903168",
                                    "name": "Xu Jiang"
                                },
                                {
                                    "authorId": "2061137049",
                                    "name": "Diogo Almeida"
                                },
                                {
                                    "authorId": "2064084601",
                                    "name": "Carroll L. Wainwright"
                                },
                                {
                                    "authorId": "2051714782",
                                    "name": "Pamela Mishkin"
                                },
                                {
                                    "authorId": null,
                                    "name": "Chong Zhang"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "2117680841",
                                    "name": "Katarina Slama"
                                },
                                {
                                    "authorId": "2064770039",
                                    "name": "Alex Ray"
                                },
                                {
                                    "authorId": "47971768",
                                    "name": "John Schulman"
                                },
                                {
                                    "authorId": "2052366271",
                                    "name": "Jacob Hilton"
                                },
                                {
                                    "authorId": "2151735262",
                                    "name": "Fraser Kelton"
                                },
                                {
                                    "authorId": "2142365973",
                                    "name": "Luke E. Miller"
                                },
                                {
                                    "authorId": "2151735251",
                                    "name": "Maddie Simens"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "2930640",
                                    "name": "P. Welinder"
                                },
                                {
                                    "authorId": "145791315",
                                    "name": "P. Christiano"
                                },
                                {
                                    "authorId": "2990741",
                                    "name": "Jan Leike"
                                },
                                {
                                    "authorId": "49407415",
                                    "name": "Ryan J. Lowe"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 13203
                        },
                        "score": 0
                    },
                    {
                        "id": "(Mitchell et al., 2023)",
                        "snippets": [
                            "In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT."
                        ],
                        "paper": {
                            "corpus_id": 256274849,
                            "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
                            "authors": [
                                {
                                    "authorId": "49688913",
                                    "name": "E. Mitchell"
                                },
                                {
                                    "authorId": "2110392124",
                                    "name": "Yoonho Lee"
                                },
                                {
                                    "authorId": "121873407",
                                    "name": "Alexander Khazatsky"
                                },
                                {
                                    "authorId": "144783904",
                                    "name": "Christopher D. Manning"
                                },
                                {
                                    "authorId": "46881670",
                                    "name": "Chelsea Finn"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 627
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Cava et al., 2024)",
                        "snippets": [
                            "The remarkable boost in human-like text generation performances achieved by Large Language Models (LLMs) in recent years has determined a rising challenge in detecting whether and to what extent texts have been generated by humans or machines [21,49,39]. In this context, the \"watermarking\" paradigm rapidly gained attention (Kirchenbauer et al., 2023)(Yoo et al., 2023)28,50], as it allows embedding specific signals into generated texts that remain invisible to humans but are algorithmically detectable. Statistical learning methods also offer advanced solutions for detecting the authorship of texts. These include probabilistic models (Mitchell et al., 2023)1,47,14], log rank information [38], perplexity [44], discourse motifs [24], and other statistical approaches (Gehrmann et al., 2019)(Tulchinskii et al., 2023)[45] More recently, we have witnessed the emergence of deep learning to detect or attribute AI-generated content, which stands as a promising body of research. Researchers have been exploiting LLMs to detect generated text [18,46], using ChatGPT itself as a detector (Bhattacharjee et al., 2023), or combining LLMs with topological aspects [43]."
                        ],
                        "paper": {
                            "corpus_id": 271161969,
                            "title": "Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text",
                            "authors": [
                                {
                                    "authorId": "2115944163",
                                    "name": "Lucio La Cava"
                                },
                                {
                                    "authorId": "2070008514",
                                    "name": "Davide Costa"
                                },
                                {
                                    "authorId": "2261507500",
                                    "name": "Andrea Tagarelli"
                                }
                            ],
                            "year": 2024,
                            "venue": "European Conference on Artificial Intelligence",
                            "n_citations": 3
                        },
                        "score": 0.96337890625
                    },
                    {
                        "id": "(Kirchenbauer et al., 2023)",
                        "snippets": [
                            "Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of\"green\"tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security."
                        ],
                        "paper": {
                            "corpus_id": 256194179,
                            "title": "A Watermark for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2166053502",
                                    "name": "John Kirchenbauer"
                                },
                                {
                                    "authorId": "8284185",
                                    "name": "Jonas Geiping"
                                },
                                {
                                    "authorId": "123191916",
                                    "name": "Yuxin Wen"
                                },
                                {
                                    "authorId": "143975296",
                                    "name": "Jonathan Katz"
                                },
                                {
                                    "authorId": "2679804",
                                    "name": "Ian Miers"
                                },
                                {
                                    "authorId": "1962083",
                                    "name": "T. Goldstein"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 507
                        },
                        "score": 0
                    },
                    {
                        "id": "(Mubarak et al., 2023)",
                        "snippets": [
                            "In the context of the rapid advancement of Large Language Models (LLMs), the ability to produce highly accurate and contextually relevant text is becoming an increasing concern in online security. The sophisticated text generation capabilities of these models can potentially facilitate harmful activities on online platforms, emphasising the importance of detecting AI-generated texts as a mitigation strategy. Several detection methodologies have been developed, each with varying degrees of effectiveness influenced by factors such as the architecture of the language model, the decoding strategy, the length of the text and the availability of raw log probabilities from the LLMs. \n\nSimple classifiers offer a basic, yet sometimes limited, approach, particularly with shorter texts and complex generation strategies. Zero-shot models provide a robust detection strategy by exploiting unique statistical patterns in machine-generated text; however, they are susceptible to adversarial actions or different generation strategies. Finetuned language models represent another approach that provides a more promising route for enhancing the detection accuracy. However, their effectiveness can be influenced by the sampling method and the length of the generated sequences. \n\nWatermarking, a more recent development, has emerged as a unique method for detection. This involves embedding watermarks in the logits of an LLM during text generation, forming a pattern that can potentially identify synthetic text. Despite the promise of watermarking, its vulnerability to spoofing/paraphrasing attacks and evasion techniques presents significant challenges that warrant further research."
                        ],
                        "paper": {
                            "corpus_id": 266369103,
                            "title": "A Survey on the Detection and Impacts of Deepfakes in Visual, Audio, and Textual Formats",
                            "authors": [
                                {
                                    "authorId": "2275319803",
                                    "name": "Rami Mubarak"
                                },
                                {
                                    "authorId": "3277359",
                                    "name": "Tariq A. A. Alsboui"
                                },
                                {
                                    "authorId": "2207298991",
                                    "name": "Omar Alshaikh"
                                },
                                {
                                    "authorId": "2276533468",
                                    "name": "Isa Inuwa-Dutse"
                                },
                                {
                                    "authorId": "143955457",
                                    "name": "Saad Khan"
                                },
                                {
                                    "authorId": "100581778",
                                    "name": "Si\u00f4n Parkinson"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 36
                        },
                        "score": 0.970703125
                    },
                    {
                        "id": "(Dathathri et al., 2024)",
                        "snippets": [
                            "Multiple strategies have emerged to address this problem. One is a retrieval-based approach, which involves keeping a growing record of all generated texts and checking against it for matches 12. This requires scale and coordination, and raises privacy concerns as it requires accessing and storing all LLM interactions."
                        ],
                        "paper": {
                            "corpus_id": 273547059,
                            "title": "Scalable watermarking for identifying large language model outputs",
                            "authors": [
                                {
                                    "authorId": "3491117",
                                    "name": "Sumanth Dathathri"
                                },
                                {
                                    "authorId": "2268761249",
                                    "name": "Abigail See"
                                },
                                {
                                    "authorId": "145053127",
                                    "name": "Sumedh Ghaisas"
                                },
                                {
                                    "authorId": "2268826600",
                                    "name": "Po-Sen Huang"
                                },
                                {
                                    "authorId": "2327318512",
                                    "name": "Rob McAdam"
                                },
                                {
                                    "authorId": "1851564",
                                    "name": "Johannes Welbl"
                                },
                                {
                                    "authorId": "2327316530",
                                    "name": "Vandana Bachani"
                                },
                                {
                                    "authorId": "2275186627",
                                    "name": "Alex Kaskasoli"
                                },
                                {
                                    "authorId": "49860489",
                                    "name": "Robert Stanforth"
                                },
                                {
                                    "authorId": "2327319709",
                                    "name": "Tatiana Matejovicova"
                                },
                                {
                                    "authorId": "2270174566",
                                    "name": "Jamie Hayes"
                                },
                                {
                                    "authorId": "2275153245",
                                    "name": "Nidhi Vyas"
                                },
                                {
                                    "authorId": "2089890458",
                                    "name": "Majd Al Merey"
                                },
                                {
                                    "authorId": "1400348545",
                                    "name": "Jonah Brown-Cohen"
                                },
                                {
                                    "authorId": "3407947",
                                    "name": "Rudy Bunel"
                                },
                                {
                                    "authorId": "2324784942",
                                    "name": "Borja Balle"
                                },
                                {
                                    "authorId": "122347685",
                                    "name": "taylan. cemgil"
                                },
                                {
                                    "authorId": "2268758347",
                                    "name": "Zahra Ahmed"
                                },
                                {
                                    "authorId": "1396786528",
                                    "name": "Kitty Stacpoole"
                                },
                                {
                                    "authorId": "47473421",
                                    "name": "Ilia Shumailov"
                                },
                                {
                                    "authorId": "2324799480",
                                    "name": "Cip Baetu"
                                },
                                {
                                    "authorId": "2071666",
                                    "name": "Sven Gowal"
                                },
                                {
                                    "authorId": "48987704",
                                    "name": "D. Hassabis"
                                },
                                {
                                    "authorId": "143967473",
                                    "name": "Pushmeet Kohli"
                                }
                            ],
                            "year": 2024,
                            "venue": "The Naturalist",
                            "n_citations": 74
                        },
                        "score": 0.9580078125
                    },
                    {
                        "id": "(Clark et al., 2021)",
                        "snippets": [
                            "Human evaluations are typically considered the gold standard in natural language generation, but as models\u2019 fluency improves, how well can evaluators detect and judge machine-generated text? We run a study assessing non-experts\u2019 ability to distinguish between human- and machine-authored text (GPT2 and GPT3) in three domains (stories, news articles, and recipes). We find that, without training, evaluators distinguished between GPT3- and human-authored text at random chance level. We explore three approaches for quickly training evaluators to better identify GPT3-authored text (detailed instructions, annotated examples, and paired examples) and find that while evaluators\u2019 accuracy improved up to 55%, it did not significantly improve across the three domains. Given the inconsistent results across text domains and the often contradictory reasons evaluators gave for their judgments, we examine the role untrained human evaluations play in NLG evaluation and provide recommendations to NLG researchers for improving human evaluations of text generated from state-of-the-art models."
                        ],
                        "paper": {
                            "corpus_id": 235694265,
                            "title": "All That\u2019s \u2018Human\u2019 Is Not Gold: Evaluating Human Evaluation of Generated Text",
                            "authors": [
                                {
                                    "authorId": "40684993",
                                    "name": "Elizabeth Clark"
                                },
                                {
                                    "authorId": "50509991",
                                    "name": "Tal August"
                                },
                                {
                                    "authorId": "38618739",
                                    "name": "Sofia Serrano"
                                },
                                {
                                    "authorId": "3465456",
                                    "name": "Nikita Haduong"
                                },
                                {
                                    "authorId": "40895369",
                                    "name": "Suchin Gururangan"
                                },
                                {
                                    "authorId": "144365875",
                                    "name": "Noah A. Smith"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 414
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jakesch et al., 2022)",
                        "snippets": [
                            "Significance Human communication is now rife with language generated by AI. Every day, across the web, chat, email, and social media, AI systems produce billions of messages that could be perceived as created by humans. In this work, we analyze human judgments of self-presentations written by humans and generated by AI systems. We find that people cannot detect AI-generated self-presentations as their judgment is misguided by intuitive but flawed heuristics for AI-generated language. We demonstrate that AI systems can exploit these heuristics to produce text perceived as \"more human than human.\" Our results raise the question of how humanity will adapt to AI-generated text, illustrating the need to reorient the development of AI language systems to ensure that they support rather than undermine human cognition."
                        ],
                        "paper": {
                            "corpus_id": 249674779,
                            "title": "Human heuristics for AI-generated language are flawed",
                            "authors": [
                                {
                                    "authorId": "113179096",
                                    "name": "Maurice Jakesch"
                                },
                                {
                                    "authorId": "1697703",
                                    "name": "Jeffrey T. Hancock"
                                },
                                {
                                    "authorId": "1687465",
                                    "name": "Mor Naaman"
                                }
                            ],
                            "year": 2022,
                            "venue": "Proceedings of the National Academy of Sciences of the United States of America",
                            "n_citations": 188
                        },
                        "score": 0
                    },
                    {
                        "id": "(Trivedi et al., 2025)",
                        "snippets": [
                            "To address these risks, various algorithms have been developed to detect AI-generated text, which include watermarking techniques [3], tools like GPTZero [4], DetectGPT [5], and OpenAI's text classifier [6]."
                        ],
                        "paper": {
                            "corpus_id": 276574900,
                            "title": "Sarang at DEFACTIFY 4.0: Detecting AI-Generated Text Using Noised Data and an Ensemble of DeBERTa Models",
                            "authors": [
                                {
                                    "authorId": "2346209847",
                                    "name": "Avinash Trivedi"
                                },
                                {
                                    "authorId": "46505742",
                                    "name": "S. Sangeetha"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9404296875
                    },
                    {
                        "id": "(Tulchinskii et al., 2023)",
                        "snippets": [
                            "Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over different text domains and varying proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant for human-written texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings for a given text sample. We show that the average intrinsic dimensionality of fluent texts in a natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\\approx 1.5$ lower, with a clear statistical separation between human-generated and AI-generated distributions. This property allows us to build a score-based artificial text detector. The proposed detector's accuracy is stable over text domains, generator models, and human writer proficiency levels, outperforming SOTA detectors in model-agnostic and cross-domain scenarios by a significant margin."
                        ],
                        "paper": {
                            "corpus_id": 259108779,
                            "title": "Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts",
                            "authors": [
                                {
                                    "authorId": "2165662874",
                                    "name": "Eduard Tulchinskii"
                                },
                                {
                                    "authorId": "2192824975",
                                    "name": "Kristian Kuznetsov"
                                },
                                {
                                    "authorId": "147328740",
                                    "name": "Laida Kushnareva"
                                },
                                {
                                    "authorId": "2037913268",
                                    "name": "D. Cherniavskii"
                                },
                                {
                                    "authorId": "101098891",
                                    "name": "S. Barannikov"
                                },
                                {
                                    "authorId": "2219690846",
                                    "name": "Irina Piontkovskaya"
                                },
                                {
                                    "authorId": "1742235",
                                    "name": "S. Nikolenko"
                                },
                                {
                                    "authorId": "51139941",
                                    "name": "Evgeny Burnaev"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 89
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Zero-Shot Detection Methods",
                "tldr": "Zero-shot detection methods identify AI-generated text without requiring training data specifically for this task. Popular approaches include DetectGPT, which analyzes the curvature of a model's log probability function, and methods that leverage statistical patterns like perplexity, burstiness, and token rank distributions. (11 sources)",
                "text": "\nZero-shot detection methods have emerged as an important category of AI-generated text detection tools that do not require training on labeled datasets of machine-generated text. These methods are particularly valuable because they can potentially generalize to new models and domains without needing additional training data <Paper corpusId=\"258762215\" paperTitle=\"(Lu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258048481\" paperTitle=\"(Chakraborty et al., 2023)\" isShortName></Paper>.\n\nOne of the most influential zero-shot detection approaches is DetectGPT, which identifies a fundamental property of language models: text generated by LLMs tends to occupy negative curvature regions of the model's log probability function <Paper corpusId=\"256274849\" paperTitle=\"(Mitchell et al., 2023)\" isShortName></Paper>. By comparing the log probability of the original text with perturbed versions, DetectGPT can detect AI-generated content without training a separate classifier. This approach has significantly improved detection performance, achieving 0.95 AUROC for fake news articles generated by GPT-NeoX-20B compared to 0.81 AUROC for previous zero-shot baselines <Paper corpusId=\"256274849\" paperTitle=\"(Mitchell et al., 2023)\" isShortName></Paper>.\n\nWhile effective, DetectGPT requires numerous queries to the source model, making it computationally expensive <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper>. Addressing this limitation, Su et al. introduced DetectLLM, which offers two variants: DetectLLM-LRR (Log Rank Ratio), a fast and efficient method, and DetectLLM-NPR (Negative Probability Ratio), which requires perturbations but achieves higher accuracy with fewer perturbations than DetectGPT <Paper corpusId=\"259129463\" paperTitle=\"(Su et al., 2023)\" isShortName></Paper>.\n\nOther statistical zero-shot methods leverage metrics such as perplexity, burstiness, entropy, and n-gram frequency distributions <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>. Tools like GPTZero specifically use perplexity and burstiness to distinguish between human and machine text, with lower values typically indicating machine-generated content <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>. The GLTR tool analyzes token rank distributions and entropy to help identify predictable patterns in AI-generated text <Paper corpusId=\"267094724\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper> <Paper corpusId=\"182952848\" paperTitle=\"(Gehrmann et al., 2019)\" isShortName></Paper>.\n\nZero-shot detection can operate in either white-box or black-box settings. In white-box scenarios, detectors have access to the source model that generated the text, while black-box detection aims to identify AI-generated content without knowing which model produced it <Paper corpusId=\"263831345\" paperTitle=\"(Bao et al., 2023)\" isShortName></Paper>. This distinction is important for practical applications, as most real-world detection scenarios involve black-box settings where the specific generative model is unknown.\n\nA key advantage of zero-shot methods is their ability to adapt to new models without retraining, though they may be less effective than supervised approaches within specific domains <Paper corpusId=\"273901392\" paperTitle=\"(Nguyen-Son et al., 2024)\" isShortName></Paper>. However, these methods face challenges with adversarial inputs and highly optimized language models that increasingly mimic human writing patterns <Paper corpusId=\"267027819\" paperTitle=\"(Bethany et al., 2024)\" isShortName></Paper>. As language models continue to evolve, developing more robust zero-shot detection methods remains an active area of research.",
                "citations": [
                    {
                        "id": "(Lu et al., 2023)",
                        "snippets": [
                            "In recent years, the research community has developed a wide range of detectors for AI-generated contents. In general, these detectors can be classified into three categories: training-based, statistical, and watermarking methods. Training-based methods treat the detection problem as a binary classification task, where neural networks are trained using AI-generated text and human-written text. Early studies utilized classifiers to identify fake reviews (Hovy, 2016) and fake news (Zellers et al., 2019). More recently, researchers have trained classifiers using text generated by LLMs, such as the GPT-3.5 detector (Guo et al., 2023) and GPT-2 detector (Solaiman et al., 2019). Statistical methods, on the other hand, focus on zero-shot detection without any additional training overhead. These methods seek to distinguish between human-written text and AI-generated text based on the statistical characteristics of text, such as the statistical irregularities in measures like entropy (Lavergne et al., 2008), perplexity (Beresneva, 2016) and token rank (Gehrmann et al., 2019). A recent method, DetectGPT (Mitchell et al., 2023), exploits the phenomenon that AI-generated text tends to lie in the negative curvature regions of log probability of text. The watermarking methods involve modifying the LLM's text generation process to imprint specific patterns on the generated text, such that it can be detected (Abdelnabi et al., 2020)Grinbaum & Adomaitis, 2022;Kirchenbauer et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 258762215,
                            "title": "Large Language Models can be Guided to Evade AI-Generated Text Detection",
                            "authors": [
                                {
                                    "authorId": "2147128045",
                                    "name": "Ning Lu"
                                },
                                {
                                    "authorId": "2152939552",
                                    "name": "Shengcai Liu"
                                },
                                {
                                    "authorId": "22272507",
                                    "name": "Ruidan He"
                                },
                                {
                                    "authorId": "2078692567",
                                    "name": "Ke Tang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 53
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Chakraborty et al., 2023)",
                        "snippets": [
                            "Traditional approaches. They involve statistical outlier detection methods, which employ statistical metrics such as entropy, perplexity, and n-gram frequency to differentiate between human and machine-generated texts (Lavergne et al., 2008)Gehrmann et al., 2019). However, with the advent of ChatGPT (OpenAI) (OpenAI, 2023), a new innovative statistical detection methodology, DetectGPT (Mitchell et al., 2023), has been developed. It operates on the principle that text generated by the model tends to lie in the negative curvature areas of the model's log probability. DetectGPT (Mitchell et al., 2023) generates and compares multiple perturbations of model-generated text to determine whether the text is machine-generated or not based on the log probability of the original text and the perturbed versions. DetectGPT significantly outperforms the majority of the existing zero-shot methods for model sample detection with very high AUC scores (note that we use the terms AUROC and AUC interchangeably for presentation convenience). \n\nClassifier-based detectors. In contrast to statistical methods, classifier-based detectors are common in natural language detection paradigms, particularly in fake news and misinformation detection (Schildhauer, 2022;Zou & Ling, 2021). OpenAI has recently fine-tuned a GPT model (OpenAI, 2023) using data from Wikipedia, WebText, and internal human demonstration data to create a web interface for a discrimination task using text generated by 34 language models. This approach combines a classifier-based approach with a human evaluation component to determine whether a given text was machine-generated or not. These recent advancements in the field of detecting AI-generated text have significant implications for detecting and preventing the spread of misinformation and fake news, thereby contributing to the betterment of society (Schildhauer, 2022;Zou & Ling, 2021;Kshetri & Voas, 2022). \n\nWatermark-based identification."
                        ],
                        "paper": {
                            "corpus_id": 258048481,
                            "title": "On the Possibilities of AI-Generated Text Detection",
                            "authors": [
                                {
                                    "authorId": "49081354",
                                    "name": "Souradip Chakraborty"
                                },
                                {
                                    "authorId": "3387859",
                                    "name": "A. S. Bedi"
                                },
                                {
                                    "authorId": "2110018762",
                                    "name": "Sicheng Zhu"
                                },
                                {
                                    "authorId": "49640821",
                                    "name": "Bang An"
                                },
                                {
                                    "authorId": "2172597446",
                                    "name": "Dinesh Manocha"
                                },
                                {
                                    "authorId": "2117426487",
                                    "name": "Furong Huang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 127
                        },
                        "score": 0.9658203125
                    },
                    {
                        "id": "(Mitchell et al., 2023)",
                        "snippets": [
                            "In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT."
                        ],
                        "paper": {
                            "corpus_id": 256274849,
                            "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
                            "authors": [
                                {
                                    "authorId": "49688913",
                                    "name": "E. Mitchell"
                                },
                                {
                                    "authorId": "2110392124",
                                    "name": "Yoonho Lee"
                                },
                                {
                                    "authorId": "121873407",
                                    "name": "Alexander Khazatsky"
                                },
                                {
                                    "authorId": "144783904",
                                    "name": "Christopher D. Manning"
                                },
                                {
                                    "authorId": "46881670",
                                    "name": "Chelsea Finn"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 627
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Deng et al., 2023)",
                        "snippets": [
                            "The detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires querying the source LLM with hundreds of its perturbations.\n\nSome methods (e.g., OpenAI, 2023b) involve training supervised classifiers, which, yet, suffer from overfitting to the training data and ineffectiveness to generalize to new test data. Zero-shot LLM-generated text detection approaches bypass these issues by leveraging the source LLM to detect its samples (Solaiman et al., 2019;Gehrmann et al., 2019b;Ippolito et al., 2020). They usually proceed by inspecting the average per-token log probability of the candidate text, but the practical detection performance can be unsatisfactory.\n\nDetectGPT (Mitchell et al., 2023) is a recent method that achieves improved zero-shot detection efficacy by exploring the probability curvature of LLMs. It generates multiple perturbations of the candidate text and scores them using the source LLM to define detection statistics. It can detect texts generated by GPT-2 (Radford et al., 2019) and GPT-NeoX-20B (Black et al., 2022). Yet, DetectGPT relies on hundreds of queries to the source LLM to estimate the local probability curvature surrounding one single candidate passage."
                        ],
                        "paper": {
                            "corpus_id": 258947640,
                            "title": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model",
                            "authors": [
                                {
                                    "authorId": "145114723",
                                    "name": "Zhijie Deng"
                                },
                                {
                                    "authorId": "2162081759",
                                    "name": "Hongcheng Gao"
                                },
                                {
                                    "authorId": "2188993538",
                                    "name": "Yibo Miao"
                                },
                                {
                                    "authorId": "46702482",
                                    "name": "Hao Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 20
                        },
                        "score": 0.9423828125
                    },
                    {
                        "id": "(Su et al., 2023)",
                        "snippets": [
                            "Previous work has studied several zero-shot methods, which require no training data. These methods achieve good performance, but there is still a lot of room for improvement. In this paper, we introduce two novel zero-shot methods for detecting machine-generated text by leveraging the log rank information. One is called DetectLLM-LRR, which is fast and efficient, and the other is called DetectLLM-NPR, which is more accurate, but slower due to the need for perturbations. Our experiments on three datasets and seven language models show that our proposed methods improve over the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover, DetectLLM-NPR needs fewer perturbations than previous work to achieve the same level of performance, which makes it more practical for real-world use."
                        ],
                        "paper": {
                            "corpus_id": 259129463,
                            "title": "DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text",
                            "authors": [
                                {
                                    "authorId": "2116966710",
                                    "name": "Jinyan Su"
                                },
                                {
                                    "authorId": "2080123731",
                                    "name": "Terry Yue Zhuo"
                                },
                                {
                                    "authorId": "2119265743",
                                    "name": "Di Wang"
                                },
                                {
                                    "authorId": "2026545715",
                                    "name": "Preslav Nakov"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 141
                        },
                        "score": 0.95849609375
                    },
                    {
                        "id": "(Tang et al., 2023)",
                        "snippets": [
                            "Recent methods such as DetectGPT and GPTZero distinguish between machine-generated and human-written text by analyzing their statistical discrepancies (Tian, 2023;Mitchell et al., 2023). DetectGPT compares the log probability computed by a model on unperturbed text and perturbed variations, leveraging the observation that text sampled from a LLM generally occupy negative curvature regions of the model's log probability function. GPTZero instead uses perplexity and burstiness to distinguish human from machine text, with lower perplexity and burstiness indicating a greater likelihood of machine-generated text. However, these heuristics do not generalize and are often fallible.\n\nDetection by Learning Classifiers Several papers have proposed to train classifiers to distinguish between AI and human generated text. During the initial GPT-2 release, OpenAI trained a RoBERTa classifier to detect GPT-2 generated text with 95% accuracy (Solaiman et al., 2019). More recently, OpenAI fine-tuned a GPT model on a dataset of machine-generated and human texts focusing on the same topic, with a true positive identification rate of 26% (Ope-nAI, 2023). Similarly, Guo et al. (2023) collected the Human ChatGPT Comparison Corpuse (HC3) and fine-tuned RoBERTa for the detection task.\n\nNotably, the capabilities of such classifiers decrease as machine-generated text becomes increasingly human-like. Sadasivan et al. (2023) show theoretically that for sufficiently advanced language models, machine-generated text detectors offer only a marginal improvement over random classifiers. Moreover, such methods are prone to adversarial attacks and are not robust to out-of-distribution text.\n\nWatermarking Large Language Models An alternative to detecting of machine-generated text using statistical discrepancies and learned classifiers is the concept of watermarks. Watermarks are hidden patterns in machinegenerated text that are imperceptible to humans, but algorithmically identifiable as synthetic. Natural language watermarks long predate the development of LLMs, relying on methods such as synonym substitution, as well as syntactic and semantic transformations (Topkara et al., 2005)."
                        ],
                        "paper": {
                            "corpus_id": 258967971,
                            "title": "Baselines for Identifying Watermarked Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2144484107",
                                    "name": "Leonard Tang"
                                },
                                {
                                    "authorId": "2143997407",
                                    "name": "Gavin Uberti"
                                },
                                {
                                    "authorId": "2211332429",
                                    "name": "Tom Shlomi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.970703125
                    },
                    {
                        "id": "(Xiong et al., 2024)",
                        "snippets": [
                            "In the realm of distinguishing texts generated by large language models (LLMs) from those authored by humans, researchers have employed a variety of methods and tools. Broadly, these approaches can be categorized into two main types: black-box and white-box detection methods (Tang et al., 2023). Black-box detection relies on API-level access to LLMs, utilizing textual samples from both human and machine sources to train classification models (Dugan et al., 2020). The study by Guo et al. (2023) integrated existing question-and-answer datasets and leveraged fine-tuning of pre-trained models to investigate the characteristics and similarities between human-generated and AI-generated texts (Guo et al., 2023). \n\nIn the context of white-box detection, Kirchenbauer et al. ( 2023) introduced a novel approach involving the embedding of watermarks in the outputs of large language models to facilitate the detection of AI-generated text (Kirchenbauer et al., 2023). Additionally, a variety of tools and methodologies, including XGBoost, decision trees, and transformer-based models, have been evaluated for their efficacy in detecting texts produced by AI (Zaitsu and Jin, 2023). These techniques incorporate multiple stylistic measurement features to differentiate between AI-generated and human-generated texts (Shijaku and Canhasi, 2023). \n\nSpecific tools and techniques in this domain include the GLTR tool developed by Gehrmann et al. (2019), which analyzes the usage of rare words in texts to distinguish between those generated by the GPT-2 model and human writers (Gehrmann et al., 2019). The DetectGPT method posits that minor rewrites of LLM-generated texts tend to reduce the log probability under the model, a hypothesis that has been explored in depth (Mitchell et al., 2023). Furthermore, intrinsic dimension analysis, including methods like the Persistent Homology Dimension estimator (PHD), has been applied to distinguish between authentic texts and those generated artificially (Tulchinskii et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 267094724,
                            "title": "Fine-tuning Large Language Models for Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection",
                            "authors": [
                                {
                                    "authorId": "2280331364",
                                    "name": "Feng Xiong"
                                },
                                {
                                    "authorId": "52196310",
                                    "name": "Thanet Markchom"
                                },
                                {
                                    "authorId": "2280391458",
                                    "name": "Ziwei Zheng"
                                },
                                {
                                    "authorId": "2280666724",
                                    "name": "Subin Jung"
                                },
                                {
                                    "authorId": "2293393169",
                                    "name": "Varun Ojha"
                                },
                                {
                                    "authorId": "2281373610",
                                    "name": "Huizhi Liang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.97705078125
                    },
                    {
                        "id": "(Gehrmann et al., 2019)",
                        "snippets": [
                            "The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by non-experts. In this work, we introduce GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across multiple sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs."
                        ],
                        "paper": {
                            "corpus_id": 182952848,
                            "title": "GLTR: Statistical Detection and Visualization of Generated Text",
                            "authors": [
                                {
                                    "authorId": "3159346",
                                    "name": "Sebastian Gehrmann"
                                },
                                {
                                    "authorId": "2879705",
                                    "name": "Hendrik Strobelt"
                                },
                                {
                                    "authorId": "2531268",
                                    "name": "Alexander M. Rush"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 546
                        },
                        "score": 0
                    },
                    {
                        "id": "(Bao et al., 2023)",
                        "snippets": [
                            "Our research centers on zero-shot detection of machine-generated text, under the premise that our model has not undergone training using any machine-generated text. This distinguishes our approach from conventional supervised methods, which commonly employ discriminative training strategies to acquire specific syntactic or semantic attributes customized for machine-generated text. In contrast, our zero-shot methodology capitalizes on the inherent capabilities of large language models to identify anomalies that function as markers of machine-generated content.\n\nThe White-box Setting. Conventional zero-shot methodologies often operate under the assumption that the source model responsible for generating machine-generated text is accessible. We refer to this context as the white-box setting, where the primary goal is to distinguish machine-generated texts produced by the source model from those generated by humans. In this white-box setting, our detection decisions are dependent on the source model, but it is not mandatory to possess detailed knowledge of the source model's architecture and parameters. For instance, within the white-box framework, a system like DetectGPT utilizes the OpenAI API to identify text generated by GPT-3, without requiring extensive knowledge of the inner workings of GPT-3.\n\nThe Black-box Setting. In real-world situations, there could be instances where we lack knowledge about the specific source models employed for content generation. This necessitates the development of a versatile detector capable of identifying texts generated by a variety of automated systems. We term this scenario the black-box setting, where the objective is to differentiate between machinegenerated texts produced by diverse, unidentified models and those composed by humans. In this context, the term \"black box\" signifies that we lack access to information about the source model or any details pertaining to it."
                        ],
                        "paper": {
                            "corpus_id": 263831345,
                            "title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
                            "authors": [
                                {
                                    "authorId": "1993226927",
                                    "name": "Guangsheng Bao"
                                },
                                {
                                    "authorId": "2257125588",
                                    "name": "Yanbin Zhao"
                                },
                                {
                                    "authorId": "2272668",
                                    "name": "Zhiyang Teng"
                                },
                                {
                                    "authorId": "2145500840",
                                    "name": "Linyi Yang"
                                },
                                {
                                    "authorId": "2256997888",
                                    "name": "Yue Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 153
                        },
                        "score": 0.95458984375
                    },
                    {
                        "id": "(Nguyen-Son et al., 2024)",
                        "snippets": [
                            "Detection of text generated by large language models uses three main techniques. Firstly, supervised learning methods (Solaiman et al., 2019;(Wang et al., 2023)(Hu et al., 2023)(Wu et al., 2023) train classifiers on datasets of original and generated text, though this requires large volumes of training data. The zero-shot approach (Bhattacharjee and Liu, 2023;(Mitchell et al., 2023) eliminates the need for training but is sensitive to out-of-distribution text. Recent research explores watermarking methodologies (Kirchenbauer et al., 2023) to force models to produce predefined words, aiding detection, but this requires modifying the models, which is impractical for proprietary models like ChatGPT."
                        ],
                        "paper": {
                            "corpus_id": 273901392,
                            "title": "SimLLM: Detecting Sentences Generated by Large Language Models Using Similarity between the Generation and its Re-generation",
                            "authors": [
                                {
                                    "authorId": "1403588130",
                                    "name": "Hoang-Quoc Nguyen-Son"
                                },
                                {
                                    "authorId": "2260782421",
                                    "name": "Minh-Son Dao"
                                },
                                {
                                    "authorId": "2238657258",
                                    "name": "Koji Zettsu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 2
                        },
                        "score": 0.95654296875
                    },
                    {
                        "id": "(Bethany et al., 2024)",
                        "snippets": [
                            "One of the subsets of statistical features leveraged in detecting machine-generated text revolves around term frequencies within text samples. Text authored by humans typically aligns with Zipf's Law, where the most frequent word in humanwritten text has roughly twice the frequency of the second most frequent word, and nearly three times that of the third most frequent word and so on (Nguyen-Son et al., 2018). Another feature-based method for detecting machine-generated text focuses on clarity and coherence. Earlier models like GPT-2 have shown that in longer outputs, there is a higher likelihood that inconsistencies, redundancy, or logical errors will emerge (See et al., 2019). Some works build classifiers on bag-of-words features and TF-IDF (Fagni et al., 2020) to analyze shorter text sequences. Other works such as Fr\u00f6hling et al. (Fr\u00f6hling et al., 2021) combine several features, including lack of syntactic and lexical diversity, repetitiveness, and extract linguistic features such as named entities and part-of-speech distributions to build a neural network classifier on this information. However, a notable limitation of these statistical and feature-based methods is their potential vulnerability to more advanced language models, which continually improve in generating human-like text patterns and linguistic diversity. \n\nZero-shot methods for detecting machine-generated text have garnered attention due to the lack of need to train models on large datasets. Works such as those by Gehrmann et al. (Gehrmann et al., 2019) find that human-written texts exhibit a broader range of word choices, whereas machine-generated texts tend to be more constrained and predictable and calculate entropy on text sequences to help distinguish machine-generated text from human-generated text. Other works used simple logprobability thresholding, where LLM token probabilities are averaged and a threshold is determined [56]. Recent works like DetectGPT [40] take this idea further by introducing the observation that the curvature of a model's log probability function is typically more negative for machine-generated text compared to human-written text."
                        ],
                        "paper": {
                            "corpus_id": 267027819,
                            "title": "Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text",
                            "authors": [
                                {
                                    "authorId": "2151516184",
                                    "name": "Mazal Bethany"
                                },
                                {
                                    "authorId": "2279750613",
                                    "name": "Brandon Wherry"
                                },
                                {
                                    "authorId": "2279757012",
                                    "name": "Emet Bethany"
                                },
                                {
                                    "authorId": "8842079",
                                    "name": "Nishant Vishwamitra"
                                },
                                {
                                    "authorId": "71756373",
                                    "name": "Peyman Najafirad"
                                }
                            ],
                            "year": 2024,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 6
                        },
                        "score": 0.955078125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Classifier-Based Detection Approaches",
                "tldr": "Classifier-based detection approaches treat AI-generated text identification as a binary classification task by training models on datasets containing both human and AI-generated texts. These methods range from traditional machine learning algorithms to fine-tuned large language models, showing strong performance within their training distribution but struggling with new generation models and adversarial scenarios. (14 sources)",
                "text": "\nClassifier-based detectors represent a prominent approach for distinguishing between human and AI-generated texts. Unlike statistical or zero-shot methods, these detectors utilize supervised learning techniques to build discriminative models trained on labeled datasets of human and machine-generated content <Paper corpusId=\"258048481\" paperTitle=\"(Chakraborty et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258714683\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper>.\n\nEarly classifier-based detection methods focused on specific domains such as fake reviews and fake news. Zellers et al. (2019) demonstrated the effectiveness of classifiers in identifying neural fake news, finding that using the same model architecture for both generation and detection yielded the best results <Paper corpusId=\"258762215\" paperTitle=\"(Lu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"168169824\" paperTitle=\"(Zellers et al., 2019)\" isShortName></Paper>. This work established a pattern where the most effective detector for a specific model's output is often the model itself or one with similar architecture.\n\nMore sophisticated approaches have emerged with the advancement of large language models. OpenAI developed a detector by fine-tuning a GPT model on datasets comprising paired human and AI-generated texts covering identical topics <Paper corpusId=\"258714683\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper>. Similarly, researchers have fine-tuned robust pre-trained language models like RoBERTa to create specialized classifiers. For example, Guo et al. created the Human ChatGPT Comparison Corpus (HC3) to fine-tune a RoBERTa-based classifier for detecting ChatGPT-generated content <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"273482129\" paperTitle=\"(Tao et al., 2024)\" isShortName></Paper>.\n\nThe effectiveness of these detectors varies significantly based on several factors. Training-based classifiers typically perform well when the test data shares a similar distribution with the training data, but their performance deteriorates when faced with out-of-distribution samples or texts from new generation models <Paper corpusId=\"258762215\" paperTitle=\"(Lu et al., 2023)\" isShortName></Paper>. This limitation becomes increasingly problematic as language models continue to evolve rapidly. Some studies have shown that for sufficiently advanced language models, machine-generated text detectors may offer only marginal improvement over random classifiers <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>.\n\nTo address these limitations, more recent approaches incorporate adversarial training techniques. The RADAR framework, for instance, jointly trains a paraphraser and a detector in an adversarial setting. The paraphraser attempts to generate realistic content that evades detection, while the detector learns to identify even these adversarially modified texts <Paper corpusId=\"277595847\" paperTitle=\"(Cao, 2025)\" isShortName></Paper> <Paper corpusId=\"259501842\" paperTitle=\"(Hu et al., 2023)\" isShortName></Paper>. This approach has shown significant improvements in robustness, especially when facing paraphrased content.\n\nOther innovations include sentence-level detection methods like SeqXGPT, which uses log-probability lists and self-attention networks to enhance detection at a more granular level <Paper corpusId=\"273901392\" paperTitle=\"(Nguyen-Son et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264128397\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. This approach is particularly valuable for identifying partially AI-generated or human-edited content.\n\nTraditional machine learning approaches using feature extraction (such as stylometric analysis) are still used in some contexts, with algorithms like logistic regression, SVM, and XGBoost offering the advantage of explainability. However, these methods generally show lower accuracy compared to deep learning approaches that can capture more nuanced aspects of writing style <Paper corpusId=\"276773353\" paperTitle=\"(Bitton et al., 2025)\" isShortName></Paper> <Paper corpusId=\"267094724\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper>.\n\nDespite their widespread use, classifier-based detectors face significant challenges. They are susceptible to adversarial attacks, lack interpretability, and struggle with human-AI collaboration scenarios where text may be partially generated or edited <Paper corpusId=\"258714683\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper>. Furthermore, they require substantial data collection and model training costs, which can be prohibitive for some applications <Paper corpusId=\"264426339\" paperTitle=\"(Wang et al._1, 2023)\" isShortName></Paper>. As language models continue to improve and generate increasingly human-like text, the effectiveness of these classifiers is expected to decline unless detection techniques evolve accordingly <Paper corpusId=\"272987651\" paperTitle=\"(Feng et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Chakraborty et al., 2023)",
                        "snippets": [
                            "Traditional approaches. They involve statistical outlier detection methods, which employ statistical metrics such as entropy, perplexity, and n-gram frequency to differentiate between human and machine-generated texts (Lavergne et al., 2008)Gehrmann et al., 2019). However, with the advent of ChatGPT (OpenAI) (OpenAI, 2023), a new innovative statistical detection methodology, DetectGPT (Mitchell et al., 2023), has been developed. It operates on the principle that text generated by the model tends to lie in the negative curvature areas of the model's log probability. DetectGPT (Mitchell et al., 2023) generates and compares multiple perturbations of model-generated text to determine whether the text is machine-generated or not based on the log probability of the original text and the perturbed versions. DetectGPT significantly outperforms the majority of the existing zero-shot methods for model sample detection with very high AUC scores (note that we use the terms AUROC and AUC interchangeably for presentation convenience). \n\nClassifier-based detectors. In contrast to statistical methods, classifier-based detectors are common in natural language detection paradigms, particularly in fake news and misinformation detection (Schildhauer, 2022;Zou & Ling, 2021). OpenAI has recently fine-tuned a GPT model (OpenAI, 2023) using data from Wikipedia, WebText, and internal human demonstration data to create a web interface for a discrimination task using text generated by 34 language models. This approach combines a classifier-based approach with a human evaluation component to determine whether a given text was machine-generated or not. These recent advancements in the field of detecting AI-generated text have significant implications for detecting and preventing the spread of misinformation and fake news, thereby contributing to the betterment of society (Schildhauer, 2022;Zou & Ling, 2021;Kshetri & Voas, 2022). \n\nWatermark-based identification."
                        ],
                        "paper": {
                            "corpus_id": 258048481,
                            "title": "On the Possibilities of AI-Generated Text Detection",
                            "authors": [
                                {
                                    "authorId": "49081354",
                                    "name": "Souradip Chakraborty"
                                },
                                {
                                    "authorId": "3387859",
                                    "name": "A. S. Bedi"
                                },
                                {
                                    "authorId": "2110018762",
                                    "name": "Sicheng Zhu"
                                },
                                {
                                    "authorId": "49640821",
                                    "name": "Bang An"
                                },
                                {
                                    "authorId": "2172597446",
                                    "name": "Dinesh Manocha"
                                },
                                {
                                    "authorId": "2117426487",
                                    "name": "Furong Huang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 127
                        },
                        "score": 0.9658203125
                    },
                    {
                        "id": "(Yang et al., 2023)",
                        "snippets": [
                            "Recent advances in large language models (LLMs) have enabled them to reach human-level proficiency across numerous professional and academic tasks [24,(Ouyang et al., 2022)[33]. One of the most impressive examples is OpenAI's ChatGPT [23], which has demonstrated remarkable prowess in answering questions, composing emails, essays, and even generating code. However, this impressive ability to create human-like text with remarkable efficiency has ignited apprehension regarding the potential abuse of LLMs for malicious purposes [6,(2023)(Liebrenz et al., 2023)(Stokel-Walker, 2022), such as phishing, disinformation campaigns, and academic dishonesty",
                            "Therefore, detecting and authenticating generated text becomes crucial to ensure the responsible and secure use of LLMs.\n\nA prevalent solution is passive detection [1, 10,18,25,28,40], where a text classifier, usually fine-tuned on a pretrained language model like RoBERTa [16] and GPT-2 [30], is adopted to distinguish between generated and human-written text. However, these learning-based methods perform well only when the input data share a similar distribution with the training data, thereby limiting their applicability to specific domains.\n\nStatistical Discrepancy Detection. Several methods distinguish between generated and human-written text by identifying statistical discrepancies between them, as exemplified by two recent tools: GPTZero [40] and DetectGPT [18]. GPTZero uses perplexity and burstiness to tell apart human-written and generated text, as language models tend to produce more predictable and consistent text based on the patterns they learned from training data, resulting in lower perplexity scores for generated text. DetectGPT exploits the negative curvature regions of a model's log probability function to identify generated text by comparing the log probability of unperturbed and perturbed text variations. However, as language models are constantly improving and becoming more sophisticated, these heuristic features struggle to achieve robustness and generalization. Deep Learning-based Detection. Deep learning-based methods rely on gathering human-written and generated samples to train classifiers. Recently, OpenAI fine-tuned a GPT model for this discrimination task using a dataset comprising paired human and AI-generated texts on identical topics [25]. Similarly, Guo et al. [10] fine-tuned a text classifier based on pre-trained autoencoding LLMs (e.g., RoBERTa) by collecting the Human ChatGPT Comparison Corpus (HC3). Deep learning-based methods exhibit strong performance under the training data distribution, but they are susceptible to adversarial attacks, lack interpretability, and struggle to provide reliable judgments in human-AI collaboration scenarios. Watermark-based Detection. Kirchenbauer et al. [12] proposed the watermarking framework for white-box language models. The watermarking operates by randomly selecting a random set of \"greenlist\" words from the model's vocabulary and softly encouraging the use of these \"greenlist\" words by interfering with the sampling process at each generation step. The watermark can be detected by testing the following null hypothesis,  0 : The text sequence is generated with no knowledge of the selection rule of \"greenlist\" words."
                        ],
                        "paper": {
                            "corpus_id": 258714683,
                            "title": "Watermarking Text Generated by Black-Box Language Models",
                            "authors": [
                                {
                                    "authorId": "2143365293",
                                    "name": "Xi Yang"
                                },
                                {
                                    "authorId": "8780109",
                                    "name": "Kejiang Chen"
                                },
                                {
                                    "authorId": "51027868",
                                    "name": "Weiming Zhang"
                                },
                                {
                                    "authorId": "40997227",
                                    "name": "Chang-rui Liu"
                                },
                                {
                                    "authorId": "2217971466",
                                    "name": "Yuang Qi"
                                },
                                {
                                    "authorId": "2155863060",
                                    "name": "Jie Zhang"
                                },
                                {
                                    "authorId": "12464207",
                                    "name": "Han Fang"
                                },
                                {
                                    "authorId": "2052212945",
                                    "name": "Neng H. Yu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 62
                        },
                        "score": 0.97021484375
                    },
                    {
                        "id": "(Lu et al., 2023)",
                        "snippets": [
                            "In recent years, the research community has developed a wide range of detectors for AI-generated contents. In general, these detectors can be classified into three categories: training-based, statistical, and watermarking methods. Training-based methods treat the detection problem as a binary classification task, where neural networks are trained using AI-generated text and human-written text. Early studies utilized classifiers to identify fake reviews (Hovy, 2016) and fake news (Zellers et al., 2019). More recently, researchers have trained classifiers using text generated by LLMs, such as the GPT-3.5 detector (Guo et al., 2023) and GPT-2 detector (Solaiman et al., 2019). Statistical methods, on the other hand, focus on zero-shot detection without any additional training overhead. These methods seek to distinguish between human-written text and AI-generated text based on the statistical characteristics of text, such as the statistical irregularities in measures like entropy (Lavergne et al., 2008), perplexity (Beresneva, 2016) and token rank (Gehrmann et al., 2019). A recent method, DetectGPT (Mitchell et al., 2023), exploits the phenomenon that AI-generated text tends to lie in the negative curvature regions of log probability of text. The watermarking methods involve modifying the LLM's text generation process to imprint specific patterns on the generated text, such that it can be detected (Abdelnabi et al., 2020)Grinbaum & Adomaitis, 2022;Kirchenbauer et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 258762215,
                            "title": "Large Language Models can be Guided to Evade AI-Generated Text Detection",
                            "authors": [
                                {
                                    "authorId": "2147128045",
                                    "name": "Ning Lu"
                                },
                                {
                                    "authorId": "2152939552",
                                    "name": "Shengcai Liu"
                                },
                                {
                                    "authorId": "22272507",
                                    "name": "Ruidan He"
                                },
                                {
                                    "authorId": "2078692567",
                                    "name": "Ke Tang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 53
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Zellers et al., 2019)",
                        "snippets": [
                            "Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. \nModern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. \nDeveloping robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news."
                        ],
                        "paper": {
                            "corpus_id": 168169824,
                            "title": "Defending Against Neural Fake News",
                            "authors": [
                                {
                                    "authorId": "2545335",
                                    "name": "Rowan Zellers"
                                },
                                {
                                    "authorId": "14487640",
                                    "name": "Ari Holtzman"
                                },
                                {
                                    "authorId": "2516777",
                                    "name": "Hannah Rashkin"
                                },
                                {
                                    "authorId": "3312309",
                                    "name": "Yonatan Bisk"
                                },
                                {
                                    "authorId": "143787583",
                                    "name": "Ali Farhadi"
                                },
                                {
                                    "authorId": "3268360",
                                    "name": "Franziska Roesner"
                                },
                                {
                                    "authorId": "1699545",
                                    "name": "Yejin Choi"
                                }
                            ],
                            "year": 2019,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1030
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tang et al., 2023)",
                        "snippets": [
                            "Recent methods such as DetectGPT and GPTZero distinguish between machine-generated and human-written text by analyzing their statistical discrepancies (Tian, 2023;Mitchell et al., 2023). DetectGPT compares the log probability computed by a model on unperturbed text and perturbed variations, leveraging the observation that text sampled from a LLM generally occupy negative curvature regions of the model's log probability function. GPTZero instead uses perplexity and burstiness to distinguish human from machine text, with lower perplexity and burstiness indicating a greater likelihood of machine-generated text. However, these heuristics do not generalize and are often fallible.\n\nDetection by Learning Classifiers Several papers have proposed to train classifiers to distinguish between AI and human generated text. During the initial GPT-2 release, OpenAI trained a RoBERTa classifier to detect GPT-2 generated text with 95% accuracy (Solaiman et al., 2019). More recently, OpenAI fine-tuned a GPT model on a dataset of machine-generated and human texts focusing on the same topic, with a true positive identification rate of 26% (Ope-nAI, 2023). Similarly, Guo et al. (2023) collected the Human ChatGPT Comparison Corpuse (HC3) and fine-tuned RoBERTa for the detection task.\n\nNotably, the capabilities of such classifiers decrease as machine-generated text becomes increasingly human-like. Sadasivan et al. (2023) show theoretically that for sufficiently advanced language models, machine-generated text detectors offer only a marginal improvement over random classifiers. Moreover, such methods are prone to adversarial attacks and are not robust to out-of-distribution text.\n\nWatermarking Large Language Models An alternative to detecting of machine-generated text using statistical discrepancies and learned classifiers is the concept of watermarks. Watermarks are hidden patterns in machinegenerated text that are imperceptible to humans, but algorithmically identifiable as synthetic. Natural language watermarks long predate the development of LLMs, relying on methods such as synonym substitution, as well as syntactic and semantic transformations (Topkara et al., 2005)."
                        ],
                        "paper": {
                            "corpus_id": 258967971,
                            "title": "Baselines for Identifying Watermarked Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2144484107",
                                    "name": "Leonard Tang"
                                },
                                {
                                    "authorId": "2143997407",
                                    "name": "Gavin Uberti"
                                },
                                {
                                    "authorId": "2211332429",
                                    "name": "Tom Shlomi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.970703125
                    },
                    {
                        "id": "(Tao et al., 2024)",
                        "snippets": [
                            "Existing detection methods are categorized into metricbased and model-based approaches. Metric-based methods quantify linguistic features, such as stylometric analysis and perplexity scoring, to determine text origin. Gehrmann et al. [22] introduced GLTR, which uses metrics like word probability and entropy for detection, while Solaiman et al. [23] proposed a zero-shot detection method leveraging pre-trained models like GPT-2 or GROVER. However, as LLMs advance, these methods have become less effective. To address these limitations, more advanced and sophisticated detection techniques have emerged. Mitchell et al. (Mitchell et al., 2023) presented DetectGPT, a zero-shot method utilizing probability curvature analysis, and Tian et al. [25] developed the Multiscale Positive-Unlabeled (MPU) framework to enhance detection across varying text lengths. While metricbased approaches rely on linguistic analysis, model-based methods use advanced machine learning to classify text as human or AI-generated. These often involve fine-tuning language models or developing specialized architectures. For instance, Guo et al. [26] fine-tuned a RoBERTa model to detect ChatGPT-generated text, while Wang et al. [27] introduced SeqXGPT, a sentence-level method using logprobability lists and self-attention networks for enhanced detection. Liu et al. (Liu et al., 2023) proposed COCO, which improves detection in low-resource settings by combining an entity coherence graph with contrastive learning."
                        ],
                        "paper": {
                            "corpus_id": 273482129,
                            "title": "Unveiling Large Language Models Generated Texts: A Multi-Level Fine-Grained Detection Framework",
                            "authors": [
                                {
                                    "authorId": "2299329349",
                                    "name": "Zhen Tao"
                                },
                                {
                                    "authorId": "2279173209",
                                    "name": "Zhiyu Li"
                                },
                                {
                                    "authorId": "2506761",
                                    "name": "Runyu Chen"
                                },
                                {
                                    "authorId": "2128365150",
                                    "name": "Dinghao Xi"
                                },
                                {
                                    "authorId": "2301200175",
                                    "name": "Wei Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.978515625
                    },
                    {
                        "id": "(Cao, 2025)",
                        "snippets": [
                            "One popular approach leverages an external LLM to classify whether a piece of text is AI-generated or not [12]. With carefully crafted prompts, these zero-shot methods can achieve moderate accuracy quickly. However, sensitivity to prompt design and the adversarial gap between the LLM detector and the text generator are common issues [22].\n\n2) Linguistic and statistical signatures: Traditional stylometric features (e.g., function words, syntax complexity, average phrase length) have long been used in authorship attribution [23], [24]. More modern detection focuses on computing perplexity or log-likelihood using reference language models, observing that LLM-generated text tends to show distinctive probability distributions. Additionally, specialized white-box methods can measure rank ordering of tokens if the generating model is partially known [25].\n\n3) Supervised classification (training-based): Labeled corpora of AI versus human text enable fine-tuning of large pre-trained transformers like RoBERTa or T5 to discriminate synthetic text [26], [27]. Researchers improve robustness with adversarial training sets that contain paraphrased or AIgenerated passages shifted in style. Tools like GPTZero [28] and RADAR [29] exemplify advanced supervised detectors. However, assembling high-quality, representative training data remains a challenge, especially as new generator architectures emerge frequently.\n\n4) Watermarking for AI text: Cooperative watermarking modifies text generation at token selection time, embedding an imperceptible pattern in the distribution of words or punctuation [30], [31]. A verifier can detect such patterns after the fact. While promising for major industrial LLMs that adopt the standard, watermarking fails if malicious or open-source models do not embed it, or if paraphrasing disrupts the signal [32], [33]."
                        ],
                        "paper": {
                            "corpus_id": 277595847,
                            "title": "A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content",
                            "authors": [
                                {
                                    "authorId": "2354295905",
                                    "name": "Lele Cao"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.95458984375
                    },
                    {
                        "id": "(Hu et al., 2023)",
                        "snippets": [
                            "Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a robust AI-text detector via adversarial learning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic content to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5-Turbo."
                        ],
                        "paper": {
                            "corpus_id": 259501842,
                            "title": "RADAR: Robust AI-Text Detection via Adversarial Learning",
                            "authors": [
                                {
                                    "authorId": "2199229406",
                                    "name": "Xiaomeng Hu"
                                },
                                {
                                    "authorId": "2158177948",
                                    "name": "Pin-Yu Chen"
                                },
                                {
                                    "authorId": "2103197703",
                                    "name": "Tsung-Yi Ho"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 127
                        },
                        "score": 0
                    },
                    {
                        "id": "(Nguyen-Son et al., 2024)",
                        "snippets": [
                            "Detection of text generated by large language models uses three main techniques. Firstly, supervised learning methods (Solaiman et al., 2019;(Wang et al., 2023)(Hu et al., 2023)(Wu et al., 2023) train classifiers on datasets of original and generated text, though this requires large volumes of training data. The zero-shot approach (Bhattacharjee and Liu, 2023;(Mitchell et al., 2023) eliminates the need for training but is sensitive to out-of-distribution text. Recent research explores watermarking methodologies (Kirchenbauer et al., 2023) to force models to produce predefined words, aiding detection, but this requires modifying the models, which is impractical for proprietary models like ChatGPT."
                        ],
                        "paper": {
                            "corpus_id": 273901392,
                            "title": "SimLLM: Detecting Sentences Generated by Large Language Models Using Similarity between the Generation and its Re-generation",
                            "authors": [
                                {
                                    "authorId": "1403588130",
                                    "name": "Hoang-Quoc Nguyen-Son"
                                },
                                {
                                    "authorId": "2260782421",
                                    "name": "Minh-Son Dao"
                                },
                                {
                                    "authorId": "2238657258",
                                    "name": "Koji Zettsu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 2
                        },
                        "score": 0.95654296875
                    },
                    {
                        "id": "(Wang et al., 2023)",
                        "snippets": [
                            "Widely applied large language models (LLMs) can generate human-like content, raising concerns about the abuse of LLMs. Therefore, it is important to build strong AI-generated text (AIGT) detectors. Current works only consider document-level AIGT detection, therefore, in this paper, we first introduce a sentence-level detection challenge by synthesizing a dataset that contains documents that are polished with LLMs, that is, the documents contain sentences written by humans and sentences modified by LLMs. Then we propose \\textbf{Seq}uence \\textbf{X} (Check) \\textbf{GPT}, a novel method that utilizes log probability lists from white-box LLMs as features for sentence-level AIGT detection. These features are composed like \\textit{waves} in speech processing and cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution and self-attention networks. We test it in both sentence and document-level detection challenges. Experimental results show that previous methods struggle in solving sentence-level AIGT detection, while our method not only significantly surpasses baseline methods in both sentence and document-level detection challenges but also exhibits strong generalization capabilities."
                        ],
                        "paper": {
                            "corpus_id": 264128397,
                            "title": "SeqXGPT: Sentence-Level AI-Generated Text Detection",
                            "authors": [
                                {
                                    "authorId": "2199600227",
                                    "name": "Pengyu Wang"
                                },
                                {
                                    "authorId": "2107897400",
                                    "name": "Linyang Li"
                                },
                                {
                                    "authorId": "2258550235",
                                    "name": "Ke Ren"
                                },
                                {
                                    "authorId": "2258680333",
                                    "name": "Botian Jiang"
                                },
                                {
                                    "authorId": "2109797247",
                                    "name": "Dong Zhang"
                                },
                                {
                                    "authorId": "2188058565",
                                    "name": "Xipeng Qiu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 57
                        },
                        "score": 0
                    },
                    {
                        "id": "(Bitton et al., 2025)",
                        "snippets": [
                            "Classical approaches applied feature extraction on the texts in order to quantify stylistic or syntactic features such as perplexity, burstiness, and word density. These features are then fed into machine learning algorithms such as logistic regression or SVM (which provide an advantage of explainability), or into \"deep\" neural networks such as LSTMs. More modern approaches use pre-trained language models such as BERT, and fine-tune them for the specific classification task. This eliminates the need to manually define a list of features, and overall presents a much higher accuracy compared to feature-based methods, mainly because this approach is able to capture more general elements and nuances of the writing style, and is less dependent on a specific domain or topic. [20](Yadagiri et al., 2024)(Dathathri et al., 2024)(Kwon et al., 2025) Approaches that rely solely on capturing the differences between AI and human texts operate under the assumption that such differences exist; however, in reality, the differences between machine and human text may diminish over time as these systems evolve. A \"watermarking\" approach has recently emerged in an attempt to overcome these shortcomings. \"Watermarks\" can be added to the text during the generation phase (generative watermarking) or after it (edit-based watermarking). (Dathathri et al., 2024) However, this approach has its own limitations, such as lower effectiveness on factual responses (as there are fewer options to control the output), and higher sensitivity to small edits of the watermarked text."
                        ],
                        "paper": {
                            "corpus_id": 276773353,
                            "title": "Detecting Stylistic Fingerprints of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "1405506735",
                                    "name": "Yehonatan Bitton"
                                },
                                {
                                    "authorId": "2348468222",
                                    "name": "Elad Bitton"
                                },
                                {
                                    "authorId": "2348465980",
                                    "name": "Shai Nisan"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.94775390625
                    },
                    {
                        "id": "(Xiong et al., 2024)",
                        "snippets": [
                            "In the realm of distinguishing texts generated by large language models (LLMs) from those authored by humans, researchers have employed a variety of methods and tools. Broadly, these approaches can be categorized into two main types: black-box and white-box detection methods (Tang et al., 2023). Black-box detection relies on API-level access to LLMs, utilizing textual samples from both human and machine sources to train classification models (Dugan et al., 2020). The study by Guo et al. (2023) integrated existing question-and-answer datasets and leveraged fine-tuning of pre-trained models to investigate the characteristics and similarities between human-generated and AI-generated texts (Guo et al., 2023). \n\nIn the context of white-box detection, Kirchenbauer et al. ( 2023) introduced a novel approach involving the embedding of watermarks in the outputs of large language models to facilitate the detection of AI-generated text (Kirchenbauer et al., 2023). Additionally, a variety of tools and methodologies, including XGBoost, decision trees, and transformer-based models, have been evaluated for their efficacy in detecting texts produced by AI (Zaitsu and Jin, 2023). These techniques incorporate multiple stylistic measurement features to differentiate between AI-generated and human-generated texts (Shijaku and Canhasi, 2023). \n\nSpecific tools and techniques in this domain include the GLTR tool developed by Gehrmann et al. (2019), which analyzes the usage of rare words in texts to distinguish between those generated by the GPT-2 model and human writers (Gehrmann et al., 2019). The DetectGPT method posits that minor rewrites of LLM-generated texts tend to reduce the log probability under the model, a hypothesis that has been explored in depth (Mitchell et al., 2023). Furthermore, intrinsic dimension analysis, including methods like the Persistent Homology Dimension estimator (PHD), has been applied to distinguish between authentic texts and those generated artificially (Tulchinskii et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 267094724,
                            "title": "Fine-tuning Large Language Models for Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection",
                            "authors": [
                                {
                                    "authorId": "2280331364",
                                    "name": "Feng Xiong"
                                },
                                {
                                    "authorId": "52196310",
                                    "name": "Thanet Markchom"
                                },
                                {
                                    "authorId": "2280391458",
                                    "name": "Ziwei Zheng"
                                },
                                {
                                    "authorId": "2280666724",
                                    "name": "Subin Jung"
                                },
                                {
                                    "authorId": "2293393169",
                                    "name": "Varun Ojha"
                                },
                                {
                                    "authorId": "2281373610",
                                    "name": "Huizhi Liang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.97705078125
                    },
                    {
                        "id": "(Wang et al._1, 2023)",
                        "snippets": [
                            "Recent research has shown promising results in the development of detection methods. The existing detectors are built on the assumption that there is a distributional difference between human-generated texts and AI-generated texts. These differences are typically identified by training classifiers or using statistical information. \n\nClassifier-based detectors. Classifier-based detectors are commonly used in natural language processing detection paradigms, especially in fake news and misinformation detection [3]. Guo et al. [4] proposed the ChatGPT Detector, where they initially constructed a dataset consisting of ChatGPT conversations with human questions and answers, and trained a text detection classifier based on this dataset. The use of these methods requires substantial data collection and incurs the cost of training these classifier models. \n\nStatistical-based detectors. Statistical-based detectors utilize statistical metrics such as entropy, perplexity, and n-gram frequency to differentiate between human-generated and AI-generated texts [1]. \n\nSome other work is based on watermark-based detectors. In previous research, watermarks have been applied in the field of image processing and computer vision to protect copyrighted content and prevent intellectual property theft (Langelaar et al., 2000). \n\nRecently, with the emergence of ChatGPT, the work by Kirchenbauer et al. [7] demonstrated how to incorporate a watermark using only the logarithmic credentials of each step to mark AI-generated texts. While watermark-based detectors are an intriguing area of research, adding watermarks may affect the readability of the texts, and the removal of watermarks is also a challenge we need to address. \n\nHowever, with the emergence of ChatGPT, an innovative statistical detection method called DetectGPT [5] has been developed. Its principle is that text generated by the model typically resides in the negative curvature region of the model's log probability. DetectGPT [5] generates and compares multiple variants of model-generated texts to determine whether the texts are machine-generated based on the log probabilities of the original texts and these variants. \n\nDetectGPT [5] outperforms the vast majority of existing zero-shot methods in terms of model sample detection, achieving very high AUC. It is based on this concept that DetectGPT-SC was proposed."
                        ],
                        "paper": {
                            "corpus_id": 264426339,
                            "title": "DetectGPT-SC: Improving Detection of Text Generated by Large Language Models through Self-Consistency with Masked Predictions",
                            "authors": [
                                {
                                    "authorId": "2261313297",
                                    "name": "Rongsheng Wang"
                                },
                                {
                                    "authorId": "2261359496",
                                    "name": "Qi Li"
                                },
                                {
                                    "authorId": "2261347599",
                                    "name": "Sihong Xie"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.970703125
                    },
                    {
                        "id": "(Feng et al., 2024)",
                        "snippets": [
                            "AI Generated Text Identification. While large language models are applied to various scenarios with their increasingly powerful text generation capabilities, such as story generation (Yuan et al., 2022), article translation [59] and code assistance [53], the potential risks they bring [7,(Lee et al., 2022)(Yanagi et al., 2020) have made the identi- Direct detection methods often identify AI-generated text by uncovering distinguishing characteristics between AIgenerated and human-written texts or utilizing an extensive dataset of AI-generated text to train a accurate text classifier for AI-generated content. For example, regarding the first kind of method, Hamed and Wu [1] discovered that the bigram similarity of texts generated by ChatGPT is higher than that of human-written texts and developed a corresponding detection method based on this finding. Mitchell et al. found that AIgenerated texts tend to occupy the negative curvature region of the model's log-probability function. Based on this observation they proposed DetectGPT (Mitchell et al., 2023). Building on Mitchell et al.'s approach, Bao et al. [2] improved the perturbation step in their method, significantly enhancing both its accuracy and speed. However, with the rapid advancement in both the size of model and text generation capabilities of LLMs, the gap between AI-generated texts and human-written texts has been narrowing [16,35,38,(Pack et al., 2024). As a result, methods based on text characteristics are becoming increasingly less effective.For the second type of method, Mindner et al. (Mindner et al., 2023) employed multidimensional text feature extraction approaches to construct a classifier, with the best classifier outperforming GPTZero [14] in terms of F1-score. Chen et al. [6] and Liu et al. [32] utilized the advanced language understanding capabilities of pretrained LLMs [33](Raffel et al., 2019), finetuning them as binary classifiers on various text datasets for AI-generated text detection. While these methods perform well on their respective test datasets, their effectiveness may be limited when applied to texts generated by newly emerging models. Likewise, as the capabilities of large language models continue to advance, their effectiveness remains a question. \n\nWatermarking, as an alternative method for AI-generated text identification, is generally more effective, versatile, and interpretable compared to direct detection."
                        ],
                        "paper": {
                            "corpus_id": 272987651,
                            "title": "A Certified Robust Watermark For Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2323563488",
                                    "name": "Xianheng Feng"
                                },
                                {
                                    "authorId": "2279667983",
                                    "name": "Jian Liu"
                                },
                                {
                                    "authorId": "2249553637",
                                    "name": "Kui Ren"
                                },
                                {
                                    "authorId": "2331522408",
                                    "name": "Chun Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.95361328125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Watermarking Techniques",
                "tldr": "Watermarking embeds invisible signals in AI-generated text that can be algorithmically detected while remaining imperceptible to humans. These techniques include vocabulary partitioning into \"green/red\" lists, semantic embedding approaches, and rejection sampling methods that offer more robust detection than classifier-based approaches. (16 sources)",
                "text": "\nWatermarking has emerged as a proactive approach to AI-generated text detection that fundamentally differs from classifier and zero-shot methods. Rather than attempting to identify statistical patterns in already-generated text, watermarking embeds detectable signals directly during the text generation process <Paper corpusId=\"258714683\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>. This technique creates text with embedded patterns that remain invisible to human readers but can be algorithmically identified with high confidence <Paper corpusId=\"268819799\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nThe most influential watermarking framework, introduced by Kirchenbauer et al., operates by partitioning the model's vocabulary into \"greenlist\" and \"redlist\" tokens based on a cryptographic hash of previously generated content <Paper corpusId=\"258987266\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"256194179\" paperTitle=\"(Kirchenbauer et al., 2023)\" isShortName></Paper>. During generation, the model softly encourages the selection of \"greenlist\" tokens, creating a statistical pattern that can be detected through hypothesis testing with interpretable p-values <Paper corpusId=\"260351507\" paperTitle=\"(Fernandez et al., 2023)\" isShortName></Paper>. This approach requires no access to the language model's API or parameters for detection and has minimal impact on text quality <Paper corpusId=\"273849981\" paperTitle=\"(Lau et al., 2024)\" isShortName></Paper> <Paper corpusId=\"256194179\" paperTitle=\"(Kirchenbauer et al., 2023)\" isShortName></Paper>.\n\nWatermarking techniques generally fall into two categories: token sampling biasing and token probability biasing <Paper corpusId=\"273901404\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>. The first approach forces models to select only \"greenlist\" tokens during generation, while the second introduces a bias to the probability distribution to softly encourage generating \"greenlist\" tokens while preserving semantic richness <Paper corpusId=\"273901404\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>. Methods like NS-Mark constrain biasing frequency, while DiPMark approximates the biased probability distribution to the original to minimize impact on text quality <Paper corpusId=\"273901404\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.\n\nMore recent watermarking innovations have focused on improving robustness against paraphrasing attacks, which represent a significant challenge to token-level watermarks <Paper corpusId=\"277595847\" paperTitle=\"(Cao, 2025)\" isShortName></Paper> <Paper corpusId=\"263831179\" paperTitle=\"(Hou et al., 2024)\" isShortName></Paper>. SemStamp, for example, implements semantic-level watermarking using locality-sensitive hashing to partition the semantic space of sentences, showing greater resistance to paraphrase attacks than token-level approaches <Paper corpusId=\"263831179\" paperTitle=\"(Hou et al., 2024)\" isShortName></Paper>. Similarly, SimMark leverages the similarity of semantic sentence embeddings and rejection sampling to impose detectable patterns that remain robust against paraphrasing <Paper corpusId=\"276116307\" paperTitle=\"(Dabiriaghdam et al., 2025)\" isShortName></Paper>.\n\nOther notable watermarking techniques include DeepTextMark, which applies Word2Vec and sentence encoding for watermark insertion and uses a transformer-based classifier for detection <Paper corpusId=\"258588289\" paperTitle=\"(Munyer et al., 2023)\" isShortName></Paper>, and the Adversarial Watermarking Transformer (AWT), which can embed binary messages into text while minimizing changes to semantics and correctness <Paper corpusId=\"258762215\" paperTitle=\"(Lu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"221516138\" paperTitle=\"(Abdelnabi et al., 2020)\" isShortName></Paper>.\n\nDespite their effectiveness, watermarking techniques face several limitations. First, they can only be implemented by the developers of language models, meaning detection depends on model creators choosing to incorporate watermarks <Paper corpusId=\"261697402\" paperTitle=\"(Anand, 2023)\" isShortName></Paper>. Second, watermarks can potentially be removed through text manipulation techniques like paraphrasing, word insertion/deletion, or sentence reordering <Paper corpusId=\"261697402\" paperTitle=\"(Anand, 2023)\" isShortName></Paper>. Finally, there is an inherent trade-off between watermark robustness and text quality, as stronger watermarks may impact the naturalness and fluidity of the generated content <Paper corpusId=\"273901404\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.\n\nOverall, watermarking represents a promising approach for AI-generated text detection that complements classifier-based and zero-shot methods. As language models continue to improve and generate increasingly human-like text, watermarking provides a more reliable detection mechanism that works regardless of how natural the text appears <Paper corpusId=\"273228395\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Yang et al., 2023)",
                        "snippets": [
                            "Recent advances in large language models (LLMs) have enabled them to reach human-level proficiency across numerous professional and academic tasks [24,(Ouyang et al., 2022)[33]. One of the most impressive examples is OpenAI's ChatGPT [23], which has demonstrated remarkable prowess in answering questions, composing emails, essays, and even generating code. However, this impressive ability to create human-like text with remarkable efficiency has ignited apprehension regarding the potential abuse of LLMs for malicious purposes [6,(2023)(Liebrenz et al., 2023)(Stokel-Walker, 2022), such as phishing, disinformation campaigns, and academic dishonesty",
                            "Therefore, detecting and authenticating generated text becomes crucial to ensure the responsible and secure use of LLMs.\n\nA prevalent solution is passive detection [1, 10,18,25,28,40], where a text classifier, usually fine-tuned on a pretrained language model like RoBERTa [16] and GPT-2 [30], is adopted to distinguish between generated and human-written text. However, these learning-based methods perform well only when the input data share a similar distribution with the training data, thereby limiting their applicability to specific domains.\n\nStatistical Discrepancy Detection. Several methods distinguish between generated and human-written text by identifying statistical discrepancies between them, as exemplified by two recent tools: GPTZero [40] and DetectGPT [18]. GPTZero uses perplexity and burstiness to tell apart human-written and generated text, as language models tend to produce more predictable and consistent text based on the patterns they learned from training data, resulting in lower perplexity scores for generated text. DetectGPT exploits the negative curvature regions of a model's log probability function to identify generated text by comparing the log probability of unperturbed and perturbed text variations. However, as language models are constantly improving and becoming more sophisticated, these heuristic features struggle to achieve robustness and generalization. Deep Learning-based Detection. Deep learning-based methods rely on gathering human-written and generated samples to train classifiers. Recently, OpenAI fine-tuned a GPT model for this discrimination task using a dataset comprising paired human and AI-generated texts on identical topics [25]. Similarly, Guo et al. [10] fine-tuned a text classifier based on pre-trained autoencoding LLMs (e.g., RoBERTa) by collecting the Human ChatGPT Comparison Corpus (HC3). Deep learning-based methods exhibit strong performance under the training data distribution, but they are susceptible to adversarial attacks, lack interpretability, and struggle to provide reliable judgments in human-AI collaboration scenarios. Watermark-based Detection. Kirchenbauer et al. [12] proposed the watermarking framework for white-box language models. The watermarking operates by randomly selecting a random set of \"greenlist\" words from the model's vocabulary and softly encouraging the use of these \"greenlist\" words by interfering with the sampling process at each generation step. The watermark can be detected by testing the following null hypothesis,  0 : The text sequence is generated with no knowledge of the selection rule of \"greenlist\" words."
                        ],
                        "paper": {
                            "corpus_id": 258714683,
                            "title": "Watermarking Text Generated by Black-Box Language Models",
                            "authors": [
                                {
                                    "authorId": "2143365293",
                                    "name": "Xi Yang"
                                },
                                {
                                    "authorId": "8780109",
                                    "name": "Kejiang Chen"
                                },
                                {
                                    "authorId": "51027868",
                                    "name": "Weiming Zhang"
                                },
                                {
                                    "authorId": "40997227",
                                    "name": "Chang-rui Liu"
                                },
                                {
                                    "authorId": "2217971466",
                                    "name": "Yuang Qi"
                                },
                                {
                                    "authorId": "2155863060",
                                    "name": "Jie Zhang"
                                },
                                {
                                    "authorId": "12464207",
                                    "name": "Han Fang"
                                },
                                {
                                    "authorId": "2052212945",
                                    "name": "Neng H. Yu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 62
                        },
                        "score": 0.97021484375
                    },
                    {
                        "id": "(Tang et al., 2023)",
                        "snippets": [
                            "Recent methods such as DetectGPT and GPTZero distinguish between machine-generated and human-written text by analyzing their statistical discrepancies (Tian, 2023;Mitchell et al., 2023). DetectGPT compares the log probability computed by a model on unperturbed text and perturbed variations, leveraging the observation that text sampled from a LLM generally occupy negative curvature regions of the model's log probability function. GPTZero instead uses perplexity and burstiness to distinguish human from machine text, with lower perplexity and burstiness indicating a greater likelihood of machine-generated text. However, these heuristics do not generalize and are often fallible.\n\nDetection by Learning Classifiers Several papers have proposed to train classifiers to distinguish between AI and human generated text. During the initial GPT-2 release, OpenAI trained a RoBERTa classifier to detect GPT-2 generated text with 95% accuracy (Solaiman et al., 2019). More recently, OpenAI fine-tuned a GPT model on a dataset of machine-generated and human texts focusing on the same topic, with a true positive identification rate of 26% (Ope-nAI, 2023). Similarly, Guo et al. (2023) collected the Human ChatGPT Comparison Corpuse (HC3) and fine-tuned RoBERTa for the detection task.\n\nNotably, the capabilities of such classifiers decrease as machine-generated text becomes increasingly human-like. Sadasivan et al. (2023) show theoretically that for sufficiently advanced language models, machine-generated text detectors offer only a marginal improvement over random classifiers. Moreover, such methods are prone to adversarial attacks and are not robust to out-of-distribution text.\n\nWatermarking Large Language Models An alternative to detecting of machine-generated text using statistical discrepancies and learned classifiers is the concept of watermarks. Watermarks are hidden patterns in machinegenerated text that are imperceptible to humans, but algorithmically identifiable as synthetic. Natural language watermarks long predate the development of LLMs, relying on methods such as synonym substitution, as well as syntactic and semantic transformations (Topkara et al., 2005)."
                        ],
                        "paper": {
                            "corpus_id": 258967971,
                            "title": "Baselines for Identifying Watermarked Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2144484107",
                                    "name": "Leonard Tang"
                                },
                                {
                                    "authorId": "2143997407",
                                    "name": "Gavin Uberti"
                                },
                                {
                                    "authorId": "2211332429",
                                    "name": "Tom Shlomi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.970703125
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart."
                        ],
                        "paper": {
                            "corpus_id": 268819799,
                            "title": "A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules",
                            "authors": [
                                {
                                    "authorId": "2294777336",
                                    "name": "Xiang Li"
                                },
                                {
                                    "authorId": "2294359739",
                                    "name": "Feng Ruan"
                                },
                                {
                                    "authorId": "2294380671",
                                    "name": "Huiyuan Wang"
                                },
                                {
                                    "authorId": "2055685346",
                                    "name": "Qi Long"
                                },
                                {
                                    "authorId": "2288099064",
                                    "name": "Weijie J. Su"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 22
                        },
                        "score": 0.96044921875
                    },
                    {
                        "id": "(Shi et al., 2023)",
                        "snippets": [
                            "Recent detectors for AI-generated text mostly fall into three categories. First, classifier-based detectors are trained with labeled data to distinguish human-written text and AI-generated text. For example, the AI Text Classifier developed by OpenAI (OpenAI, 2023a) is a fine-tuned language model. Second, watermarking methods introduce distinct patterns into AI-generated text, allowing for its identification. Among them, (Kirchenbauer et al., 2023) randomly partition the vocabulary into a greenlist and a redlist during the generation, where the division is based on the hash of the previously generated tokens. The language model only uses words in the greenlists, and thereby the generated text has a different pattern compared to human-written text which does not consider such greenlists and redlists. Third, De-tectGPT (Mitchell et al., 2023) uses the likelihood of the generated text for the detection, as they find that text generated by language models tends to reside in the negative curvature region of the log probability function. Consequently, they define a curvature-based criterion for the detection."
                        ],
                        "paper": {
                            "corpus_id": 258987266,
                            "title": "Red Teaming Language Model Detectors with Language Models",
                            "authors": [
                                {
                                    "authorId": "2987927",
                                    "name": "Zhouxing Shi"
                                },
                                {
                                    "authorId": "2108927851",
                                    "name": "Yihan Wang"
                                },
                                {
                                    "authorId": "2065089223",
                                    "name": "Fan Yin"
                                },
                                {
                                    "authorId": "2143737082",
                                    "name": "Xiangning Chen"
                                },
                                {
                                    "authorId": "2782886",
                                    "name": "Kai-Wei Chang"
                                },
                                {
                                    "authorId": "1793529",
                                    "name": "Cho-Jui Hsieh"
                                }
                            ],
                            "year": 2023,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 56
                        },
                        "score": 0.9453125
                    },
                    {
                        "id": "(Kirchenbauer et al., 2023)",
                        "snippets": [
                            "Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of\"green\"tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security."
                        ],
                        "paper": {
                            "corpus_id": 256194179,
                            "title": "A Watermark for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2166053502",
                                    "name": "John Kirchenbauer"
                                },
                                {
                                    "authorId": "8284185",
                                    "name": "Jonas Geiping"
                                },
                                {
                                    "authorId": "123191916",
                                    "name": "Yuxin Wen"
                                },
                                {
                                    "authorId": "143975296",
                                    "name": "Jonathan Katz"
                                },
                                {
                                    "authorId": "2679804",
                                    "name": "Ian Miers"
                                },
                                {
                                    "authorId": "1962083",
                                    "name": "T. Goldstein"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 507
                        },
                        "score": 0
                    },
                    {
                        "id": "(Fernandez et al., 2023)",
                        "snippets": [
                            "Discerning between generated and natural texts is increasingly challenging. In this context, watermarking emerges as a promising technique for ascribing text to a specific generative model. It alters the sampling generation process to leave an invisible trace in the output, facilitating later detection. This research consolidates watermarks for large language models based on three theoretical and empirical considerations. First, we introduce new statistical tests that offer robust theoretical guarantees which remain valid even at low false-positive rates (less than 10\u22126). Second, we compare the effectiveness of watermarks using classical benchmarks in the field of natural language processing, gaining insights into their real-world applicability. Third, we develop advanced detection schemes for scenarios where access to the LLM is available, as well as multi-bit watermarking."
                        ],
                        "paper": {
                            "corpus_id": 260351507,
                            "title": "Three Bricks to Consolidate Watermarks for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2147013351",
                                    "name": "Pierre Fernandez"
                                },
                                {
                                    "authorId": "2129106958",
                                    "name": "Antoine Chaffin"
                                },
                                {
                                    "authorId": "2144093662",
                                    "name": "Karim Tit"
                                },
                                {
                                    "authorId": "2686030",
                                    "name": "Vivien Chappelier"
                                },
                                {
                                    "authorId": "1775704",
                                    "name": "T. Furon"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Workshop on Information Forensics and Security",
                            "n_citations": 55
                        },
                        "score": 0.9736328125
                    },
                    {
                        "id": "(Lau et al., 2024)",
                        "snippets": [
                            "Currently, there are 2 major streams of LLM-generated text detectors: (i) zero-shot classifiers [25,33], which identify LLM-generated text based on the pattern and characteristics of the input, and (ii) watermark detectors [17], which rely on detecting the presence of watermarks which are imprinted into the text during the generation process (Kirchenbauer et al., 2023), and are effective in the cases where the watermarks have been added by the LLM. The detectors then examine the input, classifying it as LLM-generated if the level of watermarking exceeds a set threshold, or as human-generated otherwise."
                        ],
                        "paper": {
                            "corpus_id": 273849981,
                            "title": "Understanding the Effects of Human-written Paraphrases in LLM-generated Text Detection",
                            "authors": [
                                {
                                    "authorId": "2329369337",
                                    "name": "Hiu Ting Lau"
                                },
                                {
                                    "authorId": "2805349",
                                    "name": "A. Zubiaga"
                                }
                            ],
                            "year": 2024,
                            "venue": "Natural Language Processing Journal",
                            "n_citations": 1
                        },
                        "score": 0.9638671875
                    },
                    {
                        "id": "(Guo et al., 2024)",
                        "snippets": [
                            "Watermarking enables people to determine whether the text is generated by a specific model. It injects a unique signature based on the \"green-red\" list that can be tracked during detection, where the words in green lists are encouraged to be generated. Recent researchers propose to fix the green/red lists or increase the proportion of green tokens to defend against paraphrasing attacks. However, these methods cause degradation of text quality due to semantic disparities between the watermarked text and the unwatermarked text.\n\nCurrent watermark methods mainly inject the signature according to \"green-red list\" (Ren et al., 2023): they divide vocabulary into green/red lists, regard words in green lists as the unique signature, and encourage to generate green tokens, which is easy to be recognized. The methods can be divided into two categories: (1) Token sampling biasing-based watermark forces LLMs to select only green tokens during generation. EXP-Edit (Kuditipudi et al., 2023) intervenes in the sampling process of each token. However, forcing LLMs to sample green tokens restricts the semantic richness of LLM-generated text, thus undermining its text quality. (2) To improve the generation quality, researchers further propose token probability biasing-based watermark, which enriches the semantics of watermarked texts by introducing a bias to the probability distribution to softly encourage generating green tokens instead of restricting to select green tokens. Takezawa et al. (2023) proposed NS-Mark to constrain the frequency of biasing. Wu et al. (2023) introduced DiPMark to approximate the biased probability distribution to the original one. These methods mitigate the impact of biasing on text quality and ensure superior text quality."
                        ],
                        "paper": {
                            "corpus_id": 273901404,
                            "title": "Context-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2330513747",
                                    "name": "Yuxuan Guo"
                                },
                                {
                                    "authorId": "2260832680",
                                    "name": "Zhiliang Tian"
                                },
                                {
                                    "authorId": "2261069334",
                                    "name": "Yiping Song"
                                },
                                {
                                    "authorId": "2329747280",
                                    "name": "Tianlun Liu"
                                },
                                {
                                    "authorId": "46573238",
                                    "name": "Liang Ding"
                                },
                                {
                                    "authorId": "2163335292",
                                    "name": "Dongsheng Li"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 2
                        },
                        "score": 0.9638671875
                    },
                    {
                        "id": "(Cao, 2025)",
                        "snippets": [
                            "One popular approach leverages an external LLM to classify whether a piece of text is AI-generated or not [12]. With carefully crafted prompts, these zero-shot methods can achieve moderate accuracy quickly. However, sensitivity to prompt design and the adversarial gap between the LLM detector and the text generator are common issues [22].\n\n2) Linguistic and statistical signatures: Traditional stylometric features (e.g., function words, syntax complexity, average phrase length) have long been used in authorship attribution [23], [24]. More modern detection focuses on computing perplexity or log-likelihood using reference language models, observing that LLM-generated text tends to show distinctive probability distributions. Additionally, specialized white-box methods can measure rank ordering of tokens if the generating model is partially known [25].\n\n3) Supervised classification (training-based): Labeled corpora of AI versus human text enable fine-tuning of large pre-trained transformers like RoBERTa or T5 to discriminate synthetic text [26], [27]. Researchers improve robustness with adversarial training sets that contain paraphrased or AIgenerated passages shifted in style. Tools like GPTZero [28] and RADAR [29] exemplify advanced supervised detectors. However, assembling high-quality, representative training data remains a challenge, especially as new generator architectures emerge frequently.\n\n4) Watermarking for AI text: Cooperative watermarking modifies text generation at token selection time, embedding an imperceptible pattern in the distribution of words or punctuation [30], [31]. A verifier can detect such patterns after the fact. While promising for major industrial LLMs that adopt the standard, watermarking fails if malicious or open-source models do not embed it, or if paraphrasing disrupts the signal [32], [33]."
                        ],
                        "paper": {
                            "corpus_id": 277595847,
                            "title": "A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content",
                            "authors": [
                                {
                                    "authorId": "2354295905",
                                    "name": "Lele Cao"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.95458984375
                    },
                    {
                        "id": "(Hou et al., 2024)",
                        "snippets": [
                            "Existing watermarked generation algorithms employ token-level designs and therefore, are vulnerable to paraphrase attacks. To address this issue, we introduce watermarking on the semantic representation of sentences. We propose SemStamp, a robust sentence-level semantic watermarking algorithm that uses locality-sensitive hashing (LSH) to partition the semantic space of sentences. The algorithm encodes and LSH-hashes a candidate sentence generated by a language model, and conducts rejection sampling until the sampled sentence falls in watermarked partitions in the semantic embedding space. To test the paraphrastic robustness of watermarking algorithms, we propose a \"bigram paraphrase\" attack that produces paraphrases with small bigram overlap with the original sentence. This attack is shown to be effective against existing token-level watermark algorithms, while posing only minor degradations to SemStamp. Experimental results show that our novel semantic watermark algorithm is not only more robust than the previous state-of-the-art method on various paraphrasers and domains, but also better at preserving the quality of generation."
                        ],
                        "paper": {
                            "corpus_id": 263831179,
                            "title": "SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation",
                            "authors": [
                                {
                                    "authorId": "2257002716",
                                    "name": "A. Hou"
                                },
                                {
                                    "authorId": "2167508843",
                                    "name": "Jingyu (Jack) Zhang"
                                },
                                {
                                    "authorId": "2249540815",
                                    "name": "Tianxing He"
                                },
                                {
                                    "authorId": "2125062703",
                                    "name": "Yichen Wang"
                                },
                                {
                                    "authorId": "2257002712",
                                    "name": "Yung-Sung Chuang"
                                },
                                {
                                    "authorId": "2257103558",
                                    "name": "Hongwei Wang"
                                },
                                {
                                    "authorId": "2121272448",
                                    "name": "Lingfeng Shen"
                                },
                                {
                                    "authorId": "7536576",
                                    "name": "Benjamin Van Durme"
                                },
                                {
                                    "authorId": "1783281",
                                    "name": "Daniel Khashabi"
                                },
                                {
                                    "authorId": "2249583325",
                                    "name": "Yulia Tsvetkov"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 66
                        },
                        "score": 0
                    },
                    {
                        "id": "(Dabiriaghdam et al., 2025)",
                        "snippets": [
                            "The rapid proliferation of large language models (LLMs) has created an urgent need for reliable methods to detect whether a text is generated by such models. In this paper, we propose SimMark, a posthoc watermarking algorithm that makes LLMs' outputs traceable without requiring access to the model's internal logits, enabling compatibility with a wide range of LLMs, including API-only models. By leveraging the similarity of semantic sentence embeddings and rejection sampling to impose detectable statistical patterns imperceptible to humans, and employing a soft counting mechanism, SimMark achieves robustness against paraphrasing attacks."
                        ],
                        "paper": {
                            "corpus_id": 276116307,
                            "title": "SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2210267013",
                                    "name": "Amirhossein Dabiriaghdam"
                                },
                                {
                                    "authorId": "2294899071",
                                    "name": "Lele Wang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.98388671875
                    },
                    {
                        "id": "(Munyer et al., 2023)",
                        "snippets": [
                            "Several related works have attempted to solve this problem using binary classifiers that categorize input text as human-written or LLM-generated. However, these classifiers have been shown to be unreliable. As impactful decisions could be made based on the result of the classification, the text source detection needs to be high-quality. To this end, this paper presents DeepTextMark, a deep learning-based text watermarking method for text source detection. Applying Word2Vec and Sentence Encoding for watermark insertion and a transformer-based classifier for watermark detection, DeepTextMark achieves blindness, robustness, imperceptibility, and reliability simultaneously."
                        ],
                        "paper": {
                            "corpus_id": 258588289,
                            "title": "DeepTextMark: Deep Learning based Text Watermarking for Detection of Large Language Model Generated Text",
                            "authors": [
                                {
                                    "authorId": "2106504469",
                                    "name": "Travis J. E. Munyer"
                                },
                                {
                                    "authorId": "2113310369",
                                    "name": "Xin Zhong"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 32
                        },
                        "score": 0.96337890625
                    },
                    {
                        "id": "(Lu et al., 2023)",
                        "snippets": [
                            "In recent years, the research community has developed a wide range of detectors for AI-generated contents. In general, these detectors can be classified into three categories: training-based, statistical, and watermarking methods. Training-based methods treat the detection problem as a binary classification task, where neural networks are trained using AI-generated text and human-written text. Early studies utilized classifiers to identify fake reviews (Hovy, 2016) and fake news (Zellers et al., 2019). More recently, researchers have trained classifiers using text generated by LLMs, such as the GPT-3.5 detector (Guo et al., 2023) and GPT-2 detector (Solaiman et al., 2019). Statistical methods, on the other hand, focus on zero-shot detection without any additional training overhead. These methods seek to distinguish between human-written text and AI-generated text based on the statistical characteristics of text, such as the statistical irregularities in measures like entropy (Lavergne et al., 2008), perplexity (Beresneva, 2016) and token rank (Gehrmann et al., 2019). A recent method, DetectGPT (Mitchell et al., 2023), exploits the phenomenon that AI-generated text tends to lie in the negative curvature regions of log probability of text. The watermarking methods involve modifying the LLM's text generation process to imprint specific patterns on the generated text, such that it can be detected (Abdelnabi et al., 2020)Grinbaum & Adomaitis, 2022;Kirchenbauer et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 258762215,
                            "title": "Large Language Models can be Guided to Evade AI-Generated Text Detection",
                            "authors": [
                                {
                                    "authorId": "2147128045",
                                    "name": "Ning Lu"
                                },
                                {
                                    "authorId": "2152939552",
                                    "name": "Shengcai Liu"
                                },
                                {
                                    "authorId": "22272507",
                                    "name": "Ruidan He"
                                },
                                {
                                    "authorId": "2078692567",
                                    "name": "Ke Tang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 53
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Abdelnabi et al., 2020)",
                        "snippets": [
                            "Recent advances in natural language generation have introduced powerful language models with high-quality output text. However, this raises concerns about the potential misuse of such models for malicious purposes. In this paper, we study natural language watermarking as a defense to help better mark and trace the provenance of text. We introduce the Adversarial Watermarking Transformer (AWT) with a jointly trained encoder-decoder and adversarial training that, given an input text and a binary message, generates an output text that is unobtrusively encoded with the given message. We further study different training and inference strategies to achieve minimal changes to the semantics and correctness of the input text.AWT is the first end-to-end model to hide data in text by automatically learning -without ground truth- word substitutions along with their locations in order to encode the message. We empirically show that our model is effective in largely preserving text utility and decoding the watermark while hiding its presence against adversaries. Additionally, we demonstrate that our method is robust against a range of attacks."
                        ],
                        "paper": {
                            "corpus_id": 221516138,
                            "title": "Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding",
                            "authors": [
                                {
                                    "authorId": "1383113350",
                                    "name": "Sahar Abdelnabi"
                                },
                                {
                                    "authorId": "1739548",
                                    "name": "Mario Fritz"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE Symposium on Security and Privacy",
                            "n_citations": 152
                        },
                        "score": 0
                    },
                    {
                        "id": "(Anand, 2023)",
                        "snippets": [
                            "Watermarking in text generation is a technique that introduces hidden signals into generated text, which can be used to detect whether the text is artificial. In this approach, the language model generates a probability distribution for the next word based on a given sequence of words. The next word is then randomly selected from this distribution. The watermarking technique operates by pseudo-randomly sampling words from the distribution, thereby enabling the detection of text generated by the language model.\n\nIn a recent study conducted by Kirchenbauer et al. [33], the effectiveness of the watermarking technique was evaluated on a multi-billion parameter model from the Open Pre-trained Transformer (OPT) family. The proposed method involved the selection of randomized sets of green tokens (allowed tokens) and red tokens (restricted tokens) during the generation process. The use of green tokens was promoted while sampling, while red tokens were discouraged. This approach facilitated the detection of artificial text by counting the occurrences of red tokens. In a similar direction, Ope-nAI has also expressed intentions to incorporate watermarking into its language models.\n\nAs outlined in a blog post by Scott Aaronson 6 , OpenAI plans to use cryptographic pseudo-random functions to sample words and generate detectors using cryptographic keys. This approach aims to embed watermarks into the language model, allowing for the identification of artificial text.\n\nIn practice, evading watermarking detection in language models can be achieved through various techniques, such as inserting or deleting words randomly, rearranging sentence order, or paraphrasing text. These methods pose challenges to the effectiveness of watermarking as a detection mechanism. Additionally, the responsibility of embedding watermarking lies with the developer of the language model, and only they have the ability to build the corresponding detection models. This limitation means that watermarking can only mitigate the potential harms of language models to a certain extent, as it relies on the actions and choices of the developer."
                        ],
                        "paper": {
                            "corpus_id": 261697402,
                            "title": "Studying the impacts of pre-training using ChatGPT-generated text on downstream tasks",
                            "authors": [
                                {
                                    "authorId": "2313731991",
                                    "name": "Sarthak Anand"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.95166015625
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "As Large Language Models (LLMs) become increasingly sophisticated, they raise significant security concerns, including the creation of fake news and academic misuse. Most detectors for identifying model-generated text are limited by their reliance on variance in perplexity and burstiness, and they require substantial computational resources.\n\nNumerous open-source and commercial tools have been developed to detect model-generated text, such as GPT2-Detector [29], RoBERTa-QA [31], DetectGPT [30], GPTZero [32], Writer [33], and others. However, there are three significant limitations to these methods. First, their probability predictions cannot provide conclusive certainty and should only be regarded as a reference. Second, most current approaches rely on methods that differentiate human text from modelgenerated text based on variations in perplexity and burstiness. Third, methods like DetectGPT and RoBERTa-QA, which require multiple text-generation process and model retraining, consume substantial computational resources and time when detecting model-generated text. As LLMs advance, the generated text increasingly resembles human writing, making these methods based on variance in perplexity between human and model-generated text less applicable. Therefore, embedding watermarks in text generated by LLMs presents a faster, more stable, and reliable method of verification.\n\nRecent advancements in watermarking techniques for language models have been crucial in ensuring text authenticity and model protection. Abdelnabi and Fritz [20] pioneered the Adversarial Watermarking Transformer (AWT), marking a significant step in embedding binary messages into text and demonstrating resilience against multiple attacks. He et al. [21] developed CATER, a conditional watermarking framework that secures text generation APIs against imitation attacks while preserving word distribution integrity. Zhao, Li, and Wang [22] introduced Distillation-Resistant Watermarking (DRW), a method effective in protecting NLP models from distillation theft and maintaining accuracy. Kirchenbauer et al. [23] proposed a framework for embedding watermarks in language models that are detectable with minimal impact on text quality. In a similar vein, Zhao, Wang, and Li's GINSEW technique [24] innovatively embeds secret signals in the probability vector of decoding steps, enhancing IP infringement detection. Peng et al. [25] developed EmbMarker for EaaS, embedding watermarks in trigger words for effective copyright verification. Yoo et al. [26] explored robust multi-bit watermarking, focusing on error resistance and enhanced payload. Lastly, Li et al. [27] introduced a novel strategy for embedding watermarks in the weight quantization process of large language models, ensuring model security without compromising operational modes.\n\nThe advancement in language models has prompted the development of various detection tools for identifying model-generated text. Gehrmann [28] developed GLTR, a tool to assist non-experts in detecting texts generated by models. It uses statistical methods to identify generation artifacts and has been shown to improve human detection rates of fake text significantly. GPT2-Detector developed by OpenAI [29] is a RoBERTa base fine-tuned with outputs from the 1.5Bparameter GPT-2 model. Its primary function is to predict if text was generated by a GPT-2 model."
                        ],
                        "paper": {
                            "corpus_id": 273228395,
                            "title": "Signal Watermark on Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2041278503",
                                    "name": "Zhenyu Xu"
                                },
                                {
                                    "authorId": "2278314006",
                                    "name": "Victor S. Sheng"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.97705078125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Notable Detection Tools and Benchmarks",
                "tldr": "Several detection tools and benchmarks have emerged to identify AI-generated text, ranging from commercial solutions to open-source frameworks. These tools employ various approaches including statistical analysis, supervised learning, and zero-shot detection, with notable examples like DetectGPT, GPTZero, Ghostbuster, and AuthentiGPT demonstrating different strengths and limitations across evaluation datasets. (10 sources)",
                "text": "\n## Detection Tools\n\n- **DetectGPT**: A zero-shot detection method that leverages the negative curvature regions of a language model's log probability function. It identifies AI-generated text by comparing the log probability of original text with perturbed versions, achieving 0.95 AUROC for fake news articles generated by GPT-NeoX-20B. <Paper corpusId=\"256274849\" paperTitle=\"(Mitchell et al., 2023)\" isShortName></Paper>\n\n- **GPTZero**: Uses perplexity and burstiness metrics to distinguish between human and machine text. Lower perplexity and burstiness typically indicate machine-generated content. This tool is particularly geared toward educators. <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"266690717\" paperTitle=\"(Caiado et al., 2023)\" isShortName></Paper>\n\n- **Ghostbuster**: A state-of-the-art system that passes documents through weaker language models, searches for feature combinations, and trains a classifier to predict AI-generated text. It achieves 99.0 F1 score when evaluated across domains, outperforming previous models by 5.9 F1. <Paper corpusId=\"258865787\" paperTitle=\"(Verma et al., 2023)\" isShortName></Paper> <Paper corpusId=\"274581321\" paperTitle=\"(Tufts et al., 2024)\" isShortName></Paper>\n\n- **AuthentiGPT**: An efficient classifier that distinguishes between machine-generated and human-written texts with only one trainable parameter. It works by leveraging a black-box LLM to denoise input text with artificially added noise, then semantically compares the denoised text with the original. <Paper corpusId=\"265157663\" paperTitle=\"(Guo et al., 2023)\" isShortName></Paper>\n\n- **GLTR (Giant Language model Test Room)**: A tool designed to support non-experts in detecting generated text by applying baseline statistical methods that can detect generation artifacts across multiple sampling schemes. It improves human detection rates of fake text from 54% to 72% without prior training. <Paper corpusId=\"182952848\" paperTitle=\"(Gehrmann et al., 2019)\" isShortName></Paper>\n\n- **OpenAI's AI Text Classifier**: A fine-tuned language model developed by OpenAI to distinguish between human and AI-generated text. However, its effectiveness has been questioned as language models become increasingly human-like. <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>\n\n- **RADAR**: A framework that jointly trains a paraphraser and a detector in an adversarial setting to create a robust AI-text detector. The paraphraser generates realistic content to evade detection, while the detector learns to identify even adversarially modified texts. <Paper corpusId=\"259501842\" paperTitle=\"(Hu et al., 2023)\" isShortName></Paper>\n\n- **Other Commercial Tools**: Several commercial detection tools targeted at educators include Copyleaks AI Content Detector, Crossplag, Originality.ai, Turnitin AI Detection, and ZeroGPT. <Paper corpusId=\"266690717\" paperTitle=\"(Caiado et al., 2023)\" isShortName></Paper>\n\n## Benchmarks and Datasets\n\n- **Ghostbuster Benchmarks**: Three new datasets of human and AI-generated text in student essays, creative writing, and news articles domains, released alongside the Ghostbuster detection system. <Paper corpusId=\"258865787\" paperTitle=\"(Verma et al., 2023)\" isShortName></Paper>\n\n- **Human ChatGPT Comparison Corpus (HC3)**: A dataset collected to fine-tune RoBERTa for AI-generated text detection, specifically focused on ChatGPT-generated content. <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>\n\n- **Mindner et al. Corpus**: A text corpus covering 10 school topics used to classify human-generated vs. AI-generated texts, with their best systems achieving F1-scores over 96% for basic classification and more than 78% for human/AI-rephrased text classification. \n\n## Comparative Performance\n\n- Zero-shot methods like DetectGPT show strong performance (0.95 AUROC) without requiring training data, but can be computationally expensive as they require numerous queries to the source model. <Paper corpusId=\"256274849\" paperTitle=\"(Mitchell et al., 2023)\" isShortName></Paper>\n\n- Supervised approaches like Ghostbuster demonstrate superior performance (99.0 F1) when evaluated across domains and show better generalization across writing domains (+7.5 F1), prompting strategies (+2.1 F1), and language models (+4.4 F1) compared to previous approaches. <Paper corpusId=\"258865787\" paperTitle=\"(Verma et al., 2023)\" isShortName></Paper>\n\n- As language models continue to advance, detection tools based on variance in perplexity between human and AI-generated text are becoming less effective, making watermarking techniques increasingly valuable as a stable and reliable verification method. <Paper corpusId=\"273228395\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>\n\n- Most current detectors face challenges with out-of-distribution text, adversarial attacks, and human-AI collaboration scenarios where text may be partially generated or edited. <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"264439054\" paperTitle=\"(Ghosal et al., 2023)\" isShortName></Paper>\n\n- Recent research indicates that for sufficiently advanced language models, machine-generated text detectors may offer only marginal improvement over random classifiers, highlighting the need for continued innovation in detection methodologies. <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Mitchell et al., 2023)",
                        "snippets": [
                            "In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT."
                        ],
                        "paper": {
                            "corpus_id": 256274849,
                            "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
                            "authors": [
                                {
                                    "authorId": "49688913",
                                    "name": "E. Mitchell"
                                },
                                {
                                    "authorId": "2110392124",
                                    "name": "Yoonho Lee"
                                },
                                {
                                    "authorId": "121873407",
                                    "name": "Alexander Khazatsky"
                                },
                                {
                                    "authorId": "144783904",
                                    "name": "Christopher D. Manning"
                                },
                                {
                                    "authorId": "46881670",
                                    "name": "Chelsea Finn"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 627
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Tang et al., 2023)",
                        "snippets": [
                            "Recent methods such as DetectGPT and GPTZero distinguish between machine-generated and human-written text by analyzing their statistical discrepancies (Tian, 2023;Mitchell et al., 2023). DetectGPT compares the log probability computed by a model on unperturbed text and perturbed variations, leveraging the observation that text sampled from a LLM generally occupy negative curvature regions of the model's log probability function. GPTZero instead uses perplexity and burstiness to distinguish human from machine text, with lower perplexity and burstiness indicating a greater likelihood of machine-generated text. However, these heuristics do not generalize and are often fallible.\n\nDetection by Learning Classifiers Several papers have proposed to train classifiers to distinguish between AI and human generated text. During the initial GPT-2 release, OpenAI trained a RoBERTa classifier to detect GPT-2 generated text with 95% accuracy (Solaiman et al., 2019). More recently, OpenAI fine-tuned a GPT model on a dataset of machine-generated and human texts focusing on the same topic, with a true positive identification rate of 26% (Ope-nAI, 2023). Similarly, Guo et al. (2023) collected the Human ChatGPT Comparison Corpuse (HC3) and fine-tuned RoBERTa for the detection task.\n\nNotably, the capabilities of such classifiers decrease as machine-generated text becomes increasingly human-like. Sadasivan et al. (2023) show theoretically that for sufficiently advanced language models, machine-generated text detectors offer only a marginal improvement over random classifiers. Moreover, such methods are prone to adversarial attacks and are not robust to out-of-distribution text.\n\nWatermarking Large Language Models An alternative to detecting of machine-generated text using statistical discrepancies and learned classifiers is the concept of watermarks. Watermarks are hidden patterns in machinegenerated text that are imperceptible to humans, but algorithmically identifiable as synthetic. Natural language watermarks long predate the development of LLMs, relying on methods such as synonym substitution, as well as syntactic and semantic transformations (Topkara et al., 2005)."
                        ],
                        "paper": {
                            "corpus_id": 258967971,
                            "title": "Baselines for Identifying Watermarked Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2144484107",
                                    "name": "Leonard Tang"
                                },
                                {
                                    "authorId": "2143997407",
                                    "name": "Gavin Uberti"
                                },
                                {
                                    "authorId": "2211332429",
                                    "name": "Tom Shlomi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.970703125
                    },
                    {
                        "id": "(Caiado et al., 2023)",
                        "snippets": [
                            "Many tools to detect AI-generated text are now offered. Some popular tools geared toward educators are Copyleaks AI Content Detector, Crossplag, GPTZero, Hugging Face OpenAI Detector, Originality.ai, Turnitin AI Detection and ZeroGPT. The list of detectors and their capability is constantly changing following the fast-paced changes seen in the development of large language models.\n\nMost tools are based on detecting artifacts of the text generation process, including word choice, writing style, sentence length, and many more. A report by Open AI [9] lays out three AI content detection strategies, including a simple classifier learned from scratch, a classifier resulting from fine-tuning an existing language model, or using the probabilities assigned by the model to strings. Many existing tools follow the first two approaches. For example, the Hugging Face Open AI detector is a transformer-based classifier that is fine-tuned to detect GPT-2 text. Self-detection introduced in this paper is most closely related to the third approach. However, it does not require access to the model parameters to assess probabilities. It relies on the model itself to perform the detection."
                        ],
                        "paper": {
                            "corpus_id": 266690717,
                            "title": "AI Content Self-Detection for Transformer-based Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2277214778",
                                    "name": "Antonio Junior Alves Caiado"
                                },
                                {
                                    "authorId": "2277214594",
                                    "name": "Michael Hahsler"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.94189453125
                    },
                    {
                        "id": "(Verma et al., 2023)",
                        "snippets": [
                            "We introduce Ghostbuster, a state-of-the-art system for detecting AI-generated text.Our method works by passing documents through a series of weaker language models, running a structured search over possible combinations of their features, and then training a classifier on the selected features to predict whether documents are AI-generated.Crucially, Ghostbuster does not require access to token probabilities from the target model, making it useful for detecting text generated by black-box or unknown models.In conjunction with our model, we release three new datasets of human- and AI-generated text as detection benchmarks in the domains of student essays, creative writing, and news articles. We compare Ghostbuster to several existing detectors, including DetectGPT and GPTZero, as well as a new RoBERTa baseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is 5.9 F1 higher than the best preexisting model. It also outperforms all previous approaches in generalization across writing domains (+7.5 F1), prompting strategies (+2.1 F1), and language models (+4.4 F1)."
                        ],
                        "paper": {
                            "corpus_id": 258865787,
                            "title": "Ghostbuster: Detecting Text Ghostwritten by Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2143413953",
                                    "name": "V. Verma"
                                },
                                {
                                    "authorId": "1988380666",
                                    "name": "Eve Fleisig"
                                },
                                {
                                    "authorId": "1391385991",
                                    "name": "Nicholas Tomlin"
                                },
                                {
                                    "authorId": "38666915",
                                    "name": "D. Klein"
                                }
                            ],
                            "year": 2023,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 60
                        },
                        "score": 0.9482421875
                    },
                    {
                        "id": "(Tufts et al., 2024)",
                        "snippets": [
                            "Many methods have been released recently that claim to have a strong ability to detect the difference between AI-generated and human-generated texts. These detectors primarily fall into three categories: trained detectors, zero-shot detectors, and watermarking techniques (Yang et al., 2023)(Ghosal et al., 2023)Tang et al., 2023). Trained detectors utilize datasets of human and AI-generated texts and train a binary classification model to detect the source of a text (Zellers et al., 2019)(Hovy, 2016)(Hu et al., 2023)Tian and Cui, 2023;(Verma et al., 2023). Zero-shot detection utilizes a language model's inherent traits to identify text it generates, without explicit training for detection tasks other than calibrating a threshold for detection in some cases (Gehrmann et al., 2019;Mitchell et al., 2023;Bao et al., 2024;Yang et al., 2023a;Venkatraman et al., 2024)."
                        ],
                        "paper": {
                            "corpus_id": 274581321,
                            "title": "A Practical Examination of AI-Generated Text Detectors for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2334353491",
                                    "name": "Brian Tufts"
                                },
                                {
                                    "authorId": "150345512",
                                    "name": "Xuandong Zhao"
                                },
                                {
                                    "authorId": "2257344711",
                                    "name": "Lei Li"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 2
                        },
                        "score": 0.94873046875
                    },
                    {
                        "id": "(Guo et al., 2023)",
                        "snippets": [
                            "One of the major concerns is their ability to create text that closely mimics human writing, which can lead to potential misuse, such as academic misconduct, disinformation, and fraud. To address this problem, we present AuthentiGPT, an efficient classifier that distinguishes between machine-generated and human-written texts. Under the assumption that human-written text resides outside the distribution of machine-generated text, AuthentiGPT leverages a black-box LLM to denoise input text with artificially added noise, and then semantically compares the denoised text with the original to determine if the content is machine-generated. With only one trainable parameter, AuthentiGPT eliminates the need for a large training dataset, watermarking the LLM's output, or computing the log-likelihood."
                        ],
                        "paper": {
                            "corpus_id": 265157663,
                            "title": "AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising",
                            "authors": [
                                {
                                    "authorId": "2266505035",
                                    "name": "Zhen Guo"
                                },
                                {
                                    "authorId": "2266834515",
                                    "name": "Shangdi Yu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 11
                        },
                        "score": 0.9443359375
                    },
                    {
                        "id": "(Gehrmann et al., 2019)",
                        "snippets": [
                            "The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by non-experts. In this work, we introduce GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across multiple sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs."
                        ],
                        "paper": {
                            "corpus_id": 182952848,
                            "title": "GLTR: Statistical Detection and Visualization of Generated Text",
                            "authors": [
                                {
                                    "authorId": "3159346",
                                    "name": "Sebastian Gehrmann"
                                },
                                {
                                    "authorId": "2879705",
                                    "name": "Hendrik Strobelt"
                                },
                                {
                                    "authorId": "2531268",
                                    "name": "Alexander M. Rush"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 546
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hu et al., 2023)",
                        "snippets": [
                            "Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a robust AI-text detector via adversarial learning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic content to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5-Turbo."
                        ],
                        "paper": {
                            "corpus_id": 259501842,
                            "title": "RADAR: Robust AI-Text Detection via Adversarial Learning",
                            "authors": [
                                {
                                    "authorId": "2199229406",
                                    "name": "Xiaomeng Hu"
                                },
                                {
                                    "authorId": "2158177948",
                                    "name": "Pin-Yu Chen"
                                },
                                {
                                    "authorId": "2103197703",
                                    "name": "Tsung-Yi Ho"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 127
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "As Large Language Models (LLMs) become increasingly sophisticated, they raise significant security concerns, including the creation of fake news and academic misuse. Most detectors for identifying model-generated text are limited by their reliance on variance in perplexity and burstiness, and they require substantial computational resources.\n\nNumerous open-source and commercial tools have been developed to detect model-generated text, such as GPT2-Detector [29], RoBERTa-QA [31], DetectGPT [30], GPTZero [32], Writer [33], and others. However, there are three significant limitations to these methods. First, their probability predictions cannot provide conclusive certainty and should only be regarded as a reference. Second, most current approaches rely on methods that differentiate human text from modelgenerated text based on variations in perplexity and burstiness. Third, methods like DetectGPT and RoBERTa-QA, which require multiple text-generation process and model retraining, consume substantial computational resources and time when detecting model-generated text. As LLMs advance, the generated text increasingly resembles human writing, making these methods based on variance in perplexity between human and model-generated text less applicable. Therefore, embedding watermarks in text generated by LLMs presents a faster, more stable, and reliable method of verification.\n\nRecent advancements in watermarking techniques for language models have been crucial in ensuring text authenticity and model protection. Abdelnabi and Fritz [20] pioneered the Adversarial Watermarking Transformer (AWT), marking a significant step in embedding binary messages into text and demonstrating resilience against multiple attacks. He et al. [21] developed CATER, a conditional watermarking framework that secures text generation APIs against imitation attacks while preserving word distribution integrity. Zhao, Li, and Wang [22] introduced Distillation-Resistant Watermarking (DRW), a method effective in protecting NLP models from distillation theft and maintaining accuracy. Kirchenbauer et al. [23] proposed a framework for embedding watermarks in language models that are detectable with minimal impact on text quality. In a similar vein, Zhao, Wang, and Li's GINSEW technique [24] innovatively embeds secret signals in the probability vector of decoding steps, enhancing IP infringement detection. Peng et al. [25] developed EmbMarker for EaaS, embedding watermarks in trigger words for effective copyright verification. Yoo et al. [26] explored robust multi-bit watermarking, focusing on error resistance and enhanced payload. Lastly, Li et al. [27] introduced a novel strategy for embedding watermarks in the weight quantization process of large language models, ensuring model security without compromising operational modes.\n\nThe advancement in language models has prompted the development of various detection tools for identifying model-generated text. Gehrmann [28] developed GLTR, a tool to assist non-experts in detecting texts generated by models. It uses statistical methods to identify generation artifacts and has been shown to improve human detection rates of fake text significantly. GPT2-Detector developed by OpenAI [29] is a RoBERTa base fine-tuned with outputs from the 1.5Bparameter GPT-2 model. Its primary function is to predict if text was generated by a GPT-2 model."
                        ],
                        "paper": {
                            "corpus_id": 273228395,
                            "title": "Signal Watermark on Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2041278503",
                                    "name": "Zhenyu Xu"
                                },
                                {
                                    "authorId": "2278314006",
                                    "name": "Victor S. Sheng"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.97705078125
                    },
                    {
                        "id": "(Ghosal et al., 2023)",
                        "snippets": [
                            "Large Language Models (LLMs) have revolutionized the domain of natural language processing (NLP) with remarkable capabilities of generating human-like text responses. However, despite these advancements, several works in the existing literature have raised serious concerns about the potential misuse of LLMs such as spreading misinformation, generating fake news, plagiarism in academia, and contaminating the web. To address these concerns, a consensus among the research community is to develop algorithmic solutions to detect AI-generated text. The basic idea is that whenever we can tell if the given text is either written by a human or an AI, we can utilize this information to address the above-mentioned concerns. To that end, a plethora of detection frameworks have been proposed, highlighting the possibilities of AI-generated text detection. But in parallel to the development of detection frameworks, researchers have also concentrated on designing strategies to elude detection, i.e., focusing on the impossibilities of AI-generated text detection. This is a crucial step in order to make sure the detection frameworks are robust enough and it is not too easy to fool a detector. Despite the huge interest and the flurry of research in this domain, the community currently lacks a comprehensive analysis of recent developments. In this survey, we aim to provide a concise categorization and overview of current work encompassing both the prospects and the limitations of AI-generated text detection. To enrich the collective knowledge, we engage in an exhaustive discussion on critical and challenging open questions related to ongoing research on AI-generated text detection."
                        ],
                        "paper": {
                            "corpus_id": 264439054,
                            "title": "Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey",
                            "authors": [
                                {
                                    "authorId": "2655959",
                                    "name": "Soumya Suvra Ghosal"
                                },
                                {
                                    "authorId": "49081354",
                                    "name": "Souradip Chakraborty"
                                },
                                {
                                    "authorId": "8284185",
                                    "name": "Jonas Geiping"
                                },
                                {
                                    "authorId": "2261325066",
                                    "name": "Furong Huang"
                                },
                                {
                                    "authorId": "2172597446",
                                    "name": "Dinesh Manocha"
                                },
                                {
                                    "authorId": "3387859",
                                    "name": "A. S. Bedi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 37
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Limitations and Challenges",
                "tldr": "AI-generated text detection faces significant challenges including diminishing effectiveness as LLMs become more human-like, vulnerability to adversarial attacks, and difficulties with short texts and mixed human-AI content. Detection methods also struggle with out-of-distribution scenarios, cross-model generalization, and face inherent trade-offs between detection accuracy and computational efficiency. (11 sources)",
                "text": "\nAs detection methods for AI-generated text continue to evolve, they face several significant limitations and challenges that impact their practical effectiveness and reliability:\n\n## Diminishing Effectiveness with Advanced LLMs\n\nA fundamental challenge for detection is that as language models become more sophisticated, the statistical differences between human and AI-generated text continue to narrow. Sadasivan et al. demonstrated theoretically that for sufficiently advanced language models, machine-generated text detectors offer only marginal improvement over random classifiers <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>. This suggests an inherent limit to detection approaches that rely on statistical patterns to differentiate between human and machine writing.\n\n## Vulnerability to Adversarial Attacks\n\nMost detection methods are highly vulnerable to adversarial attacks and evasion techniques. Simple modifications like paraphrasing can significantly reduce detection accuracy <Paper corpusId=\"259501842\" paperTitle=\"(Hu et al., 2023)\" isShortName></Paper>. Several studies have documented methods to circumvent popular detection tools like GPTZero, with one study highlighting three distinct approaches to bypass detection <Paper corpusId=\"268357168\" paperTitle=\"(Munyer et al._1, 2023)\" isShortName></Paper>. This vulnerability extends to watermarking techniques as well, with semantic-level watermarks like SemStamp being developed specifically to address vulnerabilities to paraphrase attacks <Paper corpusId=\"277595847\" paperTitle=\"(Cao, 2025)\" isShortName></Paper> <Paper corpusId=\"263831179\" paperTitle=\"(Hou et al., 2024)\" isShortName></Paper>.\n\n## Challenges with Short Texts and Mixed Content\n\nDetection methods typically require substantial text length to function effectively. GPTZero, for example, requires a minimum of 250 characters to begin detection <Paper corpusId=\"268357168\" paperTitle=\"(Munyer et al._1, 2023)\" isShortName></Paper>. This limitation is particularly problematic for short-form content like social media posts, reviews, or brief communications. Additionally, detection becomes significantly more difficult with mixed human-AI collaboration, where parts of a text might be human-written and others AI-generated <Paper corpusId=\"276773353\" paperTitle=\"(Bitton et al., 2025)\" isShortName></Paper>.\n\n## Out-of-Distribution Performance\n\nMost detector models perform well only within their training distribution but struggle significantly with out-of-distribution texts. Zero-shot approaches are particularly sensitive to out-of-distribution scenarios <Paper corpusId=\"273901392\" paperTitle=\"(Nguyen-Son et al., 2024)\" isShortName></Paper>. This limitation is exacerbated by the rapid evolution of language models, making it difficult for detection systems to keep pace with new generation techniques and model architectures.\n\n## Model-Specific Limitations\n\nMany detection approaches are optimized for specific language models or generation techniques, limiting their generalizability. While some tools like Binoculars claim to detect machine text from various LLMs without model-specific modifications <Paper corpusId=\"277595847\" paperTitle=\"(Cao, 2025)\" isShortName></Paper> <Paper corpusId=\"267068867\" paperTitle=\"(Hans et al., 2024)\" isShortName></Paper>, most detectors show reduced effectiveness when faced with unfamiliar models or novel generation strategies <Paper corpusId=\"266369103\" paperTitle=\"(Mubarak et al., 2023)\" isShortName></Paper>.\n\n## Computational Resource Requirements\n\nMethods like DetectGPT and RoBERTa-QA, which require multiple text-generation processes and model retraining, consume substantial computational resources and time <Paper corpusId=\"273228395\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>. This computational burden makes real-time detection challenging, especially for applications requiring immediate verification of large volumes of content.\n\n## Trade-offs Between Approaches\n\nEach detection approach involves inherent trade-offs. Supervised classifiers require extensive training data but may be more accurate within their training distribution. Zero-shot methods eliminate the need for training data but may be less effective overall. Watermarking provides a more reliable verification method but requires implementation at the model level and can impact text quality <Paper corpusId=\"273228395\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"266369103\" paperTitle=\"(Mubarak et al., 2023)\" isShortName></Paper>.\n\n## Implementation Barriers for Watermarking\n\nWhile watermarking techniques offer a promising direction for verification, they can only be implemented by the developers of language models, limiting their universal applicability <Paper corpusId=\"273901392\" paperTitle=\"(Nguyen-Son et al., 2024)\" isShortName></Paper>. Additionally, watermarking faces challenges with factual responses where there are fewer options to control the output and is highly sensitive to even minor edits of the watermarked text <Paper corpusId=\"276773353\" paperTitle=\"(Bitton et al., 2025)\" isShortName></Paper> <Paper corpusId=\"273547059\" paperTitle=\"(Dathathri et al., 2024)\" isShortName></Paper>.\n\nThese challenges underscore the need for continued innovation in detection methodologies, particularly approaches that can remain effective as language models evolve and that can resist various forms of evasion and adversarial manipulation.",
                "citations": [
                    {
                        "id": "(Tang et al., 2023)",
                        "snippets": [
                            "Recent methods such as DetectGPT and GPTZero distinguish between machine-generated and human-written text by analyzing their statistical discrepancies (Tian, 2023;Mitchell et al., 2023). DetectGPT compares the log probability computed by a model on unperturbed text and perturbed variations, leveraging the observation that text sampled from a LLM generally occupy negative curvature regions of the model's log probability function. GPTZero instead uses perplexity and burstiness to distinguish human from machine text, with lower perplexity and burstiness indicating a greater likelihood of machine-generated text. However, these heuristics do not generalize and are often fallible.\n\nDetection by Learning Classifiers Several papers have proposed to train classifiers to distinguish between AI and human generated text. During the initial GPT-2 release, OpenAI trained a RoBERTa classifier to detect GPT-2 generated text with 95% accuracy (Solaiman et al., 2019). More recently, OpenAI fine-tuned a GPT model on a dataset of machine-generated and human texts focusing on the same topic, with a true positive identification rate of 26% (Ope-nAI, 2023). Similarly, Guo et al. (2023) collected the Human ChatGPT Comparison Corpuse (HC3) and fine-tuned RoBERTa for the detection task.\n\nNotably, the capabilities of such classifiers decrease as machine-generated text becomes increasingly human-like. Sadasivan et al. (2023) show theoretically that for sufficiently advanced language models, machine-generated text detectors offer only a marginal improvement over random classifiers. Moreover, such methods are prone to adversarial attacks and are not robust to out-of-distribution text.\n\nWatermarking Large Language Models An alternative to detecting of machine-generated text using statistical discrepancies and learned classifiers is the concept of watermarks. Watermarks are hidden patterns in machinegenerated text that are imperceptible to humans, but algorithmically identifiable as synthetic. Natural language watermarks long predate the development of LLMs, relying on methods such as synonym substitution, as well as syntactic and semantic transformations (Topkara et al., 2005)."
                        ],
                        "paper": {
                            "corpus_id": 258967971,
                            "title": "Baselines for Identifying Watermarked Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2144484107",
                                    "name": "Leonard Tang"
                                },
                                {
                                    "authorId": "2143997407",
                                    "name": "Gavin Uberti"
                                },
                                {
                                    "authorId": "2211332429",
                                    "name": "Tom Shlomi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.970703125
                    },
                    {
                        "id": "(Hu et al., 2023)",
                        "snippets": [
                            "Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a robust AI-text detector via adversarial learning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic content to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5-Turbo."
                        ],
                        "paper": {
                            "corpus_id": 259501842,
                            "title": "RADAR: Robust AI-Text Detection via Adversarial Learning",
                            "authors": [
                                {
                                    "authorId": "2199229406",
                                    "name": "Xiaomeng Hu"
                                },
                                {
                                    "authorId": "2158177948",
                                    "name": "Pin-Yu Chen"
                                },
                                {
                                    "authorId": "2103197703",
                                    "name": "Tsung-Yi Ho"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 127
                        },
                        "score": 0
                    },
                    {
                        "id": "(Munyer et al._1, 2023)",
                        "snippets": [
                            "Recent endeavors have been directed towards developing classifiers aimed at differentiating between LLM-generated text and human-written text.The prevailing approach entails the collection and labeling of LLM-generated and humanwritten texts, followed by the training of a binary classifier through supervised learning.Although the efficacy of these classifiers has yet to be fully established, some preliminary analyses have been reported [8,9].One study [9] elucidated three distinct methods, substantiated with examples, to circumvent the GPTZero [7] classifier detection.Another investigation [8] conducted a direct assessment of GPTZero's accuracy, uncovering inconsistencies in its ability to detect human-written text.Moreover, classifier-based LLM-generated text detectors commonly necessitate a substantial character count to perform detection accurately.For instance, GPTZero [7] required a minimum of 250 characters to initiate detection.Looking ahead, OpenAI is planning a cryptography-based watermarking system for ChatGPTgenerated text detection [15], although no definitive work has been disclosed as of yet.Zero-shot learning-based methods have also demonstrated some advancement.For example, Cer et al. [16] reported an increment in AUROC from 1% to 14% compared to other zero-shot detection strategies across various datasets; however, the accuracy might still fall short in real-world applications concerning text generated by models.\n\nA method has been proposed for detecting LLM-generated texts based on text watermarking (Kirchenbauer et al., 2023), which involves watermarking the text by modifying the LLMs (sensitive tokens are defined and excluded from the output of the LLMs).In contrast, our proposed DeepTextMark does not necessitate access to or modifications of the LLM.Distinct from model-dependent methods, DeepTextMark exhibits a model-independent feature, enabling its application to any text.Moreover, DeepTextMark employs a substantially more compact architecture with about 50 million parameters, whereas the method in [11] necessitates billions of parameters to implement the watermarking process."
                        ],
                        "paper": {
                            "corpus_id": 268357168,
                            "title": "DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for Identifying Large Language Model Generated Text",
                            "authors": [
                                {
                                    "authorId": "2106504469",
                                    "name": "Travis J. E. Munyer"
                                },
                                {
                                    "authorId": "2215112081",
                                    "name": "A. Tanvir"
                                },
                                {
                                    "authorId": "2107200061",
                                    "name": "A. Das"
                                },
                                {
                                    "authorId": "2257984128",
                                    "name": "Xin Zhong"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 15
                        },
                        "score": 0.958984375
                    },
                    {
                        "id": "(Cao, 2025)",
                        "snippets": [
                            "One popular approach leverages an external LLM to classify whether a piece of text is AI-generated or not [12]. With carefully crafted prompts, these zero-shot methods can achieve moderate accuracy quickly. However, sensitivity to prompt design and the adversarial gap between the LLM detector and the text generator are common issues [22].\n\n2) Linguistic and statistical signatures: Traditional stylometric features (e.g., function words, syntax complexity, average phrase length) have long been used in authorship attribution [23], [24]. More modern detection focuses on computing perplexity or log-likelihood using reference language models, observing that LLM-generated text tends to show distinctive probability distributions. Additionally, specialized white-box methods can measure rank ordering of tokens if the generating model is partially known [25].\n\n3) Supervised classification (training-based): Labeled corpora of AI versus human text enable fine-tuning of large pre-trained transformers like RoBERTa or T5 to discriminate synthetic text [26], [27]. Researchers improve robustness with adversarial training sets that contain paraphrased or AIgenerated passages shifted in style. Tools like GPTZero [28] and RADAR [29] exemplify advanced supervised detectors. However, assembling high-quality, representative training data remains a challenge, especially as new generator architectures emerge frequently.\n\n4) Watermarking for AI text: Cooperative watermarking modifies text generation at token selection time, embedding an imperceptible pattern in the distribution of words or punctuation [30], [31]. A verifier can detect such patterns after the fact. While promising for major industrial LLMs that adopt the standard, watermarking fails if malicious or open-source models do not embed it, or if paraphrasing disrupts the signal [32], [33]."
                        ],
                        "paper": {
                            "corpus_id": 277595847,
                            "title": "A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content",
                            "authors": [
                                {
                                    "authorId": "2354295905",
                                    "name": "Lele Cao"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.95458984375
                    },
                    {
                        "id": "(Hou et al., 2024)",
                        "snippets": [
                            "Existing watermarked generation algorithms employ token-level designs and therefore, are vulnerable to paraphrase attacks. To address this issue, we introduce watermarking on the semantic representation of sentences. We propose SemStamp, a robust sentence-level semantic watermarking algorithm that uses locality-sensitive hashing (LSH) to partition the semantic space of sentences. The algorithm encodes and LSH-hashes a candidate sentence generated by a language model, and conducts rejection sampling until the sampled sentence falls in watermarked partitions in the semantic embedding space. To test the paraphrastic robustness of watermarking algorithms, we propose a \"bigram paraphrase\" attack that produces paraphrases with small bigram overlap with the original sentence. This attack is shown to be effective against existing token-level watermark algorithms, while posing only minor degradations to SemStamp. Experimental results show that our novel semantic watermark algorithm is not only more robust than the previous state-of-the-art method on various paraphrasers and domains, but also better at preserving the quality of generation."
                        ],
                        "paper": {
                            "corpus_id": 263831179,
                            "title": "SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation",
                            "authors": [
                                {
                                    "authorId": "2257002716",
                                    "name": "A. Hou"
                                },
                                {
                                    "authorId": "2167508843",
                                    "name": "Jingyu (Jack) Zhang"
                                },
                                {
                                    "authorId": "2249540815",
                                    "name": "Tianxing He"
                                },
                                {
                                    "authorId": "2125062703",
                                    "name": "Yichen Wang"
                                },
                                {
                                    "authorId": "2257002712",
                                    "name": "Yung-Sung Chuang"
                                },
                                {
                                    "authorId": "2257103558",
                                    "name": "Hongwei Wang"
                                },
                                {
                                    "authorId": "2121272448",
                                    "name": "Lingfeng Shen"
                                },
                                {
                                    "authorId": "7536576",
                                    "name": "Benjamin Van Durme"
                                },
                                {
                                    "authorId": "1783281",
                                    "name": "Daniel Khashabi"
                                },
                                {
                                    "authorId": "2249583325",
                                    "name": "Yulia Tsvetkov"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 66
                        },
                        "score": 0
                    },
                    {
                        "id": "(Bitton et al., 2025)",
                        "snippets": [
                            "Classical approaches applied feature extraction on the texts in order to quantify stylistic or syntactic features such as perplexity, burstiness, and word density. These features are then fed into machine learning algorithms such as logistic regression or SVM (which provide an advantage of explainability), or into \"deep\" neural networks such as LSTMs. More modern approaches use pre-trained language models such as BERT, and fine-tune them for the specific classification task. This eliminates the need to manually define a list of features, and overall presents a much higher accuracy compared to feature-based methods, mainly because this approach is able to capture more general elements and nuances of the writing style, and is less dependent on a specific domain or topic. [20](Yadagiri et al., 2024)(Dathathri et al., 2024)(Kwon et al., 2025) Approaches that rely solely on capturing the differences between AI and human texts operate under the assumption that such differences exist; however, in reality, the differences between machine and human text may diminish over time as these systems evolve. A \"watermarking\" approach has recently emerged in an attempt to overcome these shortcomings. \"Watermarks\" can be added to the text during the generation phase (generative watermarking) or after it (edit-based watermarking). (Dathathri et al., 2024) However, this approach has its own limitations, such as lower effectiveness on factual responses (as there are fewer options to control the output), and higher sensitivity to small edits of the watermarked text."
                        ],
                        "paper": {
                            "corpus_id": 276773353,
                            "title": "Detecting Stylistic Fingerprints of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "1405506735",
                                    "name": "Yehonatan Bitton"
                                },
                                {
                                    "authorId": "2348468222",
                                    "name": "Elad Bitton"
                                },
                                {
                                    "authorId": "2348465980",
                                    "name": "Shai Nisan"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.94775390625
                    },
                    {
                        "id": "(Nguyen-Son et al., 2024)",
                        "snippets": [
                            "Detection of text generated by large language models uses three main techniques. Firstly, supervised learning methods (Solaiman et al., 2019;(Wang et al., 2023)(Hu et al., 2023)(Wu et al., 2023) train classifiers on datasets of original and generated text, though this requires large volumes of training data. The zero-shot approach (Bhattacharjee and Liu, 2023;(Mitchell et al., 2023) eliminates the need for training but is sensitive to out-of-distribution text. Recent research explores watermarking methodologies (Kirchenbauer et al., 2023) to force models to produce predefined words, aiding detection, but this requires modifying the models, which is impractical for proprietary models like ChatGPT."
                        ],
                        "paper": {
                            "corpus_id": 273901392,
                            "title": "SimLLM: Detecting Sentences Generated by Large Language Models Using Similarity between the Generation and its Re-generation",
                            "authors": [
                                {
                                    "authorId": "1403588130",
                                    "name": "Hoang-Quoc Nguyen-Son"
                                },
                                {
                                    "authorId": "2260782421",
                                    "name": "Minh-Son Dao"
                                },
                                {
                                    "authorId": "2238657258",
                                    "name": "Koji Zettsu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 2
                        },
                        "score": 0.95654296875
                    },
                    {
                        "id": "(Hans et al., 2024)",
                        "snippets": [
                            "Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called Binoculars, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate Binoculars on a number of text sources and in varied situations. Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data."
                        ],
                        "paper": {
                            "corpus_id": 267068867,
                            "title": "Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text",
                            "authors": [
                                {
                                    "authorId": "2280138753",
                                    "name": "Abhimanyu Hans"
                                },
                                {
                                    "authorId": "102604362",
                                    "name": "Avi Schwarzschild"
                                },
                                {
                                    "authorId": "1471176143",
                                    "name": "Valeriia Cherepanova"
                                },
                                {
                                    "authorId": "2075283338",
                                    "name": "Hamid Kazemi"
                                },
                                {
                                    "authorId": "2056290221",
                                    "name": "Aniruddha Saha"
                                },
                                {
                                    "authorId": "121592562",
                                    "name": "Micah Goldblum"
                                },
                                {
                                    "authorId": "8284185",
                                    "name": "Jonas Geiping"
                                },
                                {
                                    "authorId": "2237426712",
                                    "name": "Tom Goldstein"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 107
                        },
                        "score": 0
                    },
                    {
                        "id": "(Mubarak et al., 2023)",
                        "snippets": [
                            "In the context of the rapid advancement of Large Language Models (LLMs), the ability to produce highly accurate and contextually relevant text is becoming an increasing concern in online security. The sophisticated text generation capabilities of these models can potentially facilitate harmful activities on online platforms, emphasising the importance of detecting AI-generated texts as a mitigation strategy. Several detection methodologies have been developed, each with varying degrees of effectiveness influenced by factors such as the architecture of the language model, the decoding strategy, the length of the text and the availability of raw log probabilities from the LLMs. \n\nSimple classifiers offer a basic, yet sometimes limited, approach, particularly with shorter texts and complex generation strategies. Zero-shot models provide a robust detection strategy by exploiting unique statistical patterns in machine-generated text; however, they are susceptible to adversarial actions or different generation strategies. Finetuned language models represent another approach that provides a more promising route for enhancing the detection accuracy. However, their effectiveness can be influenced by the sampling method and the length of the generated sequences. \n\nWatermarking, a more recent development, has emerged as a unique method for detection. This involves embedding watermarks in the logits of an LLM during text generation, forming a pattern that can potentially identify synthetic text. Despite the promise of watermarking, its vulnerability to spoofing/paraphrasing attacks and evasion techniques presents significant challenges that warrant further research."
                        ],
                        "paper": {
                            "corpus_id": 266369103,
                            "title": "A Survey on the Detection and Impacts of Deepfakes in Visual, Audio, and Textual Formats",
                            "authors": [
                                {
                                    "authorId": "2275319803",
                                    "name": "Rami Mubarak"
                                },
                                {
                                    "authorId": "3277359",
                                    "name": "Tariq A. A. Alsboui"
                                },
                                {
                                    "authorId": "2207298991",
                                    "name": "Omar Alshaikh"
                                },
                                {
                                    "authorId": "2276533468",
                                    "name": "Isa Inuwa-Dutse"
                                },
                                {
                                    "authorId": "143955457",
                                    "name": "Saad Khan"
                                },
                                {
                                    "authorId": "100581778",
                                    "name": "Si\u00f4n Parkinson"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 36
                        },
                        "score": 0.970703125
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "As Large Language Models (LLMs) become increasingly sophisticated, they raise significant security concerns, including the creation of fake news and academic misuse. Most detectors for identifying model-generated text are limited by their reliance on variance in perplexity and burstiness, and they require substantial computational resources.\n\nNumerous open-source and commercial tools have been developed to detect model-generated text, such as GPT2-Detector [29], RoBERTa-QA [31], DetectGPT [30], GPTZero [32], Writer [33], and others. However, there are three significant limitations to these methods. First, their probability predictions cannot provide conclusive certainty and should only be regarded as a reference. Second, most current approaches rely on methods that differentiate human text from modelgenerated text based on variations in perplexity and burstiness. Third, methods like DetectGPT and RoBERTa-QA, which require multiple text-generation process and model retraining, consume substantial computational resources and time when detecting model-generated text. As LLMs advance, the generated text increasingly resembles human writing, making these methods based on variance in perplexity between human and model-generated text less applicable. Therefore, embedding watermarks in text generated by LLMs presents a faster, more stable, and reliable method of verification.\n\nRecent advancements in watermarking techniques for language models have been crucial in ensuring text authenticity and model protection. Abdelnabi and Fritz [20] pioneered the Adversarial Watermarking Transformer (AWT), marking a significant step in embedding binary messages into text and demonstrating resilience against multiple attacks. He et al. [21] developed CATER, a conditional watermarking framework that secures text generation APIs against imitation attacks while preserving word distribution integrity. Zhao, Li, and Wang [22] introduced Distillation-Resistant Watermarking (DRW), a method effective in protecting NLP models from distillation theft and maintaining accuracy. Kirchenbauer et al. [23] proposed a framework for embedding watermarks in language models that are detectable with minimal impact on text quality. In a similar vein, Zhao, Wang, and Li's GINSEW technique [24] innovatively embeds secret signals in the probability vector of decoding steps, enhancing IP infringement detection. Peng et al. [25] developed EmbMarker for EaaS, embedding watermarks in trigger words for effective copyright verification. Yoo et al. [26] explored robust multi-bit watermarking, focusing on error resistance and enhanced payload. Lastly, Li et al. [27] introduced a novel strategy for embedding watermarks in the weight quantization process of large language models, ensuring model security without compromising operational modes.\n\nThe advancement in language models has prompted the development of various detection tools for identifying model-generated text. Gehrmann [28] developed GLTR, a tool to assist non-experts in detecting texts generated by models. It uses statistical methods to identify generation artifacts and has been shown to improve human detection rates of fake text significantly. GPT2-Detector developed by OpenAI [29] is a RoBERTa base fine-tuned with outputs from the 1.5Bparameter GPT-2 model. Its primary function is to predict if text was generated by a GPT-2 model."
                        ],
                        "paper": {
                            "corpus_id": 273228395,
                            "title": "Signal Watermark on Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2041278503",
                                    "name": "Zhenyu Xu"
                                },
                                {
                                    "authorId": "2278314006",
                                    "name": "Victor S. Sheng"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.97705078125
                    },
                    {
                        "id": "(Dathathri et al., 2024)",
                        "snippets": [
                            "Multiple strategies have emerged to address this problem. One is a retrieval-based approach, which involves keeping a growing record of all generated texts and checking against it for matches 12. This requires scale and coordination, and raises privacy concerns as it requires accessing and storing all LLM interactions."
                        ],
                        "paper": {
                            "corpus_id": 273547059,
                            "title": "Scalable watermarking for identifying large language model outputs",
                            "authors": [
                                {
                                    "authorId": "3491117",
                                    "name": "Sumanth Dathathri"
                                },
                                {
                                    "authorId": "2268761249",
                                    "name": "Abigail See"
                                },
                                {
                                    "authorId": "145053127",
                                    "name": "Sumedh Ghaisas"
                                },
                                {
                                    "authorId": "2268826600",
                                    "name": "Po-Sen Huang"
                                },
                                {
                                    "authorId": "2327318512",
                                    "name": "Rob McAdam"
                                },
                                {
                                    "authorId": "1851564",
                                    "name": "Johannes Welbl"
                                },
                                {
                                    "authorId": "2327316530",
                                    "name": "Vandana Bachani"
                                },
                                {
                                    "authorId": "2275186627",
                                    "name": "Alex Kaskasoli"
                                },
                                {
                                    "authorId": "49860489",
                                    "name": "Robert Stanforth"
                                },
                                {
                                    "authorId": "2327319709",
                                    "name": "Tatiana Matejovicova"
                                },
                                {
                                    "authorId": "2270174566",
                                    "name": "Jamie Hayes"
                                },
                                {
                                    "authorId": "2275153245",
                                    "name": "Nidhi Vyas"
                                },
                                {
                                    "authorId": "2089890458",
                                    "name": "Majd Al Merey"
                                },
                                {
                                    "authorId": "1400348545",
                                    "name": "Jonah Brown-Cohen"
                                },
                                {
                                    "authorId": "3407947",
                                    "name": "Rudy Bunel"
                                },
                                {
                                    "authorId": "2324784942",
                                    "name": "Borja Balle"
                                },
                                {
                                    "authorId": "122347685",
                                    "name": "taylan. cemgil"
                                },
                                {
                                    "authorId": "2268758347",
                                    "name": "Zahra Ahmed"
                                },
                                {
                                    "authorId": "1396786528",
                                    "name": "Kitty Stacpoole"
                                },
                                {
                                    "authorId": "47473421",
                                    "name": "Ilia Shumailov"
                                },
                                {
                                    "authorId": "2324799480",
                                    "name": "Cip Baetu"
                                },
                                {
                                    "authorId": "2071666",
                                    "name": "Sven Gowal"
                                },
                                {
                                    "authorId": "48987704",
                                    "name": "D. Hassabis"
                                },
                                {
                                    "authorId": "143967473",
                                    "name": "Pushmeet Kohli"
                                }
                            ],
                            "year": 2024,
                            "venue": "The Naturalist",
                            "n_citations": 74
                        },
                        "score": 0.9580078125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.323412
    }
}