{
    "query": "What are the latest techniques for fine-tuning both the retriever and generator components in Retrieval Augmented Generation (RAG) systems, and how do dual fine-tuning approaches like RA-DIT and RankRAG compare in performance to single-component tuning?",
    "user_id": "lib_user",
    "task_id": "95ca235b-99c2-48b9-bf50-2067bb070b3f",
    "timestamp": "2025-06-23T21:39:15.421477",
    "n_retrieval": 256,
    "n_retrieved": 256,
    "n_candidates": 37,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.38316900000000004,
    "decomposed_query": {
        "rewritten_query": "Latest techniques for fine-tuning both retriever and generator components in Retrieval Augmented Generation (RAG) systems, and how dual fine-tuning approaches like RA-DIT and RankRAG compare in performance to single-component tuning.",
        "keyword_query": "fine-tuning retriever generator RAG RA-DIT RankRAG dual single-component tuning performance",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010629,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 78,
            "citation_count": 61,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.01219, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2273537815",
                    "name": "Xiaohua Wang"
                },
                {
                    "authorId": "2308276345",
                    "name": "Zhenghua Wang"
                },
                {
                    "authorId": "2292070745",
                    "name": "Xuan Gao"
                },
                {
                    "authorId": "2308226671",
                    "name": "Feiran Zhang"
                },
                {
                    "authorId": "2308043953",
                    "name": "Yixin Wu"
                },
                {
                    "authorId": "2308044030",
                    "name": "Zhibo Xu"
                },
                {
                    "authorId": "2308036711",
                    "name": "Tianyuan Shi"
                },
                {
                    "authorId": "2309182278",
                    "name": "Zhengyuan Wang"
                },
                {
                    "authorId": "2309656885",
                    "name": "Shizheng Li"
                },
                {
                    "authorId": "2309176521",
                    "name": "Qi Qian"
                },
                {
                    "authorId": "2292032843",
                    "name": "Ruicheng Yin"
                },
                {
                    "authorId": "2220896023",
                    "name": "Changze Lv"
                },
                {
                    "authorId": "2257315404",
                    "name": "Xiaoqing Zheng"
                },
                {
                    "authorId": "2257129987",
                    "name": "Xuanjing Huang"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a \u201cretrieval as generation\u201d strategy.",
            "corpus_id": 270870251,
            "sentences": [
                {
                    "corpus_id": "270870251",
                    "title": "Searching for Best Practices in Retrieval-Augmented Generation",
                    "text": "Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context [30][31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges.\n\nSeveral surveys have extensively discussed current RAG systems, covering aspects like text generation [7,8], integration with LLMs [6,39], multimodal [40], and AI-generated content [41].While these surveys provide comprehensive overviews of existing RAG methodologies, selecting the appro-  priate algorithm for practical implementation remains challenging.In this paper, we focus on best practices for applying RAG methods, advancing the understanding and application of RAG in LLMs.",
                    "score": 0.8231243358922785,
                    "section_title": "Retriever and Generator Fine-tuning",
                    "char_start_offset": 7670,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 94
                        },
                        {
                            "start": 94,
                            "end": 242
                        },
                        {
                            "start": 242,
                            "end": 345
                        },
                        {
                            "start": 345,
                            "end": 551
                        },
                        {
                            "start": 553,
                            "end": 739
                        },
                        {
                            "start": 739,
                            "end": 910
                        },
                        {
                            "start": 910,
                            "end": 1037
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 181,
                            "end": 185,
                            "matchedPaperCorpusId": "258865283"
                        },
                        {
                            "start": 658,
                            "end": 660,
                            "matchedPaperCorpusId": "250340214"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9775390625
                },
                {
                    "corpus_id": "270870251",
                    "title": "Searching for Best Practices in Retrieval-Augmented Generation",
                    "text": "Ensuring the accuracy of responses generated by Large Language Models (LLMs) such as Chat-GPT [13] and LLaMA [14] is essential.However, simply enlarging model size does not fundamentally address the issue of hallucinations [15,16], especially in knowledge-intensive tasks and specialized domains.Retrieval-augmented generation (RAG) addresses these challenges by retrieving relevant documents from external knowledge bases, providing accurate, real-time, domain-specific context to LLMs [6].Previous works have optimized the RAG pipeline through query and retrieval transformations, enhancing retriever performance, and fine-tuning both the retriever and generator.These optimizations improve the interaction between input queries, retrieval mechanisms, and generation processes, ensuring the accuracy and relevance of responses.",
                    "score": 0.6588955747570535,
                    "section_title": "Related Work",
                    "char_start_offset": 4844,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 127,
                            "end": 296
                        },
                        {
                            "start": 296,
                            "end": 491
                        },
                        {
                            "start": 491,
                            "end": 665
                        },
                        {
                            "start": 665,
                            "end": 829
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 227,
                            "end": 230,
                            "matchedPaperCorpusId": "266164171"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9296875
                }
            ],
            "relevance_judgement": 0.9775390625,
            "relevance_judgment_input_expanded": "# Title: Searching for Best Practices in Retrieval-Augmented Generation\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, Ruicheng Yin, Changze Lv, Xiaoqing Zheng, Xuanjing Huang\n## Abstract\nRetrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a \u201cretrieval as generation\u201d strategy.\n## Related Work\nEnsuring the accuracy of responses generated by Large Language Models (LLMs) such as Chat-GPT [13] and LLaMA [14] is essential.However, simply enlarging model size does not fundamentally address the issue of hallucinations [15,16], especially in knowledge-intensive tasks and specialized domains.Retrieval-augmented generation (RAG) addresses these challenges by retrieving relevant documents from external knowledge bases, providing accurate, real-time, domain-specific context to LLMs [6].Previous works have optimized the RAG pipeline through query and retrieval transformations, enhancing retriever performance, and fine-tuning both the retriever and generator.These optimizations improve the interaction between input queries, retrieval mechanisms, and generation processes, ensuring the accuracy and relevance of responses.\n\n## Retriever and Generator Fine-tuning\nFine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context [30][31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges.\n\nSeveral surveys have extensively discussed current RAG systems, covering aspects like text generation [7,8], integration with LLMs [6,39], multimodal [40], and AI-generated content [41].While these surveys provide comprehensive overviews of existing RAG methodologies, selecting the appro-  priate algorithm for practical implementation remains challenging.In this paper, we focus on best practices for applying RAG methods, advancing the understanding and application of RAG in LLMs.",
            "reference_string": "[270870251 | Wang et al. | 2024 | Citations: 61]"
        },
        {
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 251,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.10677, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1491233507",
                    "name": "Mingyue Cheng"
                },
                {
                    "authorId": "2208917508",
                    "name": "Yucong Luo"
                },
                {
                    "authorId": "2322501286",
                    "name": "Ouyang Jie"
                },
                {
                    "authorId": "2332691115",
                    "name": "Qi Liu"
                },
                {
                    "authorId": "2312648865",
                    "name": "Huijie Liu"
                },
                {
                    "authorId": "2291070758",
                    "name": "Li Li"
                },
                {
                    "authorId": "2322429208",
                    "name": "Shuo Yu"
                },
                {
                    "authorId": "2351226328",
                    "name": "Bohou Zhang"
                },
                {
                    "authorId": "2350426005",
                    "name": "Jiawei Cao"
                },
                {
                    "authorId": "2350427710",
                    "name": "Jie Ma"
                },
                {
                    "authorId": "2322524150",
                    "name": "Daoyu Wang"
                },
                {
                    "authorId": "2258714945",
                    "name": "Enhong Chen"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has gained significant attention in recent years for its potential to enhance natural language understanding and generation by combining large-scale retrieval systems with generative models. RAG leverages external knowledge sources, such as documents, databases, or structured data, to improve model performance and generate more accurate and contextually relevant outputs. This survey aims to provide a comprehensive overview of RAG by examining its fundamental components, including retrieval mechanisms, generation processes, and the integration between the two. We discuss the key characteristics of RAG, such as its ability to augment generative models with dynamic external knowledge, and the challenges associated with aligning retrieved information with generative objectives. We also present a taxonomy that categorizes RAG methods, ranging from basic retrieval-augmented approaches to more advanced models incorporating multi-modal data and reasoning capabilities. Additionally, we review the evaluation benchmarks and datasets commonly used to assess RAG systems, along with a detailed exploration of its applications in fields such as question answering, summarization, and information retrieval. Finally, we highlight emerging research directions and opportunities for improving RAG systems, such as enhanced retrieval efficiency, model interpretability, and domain-specific adaptations. This paper concludes by outlining the prospects for RAG in addressing real-world challenges and its potential to drive further advancements in natural language processing.",
            "corpus_id": 277043297,
            "sentences": [
                {
                    "corpus_id": "277043297",
                    "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
                    "text": "Generator-Guided Retriever Training. Conversely, generator-guided retriever training focuses on optimizing the retriever based on the generator's performance and requirements. In this paradigm, the generator's ability to produce coherent and accurate text influences the retriever's selection process. DKRR [95] leverages the generator's attention scores to fine-tune the retriever, enhancing its capability to select the most pertinent information. AAR [272] employs smaller language models to generate supervision signals that guide the retriever's training, ensuring that the retrieved documents are optimally aligned with the generator's needs. RA-DIT [140] fine-tunes large language models before training the retriever, fostering better alignment and synergy between the two components. Additionally, UPRISE [33] uses a frozen LLM to guide the fine-tuning of a prompt retriever, thereby improving its effectiveness in retrieving data that the generator can utilize more effectively. This bidirectional influence ensures that the retriever evolves in tandem with the generator, fostering a more integrated and efficient RAG system.",
                    "score": 0.744215238402276,
                    "section_title": "RAG Training",
                    "char_start_offset": 76826,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 36
                        },
                        {
                            "start": 37,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 301
                        },
                        {
                            "start": 302,
                            "end": 449
                        },
                        {
                            "start": 450,
                            "end": 648
                        },
                        {
                            "start": 649,
                            "end": 792
                        },
                        {
                            "start": 793,
                            "end": 988
                        },
                        {
                            "start": 989,
                            "end": 1136
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97412109375
                },
                {
                    "corpus_id": "277043297",
                    "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
                    "text": "refining both the retriever and the generator to achieve optimal overall system performance. This holistic approach ensures that improvements in one component reinforce enhancements in the other, fostering a synergistic relationship that maximizes the capabilities of the entire RAG system. By jointly training both components, the system can better align the retrieval of relevant documents with the generation of accurate and coherent text. Notable implementations of this approach include RAG [126] and the work by [81], which utilize a joint training paradigm in conjunction with Maximum Inner Product Search (MIPS) [53,200] to optimize the retrieval process effectively. This integrated training methodology allows the retriever to progressively improve its relevance scoring in response to the generator's feedback, while the generator concurrently learns to better utilize the retrieved information. The result is a more robust and question-answering by retrieving both images and text, while RA-CM3 [262] extends this capability by enabling simultaneous retrieval and generation of text and images. Recent developments like Transfusion [286] demonstrate the potential of unifying language modeling with diffusion to train a single transformer that can both understand and generate across modalities. Similarly, Show-o [251] presents a unified transformer that combines autoregressive and discrete diffusion modeling to flexibly support a wide range of vision-language tasks including visual question-answering, text-to-image generation, and text-guided inpainting/extrapolation. Other advances like VisRAG [268] showcase the potential of multimodal RAG by leveraging both visual and textual information to achieve more comprehensive document understanding, while LA-RAG [135] advances speech processing by incorporating fine-grained token-level speech retrieval.",
                    "score": 0.5853689848114858,
                    "section_title": "Collaborative Training. Collaborative optimization training adopts a co-optimization strategy, simultaneously",
                    "char_start_offset": 78076,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 92
                        },
                        {
                            "start": 93,
                            "end": 290
                        },
                        {
                            "start": 291,
                            "end": 442
                        },
                        {
                            "start": 443,
                            "end": 675
                        },
                        {
                            "start": 676,
                            "end": 906
                        },
                        {
                            "start": 907,
                            "end": 1106
                        },
                        {
                            "start": 1107,
                            "end": 1307
                        },
                        {
                            "start": 1308,
                            "end": 1586
                        },
                        {
                            "start": 1587,
                            "end": 1870
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 496,
                            "end": 501,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 518,
                            "end": 522,
                            "matchedPaperCorpusId": "211204736"
                        },
                        {
                            "start": 620,
                            "end": 624,
                            "matchedPaperCorpusId": "57573732"
                        },
                        {
                            "start": 624,
                            "end": 628,
                            "matchedPaperCorpusId": "514516"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9697265625
                },
                {
                    "corpus_id": "277043297",
                    "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
                    "text": "Training RAG models requires balancing the optimization of both retrieval and generation components to achieve optimal performance. Effective training strategies ensure the retriever fetches relevant information while the generator produces coherent and accurate outputs. This section reviews various methods in RAG training, including static training, unidirectional guided training, and collaborative training, shown in Figure 8. Each approach offers distinct benefits and challenges, affecting the effectiveness and adaptability of RAG models across applications. By exploring these paradigms, we can enhance the integration of retrieval and generation processes, ultimately improving RAG performance. This method is particularly advantageous in scenarios with limited computational resources or when rapid deployment is essential. For instance, fixing the retriever while optimizing the generator allows the system to benefit from established retrieval mechanisms, such as traditional BM25 [204] or pre-trained models like BERT [49], without the overhead of simultaneously training the retriever. This can lead to faster training cycles and reduced resource consumption. \n\nHowever, the primary drawback of static optimization is the potential compromise in overall system performance. \n\nSince only one component is being optimized, the synergy between retrieval and generation may not be fully realized, potentially limiting the model's ability to adapt to specific tasks or domains. To mitigate this, careful selection of the fixed component and the optimization process is essential to ensure that the evolving component can effectively leverage the fixed information. to better integrate and utilize external information. For example, RETRO [23] employs a pre-trained BERT model as the retriever to provide relevant context that enhances the generator's output. Similarly, RALMs [264] use a pre-trained COLBERTV2 retriever to fine-tune large language models (LLMs), thereby improving the generator's capacity to incorporate retrieved data effectively. ITER-RTGEN [217] utilizes S-BERT to guide the fine-tuning of a T5 generator, ensuring that the generated text aligns closely with the relevant retrieved information. Moreover, SMALLCAP [201] integrates CLIP as a retriever to direct a GPT-2 model in generating accurate and contextually appropriate captions. \n\nThis guided approach ensures that the generator benefits from a steady stream of relevant information, enhancing the overall quality and relevance of the generated content. \n\nGenerator-Guided Retriever Training.",
                    "score": 0.7139034406042973,
                    "section_title": "RAG Training",
                    "char_start_offset": 74282,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 271
                        },
                        {
                            "start": 272,
                            "end": 431
                        },
                        {
                            "start": 432,
                            "end": 566
                        },
                        {
                            "start": 567,
                            "end": 704
                        },
                        {
                            "start": 705,
                            "end": 834
                        },
                        {
                            "start": 835,
                            "end": 1100
                        },
                        {
                            "start": 1101,
                            "end": 1174
                        },
                        {
                            "start": 1177,
                            "end": 1288
                        },
                        {
                            "start": 1291,
                            "end": 1487
                        },
                        {
                            "start": 1488,
                            "end": 1674
                        },
                        {
                            "start": 1675,
                            "end": 1728
                        },
                        {
                            "start": 1729,
                            "end": 1868
                        },
                        {
                            "start": 1869,
                            "end": 2058
                        },
                        {
                            "start": 2059,
                            "end": 2224
                        },
                        {
                            "start": 2225,
                            "end": 2366
                        },
                        {
                            "start": 2369,
                            "end": 2541
                        },
                        {
                            "start": 2544,
                            "end": 2580
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 994,
                            "end": 999,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 1748,
                            "end": 1752,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 2244,
                            "end": 2249,
                            "matchedPaperCorpusId": "252668790"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96044921875
                }
            ],
            "relevance_judgement": 0.97412109375,
            "relevance_judgment_input_expanded": "# Title: A Survey on Knowledge-Oriented Retrieval-Augmented Generation\n# Venue: arXiv.org\n# Authors: Mingyue Cheng, Yucong Luo, Ouyang Jie, Qi Liu, Huijie Liu, Li Li, Shuo Yu, Bohou Zhang, Jiawei Cao, Jie Ma, Daoyu Wang, Enhong Chen\n## Abstract\nRetrieval-Augmented Generation (RAG) has gained significant attention in recent years for its potential to enhance natural language understanding and generation by combining large-scale retrieval systems with generative models. RAG leverages external knowledge sources, such as documents, databases, or structured data, to improve model performance and generate more accurate and contextually relevant outputs. This survey aims to provide a comprehensive overview of RAG by examining its fundamental components, including retrieval mechanisms, generation processes, and the integration between the two. We discuss the key characteristics of RAG, such as its ability to augment generative models with dynamic external knowledge, and the challenges associated with aligning retrieved information with generative objectives. We also present a taxonomy that categorizes RAG methods, ranging from basic retrieval-augmented approaches to more advanced models incorporating multi-modal data and reasoning capabilities. Additionally, we review the evaluation benchmarks and datasets commonly used to assess RAG systems, along with a detailed exploration of its applications in fields such as question answering, summarization, and information retrieval. Finally, we highlight emerging research directions and opportunities for improving RAG systems, such as enhanced retrieval efficiency, model interpretability, and domain-specific adaptations. This paper concludes by outlining the prospects for RAG in addressing real-world challenges and its potential to drive further advancements in natural language processing.\n## RAG Training\nTraining RAG models requires balancing the optimization of both retrieval and generation components to achieve optimal performance. Effective training strategies ensure the retriever fetches relevant information while the generator produces coherent and accurate outputs. This section reviews various methods in RAG training, including static training, unidirectional guided training, and collaborative training, shown in Figure 8. Each approach offers distinct benefits and challenges, affecting the effectiveness and adaptability of RAG models across applications. By exploring these paradigms, we can enhance the integration of retrieval and generation processes, ultimately improving RAG performance. This method is particularly advantageous in scenarios with limited computational resources or when rapid deployment is essential. For instance, fixing the retriever while optimizing the generator allows the system to benefit from established retrieval mechanisms, such as traditional BM25 [204] or pre-trained models like BERT [49], without the overhead of simultaneously training the retriever. This can lead to faster training cycles and reduced resource consumption. \n\nHowever, the primary drawback of static optimization is the potential compromise in overall system performance. \n\nSince only one component is being optimized, the synergy between retrieval and generation may not be fully realized, potentially limiting the model's ability to adapt to specific tasks or domains. To mitigate this, careful selection of the fixed component and the optimization process is essential to ensure that the evolving component can effectively leverage the fixed information. to better integrate and utilize external information. For example, RETRO [23] employs a pre-trained BERT model as the retriever to provide relevant context that enhances the generator's output. Similarly, RALMs [264] use a pre-trained COLBERTV2 retriever to fine-tune large language models (LLMs), thereby improving the generator's capacity to incorporate retrieved data effectively. ITER-RTGEN [217] utilizes S-BERT to guide the fine-tuning of a T5 generator, ensuring that the generated text aligns closely with the relevant retrieved information. Moreover, SMALLCAP [201] integrates CLIP as a retriever to direct a GPT-2 model in generating accurate and contextually appropriate captions. \n\nThis guided approach ensures that the generator benefits from a steady stream of relevant information, enhancing the overall quality and relevance of the generated content. \n\nGenerator-Guided Retriever Training.\n...\nGenerator-Guided Retriever Training. Conversely, generator-guided retriever training focuses on optimizing the retriever based on the generator's performance and requirements. In this paradigm, the generator's ability to produce coherent and accurate text influences the retriever's selection process. DKRR [95] leverages the generator's attention scores to fine-tune the retriever, enhancing its capability to select the most pertinent information. AAR [272] employs smaller language models to generate supervision signals that guide the retriever's training, ensuring that the retrieved documents are optimally aligned with the generator's needs. RA-DIT [140] fine-tunes large language models before training the retriever, fostering better alignment and synergy between the two components. Additionally, UPRISE [33] uses a frozen LLM to guide the fine-tuning of a prompt retriever, thereby improving its effectiveness in retrieving data that the generator can utilize more effectively. This bidirectional influence ensures that the retriever evolves in tandem with the generator, fostering a more integrated and efficient RAG system.\n\n## Collaborative Training. Collaborative optimization training adopts a co-optimization strategy, simultaneously\nrefining both the retriever and the generator to achieve optimal overall system performance. This holistic approach ensures that improvements in one component reinforce enhancements in the other, fostering a synergistic relationship that maximizes the capabilities of the entire RAG system. By jointly training both components, the system can better align the retrieval of relevant documents with the generation of accurate and coherent text. Notable implementations of this approach include RAG [126] and the work by [81], which utilize a joint training paradigm in conjunction with Maximum Inner Product Search (MIPS) [53,200] to optimize the retrieval process effectively. This integrated training methodology allows the retriever to progressively improve its relevance scoring in response to the generator's feedback, while the generator concurrently learns to better utilize the retrieved information. The result is a more robust and question-answering by retrieving both images and text, while RA-CM3 [262] extends this capability by enabling simultaneous retrieval and generation of text and images. Recent developments like Transfusion [286] demonstrate the potential of unifying language modeling with diffusion to train a single transformer that can both understand and generate across modalities. Similarly, Show-o [251] presents a unified transformer that combines autoregressive and discrete diffusion modeling to flexibly support a wide range of vision-language tasks including visual question-answering, text-to-image generation, and text-guided inpainting/extrapolation. Other advances like VisRAG [268] showcase the potential of multimodal RAG by leveraging both visual and textual information to achieve more comprehensive document understanding, while LA-RAG [135] advances speech processing by incorporating fine-grained token-level speech retrieval.",
            "reference_string": "[277043297 | Cheng et al. | 2025 | Citations: 6]"
        },
        {
            "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.18365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275628230",
                    "name": "Yiteng Tu"
                },
                {
                    "authorId": "2147219374",
                    "name": "Weihang Su"
                },
                {
                    "authorId": "2290870875",
                    "name": "Yujia Zhou"
                },
                {
                    "authorId": "2260835922",
                    "name": "Yiqun Liu"
                },
                {
                    "authorId": "2256982003",
                    "name": "Qingyao Ai"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.",
            "corpus_id": 275993994,
            "sentences": [
                {
                    "corpus_id": "275993994",
                    "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects",
                    "text": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.",
                    "score": 0.7212100433580074,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97265625
                }
            ],
            "relevance_judgement": 0.97265625,
            "relevance_judgment_input_expanded": "# Title: RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects\n# Venue: arXiv.org\n# Authors: Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, Qingyao Ai\n## Abstract\nRetrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.\n",
            "reference_string": "[275993994 | Tu et al. | 2025 | Citations: 6]"
        },
        {
            "title": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning",
            "venue": "",
            "year": 2025,
            "reference_count": 33,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.10493, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2361597409",
                    "name": "Shaohan Wang"
                },
                {
                    "authorId": "48378753",
                    "name": "L. Zhang"
                },
                {
                    "authorId": "2106681735",
                    "name": "Zheren Fu"
                },
                {
                    "authorId": "2349977855",
                    "name": "Zhendong Mao"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) is an effective method to enhance the capabilities of large language models (LLMs). Existing methods focus on optimizing the retriever or generator in the RAG system by directly utilizing the top-k retrieved documents. However, the documents effectiveness are various significantly across user queries, i.e. some documents provide valuable knowledge while others totally lack critical information. It hinders the retriever and generator's adaptation during training. Inspired by human cognitive learning, curriculum learning trains models using samples progressing from easy to difficult, thus enhancing their generalization ability, and we integrate this effective paradigm to the training of the RAG system. In this paper, we propose a multi-stage Curriculum Learning based RAG system training framework, named CL-RAG. We first construct training data with multiple difficulty levels for the retriever and generator separately through sample evolution. Then, we train the model in stages based on the curriculum learning approach, thereby optimizing the overall performance and generalization of the RAG system more effectively. Our CL-RAG framework demonstrates consistent effectiveness across four open-domain QA datasets, achieving performance gains of 2% to 4% over multiple advanced methods.",
            "corpus_id": 278635834,
            "sentences": [
                {
                    "corpus_id": "278635834",
                    "title": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning",
                    "text": "Using documents retrieved from extended knowledge bases to enhance the capabilities of large language models (LLMs) has been proven effective in NLP tasks, including language modeling (Borgeaud et al., 2022;Ram et al., 2023;Zhang et al., 2024) and question answering (Izacard et al., 2023;Shi et al., 2023;Yoran et al., 2023;Lin et al., 2023;Fang et al., 2024). Specifically, a Retrieval-Augmented Generator (RAG) system takes a query as input and uses a retriever to retrieve relevant documents from an external knowledge base. Then, it combines the documents with the query and feeds them into the LLM to make up for the LLM's own lack of knowledge. Optimization of the RAG system focuses on two main areas: improving the retriever and enhancing the generator (LLM) to the RALM. Replug (Shi et al., 2023) uses KL divergence to align retriever results with LLM preferences. LLM-Embedder (Zhang et al., 2024) employs a distillation objective based on LLM rankings. These methods train the retriever to better match LLM preferences. For generator, FiD (Izacard and Grave, 2020) finetunes LLM to handle retrieved documents and queries, addressing irrelevant information. Other studies introduce noise to improve LLM robustness (Yoran et al., 2023;Fang et al., 2024). Combining the strengths of both approaches, RA-DIT (Lin et al., 2023) uses modular training to optimize the retriever and LLM separately, enhancing overall RAG system performance.",
                    "score": 0.6083343866543466,
                    "section_title": "Retrieval-augmented Generation",
                    "char_start_offset": 5200,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 361
                        },
                        {
                            "start": 362,
                            "end": 528
                        },
                        {
                            "start": 529,
                            "end": 651
                        },
                        {
                            "start": 652,
                            "end": 780
                        },
                        {
                            "start": 781,
                            "end": 874
                        },
                        {
                            "start": 875,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1031
                        },
                        {
                            "start": 1032,
                            "end": 1168
                        },
                        {
                            "start": 1169,
                            "end": 1264
                        },
                        {
                            "start": 1265,
                            "end": 1444
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 184,
                            "end": 207,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 207,
                            "end": 224,
                            "matchedPaperCorpusId": "256459451"
                        },
                        {
                            "start": 224,
                            "end": 243,
                            "matchedPaperCorpusId": "271915498"
                        },
                        {
                            "start": 267,
                            "end": 289,
                            "matchedPaperCorpusId": "251371732"
                        },
                        {
                            "start": 306,
                            "end": 325,
                            "matchedPaperCorpusId": "263608822"
                        },
                        {
                            "start": 325,
                            "end": 342,
                            "matchedPaperCorpusId": "263605962"
                        },
                        {
                            "start": 888,
                            "end": 908,
                            "matchedPaperCorpusId": "271915498"
                        },
                        {
                            "start": 1225,
                            "end": 1245,
                            "matchedPaperCorpusId": "263608822"
                        },
                        {
                            "start": 1316,
                            "end": 1334,
                            "matchedPaperCorpusId": "263605962"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9697265625
                }
            ],
            "relevance_judgement": 0.9697265625,
            "relevance_judgment_input_expanded": "# Title: CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning\n# Venue: \n# Authors: Shaohan Wang, L. Zhang, Zheren Fu, Zhendong Mao\n## Abstract\nRetrieval-Augmented Generation (RAG) is an effective method to enhance the capabilities of large language models (LLMs). Existing methods focus on optimizing the retriever or generator in the RAG system by directly utilizing the top-k retrieved documents. However, the documents effectiveness are various significantly across user queries, i.e. some documents provide valuable knowledge while others totally lack critical information. It hinders the retriever and generator's adaptation during training. Inspired by human cognitive learning, curriculum learning trains models using samples progressing from easy to difficult, thus enhancing their generalization ability, and we integrate this effective paradigm to the training of the RAG system. In this paper, we propose a multi-stage Curriculum Learning based RAG system training framework, named CL-RAG. We first construct training data with multiple difficulty levels for the retriever and generator separately through sample evolution. Then, we train the model in stages based on the curriculum learning approach, thereby optimizing the overall performance and generalization of the RAG system more effectively. Our CL-RAG framework demonstrates consistent effectiveness across four open-domain QA datasets, achieving performance gains of 2% to 4% over multiple advanced methods.\n## Retrieval-augmented Generation\nUsing documents retrieved from extended knowledge bases to enhance the capabilities of large language models (LLMs) has been proven effective in NLP tasks, including language modeling (Borgeaud et al., 2022;Ram et al., 2023;Zhang et al., 2024) and question answering (Izacard et al., 2023;Shi et al., 2023;Yoran et al., 2023;Lin et al., 2023;Fang et al., 2024). Specifically, a Retrieval-Augmented Generator (RAG) system takes a query as input and uses a retriever to retrieve relevant documents from an external knowledge base. Then, it combines the documents with the query and feeds them into the LLM to make up for the LLM's own lack of knowledge. Optimization of the RAG system focuses on two main areas: improving the retriever and enhancing the generator (LLM) to the RALM. Replug (Shi et al., 2023) uses KL divergence to align retriever results with LLM preferences. LLM-Embedder (Zhang et al., 2024) employs a distillation objective based on LLM rankings. These methods train the retriever to better match LLM preferences. For generator, FiD (Izacard and Grave, 2020) finetunes LLM to handle retrieved documents and queries, addressing irrelevant information. Other studies introduce noise to improve LLM robustness (Yoran et al., 2023;Fang et al., 2024). Combining the strengths of both approaches, RA-DIT (Lin et al., 2023) uses modular training to optimize the retriever and LLM separately, enhancing overall RAG system performance.",
            "reference_string": "[278635834 | Wang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 20,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.21059, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280046531",
                    "name": "Yunfan Gao"
                },
                {
                    "authorId": "2275320371",
                    "name": "Yun Xiong"
                },
                {
                    "authorId": "2291409458",
                    "name": "Meng Wang"
                },
                {
                    "authorId": "2256769434",
                    "name": "Haofen Wang"
                }
            ],
            "abstract": "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of\"retrieve-then-generate\". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies.",
            "corpus_id": 271571401,
            "sentences": [
                {
                    "corpus_id": "271571401",
                    "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
                    "text": "RAG is continuously integrating with more LLM-related technologies. In Modular RAG, many components are composed of trainable language models. Through fine-tuning, the performance of the components and the compatibility with the overall flow can be further optimized. This section will introduce three main patterns of fine-tuning stages, namely retriever fine-tuning, generator fine-tuning, and dual finetuning. 1) Retriever FT: In the RAG flow, common methods for fine-tuning the retriever is shown in Figure 15 ,which include: \n\n\u2022 Direct supervised fine-tuning of the retriever. Constructing a specialized dataset for retrieval and fine-tuning the dense retriever. For example, using open-source retrieval datasets or constructing one based on domain-specific data. \n\n\u2022 Adding trainable adapter modules. Sometimes, direct fine-tuning of the API-base embedding model (e.g., Ope-nAI Ada-002 and Cohere) is not feasible. Incorporating an adapter module can enhance the representation of your data. Additionally, the adapter module facilitates better alignment with downstream tasks, whether for taskspecific (e.g., PRCA [42]) or general purposes (e.g., AAR [58]). \u2022 LM-supervised Retrieval (LSR). Fine-tuning the retriever based on the results generated by LLM. \u2022 LLM Reward RL. Still using the LLM output results as the supervisory signal. Employing reinforcement learning to align the retriever with the generator. The whole retrieval process is disassembled in the form of a generative Markov chain. 2) Generator FT: The primary methods for fine-tuning a generator in RAG flow is shown in Figure 16, which include: \n\n\u2022 Direct supervised fine-tuning. Fine-tuning through an external dataset can supplement the generator with additional knowledge. Another benefit is the ability to customize input and output formats. By setting the Q&A format, LLM can understand specific data formats and output according to instructions. \u2022 Distillation. When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model.",
                    "score": 0.7153928392445175,
                    "section_title": "E. Tuning Pattern",
                    "char_start_offset": 55399,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 67
                        },
                        {
                            "start": 68,
                            "end": 142
                        },
                        {
                            "start": 143,
                            "end": 267
                        },
                        {
                            "start": 268,
                            "end": 529
                        },
                        {
                            "start": 532,
                            "end": 581
                        },
                        {
                            "start": 582,
                            "end": 667
                        },
                        {
                            "start": 668,
                            "end": 768
                        },
                        {
                            "start": 771,
                            "end": 806
                        },
                        {
                            "start": 807,
                            "end": 920
                        },
                        {
                            "start": 921,
                            "end": 997
                        },
                        {
                            "start": 998,
                            "end": 1163
                        },
                        {
                            "start": 1164,
                            "end": 1196
                        },
                        {
                            "start": 1197,
                            "end": 1261
                        },
                        {
                            "start": 1262,
                            "end": 1278
                        },
                        {
                            "start": 1279,
                            "end": 1340
                        },
                        {
                            "start": 1341,
                            "end": 1416
                        },
                        {
                            "start": 1417,
                            "end": 1502
                        },
                        {
                            "start": 1503,
                            "end": 1617
                        },
                        {
                            "start": 1620,
                            "end": 1652
                        },
                        {
                            "start": 1653,
                            "end": 1748
                        },
                        {
                            "start": 1749,
                            "end": 1818
                        },
                        {
                            "start": 1819,
                            "end": 1924
                        },
                        {
                            "start": 1925,
                            "end": 1940
                        },
                        {
                            "start": 1941,
                            "end": 2144
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96435546875
                },
                {
                    "corpus_id": "271571401",
                    "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
                    "text": "When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model. \u2022 RL from LLM/human feedback. Reinforcement learning based on feedback from the final generated answers. In addition to using human evaluations, powerful LLMs can also serve as an evaluative judge. 3) Dual FT: In the RAG system, fine-tuning both the retriever and the generator simultaneously is a unique feature of the RAG system. It is important to note that the emphasis of system fine-tuning is on the coordination between the retriever and the generator. An exemplary implementation is RA-DIT [27], which fine-tunes both the LLM and the retriever. The LM-ft component updates the LLM to maximize the Fig. 16. Generator fine-tuning pattern, The main methods include SFT, distillation and RL from LLM/human feedback. Fig. 17. Dual fine-tuning pattern. In this mode, both the retriever and generator participate in fine-tuning, and their preferences will be aligned. likelihood of the correct answer given the retrieval-augmented instructions while the R-ft component updates the retriever to minimize the KL-Divergence between the retriever score distribution and the LLM preference.",
                    "score": 0.7296369220165467,
                    "section_title": "E. Tuning Pattern",
                    "char_start_offset": 57340,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 233
                        },
                        {
                            "start": 234,
                            "end": 308
                        },
                        {
                            "start": 309,
                            "end": 401
                        },
                        {
                            "start": 402,
                            "end": 535
                        },
                        {
                            "start": 536,
                            "end": 663
                        },
                        {
                            "start": 664,
                            "end": 756
                        },
                        {
                            "start": 757,
                            "end": 817
                        },
                        {
                            "start": 818,
                            "end": 923
                        },
                        {
                            "start": 924,
                            "end": 932
                        },
                        {
                            "start": 933,
                            "end": 958
                        },
                        {
                            "start": 959,
                            "end": 1072
                        },
                        {
                            "start": 1073,
                            "end": 1290
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94970703125
                },
                {
                    "corpus_id": "271571401",
                    "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
                    "text": "The retrieval process is pivotal in RAG systems. By leveraging powerful embedding models, queries and text can be efficiently represented in latent spaces, which facilitates the establishment of semantic similarity between questions and documents, thereby enhancing retrieval. Three main considerations that need to be addressed include retrieval efficiency, quality, and the alignment of tasks, data and models. \n\n1) Retriever Selection: With the widespread adoption of RAG technology, the development of embedding models has been in full swing. In addition to traditional models based on statistics and pre-trained models based on the encoder structure, embedding models fine-tuned on LLMs have also demonstrated powerful capabilities [39]. However, they often come with more parameters, leading to weaker inference and retrieval efficiency. Therefore, it is crucial to select the appropriate retriever based on different task scenarios. \n\nSparse Retriever uses statistical methods to convert queries and documents into sparse vectors. Its advantage lies in its efficiency in handling large datasets, focusing only on non-zero elements. However, it may be less effective than dense vectors in capturing complex semantics. Common methods include TF-IDF and BM25. \n\nDense Retriever employs pre-trained language models (PLMs) to provide dense representations of queries and documents. Despite higher computational and storage costs, it offers more complex semantic representations. Typical models include BERT structure PLMs, like ColBERT, and multi-task fine-tuned models like BGE [40] and GTE [41]. \n\nHybrid Retriever is to use both sparse and dense retrievers simultaneously. Two embedding techniques complement each other to enhance retrieval effectiveness. Sparse retriever can provide initial screening results. Additionally, sparse models enhance the zero-shot retrieval capabilities of dense models, particularly in handling queries with rare entities, thereby increasing system robustness. \n\n2) Retriever Fine-tuning: In cases where the context may diverge from pre-trained corpus, particularly in highly specialized fields like healthcare, law, and other domains abundant in proprietary terminology. While this adjustment demands additional effort, it can substantially enhance retrieval efficiency and domain alignment. \n\nSupervised Fine-Tuning (SFT). Fine-tuning a retrieval model based on labeled domain data is typically done using contrastive learning. This involves reducing the distance between positive samples while increasing the distance between negative samples.",
                    "score": 0.6118894132939461,
                    "section_title": "C. Retrieval",
                    "char_start_offset": 21512,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 48
                        },
                        {
                            "start": 49,
                            "end": 276
                        },
                        {
                            "start": 277,
                            "end": 412
                        },
                        {
                            "start": 415,
                            "end": 546
                        },
                        {
                            "start": 547,
                            "end": 742
                        },
                        {
                            "start": 743,
                            "end": 843
                        },
                        {
                            "start": 844,
                            "end": 939
                        },
                        {
                            "start": 942,
                            "end": 1037
                        },
                        {
                            "start": 1038,
                            "end": 1138
                        },
                        {
                            "start": 1139,
                            "end": 1223
                        },
                        {
                            "start": 1224,
                            "end": 1263
                        },
                        {
                            "start": 1266,
                            "end": 1383
                        },
                        {
                            "start": 1384,
                            "end": 1480
                        },
                        {
                            "start": 1481,
                            "end": 1599
                        },
                        {
                            "start": 1602,
                            "end": 1677
                        },
                        {
                            "start": 1678,
                            "end": 1760
                        },
                        {
                            "start": 1761,
                            "end": 1816
                        },
                        {
                            "start": 1817,
                            "end": 1997
                        },
                        {
                            "start": 2000,
                            "end": 2208
                        },
                        {
                            "start": 2209,
                            "end": 2329
                        },
                        {
                            "start": 2332,
                            "end": 2361
                        },
                        {
                            "start": 2362,
                            "end": 2466
                        },
                        {
                            "start": 2467,
                            "end": 2583
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.919921875
                }
            ],
            "relevance_judgement": 0.96435546875,
            "relevance_judgment_input_expanded": "# Title: Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n# Venue: arXiv.org\n# Authors: Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n## Abstract\nRetrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of\"retrieve-then-generate\". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies.\n## C. Retrieval\nThe retrieval process is pivotal in RAG systems. By leveraging powerful embedding models, queries and text can be efficiently represented in latent spaces, which facilitates the establishment of semantic similarity between questions and documents, thereby enhancing retrieval. Three main considerations that need to be addressed include retrieval efficiency, quality, and the alignment of tasks, data and models. \n\n1) Retriever Selection: With the widespread adoption of RAG technology, the development of embedding models has been in full swing. In addition to traditional models based on statistics and pre-trained models based on the encoder structure, embedding models fine-tuned on LLMs have also demonstrated powerful capabilities [39]. However, they often come with more parameters, leading to weaker inference and retrieval efficiency. Therefore, it is crucial to select the appropriate retriever based on different task scenarios. \n\nSparse Retriever uses statistical methods to convert queries and documents into sparse vectors. Its advantage lies in its efficiency in handling large datasets, focusing only on non-zero elements. However, it may be less effective than dense vectors in capturing complex semantics. Common methods include TF-IDF and BM25. \n\nDense Retriever employs pre-trained language models (PLMs) to provide dense representations of queries and documents. Despite higher computational and storage costs, it offers more complex semantic representations. Typical models include BERT structure PLMs, like ColBERT, and multi-task fine-tuned models like BGE [40] and GTE [41]. \n\nHybrid Retriever is to use both sparse and dense retrievers simultaneously. Two embedding techniques complement each other to enhance retrieval effectiveness. Sparse retriever can provide initial screening results. Additionally, sparse models enhance the zero-shot retrieval capabilities of dense models, particularly in handling queries with rare entities, thereby increasing system robustness. \n\n2) Retriever Fine-tuning: In cases where the context may diverge from pre-trained corpus, particularly in highly specialized fields like healthcare, law, and other domains abundant in proprietary terminology. While this adjustment demands additional effort, it can substantially enhance retrieval efficiency and domain alignment. \n\nSupervised Fine-Tuning (SFT). Fine-tuning a retrieval model based on labeled domain data is typically done using contrastive learning. This involves reducing the distance between positive samples while increasing the distance between negative samples.\n\n## E. Tuning Pattern\nRAG is continuously integrating with more LLM-related technologies. In Modular RAG, many components are composed of trainable language models. Through fine-tuning, the performance of the components and the compatibility with the overall flow can be further optimized. This section will introduce three main patterns of fine-tuning stages, namely retriever fine-tuning, generator fine-tuning, and dual finetuning. 1) Retriever FT: In the RAG flow, common methods for fine-tuning the retriever is shown in Figure 15 ,which include: \n\n\u2022 Direct supervised fine-tuning of the retriever. Constructing a specialized dataset for retrieval and fine-tuning the dense retriever. For example, using open-source retrieval datasets or constructing one based on domain-specific data. \n\n\u2022 Adding trainable adapter modules. Sometimes, direct fine-tuning of the API-base embedding model (e.g., Ope-nAI Ada-002 and Cohere) is not feasible. Incorporating an adapter module can enhance the representation of your data. Additionally, the adapter module facilitates better alignment with downstream tasks, whether for taskspecific (e.g., PRCA [42]) or general purposes (e.g., AAR [58]). \u2022 LM-supervised Retrieval (LSR). Fine-tuning the retriever based on the results generated by LLM. \u2022 LLM Reward RL. Still using the LLM output results as the supervisory signal. Employing reinforcement learning to align the retriever with the generator. The whole retrieval process is disassembled in the form of a generative Markov chain. 2) Generator FT: The primary methods for fine-tuning a generator in RAG flow is shown in Figure 16, which include: \n\n\u2022 Direct supervised fine-tuning. Fine-tuning through an external dataset can supplement the generator with additional knowledge. Another benefit is the ability to customize input and output formats. By setting the Q&A format, LLM can understand specific data formats and output according to instructions. \u2022 Distillation. When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model.\n...\nWhen using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model. \u2022 RL from LLM/human feedback. Reinforcement learning based on feedback from the final generated answers. In addition to using human evaluations, powerful LLMs can also serve as an evaluative judge. 3) Dual FT: In the RAG system, fine-tuning both the retriever and the generator simultaneously is a unique feature of the RAG system. It is important to note that the emphasis of system fine-tuning is on the coordination between the retriever and the generator. An exemplary implementation is RA-DIT [27], which fine-tunes both the LLM and the retriever. The LM-ft component updates the LLM to maximize the Fig. 16. Generator fine-tuning pattern, The main methods include SFT, distillation and RL from LLM/human feedback. Fig. 17. Dual fine-tuning pattern. In this mode, both the retriever and generator participate in fine-tuning, and their preferences will be aligned. likelihood of the correct answer given the retrieval-augmented instructions while the R-ft component updates the retriever to minimize the KL-Divergence between the retriever score distribution and the LLM preference.",
            "reference_string": "[271571401 | Gao et al. | 2024 | Citations: 20]"
        },
        {
            "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 84,
            "citation_count": 153,
            "influential_citation_count": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.01352",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.01352, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2255374957",
                    "name": "Xi Victoria Lin"
                },
                {
                    "authorId": "1769736",
                    "name": "Xilun Chen"
                },
                {
                    "authorId": "46221498",
                    "name": "Mingda Chen"
                },
                {
                    "authorId": "2254168373",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2253400960",
                    "name": "Maria Lomeli"
                },
                {
                    "authorId": "2191899140",
                    "name": "Rich James"
                },
                {
                    "authorId": "2253404757",
                    "name": "Pedro Rodriguez"
                },
                {
                    "authorId": "2253401183",
                    "name": "Jacob Kahn"
                },
                {
                    "authorId": "2253402270",
                    "name": "Gergely Szilvasy"
                },
                {
                    "authorId": "2253417398",
                    "name": "Mike Lewis"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "2253400757",
                    "name": "Scott Yih"
                }
            ],
            "abstract": "Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average.",
            "corpus_id": 263605962,
            "sentences": [
                {
                    "corpus_id": "263605962",
                    "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
                    "text": "Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average.",
                    "score": 0.6183732558070992,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96435546875
                },
                {
                    "corpus_id": "263605962",
                    "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
                    "text": "In this paper, we propose RA-DIT, a lightweight Retrieval-Augmented Dual Instruction Tuning framework that can effectively retrofit any pre-trained LLM with retrieval capabilities. RA-DIT updates the LLM with retrieval-augmented instruction tuning to make better use of retrieved knowledge and ignore irrelevant or distracting information. It also fine-tunes the retriever with supervision from the LLM to retrieve texts that can better help the LLM generate correct outputs. RA-DIT achieves state-of-the-art performance in zero-and few-shot evaluations on knowledge intensive benchmarks, surpassing un-tuned in-context RALM approaches such as REPLUG and compete effectively against methods that require extensive pre-training such as ATLAS.",
                    "score": 0.6560944389264464,
                    "section_title": "CONCLUSION",
                    "char_start_offset": 26531,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 180
                        },
                        {
                            "start": 181,
                            "end": 339
                        },
                        {
                            "start": 340,
                            "end": 475
                        },
                        {
                            "start": 476,
                            "end": 741
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93017578125
                },
                {
                    "corpus_id": "263605962",
                    "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
                    "text": "ATLAS (Izacard et al., 2022b) builds upon the T5 language model (Raffel et al., 2020), and continuosly pre-trains the framework over unsupervised text. REPLUG (Shi et al., 2023b) and In-Context RALM (Ram et al., 2023) combine off-the-shelf LLMs with general-purpose retrievers, showing that LLMs and retrievers, even when optimized independently, can be effectively fused through the emergent incontext learning capbabilities of LLMs. However, extensive pre-training of such architectures incurs high computational costs, and the off-the-shelf fusion approach also has limitations, particularly as the LLMs are not inherently trained to incorporate retrieved content. \n\nFigure 1: The RA-DIT approach separately fine-tunes the LLM and the retriever. For a given example, the LM-ft component updates the LLM to maximize the likelihood of the correct answer given the retrieval-augmented instructions ( \u00a72.3); the R-ft component updates the retriever to minimize the KL-Divergence between the retriever score distribution and the LLM preference ( \u00a72.4) \n\nIn this work, we show lightweight instruction tuning (Chung et al., 2022b;Iyer et al., 2022;Zhou et al., 2023) alone can significantly boost the performance of RALMs, especially in scenarios that require access to large, external knowledge sources. We propose Retrieval-Augmented Dual Instruction Tuning (RA-DIT), an approach that retrofits any LLM with retrieval capabilities via fine-tuning over a set of tasks selected to cultivate knowledge utilization and contextual awareness in the language model predictions. We initialize the framework using pre-trained LLAMA (Touvron et al., 2023a) and a state-of-the-art dual-encoder based dense retriever, DRAGON+ (Lin et al., 2023). Following Shi et al. (2023b), we retrieve relevant text chunks based on the language model prompt.",
                    "score": 0.562050437861938,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 1527,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 152,
                            "end": 434
                        },
                        {
                            "start": 435,
                            "end": 667
                        },
                        {
                            "start": 670,
                            "end": 748
                        },
                        {
                            "start": 749,
                            "end": 1049
                        },
                        {
                            "start": 1052,
                            "end": 1300
                        },
                        {
                            "start": 1301,
                            "end": 1568
                        },
                        {
                            "start": 1569,
                            "end": 1731
                        },
                        {
                            "start": 1732,
                            "end": 1830
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 64,
                            "end": 85,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9052734375
                }
            ],
            "relevance_judgement": 0.96435546875,
            "relevance_judgment_input_expanded": "# Title: RA-DIT: Retrieval-Augmented Dual Instruction Tuning\n# Venue: International Conference on Learning Representations\n# Authors: Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke S. Zettlemoyer, Scott Yih\n## Abstract\nRetrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average.\n## INTRODUCTION\nATLAS (Izacard et al., 2022b) builds upon the T5 language model (Raffel et al., 2020), and continuosly pre-trains the framework over unsupervised text. REPLUG (Shi et al., 2023b) and In-Context RALM (Ram et al., 2023) combine off-the-shelf LLMs with general-purpose retrievers, showing that LLMs and retrievers, even when optimized independently, can be effectively fused through the emergent incontext learning capbabilities of LLMs. However, extensive pre-training of such architectures incurs high computational costs, and the off-the-shelf fusion approach also has limitations, particularly as the LLMs are not inherently trained to incorporate retrieved content. \n\nFigure 1: The RA-DIT approach separately fine-tunes the LLM and the retriever. For a given example, the LM-ft component updates the LLM to maximize the likelihood of the correct answer given the retrieval-augmented instructions ( \u00a72.3); the R-ft component updates the retriever to minimize the KL-Divergence between the retriever score distribution and the LLM preference ( \u00a72.4) \n\nIn this work, we show lightweight instruction tuning (Chung et al., 2022b;Iyer et al., 2022;Zhou et al., 2023) alone can significantly boost the performance of RALMs, especially in scenarios that require access to large, external knowledge sources. We propose Retrieval-Augmented Dual Instruction Tuning (RA-DIT), an approach that retrofits any LLM with retrieval capabilities via fine-tuning over a set of tasks selected to cultivate knowledge utilization and contextual awareness in the language model predictions. We initialize the framework using pre-trained LLAMA (Touvron et al., 2023a) and a state-of-the-art dual-encoder based dense retriever, DRAGON+ (Lin et al., 2023). Following Shi et al. (2023b), we retrieve relevant text chunks based on the language model prompt.\n\n## CONCLUSION\nIn this paper, we propose RA-DIT, a lightweight Retrieval-Augmented Dual Instruction Tuning framework that can effectively retrofit any pre-trained LLM with retrieval capabilities. RA-DIT updates the LLM with retrieval-augmented instruction tuning to make better use of retrieved knowledge and ignore irrelevant or distracting information. It also fine-tunes the retriever with supervision from the LLM to retrieve texts that can better help the LLM generate correct outputs. RA-DIT achieves state-of-the-art performance in zero-and few-shot evaluations on knowledge intensive benchmarks, surpassing un-tuned in-context RALM approaches such as REPLUG and compete effectively against methods that require extensive pre-training such as ATLAS.",
            "reference_string": "[263605962 | Lin et al. | 2023 | Citations: 153]"
        },
        {
            "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 127,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.07688, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2270181751",
                    "name": "Zilun Zhang"
                },
                {
                    "authorId": "2174678931",
                    "name": "Haozhan Shen"
                },
                {
                    "authorId": "8200875",
                    "name": "Tiancheng Zhao"
                },
                {
                    "authorId": "2330774884",
                    "name": "Yuhao Wang"
                },
                {
                    "authorId": "2330612748",
                    "name": "Bin Chen"
                },
                {
                    "authorId": "2149196373",
                    "name": "Yuxiang Cai"
                },
                {
                    "authorId": "2093090552",
                    "name": "Yongheng Shang"
                },
                {
                    "authorId": "2111612160",
                    "name": "Jianwei Yin"
                }
            ],
            "abstract": "Ultra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000 $\\times$ 100,000 pixels or more) poses a significant challenge for current Remote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize the UHR image to standard input image size, the extensive spatial and contextual information that UHR images contain will be neglected. Otherwise, the original size of these images often exceeds the token limits of standard RSMLLMs, making it difficult to process the entire image and capture long-range dependencies to answer the query based on the abundant visual context. In this paper, we introduce ImageRAG for RS, a training-free framework to address the complexities of analyzing UHR remote sensing imagery. By transforming UHR remote sensing image analysis task to image's long context selection task, we design an innovative image contextual retrieval mechanism based on the Retrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's core innovation lies in its ability to selectively retrieve and focus on the most relevant portions of the UHR image as visual contexts that pertain to a given query. Fast path and slow path are proposed in this framework to handle this task efficiently and effectively. ImageRAG allows RSMLLMs to manage extensive context and spatial information from UHR RSI, ensuring the analysis is both accurate and efficient. Codebase will be released in https://github.com/om-ai-lab/ImageRAG",
            "corpus_id": 273969615,
            "sentences": [
                {
                    "corpus_id": "273969615",
                    "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
                    "text": "Task-specific advancements in RAG systems focus on refining retrieval-augmented models for particular applications, improving their efficiency and effectiveness in complex tasks. Demonstrate-Search-Predict [116] introduces a modular approach that breaks down complex problems into manageable tasks, enhancing performance in multi-hop reasoning and open-domain question answering. Similarly, RA-DIT [117] uses dual instruction tuning to fine-tune the retriever and generative model, optimizing their collaboration for knowledgeintensive benchmarks. These methods highlight the importance of tailoring RAG systems to specific tasks, enabling more effective and accurate solutions across diverse domains.",
                    "score": 0.6983662324014837,
                    "section_title": "C. Retrieval-Augmented Generation",
                    "char_start_offset": 69936,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 178
                        },
                        {
                            "start": 179,
                            "end": 379
                        },
                        {
                            "start": 380,
                            "end": 547
                        },
                        {
                            "start": 548,
                            "end": 701
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 398,
                            "end": 403,
                            "matchedPaperCorpusId": "263605962"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9609375
                }
            ],
            "relevance_judgement": 0.9609375,
            "relevance_judgment_input_expanded": "# Title: Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG\n# Venue: arXiv.org\n# Authors: Zilun Zhang, Haozhan Shen, Tiancheng Zhao, Yuhao Wang, Bin Chen, Yuxiang Cai, Yongheng Shang, Jianwei Yin\n## Abstract\nUltra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000 $\\times$ 100,000 pixels or more) poses a significant challenge for current Remote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize the UHR image to standard input image size, the extensive spatial and contextual information that UHR images contain will be neglected. Otherwise, the original size of these images often exceeds the token limits of standard RSMLLMs, making it difficult to process the entire image and capture long-range dependencies to answer the query based on the abundant visual context. In this paper, we introduce ImageRAG for RS, a training-free framework to address the complexities of analyzing UHR remote sensing imagery. By transforming UHR remote sensing image analysis task to image's long context selection task, we design an innovative image contextual retrieval mechanism based on the Retrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's core innovation lies in its ability to selectively retrieve and focus on the most relevant portions of the UHR image as visual contexts that pertain to a given query. Fast path and slow path are proposed in this framework to handle this task efficiently and effectively. ImageRAG allows RSMLLMs to manage extensive context and spatial information from UHR RSI, ensuring the analysis is both accurate and efficient. Codebase will be released in https://github.com/om-ai-lab/ImageRAG\n## C. Retrieval-Augmented Generation\nTask-specific advancements in RAG systems focus on refining retrieval-augmented models for particular applications, improving their efficiency and effectiveness in complex tasks. Demonstrate-Search-Predict [116] introduces a modular approach that breaks down complex problems into manageable tasks, enhancing performance in multi-hop reasoning and open-domain question answering. Similarly, RA-DIT [117] uses dual instruction tuning to fine-tune the retriever and generative model, optimizing their collaboration for knowledgeintensive benchmarks. These methods highlight the importance of tailoring RAG systems to specific tasks, enabling more effective and accurate solutions across diverse domains.",
            "reference_string": "[273969615 | Zhang et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 69,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.03075, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2195381022",
                    "name": "Zhengliang Shi"
                },
                {
                    "authorId": "1387839383",
                    "name": "Lingyong Yan"
                },
                {
                    "authorId": "2153198380",
                    "name": "Weiwei Sun"
                },
                {
                    "authorId": "2326805997",
                    "name": "Yue Feng"
                },
                {
                    "authorId": "1749477",
                    "name": "Pengjie Ren"
                },
                {
                    "authorId": "2265517632",
                    "name": "Xinyu Ma"
                },
                {
                    "authorId": "2237948548",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "2331316040",
                    "name": "Dawei Yin"
                },
                {
                    "authorId": "2265490493",
                    "name": "M. D. Rijke"
                },
                {
                    "authorId": "2261862546",
                    "name": "Zhaochun Ren"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) integrates large language models ( LLM s) with retrievers to access external knowledge, improving the factuality of LLM generation in knowledge-grounded tasks. To optimize the RAG performance, most previous work independently fine-tunes the retriever to adapt to frozen LLM s or trains the LLMs to use documents retrieved by off-the-shelf retrievers, lacking end-to-end training supervision. Recent work addresses this limitation by jointly training these two components but relies on overly simplifying assumptions of document independence, which has been criticized for being far from real-world scenarios. Thus, effectively optimizing the overall RAG performance remains a critical challenge. We propose a direct retrieval-augmented optimization framework, named DRO, that enables end-to-end training of two key components: (i) a generative knowledge selection model and (ii) an LLM generator. DRO alternates between two phases: (i) document permutation estimation and (ii) re-weighted maximization, progressively improving RAG components through a variational approach. In the estimation step, we treat document permutation as a latent variable and directly estimate its distribution from the selection model by applying an importance sampling strategy. In the maximization step, we calibrate the optimization expectation using importance weights and jointly train the selection model and LLM generator. Our theoretical analysis reveals that DRO is analogous to policy-gradient methods in reinforcement learning. Extensive experiments conducted on five datasets illustrate that DRO outperforms the best baseline with 5%-15% improvements in EM and F1. We also provide in-depth experiments to qualitatively analyze the stability, convergence, and variance of DRO.",
            "corpus_id": 278339057,
            "sentences": [
                {
                    "corpus_id": "278339057",
                    "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models",
                    "text": "Retrieval Fig. 1. Overview of DRO objective. The selection model directly estimate a document permutation for the generator to predict an answer, with both components trained jointly. \n\n1 Introduction Large language models (LLMs) have shown remarkable text generation abilities; however, they often provide factually incorrect content [4,53,73] due to the hallucination [16] or out-of-date information [9]. To mitigate these limitations, retrieval-augmented generation (RAG) is proposed to integrate external retrievers with LLMs, which enables the model to access extensive corpora and retrieve relevant documents for references, thereby enhancing factuality. By integrating the retriever with LLMs, RAG has shown superior performance in knowledge-intensive tasks such as question answering [49,61] and conversational information seeking [5,24,68]. \n\nFollowing the most widely used architecture [9,11,23], RAG typically includes two components to answer an input query: (i) knowledge selection, where retrieval and re-ranking models select target documents, (ii) answer generation, where an LLM generator generates correct answers conditioned on the selected documents. To enhance coverage and improve answer quality, RAG models often provide multiple retrieved documents as input to the generator. The interrelationships among these documents are crucial for final performance [15,28,32,72]. We refer to a specific selection of retrieved documents as a document permutation. \n\nImproving RAG performance. To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45]. \n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25,51].",
                    "score": 0.68585837654152,
                    "section_title": "Generative selector",
                    "char_start_offset": 138,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 17
                        },
                        {
                            "start": 18,
                            "end": 44
                        },
                        {
                            "start": 45,
                            "end": 183
                        },
                        {
                            "start": 186,
                            "end": 406
                        },
                        {
                            "start": 407,
                            "end": 660
                        },
                        {
                            "start": 661,
                            "end": 849
                        },
                        {
                            "start": 852,
                            "end": 1170
                        },
                        {
                            "start": 1171,
                            "end": 1299
                        },
                        {
                            "start": 1300,
                            "end": 1393
                        },
                        {
                            "start": 1394,
                            "end": 1476
                        },
                        {
                            "start": 1479,
                            "end": 1505
                        },
                        {
                            "start": 1506,
                            "end": 1656
                        },
                        {
                            "start": 1657,
                            "end": 1853
                        },
                        {
                            "start": 1854,
                            "end": 2044
                        },
                        {
                            "start": 2047,
                            "end": 2158
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 338,
                            "end": 341,
                            "matchedPaperCorpusId": "271162228"
                        },
                        {
                            "start": 370,
                            "end": 374,
                            "matchedPaperCorpusId": "265067168"
                        },
                        {
                            "start": 402,
                            "end": 405,
                            "matchedPaperCorpusId": "269740933"
                        },
                        {
                            "start": 796,
                            "end": 799,
                            "matchedPaperCorpusId": "270370889"
                        },
                        {
                            "start": 842,
                            "end": 845,
                            "matchedPaperCorpusId": "267406766"
                        },
                        {
                            "start": 845,
                            "end": 848,
                            "matchedPaperCorpusId": "269983269"
                        },
                        {
                            "start": 896,
                            "end": 899,
                            "matchedPaperCorpusId": "269740933"
                        },
                        {
                            "start": 902,
                            "end": 905,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 1379,
                            "end": 1383,
                            "matchedPaperCorpusId": "252568176"
                        },
                        {
                            "start": 1389,
                            "end": 1392,
                            "matchedPaperCorpusId": "269605438"
                        },
                        {
                            "start": 1645,
                            "end": 1649,
                            "matchedPaperCorpusId": "10986612"
                        },
                        {
                            "start": 1753,
                            "end": 1757,
                            "matchedPaperCorpusId": "270199429"
                        },
                        {
                            "start": 2040,
                            "end": 2043,
                            "matchedPaperCorpusId": "269293655"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96044921875
                },
                {
                    "corpus_id": "278339057",
                    "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models",
                    "text": "Retrieval-augmented generation (RAG) aims to integrate external knowledge into LLMs, improving their factuality [11]. Given an input query, the first process of RAG is to select relevant knowledge, which is typically done by retrieval [20] or ranking model [33]. Subsequently, an LLM generator incorporates these candidate documents to generate an answer. An active research question is how to improve the overall RAG performance. Some studies aim to improve the knowledge accuracy [6,43], such as fine-tuning an answer-aware dense retriever [44,48] or introducing additional modules for document filtering [63,66]. Other work alternatively enhances the robustness of LLMs to irrelevant content, enabling LLMs to adaptively extract supporting facts from the retrieved documents [70,76]. However, these methods either optimize the retrieval or the generation process without dual enhancement, potentially leading to sub-optimal performance [27]. Although existing work proposes the end-to-end training paradigm, they overly simplify a marginalization optimization through independent top-k approximation [43,72], where they simply feed top-k documents into downstream LLMs one-by-one and re-score their relevance to optimize the retriever [23,27]. This has been criticized far from the practical scenarios as the RAG system typically consumes multiple documents [72], while exhaustively enumerating all possible document permutations is cost-intensive and typically infeasible in practice. In this work, we propose DRO, which directly treats the document permutation as a latent variable and estimates its distribution for optimization.",
                    "score": 0.5639408578197876,
                    "section_title": "Retrieval-augmented generation",
                    "char_start_offset": 8098,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 117
                        },
                        {
                            "start": 118,
                            "end": 262
                        },
                        {
                            "start": 263,
                            "end": 355
                        },
                        {
                            "start": 356,
                            "end": 430
                        },
                        {
                            "start": 431,
                            "end": 615
                        },
                        {
                            "start": 616,
                            "end": 786
                        },
                        {
                            "start": 787,
                            "end": 944
                        },
                        {
                            "start": 945,
                            "end": 1246
                        },
                        {
                            "start": 1247,
                            "end": 1488
                        },
                        {
                            "start": 1489,
                            "end": 1635
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 235,
                            "end": 239,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 257,
                            "end": 261,
                            "matchedPaperCorpusId": "10986612"
                        },
                        {
                            "start": 485,
                            "end": 488,
                            "matchedPaperCorpusId": "230437591"
                        },
                        {
                            "start": 1103,
                            "end": 1107,
                            "matchedPaperCorpusId": "230437591"
                        },
                        {
                            "start": 1107,
                            "end": 1110,
                            "matchedPaperCorpusId": "269605438"
                        },
                        {
                            "start": 1238,
                            "end": 1242,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 1361,
                            "end": 1365,
                            "matchedPaperCorpusId": "269605438"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9072265625
                }
            ],
            "relevance_judgement": 0.96044921875,
            "relevance_judgment_input_expanded": "# Title: Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models\n# Venue: arXiv.org\n# Authors: Zhengliang Shi, Lingyong Yan, Weiwei Sun, Yue Feng, Pengjie Ren, Xinyu Ma, Shuaiqiang Wang, Dawei Yin, M. D. Rijke, Zhaochun Ren\n## Abstract\nRetrieval-augmented generation (RAG) integrates large language models ( LLM s) with retrievers to access external knowledge, improving the factuality of LLM generation in knowledge-grounded tasks. To optimize the RAG performance, most previous work independently fine-tunes the retriever to adapt to frozen LLM s or trains the LLMs to use documents retrieved by off-the-shelf retrievers, lacking end-to-end training supervision. Recent work addresses this limitation by jointly training these two components but relies on overly simplifying assumptions of document independence, which has been criticized for being far from real-world scenarios. Thus, effectively optimizing the overall RAG performance remains a critical challenge. We propose a direct retrieval-augmented optimization framework, named DRO, that enables end-to-end training of two key components: (i) a generative knowledge selection model and (ii) an LLM generator. DRO alternates between two phases: (i) document permutation estimation and (ii) re-weighted maximization, progressively improving RAG components through a variational approach. In the estimation step, we treat document permutation as a latent variable and directly estimate its distribution from the selection model by applying an importance sampling strategy. In the maximization step, we calibrate the optimization expectation using importance weights and jointly train the selection model and LLM generator. Our theoretical analysis reveals that DRO is analogous to policy-gradient methods in reinforcement learning. Extensive experiments conducted on five datasets illustrate that DRO outperforms the best baseline with 5%-15% improvements in EM and F1. We also provide in-depth experiments to qualitatively analyze the stability, convergence, and variance of DRO.\n## Generative selector\nRetrieval Fig. 1. Overview of DRO objective. The selection model directly estimate a document permutation for the generator to predict an answer, with both components trained jointly. \n\n1 Introduction Large language models (LLMs) have shown remarkable text generation abilities; however, they often provide factually incorrect content [4,53,73] due to the hallucination [16] or out-of-date information [9]. To mitigate these limitations, retrieval-augmented generation (RAG) is proposed to integrate external retrievers with LLMs, which enables the model to access extensive corpora and retrieve relevant documents for references, thereby enhancing factuality. By integrating the retriever with LLMs, RAG has shown superior performance in knowledge-intensive tasks such as question answering [49,61] and conversational information seeking [5,24,68]. \n\nFollowing the most widely used architecture [9,11,23], RAG typically includes two components to answer an input query: (i) knowledge selection, where retrieval and re-ranking models select target documents, (ii) answer generation, where an LLM generator generates correct answers conditioned on the selected documents. To enhance coverage and improve answer quality, RAG models often provide multiple retrieved documents as input to the generator. The interrelationships among these documents are crucial for final performance [15,28,32,72]. We refer to a specific selection of retrieved documents as a document permutation. \n\nImproving RAG performance. To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45]. \n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25,51].\n\n## Retrieval-augmented generation\nRetrieval-augmented generation (RAG) aims to integrate external knowledge into LLMs, improving their factuality [11]. Given an input query, the first process of RAG is to select relevant knowledge, which is typically done by retrieval [20] or ranking model [33]. Subsequently, an LLM generator incorporates these candidate documents to generate an answer. An active research question is how to improve the overall RAG performance. Some studies aim to improve the knowledge accuracy [6,43], such as fine-tuning an answer-aware dense retriever [44,48] or introducing additional modules for document filtering [63,66]. Other work alternatively enhances the robustness of LLMs to irrelevant content, enabling LLMs to adaptively extract supporting facts from the retrieved documents [70,76]. However, these methods either optimize the retrieval or the generation process without dual enhancement, potentially leading to sub-optimal performance [27]. Although existing work proposes the end-to-end training paradigm, they overly simplify a marginalization optimization through independent top-k approximation [43,72], where they simply feed top-k documents into downstream LLMs one-by-one and re-score their relevance to optimize the retriever [23,27]. This has been criticized far from the practical scenarios as the RAG system typically consumes multiple documents [72], while exhaustively enumerating all possible document permutations is cost-intensive and typically infeasible in practice. In this work, we propose DRO, which directly treats the document permutation as a latent variable and estimates its distribution for optimization.",
            "reference_string": "[278339057 | Shi et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Graph Retrieval-Augmented Generation: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 178,
            "citation_count": 110,
            "influential_citation_count": 7,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.08921, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2314827534",
                    "name": "Boci Peng"
                },
                {
                    "authorId": "2257195454",
                    "name": "Yun Zhu"
                },
                {
                    "authorId": "2313693489",
                    "name": "Yongchao Liu"
                },
                {
                    "authorId": "2316431106",
                    "name": "Xiaohe Bo"
                },
                {
                    "authorId": "2313685962",
                    "name": "Haizhou Shi"
                },
                {
                    "authorId": "2313754922",
                    "name": "Chuntao Hong"
                },
                {
                    "authorId": "2316581992",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2257997261",
                    "name": "Siliang Tang"
                }
            ],
            "abstract": "Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable success in addressing the challenges of Large Language Models (LLMs) without necessitating retraining. By referencing an external knowledge base, RAG refines LLM outputs, effectively mitigating issues such as ``hallucination'', lack of domain-specific knowledge, and outdated information. However, the complex structure of relationships among different entities in databases presents challenges for RAG systems. In response, GraphRAG leverages structural information across entities to enable more precise and comprehensive retrieval, capturing relational knowledge and facilitating more accurate, context-aware responses. Given the novelty and potential of GraphRAG, a systematic review of current technologies is imperative. This paper provides the first comprehensive overview of GraphRAG methodologies. We formalize the GraphRAG workflow, encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced Generation. We then outline the core technologies and training methods at each stage. Additionally, we examine downstream tasks, application domains, evaluation methodologies, and industrial use cases of GraphRAG. Finally, we explore future research directions to inspire further inquiries and advance progress in the field. In order to track recent progress in this field, we set up a repository at \\url{https://github.com/pengboci/GraphRAG-Survey}.",
            "corpus_id": 271903170,
            "sentences": [
                {
                    "corpus_id": "271903170",
                    "title": "Graph Retrieval-Augmented Generation: A Survey",
                    "text": "Jointly training retrievers and generators simultaneously enhances performance on downstream tasks by leveraging their complementary strengths. Some approaches unify retrievers and generators into a single model, typically LLMs, and train them with both retrieval and generation objectives simultaneously [112]. This method capitalizes on the cohesive capabilities of a unified architecture, enabling the model to seamlessly retrieve relevant information and generate coherent responses within a single framework. \n\nOther methodologies involve initially training retrievers and generators separately, followed by joint training techniques to fine-tune both components. For instance, Subgraph Retriever [196] adopts an alternating training paradigm, where the retriever's parameters are fixed to use the graph data for training the generator. Subsequently, the generator's parameters are fixed, and feedback from the generator is used to guide the retriever's training. This iterative process helps both components refine their performance in a coordinated manner.",
                    "score": 0.6046818482864046,
                    "section_title": "Joint Training",
                    "char_start_offset": 76071,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 311
                        },
                        {
                            "start": 312,
                            "end": 513
                        },
                        {
                            "start": 516,
                            "end": 668
                        },
                        {
                            "start": 669,
                            "end": 841
                        },
                        {
                            "start": 842,
                            "end": 968
                        },
                        {
                            "start": 969,
                            "end": 1063
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 702,
                            "end": 707,
                            "matchedPaperCorpusId": "247158305"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96044921875
                }
            ],
            "relevance_judgement": 0.96044921875,
            "relevance_judgment_input_expanded": "# Title: Graph Retrieval-Augmented Generation: A Survey\n# Venue: arXiv.org\n# Authors: Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, Siliang Tang\n## Abstract\nRecently, Retrieval-Augmented Generation (RAG) has achieved remarkable success in addressing the challenges of Large Language Models (LLMs) without necessitating retraining. By referencing an external knowledge base, RAG refines LLM outputs, effectively mitigating issues such as ``hallucination'', lack of domain-specific knowledge, and outdated information. However, the complex structure of relationships among different entities in databases presents challenges for RAG systems. In response, GraphRAG leverages structural information across entities to enable more precise and comprehensive retrieval, capturing relational knowledge and facilitating more accurate, context-aware responses. Given the novelty and potential of GraphRAG, a systematic review of current technologies is imperative. This paper provides the first comprehensive overview of GraphRAG methodologies. We formalize the GraphRAG workflow, encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced Generation. We then outline the core technologies and training methods at each stage. Additionally, we examine downstream tasks, application domains, evaluation methodologies, and industrial use cases of GraphRAG. Finally, we explore future research directions to inspire further inquiries and advance progress in the field. In order to track recent progress in this field, we set up a repository at \\url{https://github.com/pengboci/GraphRAG-Survey}.\n## Joint Training\nJointly training retrievers and generators simultaneously enhances performance on downstream tasks by leveraging their complementary strengths. Some approaches unify retrievers and generators into a single model, typically LLMs, and train them with both retrieval and generation objectives simultaneously [112]. This method capitalizes on the cohesive capabilities of a unified architecture, enabling the model to seamlessly retrieve relevant information and generate coherent responses within a single framework. \n\nOther methodologies involve initially training retrievers and generators separately, followed by joint training techniques to fine-tune both components. For instance, Subgraph Retriever [196] adopts an alternating training paradigm, where the retriever's parameters are fixed to use the graph data for training the generator. Subsequently, the generator's parameters are fixed, and feedback from the generator is used to guide the retriever's training. This iterative process helps both components refine their performance in a coordinated manner.",
            "reference_string": "[271903170 | Peng et al. | 2024 | Citations: 110]"
        },
        {
            "title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 163,
            "citation_count": 39,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.13193, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2112539433",
                    "name": "Shangyu Wu"
                },
                {
                    "authorId": "2303313918",
                    "name": "Ying Xiong"
                },
                {
                    "authorId": "2301404967",
                    "name": "Yufei Cui"
                },
                {
                    "authorId": "107747459",
                    "name": "Haolun Wu"
                },
                {
                    "authorId": "2243412535",
                    "name": "Can Chen"
                },
                {
                    "authorId": "2283264350",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "2303518782",
                    "name": "Lianming Huang"
                },
                {
                    "authorId": "2272581493",
                    "name": "Xue Liu"
                },
                {
                    "authorId": "2271790635",
                    "name": "Tei-Wei Kuo"
                },
                {
                    "authorId": "2290008872",
                    "name": "Nan Guan"
                },
                {
                    "authorId": "2302177675",
                    "name": "C. Xue"
                }
            ],
            "abstract": "Large language models (LLMs) have demonstrated great success in various fields, benefiting from their huge amount of parameters that store knowledge. However, LLMs still suffer from several key issues, such as hallucination problems, knowledge update issues, and lacking domain-specific expertise. The appearance of retrieval-augmented generation (RAG), which leverages an external knowledge database to augment LLMs, makes up those drawbacks of LLMs. This paper reviews all significant techniques of RAG, especially in the retriever and the retrieval fusions. Besides, tutorial codes are provided for implementing the representative techniques in RAG. This paper further discusses the RAG update, including RAG with/without knowledge update. Then, we introduce RAG evaluation and benchmarking, as well as the application of RAG in representative NLP tasks and industrial scenarios. Finally, this paper discusses RAG's future directions and challenges for promoting this field's development.",
            "corpus_id": 271270644,
            "sentences": [
                {
                    "corpus_id": "271270644",
                    "title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
                    "text": "Training the generator involves updating its parameters or those in the retrieval fusion modules. Since the generator is generally an LLM, training the LLM is a resource-and time-consuming process. Fortunately, several parameter-efficient fine-tuning techniques, such as LoRA [57], are proposed to address the fine-tuning problem of LLMs. Although the parameters in the retrieval fusion modules are less than those in the generator, only fine-tuning those parameters may encounter some training problems, such as low convergence and overfitting. Jointly tuning the parameters in the generator and the retrieval fusion modules is a better way to train the generator and the retrieval fusion modules if there are sufficient and powerful resources.",
                    "score": 0.5939689111540323,
                    "section_title": "Training generator.",
                    "char_start_offset": 32926,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 97
                        },
                        {
                            "start": 98,
                            "end": 197
                        },
                        {
                            "start": 198,
                            "end": 338
                        },
                        {
                            "start": 339,
                            "end": 545
                        },
                        {
                            "start": 546,
                            "end": 745
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 276,
                            "end": 280,
                            "matchedPaperCorpusId": "235458009"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95166015625
                },
                {
                    "corpus_id": "271270644",
                    "title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
                    "text": "This section gives an overview of RAG for NLP. As shown in Figure 1, RAG typically consists of three modules, the retriever, the generator, and retrieval fusions. Retriever module usually comprises three components: an encoder for encoding inputs into embeddings, an efficient indexing that supports approximate nearest neighbor search, and the datastore for storing external knowledge in the form of key-value pairs. The main challenge in the retriever module is finding the optimal trade-off between retrieval efficiency and retrieval quality. The retrieval efficiency refers to how fast the relevant information can be obtained, which involves accelerating encoding, efficient indexing, batch querying in the datastore, etc. The retrieval quality refers to how relevant the information can be retrieved, which involves chunk representation learning, advanced approximate nearest neighbor search algorithms, etc. \n\nRetrieval Fusions aims to leverage the retrieved information to augment the generation. These fusion techniques can be categorized into three major types: query-based fusion, latent fusion, and logits-based fusion. The query-based fusion augments inputs with retrievals before feeding them into the generators. The logits-based fusion focuses on the output logits of generators and fuses the retrievals logits for more robust logits. The latent fusion refers to introducing retrieval representations into the latent representations of generators, thus implicitly improving the models' performance. \n\nGenerator module can be classified into two branches of generators: default generators and retrieval-augmented (RA) generators. The default generators include most pre-trained/fine-tuned large language models, such as GPT-series models [11,114,118,119], Mistral models [71], and Gemini-series models [4,108,124]. The RA generators refer to the pre-trained/fine-tuned generators that consist of modules for fusing retrievals, such RETRO [10] and Enc-Dec [93]. Those generators generate responses or make predictions. \n\nThe workflow of RAG involves three steps: (1) retrieving the relevant information from external databases based on given inputs; \n\n(2) fusing the retrieved information with inputs or intermediate states based on the fusion techniques; (3) making predictions by generators based on the input and corresponding retrievals.",
                    "score": 0.806213815484089,
                    "section_title": "OVERVIEW OF RETRIEVAL-AUGMENTED GENERATION",
                    "char_start_offset": 3632,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 46
                        },
                        {
                            "start": 47,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 545
                        },
                        {
                            "start": 546,
                            "end": 727
                        },
                        {
                            "start": 728,
                            "end": 914
                        },
                        {
                            "start": 917,
                            "end": 1004
                        },
                        {
                            "start": 1005,
                            "end": 1131
                        },
                        {
                            "start": 1132,
                            "end": 1227
                        },
                        {
                            "start": 1228,
                            "end": 1350
                        },
                        {
                            "start": 1351,
                            "end": 1514
                        },
                        {
                            "start": 1517,
                            "end": 1644
                        },
                        {
                            "start": 1645,
                            "end": 1829
                        },
                        {
                            "start": 1830,
                            "end": 1975
                        },
                        {
                            "start": 1976,
                            "end": 2032
                        },
                        {
                            "start": 2035,
                            "end": 2163
                        },
                        {
                            "start": 2166,
                            "end": 2355
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1761,
                            "end": 1765,
                            "matchedPaperCorpusId": "49313245"
                        },
                        {
                            "start": 1765,
                            "end": 1769,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 1953,
                            "end": 1957,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 1970,
                            "end": 1974,
                            "matchedPaperCorpusId": "252846580"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92578125
                },
                {
                    "corpus_id": "271270644",
                    "title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
                    "text": "As introduced in Section 6, RAG training includes two branch of works, RAG with/without datastore update. For RAG without datastore update, the main challenge is how to jointly optimize all parameters in RAG. This may involves new loss functions with multiple objectives, new optimizations for efficient tuning parameters in retriever and generator, or other training strategies. \n\nFor RAG with datastore update, one challenge is how to align the retrieval representations with the generator's representations. Although the time cost of the update operation in datastore cannot be ignored, some works [14] reduce the update frequency by asychronously updating, thus achieving the alignment of knowledge representation and model's representation. Another challenge is when to retrain/fine-tune the generator in RAG when new corpus is added. Due to the in-context learning capability of exisitng LLM-based generators and high training overhead, retraining/finetuning the generator or directly inferring the generator becomes a challenging choice for different scenarios. Recently, some efficient training strategies [28,57] have been proposed to accelerate the fine-tuning process, which can be taken into considerations.",
                    "score": 0.656839288322697,
                    "section_title": "RAG Training",
                    "char_start_offset": 59907,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 105
                        },
                        {
                            "start": 106,
                            "end": 208
                        },
                        {
                            "start": 209,
                            "end": 379
                        },
                        {
                            "start": 382,
                            "end": 510
                        },
                        {
                            "start": 511,
                            "end": 745
                        },
                        {
                            "start": 746,
                            "end": 839
                        },
                        {
                            "start": 840,
                            "end": 1068
                        },
                        {
                            "start": 1069,
                            "end": 1219
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 601,
                            "end": 605,
                            "matchedPaperCorpusId": "249191271"
                        },
                        {
                            "start": 1114,
                            "end": 1118,
                            "matchedPaperCorpusId": "258841328"
                        },
                        {
                            "start": 1118,
                            "end": 1121,
                            "matchedPaperCorpusId": "235458009"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9140625
                }
            ],
            "relevance_judgement": 0.95166015625,
            "relevance_judgment_input_expanded": "# Title: Retrieval-Augmented Generation for Natural Language Processing: A Survey\n# Venue: arXiv.org\n# Authors: Shangyu Wu, Ying Xiong, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, C. Xue\n## Abstract\nLarge language models (LLMs) have demonstrated great success in various fields, benefiting from their huge amount of parameters that store knowledge. However, LLMs still suffer from several key issues, such as hallucination problems, knowledge update issues, and lacking domain-specific expertise. The appearance of retrieval-augmented generation (RAG), which leverages an external knowledge database to augment LLMs, makes up those drawbacks of LLMs. This paper reviews all significant techniques of RAG, especially in the retriever and the retrieval fusions. Besides, tutorial codes are provided for implementing the representative techniques in RAG. This paper further discusses the RAG update, including RAG with/without knowledge update. Then, we introduce RAG evaluation and benchmarking, as well as the application of RAG in representative NLP tasks and industrial scenarios. Finally, this paper discusses RAG's future directions and challenges for promoting this field's development.\n## OVERVIEW OF RETRIEVAL-AUGMENTED GENERATION\nThis section gives an overview of RAG for NLP. As shown in Figure 1, RAG typically consists of three modules, the retriever, the generator, and retrieval fusions. Retriever module usually comprises three components: an encoder for encoding inputs into embeddings, an efficient indexing that supports approximate nearest neighbor search, and the datastore for storing external knowledge in the form of key-value pairs. The main challenge in the retriever module is finding the optimal trade-off between retrieval efficiency and retrieval quality. The retrieval efficiency refers to how fast the relevant information can be obtained, which involves accelerating encoding, efficient indexing, batch querying in the datastore, etc. The retrieval quality refers to how relevant the information can be retrieved, which involves chunk representation learning, advanced approximate nearest neighbor search algorithms, etc. \n\nRetrieval Fusions aims to leverage the retrieved information to augment the generation. These fusion techniques can be categorized into three major types: query-based fusion, latent fusion, and logits-based fusion. The query-based fusion augments inputs with retrievals before feeding them into the generators. The logits-based fusion focuses on the output logits of generators and fuses the retrievals logits for more robust logits. The latent fusion refers to introducing retrieval representations into the latent representations of generators, thus implicitly improving the models' performance. \n\nGenerator module can be classified into two branches of generators: default generators and retrieval-augmented (RA) generators. The default generators include most pre-trained/fine-tuned large language models, such as GPT-series models [11,114,118,119], Mistral models [71], and Gemini-series models [4,108,124]. The RA generators refer to the pre-trained/fine-tuned generators that consist of modules for fusing retrievals, such RETRO [10] and Enc-Dec [93]. Those generators generate responses or make predictions. \n\nThe workflow of RAG involves three steps: (1) retrieving the relevant information from external databases based on given inputs; \n\n(2) fusing the retrieved information with inputs or intermediate states based on the fusion techniques; (3) making predictions by generators based on the input and corresponding retrievals.\n\n## Training generator.\nTraining the generator involves updating its parameters or those in the retrieval fusion modules. Since the generator is generally an LLM, training the LLM is a resource-and time-consuming process. Fortunately, several parameter-efficient fine-tuning techniques, such as LoRA [57], are proposed to address the fine-tuning problem of LLMs. Although the parameters in the retrieval fusion modules are less than those in the generator, only fine-tuning those parameters may encounter some training problems, such as low convergence and overfitting. Jointly tuning the parameters in the generator and the retrieval fusion modules is a better way to train the generator and the retrieval fusion modules if there are sufficient and powerful resources.\n\n## RAG Training\nAs introduced in Section 6, RAG training includes two branch of works, RAG with/without datastore update. For RAG without datastore update, the main challenge is how to jointly optimize all parameters in RAG. This may involves new loss functions with multiple objectives, new optimizations for efficient tuning parameters in retriever and generator, or other training strategies. \n\nFor RAG with datastore update, one challenge is how to align the retrieval representations with the generator's representations. Although the time cost of the update operation in datastore cannot be ignored, some works [14] reduce the update frequency by asychronously updating, thus achieving the alignment of knowledge representation and model's representation. Another challenge is when to retrain/fine-tune the generator in RAG when new corpus is added. Due to the in-context learning capability of exisitng LLM-based generators and high training overhead, retraining/finetuning the generator or directly inferring the generator becomes a challenging choice for different scenarios. Recently, some efficient training strategies [28,57] have been proposed to accelerate the fine-tuning process, which can be taken into considerations.",
            "reference_string": "[271270644 | Wu et al. | 2024 | Citations: 39]"
        },
        {
            "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2025,
            "reference_count": 44,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.08356, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256382094",
                    "name": "Kushagra Bhushan"
                },
                {
                    "authorId": "1392630568",
                    "name": "Yatin Nandwani"
                },
                {
                    "authorId": "2345003462",
                    "name": "Dinesh Khandelwal"
                },
                {
                    "authorId": "2320314900",
                    "name": "Sonam Gupta"
                },
                {
                    "authorId": "2345005348",
                    "name": "Gaurav Pandey"
                },
                {
                    "authorId": "1916865",
                    "name": "Dinesh Raghu"
                },
                {
                    "authorId": "2243011716",
                    "name": "Sachindra Joshi"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a prominent method for incorporating domain knowledge into Large Language Models (LLMs). While RAG enhances response relevance by incorporating retrieved domain knowledge in the context, retrieval errors can still lead to hallucinations and incorrect answers. To recover from retriever failures, domain knowledge is injected by fine-tuning the model to generate the correct response, even in the case of retrieval errors. However, we observe that without systematic knowledge augmentation, fine-tuned LLMs may memorize new information but still fail to extract relevant domain knowledge, leading to poor performance. In this work, we present a novel framework that significantly enhances the fine-tuning process by augmenting the training data in two ways -- context augmentation and knowledge paraphrasing. In context augmentation, we create multiple training samples for a given QA pair by varying the relevance of the retrieved information, teaching the model when to ignore and when to rely on retrieved content. In knowledge paraphrasing, we fine-tune with multiple answers to the same question, enabling LLMs to better internalize specialized knowledge. To mitigate catastrophic forgetting due to fine-tuning, we add a domain-specific identifier to a question and also utilize a replay buffer containing general QA pairs. Experimental results demonstrate the efficacy of our method over existing techniques, achieving up to 10\\% relative gain in token-level recall while preserving the LLM's generalization capabilities.",
            "corpus_id": 276287820,
            "sentences": [
                {
                    "corpus_id": "276287820",
                    "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG",
                    "text": "Retrieval Augmented Generation: RAG enhances Large Language Models (LLMs) by integrating external data sources, such as knowledge bases, to improve relevance and accuracy (Lewis et al., 2020;Guu et al., 2020;Karpukhin et al., 2020). Recent advancements have extended its applicability across domains (Asai et al., 2024;Kim et al., 2024;Yan et al., 2024;Liu et al., 2024), but RAG systems still face key challenges: hallucinations due to mismatches between retrieved data and the LLM's pre-existing knowledge (Setty et al., 2024;Jin et al., 2024), difficulty with complex multi-document reasoning (Setty et al., 2024), and an inability to fully leverage fixed-domain settings where all domain-specific documents are available beforehand because typically neither the retriever nor the generator LLM are trained on the domain data. Domain-Aware Fine-Tuning for RAG: Joint training of the retriever and LLM has been proposed as a way to improve RAG's domain-specific performance (Guu et al., 2020;Sachan et al., 2021;Siriwardhana et al., 2023;Shi et al., 2024). By jointly training the retriever and LLM, the system can better adapt to domain-specific contexts. However, this approach introduces complexities, including the need for specialized loss functions and frequent retriever updates. \n\nAnother line of work (Mecklenburg et al., 2024;Zhang et al., 2024b) focuses solely on adding domain knowledge to LLMs as an alternative to RAG. These approaches fine-tune LLMs using questionanswer (QA) pairs derived from domain data and aim to answer any new test query without retrieving any document. As a result, they fail to leverage access to the domain documents during inference. \n\nRecently, Zhang et al. introduced Retrieval-Augmented Fine-Tuning (RAFT), a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts.",
                    "score": 0.6795535119413307,
                    "section_title": "Related Work",
                    "char_start_offset": 5971,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 232
                        },
                        {
                            "start": 233,
                            "end": 829
                        },
                        {
                            "start": 830,
                            "end": 1058
                        },
                        {
                            "start": 1059,
                            "end": 1158
                        },
                        {
                            "start": 1159,
                            "end": 1288
                        },
                        {
                            "start": 1291,
                            "end": 1434
                        },
                        {
                            "start": 1435,
                            "end": 1593
                        },
                        {
                            "start": 1594,
                            "end": 1677
                        },
                        {
                            "start": 1680,
                            "end": 1854
                        },
                        {
                            "start": 1855,
                            "end": 1980
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94189453125
                }
            ],
            "relevance_judgement": 0.94189453125,
            "relevance_judgment_input_expanded": "# Title: Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Kushagra Bhushan, Yatin Nandwani, Dinesh Khandelwal, Sonam Gupta, Gaurav Pandey, Dinesh Raghu, Sachindra Joshi\n## Abstract\nRetrieval-Augmented Generation (RAG) has emerged as a prominent method for incorporating domain knowledge into Large Language Models (LLMs). While RAG enhances response relevance by incorporating retrieved domain knowledge in the context, retrieval errors can still lead to hallucinations and incorrect answers. To recover from retriever failures, domain knowledge is injected by fine-tuning the model to generate the correct response, even in the case of retrieval errors. However, we observe that without systematic knowledge augmentation, fine-tuned LLMs may memorize new information but still fail to extract relevant domain knowledge, leading to poor performance. In this work, we present a novel framework that significantly enhances the fine-tuning process by augmenting the training data in two ways -- context augmentation and knowledge paraphrasing. In context augmentation, we create multiple training samples for a given QA pair by varying the relevance of the retrieved information, teaching the model when to ignore and when to rely on retrieved content. In knowledge paraphrasing, we fine-tune with multiple answers to the same question, enabling LLMs to better internalize specialized knowledge. To mitigate catastrophic forgetting due to fine-tuning, we add a domain-specific identifier to a question and also utilize a replay buffer containing general QA pairs. Experimental results demonstrate the efficacy of our method over existing techniques, achieving up to 10\\% relative gain in token-level recall while preserving the LLM's generalization capabilities.\n## Related Work\nRetrieval Augmented Generation: RAG enhances Large Language Models (LLMs) by integrating external data sources, such as knowledge bases, to improve relevance and accuracy (Lewis et al., 2020;Guu et al., 2020;Karpukhin et al., 2020). Recent advancements have extended its applicability across domains (Asai et al., 2024;Kim et al., 2024;Yan et al., 2024;Liu et al., 2024), but RAG systems still face key challenges: hallucinations due to mismatches between retrieved data and the LLM's pre-existing knowledge (Setty et al., 2024;Jin et al., 2024), difficulty with complex multi-document reasoning (Setty et al., 2024), and an inability to fully leverage fixed-domain settings where all domain-specific documents are available beforehand because typically neither the retriever nor the generator LLM are trained on the domain data. Domain-Aware Fine-Tuning for RAG: Joint training of the retriever and LLM has been proposed as a way to improve RAG's domain-specific performance (Guu et al., 2020;Sachan et al., 2021;Siriwardhana et al., 2023;Shi et al., 2024). By jointly training the retriever and LLM, the system can better adapt to domain-specific contexts. However, this approach introduces complexities, including the need for specialized loss functions and frequent retriever updates. \n\nAnother line of work (Mecklenburg et al., 2024;Zhang et al., 2024b) focuses solely on adding domain knowledge to LLMs as an alternative to RAG. These approaches fine-tune LLMs using questionanswer (QA) pairs derived from domain data and aim to answer any new test query without retrieving any document. As a result, they fail to leverage access to the domain documents during inference. \n\nRecently, Zhang et al. introduced Retrieval-Augmented Fine-Tuning (RAFT), a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts.",
            "reference_string": "[276287820 | Bhushan et al. | 2025 | Citations: 2]"
        },
        {
            "title": "OpenRAG: Optimizing RAG End-to-End via In-Context Retrieval Learning",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 59,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.08398, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2300037598",
                    "name": "Jiawei Zhou"
                },
                {
                    "authorId": "2299947072",
                    "name": "Lei Chen"
                }
            ],
            "abstract": "In this paper, we analyze and empirically show that the learned relevance for conventional information retrieval (IR) scenarios may be inconsistent in retrieval-augmented generation (RAG) scenarios. To bridge this gap, we introduce OpenRAG, a RAG framework that is optimized end-to-end by tuning the retriever to capture in-context relevance, enabling adaptation to the diverse and evolving needs. Extensive experiments across a wide range of tasks demonstrate that OpenRAG, by tuning a retriever end-to-end, leads to a consistent improvement of 4.0% over the original retriever, consistently outperforming existing state-of-the-art retrievers by 2.1%. Additionally, our results indicate that for some tasks, an end-to-end tuned 0.2B retriever can achieve improvements that surpass those of RAG-oriented or instruction-tuned 8B large language models (LLMs), highlighting the cost-effectiveness of our approach in enhancing RAG systems.",
            "corpus_id": 276928032,
            "sentences": [
                {
                    "corpus_id": "276928032",
                    "title": "OpenRAG: Optimizing RAG End-to-End via In-Context Retrieval Learning",
                    "text": "In this paper, we analyze and empirically show that the learned relevance for conventional information retrieval (IR) scenarios may be inconsistent in retrieval-augmented generation (RAG) scenarios. To bridge this gap, we introduce OpenRAG, a RAG framework that is optimized end-to-end by tuning the retriever to capture in-context relevance, enabling adaptation to the diverse and evolving needs. Extensive experiments across a wide range of tasks demonstrate that OpenRAG, by tuning a retriever end-to-end, leads to a consistent improvement of 4.0% over the original retriever, consistently outperforming existing state-of-the-art retrievers by 2.1%. Additionally, our results indicate that for some tasks, an end-to-end tuned 0.2B retriever can achieve improvements that surpass those of RAG-oriented or instruction-tuned 8B large language models (LLMs), highlighting the cost-effectiveness of our approach in enhancing RAG systems.",
                    "score": 0.5662431339560703,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94091796875
                }
            ],
            "relevance_judgement": 0.94091796875,
            "relevance_judgment_input_expanded": "# Title: OpenRAG: Optimizing RAG End-to-End via In-Context Retrieval Learning\n# Venue: arXiv.org\n# Authors: Jiawei Zhou, Lei Chen\n## Abstract\nIn this paper, we analyze and empirically show that the learned relevance for conventional information retrieval (IR) scenarios may be inconsistent in retrieval-augmented generation (RAG) scenarios. To bridge this gap, we introduce OpenRAG, a RAG framework that is optimized end-to-end by tuning the retriever to capture in-context relevance, enabling adaptation to the diverse and evolving needs. Extensive experiments across a wide range of tasks demonstrate that OpenRAG, by tuning a retriever end-to-end, leads to a consistent improvement of 4.0% over the original retriever, consistently outperforming existing state-of-the-art retrievers by 2.1%. Additionally, our results indicate that for some tasks, an end-to-end tuned 0.2B retriever can achieve improvements that surpass those of RAG-oriented or instruction-tuned 8B large language models (LLMs), highlighting the cost-effectiveness of our approach in enhancing RAG systems.\n",
            "reference_string": "[276928032 | Zhou et al. | 2025 | Citations: 1]"
        },
        {
            "title": "W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 52,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.08444, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316325542",
                    "name": "Jinming Nian"
                },
                {
                    "authorId": "2113952662",
                    "name": "Zhiyuan Peng"
                },
                {
                    "authorId": "2260433198",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "2305661352",
                    "name": "Yi Fang"
                }
            ],
            "abstract": "In knowledge-intensive tasks such as open-domain question answering (OpenQA), large language models (LLMs) often struggle to generate factual answers, relying solely on their internal (parametric) knowledge. To address this limitation, Retrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving relevant information from external sources, thereby positioning the retriever as a pivotal component. Although dense retrieval demonstrates state-of-the-art performance, its training poses challenges due to the scarcity of ground-truth evidence, largely attributed to the high costs of human annotation. In this paper, we propose W-RAG, a method that draws weak training signals from the downstream task (such as OpenQA) of an LLM, and fine-tunes the retriever to prioritize passages that most benefit the task. Specifically, we rerank the top-$k$ passages retrieved via BM25 by assessing the probability that the LLM will generate the correct answer for a question given each passage. The highest-ranking passages are then used as positive fine-tuning examples for dense retrieval. We conduct comprehensive experiments across four publicly available OpenQA datasets to demonstrate that our approach enhances both retrieval and OpenQA performance compared to baseline models, achieving results comparable to models fine-tuned with human-labeled data.",
            "corpus_id": 271891920,
            "sentences": [
                {
                    "corpus_id": "271891920",
                    "title": "W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering",
                    "text": "In knowledge-intensive tasks such as open-domain question answering (OpenQA), large language models (LLMs) often struggle to generate factual answers, relying solely on their internal (parametric) knowledge. To address this limitation, Retrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving relevant information from external sources, thereby positioning the retriever as a pivotal component. Although dense retrieval demonstrates state-of-the-art performance, its training poses challenges due to the scarcity of ground-truth evidence, largely attributed to the high costs of human annotation. In this paper, we propose W-RAG, a method that draws weak training signals from the downstream task (such as OpenQA) of an LLM, and fine-tunes the retriever to prioritize passages that most benefit the task. Specifically, we rerank the top-$k$ passages retrieved via BM25 by assessing the probability that the LLM will generate the correct answer for a question given each passage. The highest-ranking passages are then used as positive fine-tuning examples for dense retrieval. We conduct comprehensive experiments across four publicly available OpenQA datasets to demonstrate that our approach enhances both retrieval and OpenQA performance compared to baseline models, achieving results comparable to models fine-tuned with human-labeled data.",
                    "score": 0.572705640332384,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93994140625
                }
            ],
            "relevance_judgement": 0.93994140625,
            "relevance_judgment_input_expanded": "# Title: W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering\n# Venue: arXiv.org\n# Authors: Jinming Nian, Zhiyuan Peng, Qifan Wang, Yi Fang\n## Abstract\nIn knowledge-intensive tasks such as open-domain question answering (OpenQA), large language models (LLMs) often struggle to generate factual answers, relying solely on their internal (parametric) knowledge. To address this limitation, Retrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving relevant information from external sources, thereby positioning the retriever as a pivotal component. Although dense retrieval demonstrates state-of-the-art performance, its training poses challenges due to the scarcity of ground-truth evidence, largely attributed to the high costs of human annotation. In this paper, we propose W-RAG, a method that draws weak training signals from the downstream task (such as OpenQA) of an LLM, and fine-tunes the retriever to prioritize passages that most benefit the task. Specifically, we rerank the top-$k$ passages retrieved via BM25 by assessing the probability that the LLM will generate the correct answer for a question given each passage. The highest-ranking passages are then used as positive fine-tuning examples for dense retrieval. We conduct comprehensive experiments across four publicly available OpenQA datasets to demonstrate that our approach enhances both retrieval and OpenQA performance compared to baseline models, achieving results comparable to models fine-tuned with human-labeled data.\n",
            "reference_string": "[271891920 | Nian et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 151,
            "citation_count": 39,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.10102, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118788278",
                    "name": "Yujia Zhou"
                },
                {
                    "authorId": "2319219743",
                    "name": "Yan Liu"
                },
                {
                    "authorId": "2144456832",
                    "name": "Xiaoxi Li"
                },
                {
                    "authorId": "4376097",
                    "name": "Jiajie Jin"
                },
                {
                    "authorId": "1972030827",
                    "name": "Hongjin Qian"
                },
                {
                    "authorId": "2284309569",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "2321448388",
                    "name": "Chaozhuo Li"
                },
                {
                    "authorId": "2257039188",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "2321412234",
                    "name": "Tsung-Yi Ho"
                },
                {
                    "authorId": "2255871958",
                    "name": "Philip S. Yu"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal paradigm in the development of Large Language Models (LLMs). While much of the current research in this field focuses on performance optimization, particularly in terms of accuracy and efficiency, the trustworthiness of RAG systems remains an area still under exploration. From a positive perspective, RAG systems are promising to enhance LLMs by providing them with useful and up-to-date knowledge from vast external databases, thereby mitigating the long-standing problem of hallucination. While from a negative perspective, RAG systems are at the risk of generating undesirable contents if the retrieved information is either inappropriate or poorly utilized. To address these concerns, we propose a unified framework that assesses the trustworthiness of RAG systems across six key dimensions: factuality, robustness, fairness, transparency, accountability, and privacy. Within this framework, we thoroughly review the existing literature on each dimension. Additionally, we create the evaluation benchmark regarding the six dimensions and conduct comprehensive evaluations for a variety of proprietary and open-source models. Finally, we identify the potential challenges for future research based on our investigation results. Through this work, we aim to lay a structured foundation for future investigations and provide practical insights for enhancing the trustworthiness of RAG systems in real-world applications.",
            "corpus_id": 272689561,
            "sentences": [
                {
                    "corpus_id": "272689561",
                    "title": "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey",
                    "text": "These enhanced systems, known as Advanced RAG, introduce specialized modules at different stages of the retrieval and generation pipeline, which can be categorized as pre-retrieval and post-retrieval components. In the pre-retrieval stage, a common issue is that the original query may be too short or vague, resulting in irrelevant retrieval results. To address this, a rewriter is introduced to clarify or expand the query. Rewriting methods include directly prompting the LLM [34,35] or training a rewriter model using feedback from the generator [36]. In the post-retrieval stage, the generator often faces challenges due to the length or noise of the retrieved content, which can affect the generation quality [33,37]. To mitigate this, a reranker is used to reorder the retrieval results [38]. Rerankers, often using cross-encoder architectures, better measure the similarity between the query and retrieved documents, pushing more relevant documents forward and removing less relevant ones. Another optimization component is the refiner, which summarizes or compresses retrieved content using techniques like prompting the LLM to summarize [39,40], or training a summarizer through supervised fine-tuning or reinforcement learning [41][42][43]. Despite the flexibility of Advanced RAG, its sequential structure limits adaptability in complex scenarios, such as queries requiring step-by-step reasoning. \n\nModular RAG. As RAG research evolves, it has entered the modular RAG stage, where components are treated as flexible modules that can be combined to create customized pipelines for different scenarios, offering greater flexibility and adaptability. Research now focuses on optimizing these pipelines, which come in four main types: Sequential, Conditional, Branching, and Loop. Sequential Pipelines process queries linearly, similar to advanced RAG, with pre-retrieval and post-retrieval stages. Conditional Pipelines route queries along different execution paths based on their type. For instance, SKR [44] identifies queries that the LLM can answer without retrieval, while Adaptive-RAG [45] classifies queries as simple or complex, using multi-round retrieval for complex ones. Branching Pipelines execute multiple paths simultaneously for a query, combining the results to form the final output.",
                    "score": 0.5867210051639979,
                    "section_title": "Retrieval-augmented Generation System",
                    "char_start_offset": 8637,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 351
                        },
                        {
                            "start": 352,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 555
                        },
                        {
                            "start": 556,
                            "end": 723
                        },
                        {
                            "start": 724,
                            "end": 799
                        },
                        {
                            "start": 800,
                            "end": 997
                        },
                        {
                            "start": 998,
                            "end": 1251
                        },
                        {
                            "start": 1252,
                            "end": 1409
                        },
                        {
                            "start": 1412,
                            "end": 1424
                        },
                        {
                            "start": 1425,
                            "end": 1660
                        },
                        {
                            "start": 1661,
                            "end": 1789
                        },
                        {
                            "start": 1790,
                            "end": 1907
                        },
                        {
                            "start": 1908,
                            "end": 1996
                        },
                        {
                            "start": 1997,
                            "end": 2192
                        },
                        {
                            "start": 2193,
                            "end": 2311
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 479,
                            "end": 483,
                            "matchedPaperCorpusId": "263830368"
                        },
                        {
                            "start": 483,
                            "end": 486,
                            "matchedPaperCorpusId": "252519173"
                        },
                        {
                            "start": 550,
                            "end": 554,
                            "matchedPaperCorpusId": "258841283"
                        },
                        {
                            "start": 715,
                            "end": 719,
                            "matchedPaperCorpusId": "256459776"
                        },
                        {
                            "start": 794,
                            "end": 798,
                            "matchedPaperCorpusId": "250391085"
                        },
                        {
                            "start": 1147,
                            "end": 1151,
                            "matchedPaperCorpusId": "263831502"
                        },
                        {
                            "start": 1151,
                            "end": 1154,
                            "matchedPaperCorpusId": "269293435"
                        },
                        {
                            "start": 1238,
                            "end": 1242,
                            "matchedPaperCorpusId": "271745607"
                        },
                        {
                            "start": 1242,
                            "end": 1246,
                            "matchedPaperCorpusId": "267751485"
                        },
                        {
                            "start": 1246,
                            "end": 1250,
                            "matchedPaperCorpusId": "264590451"
                        },
                        {
                            "start": 2015,
                            "end": 2019,
                            "matchedPaperCorpusId": "263828724"
                        },
                        {
                            "start": 2101,
                            "end": 2105,
                            "matchedPaperCorpusId": "268553748"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.939453125
                }
            ],
            "relevance_judgement": 0.939453125,
            "relevance_judgment_input_expanded": "# Title: Trustworthiness in Retrieval-Augmented Generation Systems: A Survey\n# Venue: arXiv.org\n# Authors: Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo Li, Zhicheng Dou, Tsung-Yi Ho, Philip S. Yu\n## Abstract\nRetrieval-Augmented Generation (RAG) has quickly grown into a pivotal paradigm in the development of Large Language Models (LLMs). While much of the current research in this field focuses on performance optimization, particularly in terms of accuracy and efficiency, the trustworthiness of RAG systems remains an area still under exploration. From a positive perspective, RAG systems are promising to enhance LLMs by providing them with useful and up-to-date knowledge from vast external databases, thereby mitigating the long-standing problem of hallucination. While from a negative perspective, RAG systems are at the risk of generating undesirable contents if the retrieved information is either inappropriate or poorly utilized. To address these concerns, we propose a unified framework that assesses the trustworthiness of RAG systems across six key dimensions: factuality, robustness, fairness, transparency, accountability, and privacy. Within this framework, we thoroughly review the existing literature on each dimension. Additionally, we create the evaluation benchmark regarding the six dimensions and conduct comprehensive evaluations for a variety of proprietary and open-source models. Finally, we identify the potential challenges for future research based on our investigation results. Through this work, we aim to lay a structured foundation for future investigations and provide practical insights for enhancing the trustworthiness of RAG systems in real-world applications.\n## Retrieval-augmented Generation System\nThese enhanced systems, known as Advanced RAG, introduce specialized modules at different stages of the retrieval and generation pipeline, which can be categorized as pre-retrieval and post-retrieval components. In the pre-retrieval stage, a common issue is that the original query may be too short or vague, resulting in irrelevant retrieval results. To address this, a rewriter is introduced to clarify or expand the query. Rewriting methods include directly prompting the LLM [34,35] or training a rewriter model using feedback from the generator [36]. In the post-retrieval stage, the generator often faces challenges due to the length or noise of the retrieved content, which can affect the generation quality [33,37]. To mitigate this, a reranker is used to reorder the retrieval results [38]. Rerankers, often using cross-encoder architectures, better measure the similarity between the query and retrieved documents, pushing more relevant documents forward and removing less relevant ones. Another optimization component is the refiner, which summarizes or compresses retrieved content using techniques like prompting the LLM to summarize [39,40], or training a summarizer through supervised fine-tuning or reinforcement learning [41][42][43]. Despite the flexibility of Advanced RAG, its sequential structure limits adaptability in complex scenarios, such as queries requiring step-by-step reasoning. \n\nModular RAG. As RAG research evolves, it has entered the modular RAG stage, where components are treated as flexible modules that can be combined to create customized pipelines for different scenarios, offering greater flexibility and adaptability. Research now focuses on optimizing these pipelines, which come in four main types: Sequential, Conditional, Branching, and Loop. Sequential Pipelines process queries linearly, similar to advanced RAG, with pre-retrieval and post-retrieval stages. Conditional Pipelines route queries along different execution paths based on their type. For instance, SKR [44] identifies queries that the LLM can answer without retrieval, while Adaptive-RAG [45] classifies queries as simple or complex, using multi-round retrieval for complex ones. Branching Pipelines execute multiple paths simultaneously for a query, combining the results to form the final output.",
            "reference_string": "[272689561 | Zhou et al. | 2024 | Citations: 39]"
        },
        {
            "title": "Improving Retrieval-Augmented Deep Assertion Generation via Joint Training",
            "venue": "IEEE Transactions on Software Engineering",
            "year": 2025,
            "reference_count": 84,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.10696, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1409701329",
                    "name": "Quanjun Zhang"
                },
                {
                    "authorId": "2239197945",
                    "name": "Chunrong Fang"
                },
                {
                    "authorId": "2346015117",
                    "name": "Yi Zheng"
                },
                {
                    "authorId": "2133779426",
                    "name": "Ruixiang Qian"
                },
                {
                    "authorId": "150311588",
                    "name": "Shengcheng Yu"
                },
                {
                    "authorId": "2285983793",
                    "name": "Yuan Zhao"
                },
                {
                    "authorId": "2296059777",
                    "name": "Jianyi Zhou"
                },
                {
                    "authorId": "2276454592",
                    "name": "Yun Yang"
                },
                {
                    "authorId": "2322486553",
                    "name": "Tao Zheng"
                },
                {
                    "authorId": "2238950128",
                    "name": "Zhenyu Chen"
                }
            ],
            "abstract": "Unit testing attempts to validate the correctness of basic units of the software system under test and has a crucial role in software development and testing. However, testing experts have to spend a huge amount of effort to write unit test cases manually. Very recent work proposes a retrieve-and-edit approach to automatically generate unit test oracles, i.e., assertions. Despite being promising, it is still far from perfect due to some limitations, such as splitting assertion retrieval and generation into two separate components without benefiting each other. In this paper, we propose AG-RAG, a retrieval-augmented automated assertion generation (AG) approach that leverages external codebases and joint training to address various technical limitations of prior work. Inspired by the plastic surgery hypothesis, AG-RAG attempts to combine relevant unit tests and advanced pre-trained language models (PLMs) with retrieval-augmented fine-tuning. The key insight of AG-RAG is to simultaneously optimize the retriever and the generator as a whole pipeline with a joint training strategy, enabling them to learn from each other. Particularly, AG-RAG builds a dense retriever to search for relevant test-assert pairs (TAPs) with semantic matching and a retrieval-augmented generator to synthesize accurate assertions with the focal-test and retrieved TAPs as input. Besides, AG-RAG leverages a code-aware language model CodeT5 as the cornerstone to facilitate both assertion retrieval and generation tasks. Furthermore, AG-RAG designs a joint training strategy that allows the retriever to learn from the feedback provided by the generator. This unified design fully adapts both components specifically for retrieving more useful TAPs, thereby generating accurate assertions. AG-RAG is a generic framework that can be adapted to various off-the-shelf PLMs. We extensively evaluate AG-RAG against six state-of-the-art AG approaches on two benchmarks and three metrics. Experimental results show that AG-RAG significantly outperforms previous AG approaches on all benchmarks and metrics, e.g., improving the most recent baseline EditAS by 20.82% and 26.98% in terms of accuracy. AG-RAG also correctly generates 1739 and 2866 unique assertions that all baselines fail to generate, 3.45X and 9.20X more than EditAS. We further demonstrate the positive contribution of our joint training strategy, e.g., AG-RAG improving a variant without the retriever by an average accuracy of 14.11%. Besides, adopting other PLMs can provide substantial advancement, e.g., AG-RAG with four different PLMs improving EditAS by an average accuracy of 9.02%, highlighting the generalizability of our framework. Overall, our work demonstrates the promising potential of jointly fine-tuning the PLM-based retriever and generator to predict accurate assertions by incorporating external knowledge sources, thereby reducing the manual efforts of unit testing experts in practical scenarios.",
            "corpus_id": 276408682,
            "sentences": [
                {
                    "corpus_id": "276408682",
                    "title": "Improving Retrieval-Augmented Deep Assertion Generation via Joint Training",
                    "text": "The overall framework of AG-RAG is illustrated in Fig. 2. In the assertion retrieval stage, AG-RAG searches for similar TAPs from the external codebase by calculating their semantic similarity by a dense retriever. In the assertion generation stage, AG-RAG fine-tunes a pre-trained encoderdecoder generator with the retrieval-augmented inputs, i.e., the original focal-test and similar TAPs. During the training process, both the retriever and generator are optimized with a novel joint training strategy, which better adapts them as a whole pipeline to our task. In the assertion inference stage, after the retriever and a generator are well trained, given a focal-test input and retrieval codebase, the beam search strategy is leveraged to generate a ranked list of candidate assertions and return the one with the highest probability of being correct.",
                    "score": 0.5750553170477086,
                    "section_title": "APPROACH",
                    "char_start_offset": 15820,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 214
                        },
                        {
                            "start": 215,
                            "end": 391
                        },
                        {
                            "start": 392,
                            "end": 563
                        },
                        {
                            "start": 564,
                            "end": 854
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.939453125
                }
            ],
            "relevance_judgement": 0.939453125,
            "relevance_judgment_input_expanded": "# Title: Improving Retrieval-Augmented Deep Assertion Generation via Joint Training\n# Venue: IEEE Transactions on Software Engineering\n# Authors: Quanjun Zhang, Chunrong Fang, Yi Zheng, Ruixiang Qian, Shengcheng Yu, Yuan Zhao, Jianyi Zhou, Yun Yang, Tao Zheng, Zhenyu Chen\n## Abstract\nUnit testing attempts to validate the correctness of basic units of the software system under test and has a crucial role in software development and testing. However, testing experts have to spend a huge amount of effort to write unit test cases manually. Very recent work proposes a retrieve-and-edit approach to automatically generate unit test oracles, i.e., assertions. Despite being promising, it is still far from perfect due to some limitations, such as splitting assertion retrieval and generation into two separate components without benefiting each other. In this paper, we propose AG-RAG, a retrieval-augmented automated assertion generation (AG) approach that leverages external codebases and joint training to address various technical limitations of prior work. Inspired by the plastic surgery hypothesis, AG-RAG attempts to combine relevant unit tests and advanced pre-trained language models (PLMs) with retrieval-augmented fine-tuning. The key insight of AG-RAG is to simultaneously optimize the retriever and the generator as a whole pipeline with a joint training strategy, enabling them to learn from each other. Particularly, AG-RAG builds a dense retriever to search for relevant test-assert pairs (TAPs) with semantic matching and a retrieval-augmented generator to synthesize accurate assertions with the focal-test and retrieved TAPs as input. Besides, AG-RAG leverages a code-aware language model CodeT5 as the cornerstone to facilitate both assertion retrieval and generation tasks. Furthermore, AG-RAG designs a joint training strategy that allows the retriever to learn from the feedback provided by the generator. This unified design fully adapts both components specifically for retrieving more useful TAPs, thereby generating accurate assertions. AG-RAG is a generic framework that can be adapted to various off-the-shelf PLMs. We extensively evaluate AG-RAG against six state-of-the-art AG approaches on two benchmarks and three metrics. Experimental results show that AG-RAG significantly outperforms previous AG approaches on all benchmarks and metrics, e.g., improving the most recent baseline EditAS by 20.82% and 26.98% in terms of accuracy. AG-RAG also correctly generates 1739 and 2866 unique assertions that all baselines fail to generate, 3.45X and 9.20X more than EditAS. We further demonstrate the positive contribution of our joint training strategy, e.g., AG-RAG improving a variant without the retriever by an average accuracy of 14.11%. Besides, adopting other PLMs can provide substantial advancement, e.g., AG-RAG with four different PLMs improving EditAS by an average accuracy of 9.02%, highlighting the generalizability of our framework. Overall, our work demonstrates the promising potential of jointly fine-tuning the PLM-based retriever and generator to predict accurate assertions by incorporating external knowledge sources, thereby reducing the manual efforts of unit testing experts in practical scenarios.\n## APPROACH\nThe overall framework of AG-RAG is illustrated in Fig. 2. In the assertion retrieval stage, AG-RAG searches for similar TAPs from the external codebase by calculating their semantic similarity by a dense retriever. In the assertion generation stage, AG-RAG fine-tunes a pre-trained encoderdecoder generator with the retrieval-augmented inputs, i.e., the original focal-test and similar TAPs. During the training process, both the retriever and generator are optimized with a novel joint training strategy, which better adapts them as a whole pipeline to our task. In the assertion inference stage, after the retriever and a generator are well trained, given a focal-test input and retrieval codebase, the beam search strategy is leveraged to generate a ranked list of candidate assertions and return the one with the highest probability of being correct.",
            "reference_string": "[276408682 | Zhang et al. | 2025 | Citations: 1]"
        },
        {
            "title": "RankCoT: Refining Knowledge for Retrieval-Augmented Generation through Ranking Chain-of-Thoughts",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 54,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.17888, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2347252099",
                    "name": "Mingyan Wu"
                },
                {
                    "authorId": "2323176343",
                    "name": "Zhenghao Liu"
                },
                {
                    "authorId": "2277242040",
                    "name": "Yukun Yan"
                },
                {
                    "authorId": "2261354998",
                    "name": "Xinze Li"
                },
                {
                    "authorId": "2314785970",
                    "name": "Shi Yu"
                },
                {
                    "authorId": "1633538428",
                    "name": "Zheni Zeng"
                },
                {
                    "authorId": "2261295920",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "2204644192",
                    "name": "Ge Yu"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) enhances the performance of Large Language Models (LLMs) by incorporating external knowledge. However, LLMs still encounter challenges in effectively utilizing the knowledge from retrieved documents, often being misled by irrelevant or noisy information. To address this issue, we introduce RankCoT, a knowledge refinement method that incorporates reranking signals in generating CoT-based summarization for knowledge refinement based on given query and all retrieval documents. During training, RankCoT prompts the LLM to generate Chain-of-Thought (CoT) candidates based on the query and individual documents. It then fine-tunes the LLM to directly reproduce the best CoT from these candidate outputs based on all retrieved documents, which requires LLM to filter out irrelevant documents during generating CoT-style summarization. Additionally, RankCoT incorporates a self-reflection mechanism that further refines the CoT outputs, resulting in higher-quality training data. Our experiments demonstrate the effectiveness of RankCoT, showing its superior performance over other knowledge refinement models. Further analysis reveals that RankCoT can provide shorter but effective refinement results, enabling the generator to produce more accurate answers. All code and data are available at https://github.com/NEUIR/RankCoT.",
            "corpus_id": 276580741,
            "sentences": [
                {
                    "corpus_id": "276580741",
                    "title": "RankCoT: Refining Knowledge for Retrieval-Augmented Generation through Ranking Chain-of-Thoughts",
                    "text": "Modular RAG systems (Gao et al., 2024;Xu et al., 2024c) focus on refining external knowledge through different modules implemented by LLMs, which have become a key trend in the RAG area. For instance, Self-RAG (Asai et al., 2024a) uses different tags for adaptive retrieval (Jiang et al., 2023) and self-reflection to refine knowledge. Some approaches also focus on reformulating queries to identify more useful documents for answering ques-tions (Yan et al., 2024;Trivedi et al., 2023). Yan et al. (2024) introduce a retrieval evaluator that acts as a judge to trigger query reformulation, search, and knowledge refinement actions to supply more accurate evidence for generation. \n\nTo further improve the performance of modular RAG systems, these models focus on fine-tuning various components of the RAG framework. Some efforts aim to align the information needs between the retriever and the generator by optimizing the retrievers based on feedback from the generation models (Yu et al., 2023;Shi et al., 2024;Izacard and Grave, 2021). Lin et al. (2024) adapt LLMs within the RAG setting by constructing instructiontuning data for Supervised Fine-Tuning (SFT), enabling the models to better leverage the retrieved documents. Additionally, Li et al. (2024) use Direct Preference Optimization (DPO) (Rafailov et al., 2024) to jointly optimize the modules in a RAG system, aligning their data preferences.",
                    "score": 0.6555963627496948,
                    "section_title": "Related Work",
                    "char_start_offset": 5892,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 186
                        },
                        {
                            "start": 187,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 487
                        },
                        {
                            "start": 488,
                            "end": 680
                        },
                        {
                            "start": 683,
                            "end": 816
                        },
                        {
                            "start": 817,
                            "end": 1038
                        },
                        {
                            "start": 1039,
                            "end": 1227
                        },
                        {
                            "start": 1228,
                            "end": 1405
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 210,
                            "end": 230,
                            "matchedPaperCorpusId": "264288947"
                        },
                        {
                            "start": 274,
                            "end": 294,
                            "matchedPaperCorpusId": "258615731"
                        },
                        {
                            "start": 465,
                            "end": 486,
                            "matchedPaperCorpusId": "254877499"
                        },
                        {
                            "start": 979,
                            "end": 996,
                            "matchedPaperCorpusId": "258960666"
                        },
                        {
                            "start": 996,
                            "end": 1013,
                            "matchedPaperCorpusId": "256389797"
                        },
                        {
                            "start": 1013,
                            "end": 1037,
                            "matchedPaperCorpusId": "227746078"
                        },
                        {
                            "start": 1300,
                            "end": 1323,
                            "matchedPaperCorpusId": "258959321"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93212890625
                }
            ],
            "relevance_judgement": 0.93212890625,
            "relevance_judgment_input_expanded": "# Title: RankCoT: Refining Knowledge for Retrieval-Augmented Generation through Ranking Chain-of-Thoughts\n# Venue: arXiv.org\n# Authors: Mingyan Wu, Zhenghao Liu, Yukun Yan, Xinze Li, Shi Yu, Zheni Zeng, Yu Gu, Ge Yu\n## Abstract\nRetrieval-Augmented Generation (RAG) enhances the performance of Large Language Models (LLMs) by incorporating external knowledge. However, LLMs still encounter challenges in effectively utilizing the knowledge from retrieved documents, often being misled by irrelevant or noisy information. To address this issue, we introduce RankCoT, a knowledge refinement method that incorporates reranking signals in generating CoT-based summarization for knowledge refinement based on given query and all retrieval documents. During training, RankCoT prompts the LLM to generate Chain-of-Thought (CoT) candidates based on the query and individual documents. It then fine-tunes the LLM to directly reproduce the best CoT from these candidate outputs based on all retrieved documents, which requires LLM to filter out irrelevant documents during generating CoT-style summarization. Additionally, RankCoT incorporates a self-reflection mechanism that further refines the CoT outputs, resulting in higher-quality training data. Our experiments demonstrate the effectiveness of RankCoT, showing its superior performance over other knowledge refinement models. Further analysis reveals that RankCoT can provide shorter but effective refinement results, enabling the generator to produce more accurate answers. All code and data are available at https://github.com/NEUIR/RankCoT.\n## Related Work\nModular RAG systems (Gao et al., 2024;Xu et al., 2024c) focus on refining external knowledge through different modules implemented by LLMs, which have become a key trend in the RAG area. For instance, Self-RAG (Asai et al., 2024a) uses different tags for adaptive retrieval (Jiang et al., 2023) and self-reflection to refine knowledge. Some approaches also focus on reformulating queries to identify more useful documents for answering ques-tions (Yan et al., 2024;Trivedi et al., 2023). Yan et al. (2024) introduce a retrieval evaluator that acts as a judge to trigger query reformulation, search, and knowledge refinement actions to supply more accurate evidence for generation. \n\nTo further improve the performance of modular RAG systems, these models focus on fine-tuning various components of the RAG framework. Some efforts aim to align the information needs between the retriever and the generator by optimizing the retrievers based on feedback from the generation models (Yu et al., 2023;Shi et al., 2024;Izacard and Grave, 2021). Lin et al. (2024) adapt LLMs within the RAG setting by constructing instructiontuning data for Supervised Fine-Tuning (SFT), enabling the models to better leverage the retrieved documents. Additionally, Li et al. (2024) use Direct Preference Optimization (DPO) (Rafailov et al., 2024) to jointly optimize the modules in a RAG system, aligning their data preferences.",
            "reference_string": "[276580741 | Wu et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 62,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.01281, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2203079037",
                    "name": "Sakhinana Sagar Srinivas"
                },
                {
                    "authorId": "2139833562",
                    "name": "Venkataramana Runkana"
                }
            ],
            "abstract": "We present a comprehensive framework for enhancing Retrieval-Augmented Generation (RAG) systems through dynamic retrieval strategies and reinforcement fine-tuning. This approach significantly improves large language models on knowledge-intensive tasks, including opendomain question answering and complex reasoning. Our framework integrates two complementary techniques: Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS), which dynamically determines retrieval timing and content based on contextual needs. Together, these techniques enhance both the utilization and relevance of retrieved content, improving factual accuracy and response quality. Designed as a lightweight solution compatible with any Transformer-based LLM without requiring additional training, our framework excels in knowledge-intensive tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a novel method to selectively compress key-value caches by token importance, mitigating memory bottlenecks in long-context applications. The framework also incorporates test-time scaling techniques to dynamically balance reasoning depth and computational resources, alongside optimized decoding strategies for faster inference. Experiments on benchmark datasets show that our framework reduces hallucinations, strengthens domain-specific reasoning, and achieves significant efficiency and scalability gains over traditional RAG systems. This integrated approach advances the development of robust, efficient, and scalable RAG systems across diverse applications.",
            "corpus_id": 277501853,
            "sentences": [
                {
                    "corpus_id": "277501853",
                    "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
                    "text": "We present a comprehensive framework for enhancing Retrieval-Augmented Generation (RAG) systems through dynamic retrieval strategies and reinforcement fine-tuning. This approach significantly improves large language models on knowledge-intensive tasks, including opendomain question answering and complex reasoning. Our framework integrates two complementary techniques: Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS), which dynamically determines retrieval timing and content based on contextual needs. Together, these techniques enhance both the utilization and relevance of retrieved content, improving factual accuracy and response quality. Designed as a lightweight solution compatible with any Transformer-based LLM without requiring additional training, our framework excels in knowledge-intensive tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a novel method to selectively compress key-value caches by token importance, mitigating memory bottlenecks in long-context applications. The framework also incorporates test-time scaling techniques to dynamically balance reasoning depth and computational resources, alongside optimized decoding strategies for faster inference. Experiments on benchmark datasets show that our framework reduces hallucinations, strengthens domain-specific reasoning, and achieves significant efficiency and scalability gains over traditional RAG systems. This integrated approach advances the development of robust, efficient, and scalable RAG systems across diverse applications.",
                    "score": 0.5822818467111301,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92919921875
                }
            ],
            "relevance_judgement": 0.92919921875,
            "relevance_judgment_input_expanded": "# Title: Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding\n# Venue: arXiv.org\n# Authors: Sakhinana Sagar Srinivas, Venkataramana Runkana\n## Abstract\nWe present a comprehensive framework for enhancing Retrieval-Augmented Generation (RAG) systems through dynamic retrieval strategies and reinforcement fine-tuning. This approach significantly improves large language models on knowledge-intensive tasks, including opendomain question answering and complex reasoning. Our framework integrates two complementary techniques: Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS), which dynamically determines retrieval timing and content based on contextual needs. Together, these techniques enhance both the utilization and relevance of retrieved content, improving factual accuracy and response quality. Designed as a lightweight solution compatible with any Transformer-based LLM without requiring additional training, our framework excels in knowledge-intensive tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a novel method to selectively compress key-value caches by token importance, mitigating memory bottlenecks in long-context applications. The framework also incorporates test-time scaling techniques to dynamically balance reasoning depth and computational resources, alongside optimized decoding strategies for faster inference. Experiments on benchmark datasets show that our framework reduces hallucinations, strengthens domain-specific reasoning, and achieves significant efficiency and scalability gains over traditional RAG systems. This integrated approach advances the development of robust, efficient, and scalable RAG systems across diverse applications.\n",
            "reference_string": "[277501853 | Srinivas et al. | 2025 | Citations: 1]"
        },
        {
            "title": "From Efficient Multimodal Models to World Models: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 121,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.00118, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2276937443",
                    "name": "Xinji Mai"
                },
                {
                    "authorId": "2261831274",
                    "name": "Zeng Tao"
                },
                {
                    "authorId": "2261891655",
                    "name": "Junxiong Lin"
                },
                {
                    "authorId": "2276807843",
                    "name": "Haoran Wang"
                },
                {
                    "authorId": "2276969811",
                    "name": "Yang Chang"
                },
                {
                    "authorId": "2212014366",
                    "name": "Yanlan Kang"
                },
                {
                    "authorId": "2276879376",
                    "name": "Yan Wang"
                },
                {
                    "authorId": "2276819302",
                    "name": "Wenqiang Zhang"
                }
            ],
            "abstract": "Multimodal Large Models (MLMs) are becoming a significant research focus, combining powerful large language models with multimodal learning to perform complex tasks across different data modalities. This review explores the latest developments and challenges in MLMs, emphasizing their potential in achieving artificial general intelligence and as a pathway to world models. We provide an overview of key techniques such as Multimodal Chain of Thought (M-COT), Multimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning (M-ICL). Additionally, we discuss both the fundamental and specific technologies of multimodal models, highlighting their applications, input/output modalities, and design characteristics. Despite significant advancements, the development of a unified multimodal model remains elusive. We discuss the integration of 3D generation and embodied intelligence to enhance world simulation capabilities and propose incorporating external rule systems for improved reasoning and decision-making. Finally, we outline future research directions to address these challenges and advance the field.",
            "corpus_id": 270870796,
            "sentences": [
                {
                    "corpus_id": "270870796",
                    "title": "From Efficient Multimodal Models to World Models: A Survey",
                    "text": "Fine-tuning techniques involve further training pre-trained models on specific task datasets to enhance model performance in that task.Below are common fine-tuning techniques and their recent advancements:\n\nLoRA (Low-Rank Adaptation) [59] is a low-rank adaptation technique that adds low-rank matrices to pre-trained models for fine-tuning, reducing computational and storage costs while maintaining performance.QLoRA [60] is an improved version that further optimizes the fine-tuning process through quantization techniques.Retrieval-Augmented Generation (RAG) [61] combines information retrieval and generative models, enhancing generative model performance by retrieving relevant information from external data sources.The LangChain [62] library provides various tools allowing large models to access real-time information from sources like Google Search, vector databases, or knowledge graphs, further improving RAG effectiveness.LlamaIndex (GPT Index) [63], [64] is an integrated data framework designed to enhance large language models (LLMs) by enabling the use of private or custom data.LlamaIndex provides data connectors, indexing and graph-building mechanisms, and advanced retrieval and query interfaces, simplifying data integration and information retrieval processes.\n\nBy applying these fine-tuning techniques appropriately, pretrained model knowledge can be fully utilized, improving performance in new tasks while reducing training time and computational resource consumption.",
                    "score": 0.5874777786508157,
                    "section_title": "G. Fine-Tuning Techniques for Model Architectures",
                    "char_start_offset": 32883,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 135
                        },
                        {
                            "start": 135,
                            "end": 205
                        },
                        {
                            "start": 207,
                            "end": 412
                        },
                        {
                            "start": 412,
                            "end": 525
                        },
                        {
                            "start": 525,
                            "end": 722
                        },
                        {
                            "start": 722,
                            "end": 934
                        },
                        {
                            "start": 934,
                            "end": 1095
                        },
                        {
                            "start": 1095,
                            "end": 1282
                        },
                        {
                            "start": 1284,
                            "end": 1493
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 418,
                            "end": 422,
                            "matchedPaperCorpusId": "258841328"
                        },
                        {
                            "start": 562,
                            "end": 566,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 736,
                            "end": 740,
                            "matchedPaperCorpusId": "260223847"
                        },
                        {
                            "start": 963,
                            "end": 967,
                            "matchedPaperCorpusId": "73651944"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9267578125
                }
            ],
            "relevance_judgement": 0.9267578125,
            "relevance_judgment_input_expanded": "# Title: From Efficient Multimodal Models to World Models: A Survey\n# Venue: arXiv.org\n# Authors: Xinji Mai, Zeng Tao, Junxiong Lin, Haoran Wang, Yang Chang, Yanlan Kang, Yan Wang, Wenqiang Zhang\n## Abstract\nMultimodal Large Models (MLMs) are becoming a significant research focus, combining powerful large language models with multimodal learning to perform complex tasks across different data modalities. This review explores the latest developments and challenges in MLMs, emphasizing their potential in achieving artificial general intelligence and as a pathway to world models. We provide an overview of key techniques such as Multimodal Chain of Thought (M-COT), Multimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning (M-ICL). Additionally, we discuss both the fundamental and specific technologies of multimodal models, highlighting their applications, input/output modalities, and design characteristics. Despite significant advancements, the development of a unified multimodal model remains elusive. We discuss the integration of 3D generation and embodied intelligence to enhance world simulation capabilities and propose incorporating external rule systems for improved reasoning and decision-making. Finally, we outline future research directions to address these challenges and advance the field.\n## G. Fine-Tuning Techniques for Model Architectures\nFine-tuning techniques involve further training pre-trained models on specific task datasets to enhance model performance in that task.Below are common fine-tuning techniques and their recent advancements:\n\nLoRA (Low-Rank Adaptation) [59] is a low-rank adaptation technique that adds low-rank matrices to pre-trained models for fine-tuning, reducing computational and storage costs while maintaining performance.QLoRA [60] is an improved version that further optimizes the fine-tuning process through quantization techniques.Retrieval-Augmented Generation (RAG) [61] combines information retrieval and generative models, enhancing generative model performance by retrieving relevant information from external data sources.The LangChain [62] library provides various tools allowing large models to access real-time information from sources like Google Search, vector databases, or knowledge graphs, further improving RAG effectiveness.LlamaIndex (GPT Index) [63], [64] is an integrated data framework designed to enhance large language models (LLMs) by enabling the use of private or custom data.LlamaIndex provides data connectors, indexing and graph-building mechanisms, and advanced retrieval and query interfaces, simplifying data integration and information retrieval processes.\n\nBy applying these fine-tuning techniques appropriately, pretrained model knowledge can be fully utilized, improving performance in new tasks while reducing training time and computational resource consumption.",
            "reference_string": "[270870796 | Mai et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 229,
            "citation_count": 1819,
            "influential_citation_count": 106,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.10997, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280046531",
                    "name": "Yunfan Gao"
                },
                {
                    "authorId": "2275320371",
                    "name": "Yun Xiong"
                },
                {
                    "authorId": "2275341478",
                    "name": "Xinyu Gao"
                },
                {
                    "authorId": "2275191447",
                    "name": "Kangxiang Jia"
                },
                {
                    "authorId": "2275530552",
                    "name": "Jinliu Pan"
                },
                {
                    "authorId": "2275171009",
                    "name": "Yuxi Bi"
                },
                {
                    "authorId": "2276187454",
                    "name": "Yi Dai"
                },
                {
                    "authorId": "2275540959",
                    "name": "Jiawei Sun"
                },
                {
                    "authorId": "2258800561",
                    "name": "Qianyu Guo"
                },
                {
                    "authorId": "2291409458",
                    "name": "Meng Wang"
                },
                {
                    "authorId": "2256769434",
                    "name": "Haofen Wang"
                }
            ],
            "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.",
            "corpus_id": 266359151,
            "sentences": [
                {
                    "corpus_id": "266359151",
                    "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
                    "text": "2) New Patterns: Modular RAG offers remarkable adaptability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple \"Retrieve\" and \"Read\" mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks. \n\nInnovations such as the Rewrite-Retrieve-Read [7]model leverage the LLM's capabilities to refine retrieval queries through a rewriting module and a LM-feedback mechanism to update rewriting model., improving task performance. Similarly, approaches like Generate-Read [13] replace traditional retrieval with LLM-generated content, while Recite-Read [22] emphasizes retrieval from model weights, enhancing the model's ability to handle knowledge-intensive tasks. Hybrid retrieval strategies integrate keyword, semantic, and vector searches to cater to diverse queries. Additionally, employing sub-queries and hypothetical document embeddings (HyDE) [11] seeks to improve retrieval relevance by focusing on embedding similarities between generated answers and real documents. \n\nAdjustments in module arrangement and interaction, such as the Demonstrate-Search-Predict (DSP) [23] framework and the iterative Retrieve-Read-Retrieve-Read flow of ITER-RETGEN [14], showcase the dynamic use of module outputs to bolster another module's functionality, illustrating a sophisticated understanding of enhancing module synergy. The flexible orchestration of Modular RAG Flow showcases the benefits of adaptive retrieval through techniques such as FLARE [24] and Self-RAG [25]. This approach transcends the fixed RAG retrieval process by evaluating the necessity of retrieval based on different scenarios. Another benefit of a flexible architecture is that the RAG system can more easily integrate with other technologies (such as fine-tuning or reinforcement learning) [26]. For example, this can involve fine-tuning the retriever for better retrieval results, fine-tuning the generator for more personalized outputs, or engaging in collaborative fine-tuning [27].",
                    "score": 0.6096042122869251,
                    "section_title": "C. Modular RAG",
                    "char_start_offset": 13909,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 142
                        },
                        {
                            "start": 143,
                            "end": 266
                        },
                        {
                            "start": 267,
                            "end": 443
                        },
                        {
                            "start": 446,
                            "end": 671
                        },
                        {
                            "start": 672,
                            "end": 906
                        },
                        {
                            "start": 907,
                            "end": 1012
                        },
                        {
                            "start": 1013,
                            "end": 1218
                        },
                        {
                            "start": 1221,
                            "end": 1561
                        },
                        {
                            "start": 1562,
                            "end": 1710
                        },
                        {
                            "start": 1711,
                            "end": 1838
                        },
                        {
                            "start": 1839,
                            "end": 2008
                        },
                        {
                            "start": 2009,
                            "end": 2198
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9228515625
                },
                {
                    "corpus_id": "266359151",
                    "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
                    "text": "Targeted fine-tuning based on the scenario and data characteristics on LLMs can yield better results. This is also one of the greatest advantages of using on-premise LLMs. When LLMs lack data in a specific domain, additional knowledge can be provided to the LLM through fine-tuning. Huggingface's fine-tuning data can also be used as an initial step. \n\nAnother benefit of fine-tuning is the ability to adjust the model's input and output. For example, it can enable LLM to adapt to specific data formats and generate responses in a particular style as instructed [37]. For retrieval tasks that engage with structured data, the SANTA framework [76] implements a tripartite training regimen to effectively encapsulate both structural and semantic nuances. The initial phase focuses on the retriever, where contrastive learning is harnessed to refine the query and document embeddings. \n\nAligning LLM outputs with human or retriever preferences through reinforcement learning is a potential approach. For instance, manually annotating the final generated answers and then providing feedback through reinforcement learning. In addition to aligning with human preferences, it is also possible to align with the preferences of fine-tuned models and retrievers [79]. When circumstances prevent access to powerful proprietary models or larger parameter open-source models, a simple and effective method is to distill the more powerful models(e.g. GPT-4). Fine-tuning of LLM can also be coordinated with fine-tuning of the retriever to align preferences. A typical approach, such as RA-DIT [27], aligns the scoring functions between Retriever and Generator using KL divergence.",
                    "score": 0.6070750554025748,
                    "section_title": "B. LLM Fine-tuning",
                    "char_start_offset": 40474,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 101
                        },
                        {
                            "start": 102,
                            "end": 171
                        },
                        {
                            "start": 172,
                            "end": 282
                        },
                        {
                            "start": 283,
                            "end": 350
                        },
                        {
                            "start": 353,
                            "end": 438
                        },
                        {
                            "start": 439,
                            "end": 568
                        },
                        {
                            "start": 569,
                            "end": 753
                        },
                        {
                            "start": 754,
                            "end": 882
                        },
                        {
                            "start": 885,
                            "end": 997
                        },
                        {
                            "start": 998,
                            "end": 1119
                        },
                        {
                            "start": 1120,
                            "end": 1259
                        },
                        {
                            "start": 1260,
                            "end": 1438
                        },
                        {
                            "start": 1439,
                            "end": 1446
                        },
                        {
                            "start": 1447,
                            "end": 1545
                        },
                        {
                            "start": 1546,
                            "end": 1668
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91796875
                }
            ],
            "relevance_judgement": 0.9228515625,
            "relevance_judgment_input_expanded": "# Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n# Venue: arXiv.org\n# Authors: Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang\n## Abstract\nLarge Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\n## C. Modular RAG\n2) New Patterns: Modular RAG offers remarkable adaptability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple \"Retrieve\" and \"Read\" mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks. \n\nInnovations such as the Rewrite-Retrieve-Read [7]model leverage the LLM's capabilities to refine retrieval queries through a rewriting module and a LM-feedback mechanism to update rewriting model., improving task performance. Similarly, approaches like Generate-Read [13] replace traditional retrieval with LLM-generated content, while Recite-Read [22] emphasizes retrieval from model weights, enhancing the model's ability to handle knowledge-intensive tasks. Hybrid retrieval strategies integrate keyword, semantic, and vector searches to cater to diverse queries. Additionally, employing sub-queries and hypothetical document embeddings (HyDE) [11] seeks to improve retrieval relevance by focusing on embedding similarities between generated answers and real documents. \n\nAdjustments in module arrangement and interaction, such as the Demonstrate-Search-Predict (DSP) [23] framework and the iterative Retrieve-Read-Retrieve-Read flow of ITER-RETGEN [14], showcase the dynamic use of module outputs to bolster another module's functionality, illustrating a sophisticated understanding of enhancing module synergy. The flexible orchestration of Modular RAG Flow showcases the benefits of adaptive retrieval through techniques such as FLARE [24] and Self-RAG [25]. This approach transcends the fixed RAG retrieval process by evaluating the necessity of retrieval based on different scenarios. Another benefit of a flexible architecture is that the RAG system can more easily integrate with other technologies (such as fine-tuning or reinforcement learning) [26]. For example, this can involve fine-tuning the retriever for better retrieval results, fine-tuning the generator for more personalized outputs, or engaging in collaborative fine-tuning [27].\n\n## B. LLM Fine-tuning\nTargeted fine-tuning based on the scenario and data characteristics on LLMs can yield better results. This is also one of the greatest advantages of using on-premise LLMs. When LLMs lack data in a specific domain, additional knowledge can be provided to the LLM through fine-tuning. Huggingface's fine-tuning data can also be used as an initial step. \n\nAnother benefit of fine-tuning is the ability to adjust the model's input and output. For example, it can enable LLM to adapt to specific data formats and generate responses in a particular style as instructed [37]. For retrieval tasks that engage with structured data, the SANTA framework [76] implements a tripartite training regimen to effectively encapsulate both structural and semantic nuances. The initial phase focuses on the retriever, where contrastive learning is harnessed to refine the query and document embeddings. \n\nAligning LLM outputs with human or retriever preferences through reinforcement learning is a potential approach. For instance, manually annotating the final generated answers and then providing feedback through reinforcement learning. In addition to aligning with human preferences, it is also possible to align with the preferences of fine-tuned models and retrievers [79]. When circumstances prevent access to powerful proprietary models or larger parameter open-source models, a simple and effective method is to distill the more powerful models(e.g. GPT-4). Fine-tuning of LLM can also be coordinated with fine-tuning of the retriever to align preferences. A typical approach, such as RA-DIT [27], aligns the scoring functions between Retriever and Generator using KL divergence.",
            "reference_string": "[266359151 | Gao et al. | 2023 | Citations: 1819]"
        },
        {
            "title": "RuleRAG: Rule-guided retrieval-augmented generation with language models for question answering",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 67,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.22353, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2165306772",
                    "name": "Zhongwu Chen"
                },
                {
                    "authorId": "2250617116",
                    "name": "Chengjin Xu"
                },
                {
                    "authorId": "2329140108",
                    "name": "Dingmin Wang"
                },
                {
                    "authorId": "2273614102",
                    "name": "Zhen Huang"
                },
                {
                    "authorId": "67069932",
                    "name": "Yong Dou"
                },
                {
                    "authorId": "2284217200",
                    "name": "Jian Guo"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) has shown promising potential in knowledge intensive question answering (QA). However, existing approaches only consider the query itself, neither specifying the retrieval preferences for the retrievers nor informing the generators of how to refer to the retrieved documents for the answers, which poses a significant challenge to the QA performance. To address these issues, we propose Rule-guided Retrieval-Augmented Generation with LMs, which explicitly introduces rules for in-context learning (RuleRAG-ICL) to guide retrievers to recall related documents in the directions of rules and uniformly guide generators to reason attributed by the same rules. Moreover, most existing RAG datasets were constructed without considering rules and Knowledge Graphs (KGs) are recognized as providing high-quality rules. Therefore, we construct five rule-aware RAG benchmarks for QA, RuleQA, based on KGs to stress the significance of retrieval and reasoning with rules. Experiments on RuleQA demonstrate RuleRAG-ICL improves the retrieval quality of +89.2% in Recall@10 and answer accuracy of +103.1% in Exact Match, and RuleRAG-FT yields more enhancement. In addition, experiments on four existing RAG datasets show RuleRAG is also effective by offering rules in RuleQA to them, further proving the generalization of rule guidance in RuleRAG.",
            "corpus_id": 273695367,
            "sentences": [
                {
                    "corpus_id": "273695367",
                    "title": "RuleRAG: Rule-guided retrieval-augmented generation with language models for question answering",
                    "text": "In this section, we present details of our proposed novel rule-guided retrieval-augmented generation with LMs (RuleRAG) for solving the task of knowledge-intensive factual queries. Notably, RuleRAG includes training-free RuleRAG-ICL and fine-tuned RuleRAG-FT. First, we prompt RuleRAG-ICL with queries and rules for in-context learning during retrieving and inferring. The rules are aimed to guide retrievers to recall logically supportive documents and guide generators to predict attributable answers. Then, RuleRAG-FT further fine-tunes the retrievers and generators to explicitly enhance their rule-following ability by our introduced rule-guided fine-tuning (RGFT), where we leverage the queries combining rules as fine-tuning data and the ground truth answers as supervision data. \n\nThe inferring process of RuleRAG-FT is the same as RuleRAG-ICL.",
                    "score": 0.57940649977859,
                    "section_title": "PROPOSED METHOD: RULERAG",
                    "char_start_offset": 10390,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 180
                        },
                        {
                            "start": 181,
                            "end": 259
                        },
                        {
                            "start": 260,
                            "end": 368
                        },
                        {
                            "start": 369,
                            "end": 503
                        },
                        {
                            "start": 504,
                            "end": 786
                        },
                        {
                            "start": 789,
                            "end": 852
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92236328125
                },
                {
                    "corpus_id": "273695367",
                    "title": "RuleRAG: Rule-guided retrieval-augmented generation with language models for question answering",
                    "text": "The overview of our proposed rule-guided retriever and generator fine-tuning in RuleRAG-FT are illustrated in Figure 2 (b) and (c), respectively. For rule-guided retriever fine-tuning (RGFT-retriever), we update the LM encoders in a contrastive learning objective (Chen et al., 2020) and train over supervised fine-tuning data F R provided in our constructed benchmarks, where inputs are the queries plus rules and supervised labels are heuristic oracle documents. Compared with retrievers employed with simple retrieval principles, our fine-tuned retrievers can recall more relevant results, aligned with the preferences of the rules. For rule-guided generator fine-tuning (RGFT-generator), we adopt the supervised instruction-tuning objective (Iyer et al., 2023;Chung et al., 2024) while combining each query q with two components: retrieved documents D q from the retrieval phase and the same set of rules R q consistent with the retrieval phase. The rules introduced in the RGFT-generator train LLMs on how to optimally attribute from the retrieved context into answers by following rules, making RuleRAG leverage the fine-tuned retrievers more rationally. Experiments show our proposed RGFT can further guarantee and boost the retrieval quality and answering accuracy of RuleRAG-FT than RuleRAG-ICL. \n\nRule-guided retriever fine-tuning (RGFT-retriever). We utilize two main types of retrievers: sparse retrievers and dense retrievers. As the sparse retriever, we use Pyserini1 to implement the standard training-free BM25 (Robertson & Zaragoza, 2009), which relies on word-level frequencies. As the dense retrievers, we adopt the dual-encoder based retriever architecture, such as DPR2 and SimCSE3 . We freeze the document encoder and tune the query encoder for high retrieval efficiency (Lewis et al., 2020). Given a ((q, r) , D o ) pair in the fine-tuning data F R where D o serve as the oracle documents, each d + i \u2208 D o is a positive learning example while each in-batch d \u2212 j \u0338 \u2208 D o is a negative example.",
                    "score": 0.7073053144796827,
                    "section_title": "RULERAG-FT",
                    "char_start_offset": 13388,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 464
                        },
                        {
                            "start": 465,
                            "end": 635
                        },
                        {
                            "start": 636,
                            "end": 949
                        },
                        {
                            "start": 950,
                            "end": 1160
                        },
                        {
                            "start": 1161,
                            "end": 1304
                        },
                        {
                            "start": 1307,
                            "end": 1358
                        },
                        {
                            "start": 1359,
                            "end": 1439
                        },
                        {
                            "start": 1440,
                            "end": 1596
                        },
                        {
                            "start": 1597,
                            "end": 1704
                        },
                        {
                            "start": 1705,
                            "end": 1814
                        },
                        {
                            "start": 1815,
                            "end": 2017
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 264,
                            "end": 283,
                            "matchedPaperCorpusId": "211096730"
                        },
                        {
                            "start": 1527,
                            "end": 1555,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 1793,
                            "end": 1813,
                            "matchedPaperCorpusId": "218869575"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90869140625
                }
            ],
            "relevance_judgement": 0.92236328125,
            "relevance_judgment_input_expanded": "# Title: RuleRAG: Rule-guided retrieval-augmented generation with language models for question answering\n# Venue: arXiv.org\n# Authors: Zhongwu Chen, Chengjin Xu, Dingmin Wang, Zhen Huang, Yong Dou, Jian Guo\n## Abstract\nRetrieval-augmented generation (RAG) has shown promising potential in knowledge intensive question answering (QA). However, existing approaches only consider the query itself, neither specifying the retrieval preferences for the retrievers nor informing the generators of how to refer to the retrieved documents for the answers, which poses a significant challenge to the QA performance. To address these issues, we propose Rule-guided Retrieval-Augmented Generation with LMs, which explicitly introduces rules for in-context learning (RuleRAG-ICL) to guide retrievers to recall related documents in the directions of rules and uniformly guide generators to reason attributed by the same rules. Moreover, most existing RAG datasets were constructed without considering rules and Knowledge Graphs (KGs) are recognized as providing high-quality rules. Therefore, we construct five rule-aware RAG benchmarks for QA, RuleQA, based on KGs to stress the significance of retrieval and reasoning with rules. Experiments on RuleQA demonstrate RuleRAG-ICL improves the retrieval quality of +89.2% in Recall@10 and answer accuracy of +103.1% in Exact Match, and RuleRAG-FT yields more enhancement. In addition, experiments on four existing RAG datasets show RuleRAG is also effective by offering rules in RuleQA to them, further proving the generalization of rule guidance in RuleRAG.\n## PROPOSED METHOD: RULERAG\nIn this section, we present details of our proposed novel rule-guided retrieval-augmented generation with LMs (RuleRAG) for solving the task of knowledge-intensive factual queries. Notably, RuleRAG includes training-free RuleRAG-ICL and fine-tuned RuleRAG-FT. First, we prompt RuleRAG-ICL with queries and rules for in-context learning during retrieving and inferring. The rules are aimed to guide retrievers to recall logically supportive documents and guide generators to predict attributable answers. Then, RuleRAG-FT further fine-tunes the retrievers and generators to explicitly enhance their rule-following ability by our introduced rule-guided fine-tuning (RGFT), where we leverage the queries combining rules as fine-tuning data and the ground truth answers as supervision data. \n\nThe inferring process of RuleRAG-FT is the same as RuleRAG-ICL.\n\n## RULERAG-FT\nThe overview of our proposed rule-guided retriever and generator fine-tuning in RuleRAG-FT are illustrated in Figure 2 (b) and (c), respectively. For rule-guided retriever fine-tuning (RGFT-retriever), we update the LM encoders in a contrastive learning objective (Chen et al., 2020) and train over supervised fine-tuning data F R provided in our constructed benchmarks, where inputs are the queries plus rules and supervised labels are heuristic oracle documents. Compared with retrievers employed with simple retrieval principles, our fine-tuned retrievers can recall more relevant results, aligned with the preferences of the rules. For rule-guided generator fine-tuning (RGFT-generator), we adopt the supervised instruction-tuning objective (Iyer et al., 2023;Chung et al., 2024) while combining each query q with two components: retrieved documents D q from the retrieval phase and the same set of rules R q consistent with the retrieval phase. The rules introduced in the RGFT-generator train LLMs on how to optimally attribute from the retrieved context into answers by following rules, making RuleRAG leverage the fine-tuned retrievers more rationally. Experiments show our proposed RGFT can further guarantee and boost the retrieval quality and answering accuracy of RuleRAG-FT than RuleRAG-ICL. \n\nRule-guided retriever fine-tuning (RGFT-retriever). We utilize two main types of retrievers: sparse retrievers and dense retrievers. As the sparse retriever, we use Pyserini1 to implement the standard training-free BM25 (Robertson & Zaragoza, 2009), which relies on word-level frequencies. As the dense retrievers, we adopt the dual-encoder based retriever architecture, such as DPR2 and SimCSE3 . We freeze the document encoder and tune the query encoder for high retrieval efficiency (Lewis et al., 2020). Given a ((q, r) , D o ) pair in the fine-tuning data F R where D o serve as the oracle documents, each d + i \u2208 D o is a positive learning example while each in-batch d \u2212 j \u0338 \u2208 D o is a negative example.",
            "reference_string": "[273695367 | Chen et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 159,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.09022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2296715370",
                    "name": "Benjue Weng"
                }
            ],
            "abstract": "With the surge of ChatGPT,the use of large models has significantly increased,rapidly rising to prominence across the industry and sweeping across the internet. This article is a comprehensive review of fine-tuning methods for large models. This paper investigates the latest technological advancements and the application of advanced methods in aspects such as task-adaptive fine-tuning,domain-adaptive fine-tuning,few-shot learning,knowledge distillation,multi-task learning,parameter-efficient fine-tuning,and dynamic fine-tuning.",
            "corpus_id": 269149041,
            "sentences": [
                {
                    "corpus_id": "269149041",
                    "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
                    "text": "For every x and preceding generation y < t,the model decodes a retrieval token to evaluate the utility of retrieval.If retrieval is not required,the model predicts the next output segment,as it does in a standard LM.If retrieval is needed,the model generates: a critique token to evaluate the retrieved passage's relevance,the next response segment,and a critique token to evaluate if the information in the response segment is supported by the passage.Finally,a new critique token evaluates the overall utility of the response.\n\n\"REPLUG: Retrieval-Augmented Black-Box Language Models [115]:\" This research explores how to optimize retrieval results in black-box language models,such as those that only expose APIs without revealing embeddings,using a retrieval-augmented approach.\n\n\"Atlas: Few-shot Learning with Retrieval Augmented Language Models [45]:\" The paper discusses methods for joint training of retrievers and language models,especially for knowledge-intensive tasks.\n\n\"RA-DIT: RETRIEVAL-AUGMENTED DUAL IN-STRUCTION TUNING [66]:\" A lightweight fine-tuning method that combines RAG and SFT is proposed to enhance the performance of retrieval-augmented language models.\n\n\"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering [118]:\" This paper assesses the domain adaptability of RAG models in open-domain question answering (ODQA) tasks and proposes RAG-end2end,an extension of RAG that can adapt to specific domain knowledge bases by updating all components during training.\n\n\"RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance [8]:\" This article provides a guide on the optimization strategies of RAG,fine-tuning,and their combina-",
                    "score": 0.8420221995607047,
                    "section_title": "XIII. RAG-MEMORY-FINETUNING",
                    "char_start_offset": 145958,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 116,
                            "end": 216
                        },
                        {
                            "start": 216,
                            "end": 453
                        },
                        {
                            "start": 453,
                            "end": 528
                        },
                        {
                            "start": 530,
                            "end": 781
                        },
                        {
                            "start": 783,
                            "end": 979
                        },
                        {
                            "start": 981,
                            "end": 1179
                        },
                        {
                            "start": 1181,
                            "end": 1547
                        },
                        {
                            "start": 1549,
                            "end": 1721
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1296,
                            "end": 1301,
                            "matchedPaperCorpusId": "252735056"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9208984375
                }
            ],
            "relevance_judgement": 0.9208984375,
            "relevance_judgment_input_expanded": "# Title: Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies\n# Venue: arXiv.org\n# Authors: Benjue Weng\n## Abstract\nWith the surge of ChatGPT,the use of large models has significantly increased,rapidly rising to prominence across the industry and sweeping across the internet. This article is a comprehensive review of fine-tuning methods for large models. This paper investigates the latest technological advancements and the application of advanced methods in aspects such as task-adaptive fine-tuning,domain-adaptive fine-tuning,few-shot learning,knowledge distillation,multi-task learning,parameter-efficient fine-tuning,and dynamic fine-tuning.\n## XIII. RAG-MEMORY-FINETUNING\nFor every x and preceding generation y < t,the model decodes a retrieval token to evaluate the utility of retrieval.If retrieval is not required,the model predicts the next output segment,as it does in a standard LM.If retrieval is needed,the model generates: a critique token to evaluate the retrieved passage's relevance,the next response segment,and a critique token to evaluate if the information in the response segment is supported by the passage.Finally,a new critique token evaluates the overall utility of the response.\n\n\"REPLUG: Retrieval-Augmented Black-Box Language Models [115]:\" This research explores how to optimize retrieval results in black-box language models,such as those that only expose APIs without revealing embeddings,using a retrieval-augmented approach.\n\n\"Atlas: Few-shot Learning with Retrieval Augmented Language Models [45]:\" The paper discusses methods for joint training of retrievers and language models,especially for knowledge-intensive tasks.\n\n\"RA-DIT: RETRIEVAL-AUGMENTED DUAL IN-STRUCTION TUNING [66]:\" A lightweight fine-tuning method that combines RAG and SFT is proposed to enhance the performance of retrieval-augmented language models.\n\n\"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering [118]:\" This paper assesses the domain adaptability of RAG models in open-domain question answering (ODQA) tasks and proposes RAG-end2end,an extension of RAG that can adapt to specific domain knowledge bases by updating all components during training.\n\n\"RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance [8]:\" This article provides a guide on the optimization strategies of RAG,fine-tuning,and their combina-",
            "reference_string": "[269149041 | Weng | 2024 | Citations: 10]"
        },
        {
            "title": "Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 192,
            "citation_count": 42,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.14924, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268432582",
                    "name": "Siyun Zhao"
                },
                {
                    "authorId": "2125051198",
                    "name": "Yuqing Yang"
                },
                {
                    "authorId": "2294387070",
                    "name": "Zilong Wang"
                },
                {
                    "authorId": "2260609693",
                    "name": "Zhiyuan He"
                },
                {
                    "authorId": "2180993402",
                    "name": "Luna K. Qiu"
                },
                {
                    "authorId": "2259937079",
                    "name": "Lili Qiu"
                }
            ],
            "abstract": "Large language models (LLMs) augmented with external data have demonstrated remarkable capabilities in completing real-world tasks. Techniques for integrating external data into LLMs, such as Retrieval-Augmented Generation (RAG) and fine-tuning, are gaining increasing attention and widespread application. Nonetheless, the effective deployment of data-augmented LLMs across various specialized fields presents substantial challenges. These challenges encompass a wide range of issues, from retrieving relevant data and accurately interpreting user intent to fully harnessing the reasoning capabilities of LLMs for complex tasks. We believe that there is no one-size-fits-all solution for data-augmented LLM applications. In practice, underperformance often arises from a failure to correctly identify the core focus of a task or because the task inherently requires a blend of multiple capabilities that must be disentangled for better resolution. In this survey, we propose a RAG task categorization method, classifying user queries into four levels based on the type of external data required and primary focus of the task: explicit fact queries, implicit fact queries, interpretable rationale queries, and hidden rationale queries. We define these levels of queries, provide relevant datasets, and summarize the key challenges and most effective techniques for addressing these challenges. Finally, we discuss three main forms of integrating external data into LLMs: context, small model, and fine-tuning, highlighting their respective strengths, limitations, and the types of problems they are suited to solve. This work aims to help readers thoroughly understand and decompose the data requirements and key bottlenecks in building LLM applications, offering solutions to the different challenges and serving as a guide to systematically developing such applications.",
            "corpus_id": 272827955,
            "sentences": [
                {
                    "corpus_id": "272827955",
                    "title": "Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely",
                    "text": "Generating responses requires determining if the retrieved information is sufficient or if additional external data is needed. Handling conflicts between retrieved knowledge and the model's internal prior knowledge is also essential [84,85,86]. Supervised fine-tuning is an effective method to enhance the generation performance in RAG systems. When faced with irrelevant or erroneous information as the retrieved context, pre-trained large language models are often easily misled, resulting in incorrect responses. Many studies have shown that by subtly designing training data for RAG systems, fine-tuning or pretraining can effectively mitigate this issue [87,88,89]. Through experimental analysis, RAAT [89], demonstrated that the detrimental effects of irrelevant retrieval noise, relevant retrieval noise, and counterfactual retrieval noise on RAG models increase progressively. By incorporating with these training process, these methods enables the LLM to internally recognize noisy contexts, leading to significant improvements in response generation quality even in the presence of noisy retrievals. Furthermore, to ensure more consistent performance between the retriever and generator within the RAG system, some studies employ joint training of both retriever and generator during the training phase [90,91,92]. \n\n4 Implicit Fact Queries (L2)",
                    "score": 0.5920132829526401,
                    "section_title": "Response Generation Enhancement",
                    "char_start_offset": 25340,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 126
                        },
                        {
                            "start": 127,
                            "end": 244
                        },
                        {
                            "start": 245,
                            "end": 344
                        },
                        {
                            "start": 345,
                            "end": 515
                        },
                        {
                            "start": 516,
                            "end": 670
                        },
                        {
                            "start": 671,
                            "end": 884
                        },
                        {
                            "start": 885,
                            "end": 1109
                        },
                        {
                            "start": 1110,
                            "end": 1324
                        },
                        {
                            "start": 1327,
                            "end": 1355
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1317,
                            "end": 1320,
                            "matchedPaperCorpusId": "252568176"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92041015625
                }
            ],
            "relevance_judgement": 0.92041015625,
            "relevance_judgment_input_expanded": "# Title: Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely\n# Venue: arXiv.org\n# Authors: Siyun Zhao, Yuqing Yang, Zilong Wang, Zhiyuan He, Luna K. Qiu, Lili Qiu\n## Abstract\nLarge language models (LLMs) augmented with external data have demonstrated remarkable capabilities in completing real-world tasks. Techniques for integrating external data into LLMs, such as Retrieval-Augmented Generation (RAG) and fine-tuning, are gaining increasing attention and widespread application. Nonetheless, the effective deployment of data-augmented LLMs across various specialized fields presents substantial challenges. These challenges encompass a wide range of issues, from retrieving relevant data and accurately interpreting user intent to fully harnessing the reasoning capabilities of LLMs for complex tasks. We believe that there is no one-size-fits-all solution for data-augmented LLM applications. In practice, underperformance often arises from a failure to correctly identify the core focus of a task or because the task inherently requires a blend of multiple capabilities that must be disentangled for better resolution. In this survey, we propose a RAG task categorization method, classifying user queries into four levels based on the type of external data required and primary focus of the task: explicit fact queries, implicit fact queries, interpretable rationale queries, and hidden rationale queries. We define these levels of queries, provide relevant datasets, and summarize the key challenges and most effective techniques for addressing these challenges. Finally, we discuss three main forms of integrating external data into LLMs: context, small model, and fine-tuning, highlighting their respective strengths, limitations, and the types of problems they are suited to solve. This work aims to help readers thoroughly understand and decompose the data requirements and key bottlenecks in building LLM applications, offering solutions to the different challenges and serving as a guide to systematically developing such applications.\n## Response Generation Enhancement\nGenerating responses requires determining if the retrieved information is sufficient or if additional external data is needed. Handling conflicts between retrieved knowledge and the model's internal prior knowledge is also essential [84,85,86]. Supervised fine-tuning is an effective method to enhance the generation performance in RAG systems. When faced with irrelevant or erroneous information as the retrieved context, pre-trained large language models are often easily misled, resulting in incorrect responses. Many studies have shown that by subtly designing training data for RAG systems, fine-tuning or pretraining can effectively mitigate this issue [87,88,89]. Through experimental analysis, RAAT [89], demonstrated that the detrimental effects of irrelevant retrieval noise, relevant retrieval noise, and counterfactual retrieval noise on RAG models increase progressively. By incorporating with these training process, these methods enables the LLM to internally recognize noisy contexts, leading to significant improvements in response generation quality even in the presence of noisy retrievals. Furthermore, to ensure more consistent performance between the retriever and generator within the RAG system, some studies employ joint training of both retriever and generator during the training phase [90,91,92]. \n\n4 Implicit Fact Queries (L2)",
            "reference_string": "[272827955 | Zhao et al. | 2024 | Citations: 42]"
        },
        {
            "title": "Toward Optimal Search and Retrieval for RAG",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.07396, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284988281",
                    "name": "Alexandria Leto"
                },
                {
                    "authorId": "2276606286",
                    "name": "Cecilia Aguerrebere"
                },
                {
                    "authorId": "3286327",
                    "name": "I. Bhati"
                },
                {
                    "authorId": "2276606593",
                    "name": "Ted Willke"
                },
                {
                    "authorId": "2276606934",
                    "name": "Mariano Tepper"
                },
                {
                    "authorId": "3369353",
                    "name": "Vy A. Vo"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) is a promising method for addressing some of the memory-related challenges associated with Large Language Models (LLMs). Two separate systems form the RAG pipeline, the retriever and the reader, and the impact of each on downstream task performance is not well-understood. Here, we work towards the goal of understanding how retrievers can be optimized for RAG pipelines for common tasks such as Question Answering (QA). We conduct experiments focused on the relationship between retrieval and RAG performance on QA and attributed QA and unveil a number of insights useful to practitioners developing high-performance RAG pipelines. For example, lowering search accuracy has minor implications for RAG performance while potentially increasing retrieval speed and memory efficiency.",
            "corpus_id": 273969566,
            "sentences": [
                {
                    "corpus_id": "273969566",
                    "title": "Toward Optimal Search and Retrieval for RAG",
                    "text": "Retrieval-augmented generation (RAG) is a promising method for addressing some of the memory-related challenges associated with Large Language Models (LLMs). Two separate systems form the RAG pipeline, the retriever and the reader, and the impact of each on downstream task performance is not well-understood. Here, we work towards the goal of understanding how retrievers can be optimized for RAG pipelines for common tasks such as Question Answering (QA). We conduct experiments focused on the relationship between retrieval and RAG performance on QA and attributed QA and unveil a number of insights useful to practitioners developing high-performance RAG pipelines. For example, lowering search accuracy has minor implications for RAG performance while potentially increasing retrieval speed and memory efficiency.",
                    "score": 0.577361974119076,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9150390625
                }
            ],
            "relevance_judgement": 0.9150390625,
            "relevance_judgment_input_expanded": "# Title: Toward Optimal Search and Retrieval for RAG\n# Venue: arXiv.org\n# Authors: Alexandria Leto, Cecilia Aguerrebere, I. Bhati, Ted Willke, Mariano Tepper, Vy A. Vo\n## Abstract\nRetrieval-augmented generation (RAG) is a promising method for addressing some of the memory-related challenges associated with Large Language Models (LLMs). Two separate systems form the RAG pipeline, the retriever and the reader, and the impact of each on downstream task performance is not well-understood. Here, we work towards the goal of understanding how retrievers can be optimized for RAG pipelines for common tasks such as Question Answering (QA). We conduct experiments focused on the relationship between retrieval and RAG performance on QA and attributed QA and unveil a number of insights useful to practitioners developing high-performance RAG pipelines. For example, lowering search accuracy has minor implications for RAG performance while potentially increasing retrieval speed and memory efficiency.\n",
            "reference_string": "[273969566 | Leto et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Reward-RAG: Enhancing RAG with Reward Driven Supervision",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 106,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03780, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324796381",
                    "name": "Thang Nguyen"
                },
                {
                    "authorId": "2324790937",
                    "name": "Peter Chin"
                },
                {
                    "authorId": "2324792268",
                    "name": "Yu-Wing Tai"
                }
            ],
            "abstract": "In this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences. The versatility of our approach allows it to be effectively applied across various domains through domain-specific fine-tuning. We evaluate Reward-RAG on publicly available benchmarks from multiple domains, comparing it to state-of-the-art methods. Our experimental results demonstrate significant improvements in performance, highlighting the effectiveness of Reward-RAG in improving the relevance and quality of generated responses. These findings underscore the potential of integrating reward models with RAG to achieve superior outcomes in natural language generation tasks.",
            "corpus_id": 273186680,
            "sentences": [
                {
                    "corpus_id": "273186680",
                    "title": "Reward-RAG: Enhancing RAG with Reward Driven Supervision",
                    "text": "In this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences. The versatility of our approach allows it to be effectively applied across various domains through domain-specific fine-tuning. We evaluate Reward-RAG on publicly available benchmarks from multiple domains, comparing it to state-of-the-art methods. Our experimental results demonstrate significant improvements in performance, highlighting the effectiveness of Reward-RAG in improving the relevance and quality of generated responses. These findings underscore the potential of integrating reward models with RAG to achieve superior outcomes in natural language generation tasks.",
                    "score": 0.5597336665183067,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91455078125
                }
            ],
            "relevance_judgement": 0.91455078125,
            "relevance_judgment_input_expanded": "# Title: Reward-RAG: Enhancing RAG with Reward Driven Supervision\n# Venue: arXiv.org\n# Authors: Thang Nguyen, Peter Chin, Yu-Wing Tai\n## Abstract\nIn this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences. The versatility of our approach allows it to be effectively applied across various domains through domain-specific fine-tuning. We evaluate Reward-RAG on publicly available benchmarks from multiple domains, comparing it to state-of-the-art methods. Our experimental results demonstrate significant improvements in performance, highlighting the effectiveness of Reward-RAG in improving the relevance and quality of generated responses. These findings underscore the potential of integrating reward models with RAG to achieve superior outcomes in natural language generation tasks.\n",
            "reference_string": "[273186680 | Nguyen et al. | 2024 | Citations: 5]"
        },
        {
            "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 98,
            "citation_count": 74,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.02485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2259265562",
                    "name": "Yue Yu"
                },
                {
                    "authorId": "2253664013",
                    "name": "Wei Ping"
                },
                {
                    "authorId": "2256582287",
                    "name": "Zihan Liu"
                },
                {
                    "authorId": "2256656241",
                    "name": "Boxin Wang"
                },
                {
                    "authorId": "2287859963",
                    "name": "Jiaxuan You"
                },
                {
                    "authorId": "2256776233",
                    "name": "Chao Zhang"
                },
                {
                    "authorId": "1911755",
                    "name": "M. Shoeybi"
                },
                {
                    "authorId": "2264406909",
                    "name": "Bryan Catanzaro"
                }
            ],
            "abstract": "Large language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG). In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data. For generation, we compare our model with many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks. Specifically, our Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In addition, it also performs comparably to GPT-4 on five RAG benchmarks in the biomedical domain without instruction fine-tuning on biomedical data, demonstrating its superb capability for generalization to new domains.",
            "corpus_id": 270878612,
            "sentences": [
                {
                    "corpus_id": "270878612",
                    "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs",
                    "text": "Large language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG). In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data. For generation, we compare our model with many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks. Specifically, our Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In addition, it also performs comparably to GPT-4 on five RAG benchmarks in the biomedical domain without instruction fine-tuning on biomedical data, demonstrating its superb capability for generalization to new domains.",
                    "score": 0.641627344701692,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9140625
                }
            ],
            "relevance_judgement": 0.9140625,
            "relevance_judgment_input_expanded": "# Title: RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs\n# Venue: Neural Information Processing Systems\n# Authors: Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, M. Shoeybi, Bryan Catanzaro\n## Abstract\nLarge language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG). In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data. For generation, we compare our model with many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks. Specifically, our Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In addition, it also performs comparably to GPT-4 on five RAG benchmarks in the biomedical domain without instruction fine-tuning on biomedical data, demonstrating its superb capability for generalization to new domains.\n",
            "reference_string": "[270878612 | Yu et al. | 2024 | Citations: 74]"
        },
        {
            "title": "Contrastive Learning to Improve Retrieval for Real-World Fact Checking",
            "venue": "FEVER",
            "year": 2024,
            "reference_count": 30,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.04657, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2165382262",
                    "name": "Aniruddh Sriram"
                },
                {
                    "authorId": "2159829626",
                    "name": "Fangyuan Xu"
                },
                {
                    "authorId": "2257003422",
                    "name": "Eunsol Choi"
                },
                {
                    "authorId": "1814094",
                    "name": "Greg Durrett"
                }
            ],
            "abstract": "Recent work on fact-checking addresses a realistic setting where models incorporate evidence retrieved from the web to decide the veracity of claims. A bottleneck in this pipeline is in retrieving relevant evidence: traditional methods may surface documents directly related to a claim, but fact-checking complex claims requires more inferences. For instance, a document about how a vaccine was developed is relevant to addressing claims about what it might contain, even if it does not address them directly. We present Contrastive Fact-Checking Reranker (CFR), an improved retriever for this setting. By leveraging the AVeriTeC dataset, which annotates subquestions for claims with human written answers from evidence documents, we fine-tune Contriever with a contrastive objective based on multiple training signals, including distillation from GPT-4, evaluating subquestion answers, and gold labels in the dataset. We evaluate our model on both retrieval and end-to-end veracity judgments about claims. On the AVeriTeC dataset, we find a 6% improvement in veracity classification accuracy. We also show our gains can be transferred to FEVER, ClaimDecomp, HotpotQA, and a synthetic dataset requiring retrievers to make inferences.",
            "corpus_id": 273185619,
            "sentences": [
                {
                    "corpus_id": "273185619",
                    "title": "Contrastive Learning to Improve Retrieval for Real-World Fact Checking",
                    "text": "Retrieval-augmented generation (RAG) relies on two key modules: a retriever and a reader/generation model. For many RAG systems, noisy retrieval hurts downstream performance by providing irrelevant or misleading documents (Yoran et al., 2024). Sauchuk et al. (2022) found that adding distractors can cause a 27% drop on veracity classification accuracy on FEVER. Therefore, it's important for retrievers to find relevant documents and simultaneously avoid damaging ones. Shi et al. (2023) attempts to solve this problem by finetuning the retrieval component while fixing the reader LM, similar to our work. Other approaches like Ke et al. (2024) create a more complex system with a \"bridging\" model between the retriever and reader. Nevertheless, noisy retrieval remains a failure point in RAG systems (Barnett et al., 2024), and tangible downstream gains can be realized by further finetuning.",
                    "score": 0.6264539890680402,
                    "section_title": "Retrieval Augmented Generation Systems",
                    "char_start_offset": 5329,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 106
                        },
                        {
                            "start": 107,
                            "end": 243
                        },
                        {
                            "start": 244,
                            "end": 362
                        },
                        {
                            "start": 363,
                            "end": 470
                        },
                        {
                            "start": 471,
                            "end": 606
                        },
                        {
                            "start": 607,
                            "end": 732
                        },
                        {
                            "start": 733,
                            "end": 894
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 244,
                            "end": 265,
                            "matchedPaperCorpusId": "250340232"
                        },
                        {
                            "start": 802,
                            "end": 824,
                            "matchedPaperCorpusId": "266933076"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91357421875
                }
            ],
            "relevance_judgement": 0.91357421875,
            "relevance_judgment_input_expanded": "# Title: Contrastive Learning to Improve Retrieval for Real-World Fact Checking\n# Venue: FEVER\n# Authors: Aniruddh Sriram, Fangyuan Xu, Eunsol Choi, Greg Durrett\n## Abstract\nRecent work on fact-checking addresses a realistic setting where models incorporate evidence retrieved from the web to decide the veracity of claims. A bottleneck in this pipeline is in retrieving relevant evidence: traditional methods may surface documents directly related to a claim, but fact-checking complex claims requires more inferences. For instance, a document about how a vaccine was developed is relevant to addressing claims about what it might contain, even if it does not address them directly. We present Contrastive Fact-Checking Reranker (CFR), an improved retriever for this setting. By leveraging the AVeriTeC dataset, which annotates subquestions for claims with human written answers from evidence documents, we fine-tune Contriever with a contrastive objective based on multiple training signals, including distillation from GPT-4, evaluating subquestion answers, and gold labels in the dataset. We evaluate our model on both retrieval and end-to-end veracity judgments about claims. On the AVeriTeC dataset, we find a 6% improvement in veracity classification accuracy. We also show our gains can be transferred to FEVER, ClaimDecomp, HotpotQA, and a synthetic dataset requiring retrievers to make inferences.\n## Retrieval Augmented Generation Systems\nRetrieval-augmented generation (RAG) relies on two key modules: a retriever and a reader/generation model. For many RAG systems, noisy retrieval hurts downstream performance by providing irrelevant or misleading documents (Yoran et al., 2024). Sauchuk et al. (2022) found that adding distractors can cause a 27% drop on veracity classification accuracy on FEVER. Therefore, it's important for retrievers to find relevant documents and simultaneously avoid damaging ones. Shi et al. (2023) attempts to solve this problem by finetuning the retrieval component while fixing the reader LM, similar to our work. Other approaches like Ke et al. (2024) create a more complex system with a \"bridging\" model between the retriever and reader. Nevertheless, noisy retrieval remains a failure point in RAG systems (Barnett et al., 2024), and tangible downstream gains can be realized by further finetuning.",
            "reference_string": "[273185619 | Sriram et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services",
            "venue": "2024 IEEE International Conference on Data Mining Workshops (ICDMW)",
            "year": 2024,
            "reference_count": 24,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.16088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3333168",
                    "name": "Ali Anaissi"
                },
                {
                    "authorId": "3069261",
                    "name": "Ali Braytee"
                },
                {
                    "authorId": "1992906806",
                    "name": "Junaid Akram"
                }
            ],
            "abstract": "We present an advanced approach to medical question-answering (QA) services, using fine-tuned Large Language Models (LLMs) to improve the accuracy and reliability of healthcare information. Our study focuses on optimizing models like LLaMA-2 and Mistral, which have shown great promise in delivering precise, reliable medical answers. By leveraging comprehensive datasets, we applied fine-tuning techniques such as rsDoRA+ and ReRAG. rsDoRA+ enhances model performance through a combination of decomposed model weights, varied learning rates for low-rank matrices, and rank stabilization, leading to improved efficiency. ReRAG, which integrates retrieval on demand and question rewriting, further refines the accuracy of the responses. This approach enables healthcare providers to access fast, dependable information, aiding in more efficient decision-making and fostering greater patient trust. Our work highlights the potential of fine-tuned LLMs to significantly improve the quality and accessibility of medical information services, ultimately contributing to better healthcare outcomes for all.",
            "corpus_id": 273501949,
            "sentences": [
                {
                    "corpus_id": "273501949",
                    "title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services",
                    "text": "Retrieval-Augmented Generation (RAG) leverages both parametric and non-parametric memory, significantly enhancing the performance of Large Language Models (LLMs) in translation and question-answering tasks, as highlighted by Lewis et al. [16]. These models, equipped with extensive training datasets and substantial parameters, have led advancements in these fields. However, they face persistent challenges such as the potential for outdated or incorrect information and difficulties with real-time updates [8], [10], [12], [17]. The RAG approach improves LLM performance through pretraining, combining various memory types to generate factbased, varied, and accurate language representations. This method employs a dynamic updating mechanism to refresh the knowledge base without retraining the entire model, thereby enhancing reliability and clarity [18]. Nonetheless, RAG faces issues like noise or conflicting information during the retrieval phase, necessitating improvements for response accuracy and reliability [19]. Lin et al. [20] suggest integrating RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches. \n\nSELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation. It also incorporates selfreflection tokens that allow the model to assess the quality and integrity of its responses, thereby increasing the connectedness and correctness of its outputs [21]. Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]. Among PEFT methods, Low-Rank Adaptation (LoRA) and its advanced versions, such as LoRA+ and DoRA, have shown significant improvements. LoRA introduces trainable low-rank matrices within the Transformer architecture, enhancing the efficiency of parameter updates without altering the model architecture significantly [17], [23].",
                    "score": 0.6147044427271824,
                    "section_title": "II. RELATED WORK",
                    "char_start_offset": 3739,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 243
                        },
                        {
                            "start": 244,
                            "end": 366
                        },
                        {
                            "start": 367,
                            "end": 530
                        },
                        {
                            "start": 531,
                            "end": 694
                        },
                        {
                            "start": 695,
                            "end": 858
                        },
                        {
                            "start": 859,
                            "end": 1025
                        },
                        {
                            "start": 1026,
                            "end": 1163
                        },
                        {
                            "start": 1166,
                            "end": 1336
                        },
                        {
                            "start": 1337,
                            "end": 1498
                        },
                        {
                            "start": 1499,
                            "end": 1690
                        },
                        {
                            "start": 1691,
                            "end": 1826
                        },
                        {
                            "start": 1827,
                            "end": 2030
                        },
                        {
                            "start": 2031,
                            "end": 2165
                        },
                        {
                            "start": 2166,
                            "end": 2358
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91357421875
                }
            ],
            "relevance_judgement": 0.91357421875,
            "relevance_judgment_input_expanded": "# Title: Fine-Tuning LLMs for Reliable Medical Question-Answering Services\n# Venue: 2024 IEEE International Conference on Data Mining Workshops (ICDMW)\n# Authors: Ali Anaissi, Ali Braytee, Junaid Akram\n## Abstract\nWe present an advanced approach to medical question-answering (QA) services, using fine-tuned Large Language Models (LLMs) to improve the accuracy and reliability of healthcare information. Our study focuses on optimizing models like LLaMA-2 and Mistral, which have shown great promise in delivering precise, reliable medical answers. By leveraging comprehensive datasets, we applied fine-tuning techniques such as rsDoRA+ and ReRAG. rsDoRA+ enhances model performance through a combination of decomposed model weights, varied learning rates for low-rank matrices, and rank stabilization, leading to improved efficiency. ReRAG, which integrates retrieval on demand and question rewriting, further refines the accuracy of the responses. This approach enables healthcare providers to access fast, dependable information, aiding in more efficient decision-making and fostering greater patient trust. Our work highlights the potential of fine-tuned LLMs to significantly improve the quality and accessibility of medical information services, ultimately contributing to better healthcare outcomes for all.\n## II. RELATED WORK\nRetrieval-Augmented Generation (RAG) leverages both parametric and non-parametric memory, significantly enhancing the performance of Large Language Models (LLMs) in translation and question-answering tasks, as highlighted by Lewis et al. [16]. These models, equipped with extensive training datasets and substantial parameters, have led advancements in these fields. However, they face persistent challenges such as the potential for outdated or incorrect information and difficulties with real-time updates [8], [10], [12], [17]. The RAG approach improves LLM performance through pretraining, combining various memory types to generate factbased, varied, and accurate language representations. This method employs a dynamic updating mechanism to refresh the knowledge base without retraining the entire model, thereby enhancing reliability and clarity [18]. Nonetheless, RAG faces issues like noise or conflicting information during the retrieval phase, necessitating improvements for response accuracy and reliability [19]. Lin et al. [20] suggest integrating RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches. \n\nSELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation. It also incorporates selfreflection tokens that allow the model to assess the quality and integrity of its responses, thereby increasing the connectedness and correctness of its outputs [21]. Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]. Among PEFT methods, Low-Rank Adaptation (LoRA) and its advanced versions, such as LoRA+ and DoRA, have shown significant improvements. LoRA introduces trainable low-rank matrices within the Transformer architecture, enhancing the efficiency of parameter updates without altering the model architecture significantly [17], [23].",
            "reference_string": "[273501949 | Anaissi et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 32,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.04652, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2296597690",
                    "name": "Patrice B'echard"
                },
                {
                    "authorId": "2296597772",
                    "name": "Orlando Marquez Ayala"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying Large Language Models (LLMs), as it can address typical limitations such as generating hallucinated or outdated information. However, when building real-world RAG applications, practical issues arise. First, the retrieved information is generally domain-specific. Since it is computationally expensive to fine-tune LLMs, it is more feasible to fine-tune the retriever to improve the quality of the data included in the LLM input. Second, as more applications are deployed in the same real-world system, one cannot afford to deploy separate retrievers. Moreover, these RAG applications normally retrieve different kinds of data. Our solution is to instruction fine-tune a small retriever encoder on a variety of domain-specific tasks to allow us to deploy one encoder that can serve many use cases, thereby achieving low-cost, scalability, and speed. We show how this encoder generalizes to out-of-domain settings as well as to an unseen retrieval task on real-world enterprise use cases.",
            "corpus_id": 275357908,
            "sentences": [
                {
                    "corpus_id": "275357908",
                    "title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG",
                    "text": "Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying Large Language Models (LLMs), as it can address typical limitations such as generating hallucinated or outdated information. However, when building real-world RAG applications, practical issues arise. First, the retrieved information is generally domain-specific. Since it is computationally expensive to fine-tune LLMs, it is more feasible to fine-tune the retriever to improve the quality of the data included in the LLM input. Second, as more applications are deployed in the same real-world system, one cannot afford to deploy separate retrievers. Moreover, these RAG applications normally retrieve different kinds of data. Our solution is to instruction fine-tune a small retriever encoder on a variety of domain-specific tasks to allow us to deploy one encoder that can serve many use cases, thereby achieving low-cost, scalability, and speed. We show how this encoder generalizes to out-of-domain settings as well as to an unseen retrieval task on real-world enterprise use cases.",
                    "score": 0.6547075719841893,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9130859375
                }
            ],
            "relevance_judgement": 0.9130859375,
            "relevance_judgment_input_expanded": "# Title: Multi-task retriever fine-tuning for domain-specific and efficient RAG\n# Venue: arXiv.org\n# Authors: Patrice B'echard, Orlando Marquez Ayala\n## Abstract\nRetrieval-Augmented Generation (RAG) has become ubiquitous when deploying Large Language Models (LLMs), as it can address typical limitations such as generating hallucinated or outdated information. However, when building real-world RAG applications, practical issues arise. First, the retrieved information is generally domain-specific. Since it is computationally expensive to fine-tune LLMs, it is more feasible to fine-tune the retriever to improve the quality of the data included in the LLM input. Second, as more applications are deployed in the same real-world system, one cannot afford to deploy separate retrievers. Moreover, these RAG applications normally retrieve different kinds of data. Our solution is to instruction fine-tune a small retriever encoder on a variety of domain-specific tasks to allow us to deploy one encoder that can serve many use cases, thereby achieving low-cost, scalability, and speed. We show how this encoder generalizes to out-of-domain settings as well as to an unseen retrieval task on real-world enterprise use cases.\n",
            "reference_string": "[275357908 | B'echard et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 111,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.13019, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2345960924",
                    "name": "Sha Li"
                },
                {
                    "authorId": "2322968831",
                    "name": "Naren Ramakrishnan"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) aims to augment the capabilities of Large Language Models (LLMs) by retrieving and incorporate external documents or chunks prior to generation. However, even improved retriever relevance can brings erroneous or contextually distracting information, undermining the effectiveness of RAG in downstream tasks. We introduce a compact, efficient, and pluggable module designed to refine retrieved chunks before using them for generation. The module aims to extract and reorganize the most relevant and supportive information into a concise, query-specific format. Through a three-stage training paradigm - comprising supervised fine - tuning, contrastive multi-task learning, and reinforcement learning-based alignment - it prioritizes critical knowledge and aligns it with the generator's preferences. This approach enables LLMs to produce outputs that are more accurate, reliable, and contextually appropriate.",
            "corpus_id": 276422305,
            "sentences": [
                {
                    "corpus_id": "276422305",
                    "title": "Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation",
                    "text": "Retrieval-Augmented Generation (RAG) aims to augment the capabilities of Large Language Models (LLMs) by retrieving and incorporate external documents or chunks prior to generation. However, even improved retriever relevance can brings erroneous or contextually distracting information, undermining the effectiveness of RAG in downstream tasks. We introduce a compact, efficient, and pluggable module designed to refine retrieved chunks before using them for generation. The module aims to extract and reorganize the most relevant and supportive information into a concise, query-specific format. Through a three-stage training paradigm - comprising supervised fine - tuning, contrastive multi-task learning, and reinforcement learning-based alignment - it prioritizes critical knowledge and aligns it with the generator's preferences. This approach enables LLMs to produce outputs that are more accurate, reliable, and contextually appropriate.",
                    "score": 0.5799103969628757,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9111328125
                }
            ],
            "relevance_judgement": 0.9111328125,
            "relevance_judgment_input_expanded": "# Title: Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation\n# Venue: arXiv.org\n# Authors: Sha Li, Naren Ramakrishnan\n## Abstract\nRetrieval-Augmented Generation (RAG) aims to augment the capabilities of Large Language Models (LLMs) by retrieving and incorporate external documents or chunks prior to generation. However, even improved retriever relevance can brings erroneous or contextually distracting information, undermining the effectiveness of RAG in downstream tasks. We introduce a compact, efficient, and pluggable module designed to refine retrieved chunks before using them for generation. The module aims to extract and reorganize the most relevant and supportive information into a concise, query-specific format. Through a three-stage training paradigm - comprising supervised fine - tuning, contrastive multi-task learning, and reinforcement learning-based alignment - it prioritizes critical knowledge and aligns it with the generator's preferences. This approach enables LLMs to produce outputs that are more accurate, reliable, and contextually appropriate.\n",
            "reference_string": "[276422305 | Li et al. | 2025 | Citations: 2]"
        },
        {
            "title": "AttentionRAG: Attention-Guided Context Pruning in Retrieval-Augmented Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 29,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.10720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325115068",
                    "name": "Yixiong Fang"
                },
                {
                    "authorId": "2350599634",
                    "name": "Tianran Sun"
                },
                {
                    "authorId": "2279412470",
                    "name": "Yuling Shi"
                },
                {
                    "authorId": "2269756996",
                    "name": "Xiaodong Gu"
                }
            ],
            "abstract": "While RAG demonstrates remarkable capabilities in LLM applications, its effectiveness is hindered by the ever-increasing length of retrieved contexts, which introduces information redundancy and substantial computational overhead. Existing context pruning methods, such as LLMLingua, lack contextual awareness and offer limited flexibility in controlling compression rates, often resulting in either insufficient pruning or excessive information loss. In this paper, we propose AttentionRAG, an attention-guided context pruning method for RAG systems. The core idea of AttentionRAG lies in its attention focus mechanism, which reformulates RAG queries into a next-token prediction paradigm. This mechanism isolates the query's semantic focus to a single token, enabling precise and efficient attention calculation between queries and retrieved contexts. Extensive experiments on LongBench and Babilong benchmarks show that AttentionRAG achieves up to 6.3$\\times$ context compression while outperforming LLMLingua methods by around 10\\% in key metrics.",
            "corpus_id": 277043707,
            "sentences": [
                {
                    "corpus_id": "277043707",
                    "title": "AttentionRAG: Attention-Guided Context Pruning in Retrieval-Augmented Generation",
                    "text": "Retrieve-Augmented Generation (RAG) is a framework that enhances the capabilities of LLMs by integrating external knowledge through retrieval. \n\nA RAG system typically consists of two components: a retriever, which fetches relevant documents, called contexts, from a large corpus based on a query, and a generator, which generates an answer using both the retrieved context and the model's internal knowledge. This combination enables more accurate and contextually relevant outputs, especially for tasks requiring detailed or up-to-date information that might not be present in the model's training data.",
                    "score": 0.5585139821399403,
                    "section_title": "Retrieve-Augmented Generation",
                    "char_start_offset": 5881,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 142
                        },
                        {
                            "start": 145,
                            "end": 409
                        },
                        {
                            "start": 410,
                            "end": 605
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91064453125
                }
            ],
            "relevance_judgement": 0.91064453125,
            "relevance_judgment_input_expanded": "# Title: AttentionRAG: Attention-Guided Context Pruning in Retrieval-Augmented Generation\n# Venue: arXiv.org\n# Authors: Yixiong Fang, Tianran Sun, Yuling Shi, Xiaodong Gu\n## Abstract\nWhile RAG demonstrates remarkable capabilities in LLM applications, its effectiveness is hindered by the ever-increasing length of retrieved contexts, which introduces information redundancy and substantial computational overhead. Existing context pruning methods, such as LLMLingua, lack contextual awareness and offer limited flexibility in controlling compression rates, often resulting in either insufficient pruning or excessive information loss. In this paper, we propose AttentionRAG, an attention-guided context pruning method for RAG systems. The core idea of AttentionRAG lies in its attention focus mechanism, which reformulates RAG queries into a next-token prediction paradigm. This mechanism isolates the query's semantic focus to a single token, enabling precise and efficient attention calculation between queries and retrieved contexts. Extensive experiments on LongBench and Babilong benchmarks show that AttentionRAG achieves up to 6.3$\\times$ context compression while outperforming LLMLingua methods by around 10\\% in key metrics.\n## Retrieve-Augmented Generation\nRetrieve-Augmented Generation (RAG) is a framework that enhances the capabilities of LLMs by integrating external knowledge through retrieval. \n\nA RAG system typically consists of two components: a retriever, which fetches relevant documents, called contexts, from a large corpus based on a query, and a generator, which generates an answer using both the retrieved context and the model's internal knowledge. This combination enables more accurate and contextually relevant outputs, especially for tasks requiring detailed or up-to-date information that might not be present in the model's training data.",
            "reference_string": "[277043707 | Fang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
            "venue": "",
            "year": 2025,
            "reference_count": 31,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.10792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2362089035",
                    "name": "Zhan Peng Lee"
                },
                {
                    "authorId": "2362188632",
                    "name": "Andre Lin"
                },
                {
                    "authorId": "2363425126",
                    "name": "Calvin Tan"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to improve factuality in large language models (LLMs) by grounding their outputs in retrieved documents. However, ensuring perfect retrieval of relevant information remains challenging, and when irrelevant content is passed downstream to an LLM, it can lead to hallucinations. In this work, we propose Finetune-RAG, a simple and effective fine-tuning approach that features the first-of-its-kind RAG training dataset constructed to mimic real-world imperfections. Experimental results show that Finetune-RAG improves factual accuracy by 21.2% over the base model. We also propose Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests models under realistic imperfect retrieval scenarios. Our codebase and dataset are fully open sourced for community use.",
            "corpus_id": 278714952,
            "sentences": [
                {
                    "corpus_id": "278714952",
                    "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
                    "text": "There are several promising extensions to Finetune-RAG that could further improve its robustness and applicability: \n\n\u2022 Training with more in-context RAG: Real-world retrieval often returns more than two documents, and the context window of LLMs are increasing rapidly. At the time of our work, we focused on relatively low context window of 8k, which would realistically be used for two to three RAG documents using up to 3k context window. With increasing context window, future work can explore training with more RAG chunks to optimize LLMs RAG performance even at high level of stresses caused by more retrieved chunks. To support this, we future-proofed our dataset by including two additional relevant chunks per example to support generating more complex multi-document training scenarios. \u2022 Joint retrieval-generation optimization: While Finetune-RAG focuses on improving the generation component, combining it with learned retrieval mechanisms such as rerankeraware retrievers or contrastively trained retrievers could lead to further improvements in factual accuracy and context filtering. \u2022 Multimodal extensions: Hallucination is not limited to text-based models. Extending Finetune-RAG to multimodal settings, such as image-caption retrieval or code+documentation generation, may help build more robust grounded systems in other domains. \n\n\u2022 Evaluation on downstream tasks: While our benchmarking focuses on controlled hallucination settings, future work should assess Finetune-RAG's impact on end-to-end performance in downstream RAG applications such as open-domain question answering, legal document summarization, and domain-specific information retrieval.",
                    "score": 0.5596556641512547,
                    "section_title": "Future Work",
                    "char_start_offset": 21636,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 115
                        },
                        {
                            "start": 118,
                            "end": 269
                        },
                        {
                            "start": 270,
                            "end": 441
                        },
                        {
                            "start": 442,
                            "end": 624
                        },
                        {
                            "start": 625,
                            "end": 797
                        },
                        {
                            "start": 798,
                            "end": 1100
                        },
                        {
                            "start": 1101,
                            "end": 1176
                        },
                        {
                            "start": 1177,
                            "end": 1351
                        },
                        {
                            "start": 1354,
                            "end": 1674
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90966796875
                }
            ],
            "relevance_judgement": 0.90966796875,
            "relevance_judgment_input_expanded": "# Title: Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation\n# Venue: \n# Authors: Zhan Peng Lee, Andre Lin, Calvin Tan\n## Abstract\nRetrieval-Augmented Generation (RAG) has emerged as a powerful framework to improve factuality in large language models (LLMs) by grounding their outputs in retrieved documents. However, ensuring perfect retrieval of relevant information remains challenging, and when irrelevant content is passed downstream to an LLM, it can lead to hallucinations. In this work, we propose Finetune-RAG, a simple and effective fine-tuning approach that features the first-of-its-kind RAG training dataset constructed to mimic real-world imperfections. Experimental results show that Finetune-RAG improves factual accuracy by 21.2% over the base model. We also propose Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests models under realistic imperfect retrieval scenarios. Our codebase and dataset are fully open sourced for community use.\n## Future Work\nThere are several promising extensions to Finetune-RAG that could further improve its robustness and applicability: \n\n\u2022 Training with more in-context RAG: Real-world retrieval often returns more than two documents, and the context window of LLMs are increasing rapidly. At the time of our work, we focused on relatively low context window of 8k, which would realistically be used for two to three RAG documents using up to 3k context window. With increasing context window, future work can explore training with more RAG chunks to optimize LLMs RAG performance even at high level of stresses caused by more retrieved chunks. To support this, we future-proofed our dataset by including two additional relevant chunks per example to support generating more complex multi-document training scenarios. \u2022 Joint retrieval-generation optimization: While Finetune-RAG focuses on improving the generation component, combining it with learned retrieval mechanisms such as rerankeraware retrievers or contrastively trained retrievers could lead to further improvements in factual accuracy and context filtering. \u2022 Multimodal extensions: Hallucination is not limited to text-based models. Extending Finetune-RAG to multimodal settings, such as image-caption retrieval or code+documentation generation, may help build more robust grounded systems in other domains. \n\n\u2022 Evaluation on downstream tasks: While our benchmarking focuses on controlled hallucination settings, future work should assess Finetune-RAG's impact on end-to-end performance in downstream RAG applications such as open-domain question answering, legal document summarization, and domain-specific information retrieval.",
            "reference_string": "[278714952 | Lee et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 401,
            "citation_count": 282,
            "influential_citation_count": 11,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.19473, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268718776",
                    "name": "Penghao Zhao"
                },
                {
                    "authorId": "2288557803",
                    "name": "Hailin Zhang"
                },
                {
                    "authorId": "2289597580",
                    "name": "Qinhan Yu"
                },
                {
                    "authorId": "2288675277",
                    "name": "Zhengren Wang"
                },
                {
                    "authorId": "2288532368",
                    "name": "Yunteng Geng"
                },
                {
                    "authorId": "46182701",
                    "name": "Fangcheng Fu"
                },
                {
                    "authorId": "2249513224",
                    "name": "Ling Yang"
                },
                {
                    "authorId": "2277807793",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "2277742543",
                    "name": "Bin Cui"
                }
            ],
            "abstract": "Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.",
            "corpus_id": 268091298,
            "sentences": [
                {
                    "corpus_id": "268091298",
                    "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
                    "text": "2) Retriever Enhancement: In RAG systems, the quality of retrieved content determines the information fed into the generators. Lower content quality increases the risk of model hallucinations or other degradation. In this section, we introduce efficient ways to enhance retrieval effectiveness. Recursive Retrieval: Recursive retrieval is to perform multiple searches to retrieve richer and higher-quality contents. \n\nReACT [132] uses Chain-of-Thought (CoT) [133] to break queries down for recursive retrieval and provide richer information. RATP [134] uses the Monte-Carlo Tree Search for simulations to select optimal retrieval content, which is then templated and forwarded to the generator for output. Chunk Optimization: Chunk optimization refers to adjusting chunk size for improved retrieval results. \n\nLlamaIndex [135] incorporates a series of chunk optimization methods, one of which operates on a 'small to big' principle. The core concept here is to pinpoint finer-grained content but return richer information. For instance, Sentence-window retrieval fetches small text chunks and returns a window of relevant sentences surrounding the retrieved segment. In automerge retrieval, documents are arranged in a tree structure. The process retrieves the parent node, which encapsulates the content of its child nodes, by fetching the child node first. To address the lack of contextual information, RAPTOR [136] employs recursive embedding, clustering, and summarization of text chunks until further clustering becomes infeasible, thereby constructing a multi-level tree structure. Prompt-RAG [137] enhances retrieval accuracy by pre-generating a table of contents, enabling the model to autonomously select relevant chapters based on the query. Raina et al. [138] break text chunks into finer atomic statements to achieve higher recall and improved results. Retriever Finetuning: The retriever, central to the RAG system, relies on a proficient embedding model [139]- [142] to represent related content and feed the generator, enhancing system performance. \n\nAdditionally, embedding models with strong expressive power can be fine-tuned with domain-specific or task-related data to boost performance in targeted areas. REPLUG [86] treats LM as a black box and update the retriever model based on the final results.",
                    "score": 0.5776983522137551,
                    "section_title": "B. RAG Enhancements",
                    "char_start_offset": 27257,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 126
                        },
                        {
                            "start": 127,
                            "end": 213
                        },
                        {
                            "start": 214,
                            "end": 294
                        },
                        {
                            "start": 295,
                            "end": 415
                        },
                        {
                            "start": 418,
                            "end": 541
                        },
                        {
                            "start": 542,
                            "end": 705
                        },
                        {
                            "start": 706,
                            "end": 807
                        },
                        {
                            "start": 810,
                            "end": 932
                        },
                        {
                            "start": 933,
                            "end": 1022
                        },
                        {
                            "start": 1023,
                            "end": 1166
                        },
                        {
                            "start": 1167,
                            "end": 1234
                        },
                        {
                            "start": 1235,
                            "end": 1358
                        },
                        {
                            "start": 1359,
                            "end": 1588
                        },
                        {
                            "start": 1589,
                            "end": 1752
                        },
                        {
                            "start": 1753,
                            "end": 1865
                        },
                        {
                            "start": 1866,
                            "end": 2064
                        },
                        {
                            "start": 2067,
                            "end": 2226
                        },
                        {
                            "start": 2227,
                            "end": 2322
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9091796875
                }
            ],
            "relevance_judgement": 0.9091796875,
            "relevance_judgment_input_expanded": "# Title: Retrieval-Augmented Generation for AI-Generated Content: A Survey\n# Venue: arXiv.org\n# Authors: Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Bin Cui\n## Abstract\nAdvancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.\n## B. RAG Enhancements\n2) Retriever Enhancement: In RAG systems, the quality of retrieved content determines the information fed into the generators. Lower content quality increases the risk of model hallucinations or other degradation. In this section, we introduce efficient ways to enhance retrieval effectiveness. Recursive Retrieval: Recursive retrieval is to perform multiple searches to retrieve richer and higher-quality contents. \n\nReACT [132] uses Chain-of-Thought (CoT) [133] to break queries down for recursive retrieval and provide richer information. RATP [134] uses the Monte-Carlo Tree Search for simulations to select optimal retrieval content, which is then templated and forwarded to the generator for output. Chunk Optimization: Chunk optimization refers to adjusting chunk size for improved retrieval results. \n\nLlamaIndex [135] incorporates a series of chunk optimization methods, one of which operates on a 'small to big' principle. The core concept here is to pinpoint finer-grained content but return richer information. For instance, Sentence-window retrieval fetches small text chunks and returns a window of relevant sentences surrounding the retrieved segment. In automerge retrieval, documents are arranged in a tree structure. The process retrieves the parent node, which encapsulates the content of its child nodes, by fetching the child node first. To address the lack of contextual information, RAPTOR [136] employs recursive embedding, clustering, and summarization of text chunks until further clustering becomes infeasible, thereby constructing a multi-level tree structure. Prompt-RAG [137] enhances retrieval accuracy by pre-generating a table of contents, enabling the model to autonomously select relevant chapters based on the query. Raina et al. [138] break text chunks into finer atomic statements to achieve higher recall and improved results. Retriever Finetuning: The retriever, central to the RAG system, relies on a proficient embedding model [139]- [142] to represent related content and feed the generator, enhancing system performance. \n\nAdditionally, embedding models with strong expressive power can be fine-tuned with domain-specific or task-related data to boost performance in targeted areas. REPLUG [86] treats LM as a black box and update the retriever model based on the final results.",
            "reference_string": "[268091298 | Zhao et al. | 2024 | Citations: 282]"
        },
        {
            "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 21,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.17648, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2322992640",
                    "name": "Isaac Chung"
                },
                {
                    "authorId": "2322982756",
                    "name": "Phat Vo"
                },
                {
                    "authorId": "2322991957",
                    "name": "Arman Kizilkale"
                },
                {
                    "authorId": "2322982549",
                    "name": "Aaron Reite"
                }
            ],
            "abstract": "Retrieval Augmented Generation (RAG) is a common method for integrating external knowledge into pretrained Large Language Models (LLMs) to enhance accuracy and relevancy in question answering (QA) tasks. However, prompt engineering and resource efficiency remain significant bottlenecks in developing optimal and robust RAG solutions for real-world QA applications. Recent studies have shown success in using fine tuning to address these problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to smaller 7B models has demonstrated superior performance compared to RAG setups with much larger models such as GPT-3.5. The combination of RAFT with parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), promises an even more efficient solution, yet remains an unexplored area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage requirements and gain faster inference times while maintaining comparable RAG performance. This results in a more compute-efficient RAFT, or CRAFT, which is particularly useful for knowledge-intensive QA tasks in resource-constrained environments where internet access may be restricted and hardware resources limited.",
            "corpus_id": 272911196,
            "sentences": [
                {
                    "corpus_id": "272911196",
                    "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
                    "text": "Retrieval Augmented Generation (RAG) is a common method for integrating external knowledge into pretrained Large Language Models (LLMs) to enhance accuracy and relevancy in question answering (QA) tasks. However, prompt engineering and resource efficiency remain significant bottlenecks in developing optimal and robust RAG solutions for real-world QA applications. Recent studies have shown success in using fine tuning to address these problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to smaller 7B models has demonstrated superior performance compared to RAG setups with much larger models such as GPT-3.5. The combination of RAFT with parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), promises an even more efficient solution, yet remains an unexplored area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage requirements and gain faster inference times while maintaining comparable RAG performance. This results in a more compute-efficient RAFT, or CRAFT, which is particularly useful for knowledge-intensive QA tasks in resource-constrained environments where internet access may be restricted and hardware resources limited.",
                    "score": 0.5661076093979819,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9091796875
                },
                {
                    "corpus_id": "272911196",
                    "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
                    "text": "We release all models and generated datasets to facilitate further study on HuggingFace1 . In Section 2, we present some related work on RAG, fine tuning methods for RAG including RAFT, and PEFT techniques such as LoRA. In Section 3, we introduce our compute-efficient RAFT method, CRAFT, in detail. In Section 4, we present the experiment setup. In Section 5 we report the results, and in Section 6 we conclude the paper. \n\nRetrieval Augmented Generation (RAG) RAG (Lewis et al., 2021) enhances LLMs by retrieving relevant document chunks from external knowledge bases through semantic similarity calculations. This method mitigates the generation of factually incorrect content by referencing external knowledge rather than relying solely on knowledge the model learned during training, thereby improving the relevancy of the generated text while reducing \"hallucinations\". Despite its advantages, RAG faces challenges, particularly with domain-specific or knowledge-intensive tasks, particularly when handling queries beyond the scope of its retrieved data (Zhang et al., 2023), though at a lesser extent when compared to non-retrieval-augmented LLMs. Other major challenges with RAG includes requiring a high-performing retriever model to produce representative embeddings from the document chunks and retrieval system that balances scale and accuracy. Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024). RAG excels in dynamic environments by offering real-time knowledge updates and effective utilization of external knowledge sources with high interpretability. However, it comes with higher latency and the possibility of added noise from extraneous contexts. \n\nFine tuning for RAG Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever. RAFT (Zhang et al., 2024b) includes a fine tuning strategy that generates training data from the QA target domain data for instruction fine tuning.",
                    "score": 0.6580173737316082,
                    "section_title": "Introduction",
                    "char_start_offset": 4339,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 90
                        },
                        {
                            "start": 91,
                            "end": 219
                        },
                        {
                            "start": 220,
                            "end": 299
                        },
                        {
                            "start": 300,
                            "end": 346
                        },
                        {
                            "start": 347,
                            "end": 422
                        },
                        {
                            "start": 425,
                            "end": 611
                        },
                        {
                            "start": 612,
                            "end": 875
                        },
                        {
                            "start": 876,
                            "end": 1154
                        },
                        {
                            "start": 1155,
                            "end": 1356
                        },
                        {
                            "start": 1357,
                            "end": 1495
                        },
                        {
                            "start": 1496,
                            "end": 1654
                        },
                        {
                            "start": 1655,
                            "end": 1753
                        },
                        {
                            "start": 1756,
                            "end": 1932
                        },
                        {
                            "start": 1933,
                            "end": 2090
                        },
                        {
                            "start": 2091,
                            "end": 2235
                        },
                        {
                            "start": 2236,
                            "end": 2383
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89990234375
                }
            ],
            "relevance_judgement": 0.9091796875,
            "relevance_judgment_input_expanded": "# Title: Efficient In-Domain Question Answering for Resource-Constrained Environments\n# Venue: arXiv.org\n# Authors: Isaac Chung, Phat Vo, Arman Kizilkale, Aaron Reite\n## Abstract\nRetrieval Augmented Generation (RAG) is a common method for integrating external knowledge into pretrained Large Language Models (LLMs) to enhance accuracy and relevancy in question answering (QA) tasks. However, prompt engineering and resource efficiency remain significant bottlenecks in developing optimal and robust RAG solutions for real-world QA applications. Recent studies have shown success in using fine tuning to address these problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to smaller 7B models has demonstrated superior performance compared to RAG setups with much larger models such as GPT-3.5. The combination of RAFT with parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), promises an even more efficient solution, yet remains an unexplored area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage requirements and gain faster inference times while maintaining comparable RAG performance. This results in a more compute-efficient RAFT, or CRAFT, which is particularly useful for knowledge-intensive QA tasks in resource-constrained environments where internet access may be restricted and hardware resources limited.\n## Introduction\nWe release all models and generated datasets to facilitate further study on HuggingFace1 . In Section 2, we present some related work on RAG, fine tuning methods for RAG including RAFT, and PEFT techniques such as LoRA. In Section 3, we introduce our compute-efficient RAFT method, CRAFT, in detail. In Section 4, we present the experiment setup. In Section 5 we report the results, and in Section 6 we conclude the paper. \n\nRetrieval Augmented Generation (RAG) RAG (Lewis et al., 2021) enhances LLMs by retrieving relevant document chunks from external knowledge bases through semantic similarity calculations. This method mitigates the generation of factually incorrect content by referencing external knowledge rather than relying solely on knowledge the model learned during training, thereby improving the relevancy of the generated text while reducing \"hallucinations\". Despite its advantages, RAG faces challenges, particularly with domain-specific or knowledge-intensive tasks, particularly when handling queries beyond the scope of its retrieved data (Zhang et al., 2023), though at a lesser extent when compared to non-retrieval-augmented LLMs. Other major challenges with RAG includes requiring a high-performing retriever model to produce representative embeddings from the document chunks and retrieval system that balances scale and accuracy. Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024). RAG excels in dynamic environments by offering real-time knowledge updates and effective utilization of external knowledge sources with high interpretability. However, it comes with higher latency and the possibility of added noise from extraneous contexts. \n\nFine tuning for RAG Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever. RAFT (Zhang et al., 2024b) includes a fine tuning strategy that generates training data from the QA target domain data for instruction fine tuning.",
            "reference_string": "[272911196 | Chung et al. | 2024 | Citations: 0]"
        },
        {
            "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 47,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.04588, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292123797",
                    "name": "Hao Sun"
                },
                {
                    "authorId": "2184030467",
                    "name": "Zile Qiao"
                },
                {
                    "authorId": "5765645",
                    "name": "Jiayan Guo"
                },
                {
                    "authorId": "2359593960",
                    "name": "Xuanbo Fan"
                },
                {
                    "authorId": "2246638730",
                    "name": "Yingyan Hou"
                },
                {
                    "authorId": "2256747040",
                    "name": "Yong Jiang"
                },
                {
                    "authorId": "35930962",
                    "name": "Pengjun Xie"
                },
                {
                    "authorId": "2274264502",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2276428076",
                    "name": "Fei Huang"
                },
                {
                    "authorId": "2284490340",
                    "name": "Jingren Zhou"
                }
            ],
            "abstract": "Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a novel RL framework that incentivizes the capabilities of LLMs to use a real search engine with simulated searches during training. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both useful and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.",
            "corpus_id": 278367823,
            "sentences": [
                {
                    "corpus_id": "278367823",
                    "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
                    "text": "Retrieval-augmented generation (RAG) enhances generation performance by integrating relevant external knowledge into the generation pipeline. Early research primarily adopted prompt-based approaches, guiding LLMs through processes such as query generation, query decomposition, and multi-turn information retrieval [44,28,43,15,33,22]. Despite their effectiveness, these methods often require intricate prompt engineering and impose substantial demands on the model's reasoning capabilities. To improve efficiency and reduce dependency on strong black-box LLMs, subsequent work has proposed supervised fine-tuning strategies for smaller LLMs. For instance, Self-RAG [1] employs a self-reflection mechanism, iteratively refining model outputs through predicted reflection tokens. RetroLLM [24] integrates retrieval and generation by enabling the model to directly generate fine-grained evidence from the corpus via constrained decoding. Recent advances also include test-time scaling techniques [25,14,47,13], notably Monte Carlo Tree Search (MCTS), which dynamically expands the search space during inference. For example, RAG-star [13] integrates retrieved information into a tree-based reasoning process, while AirRAG [5] employs MCTS to activate intrinsic reasoning capabilities and expand the solution space. Despite promising results, these approaches introduce significant computational overhead, limiting their practical applicability.",
                    "score": 0.561167752858074,
                    "section_title": "Retrieval-Augmented Generation",
                    "char_start_offset": 6269,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 141
                        },
                        {
                            "start": 142,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 491
                        },
                        {
                            "start": 492,
                            "end": 642
                        },
                        {
                            "start": 643,
                            "end": 778
                        },
                        {
                            "start": 779,
                            "end": 935
                        },
                        {
                            "start": 936,
                            "end": 1109
                        },
                        {
                            "start": 1110,
                            "end": 1312
                        },
                        {
                            "start": 1313,
                            "end": 1442
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 325,
                            "end": 328,
                            "matchedPaperCorpusId": "258615731"
                        },
                        {
                            "start": 666,
                            "end": 669,
                            "matchedPaperCorpusId": "264288947"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9072265625
                }
            ],
            "relevance_judgement": 0.9072265625,
            "relevance_judgment_input_expanded": "# Title: ZeroSearch: Incentivize the Search Capability of LLMs without Searching\n# Venue: arXiv.org\n# Authors: Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, Jingren Zhou\n## Abstract\nEffective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a novel RL framework that incentivizes the capabilities of LLMs to use a real search engine with simulated searches during training. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both useful and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.\n## Retrieval-Augmented Generation\nRetrieval-augmented generation (RAG) enhances generation performance by integrating relevant external knowledge into the generation pipeline. Early research primarily adopted prompt-based approaches, guiding LLMs through processes such as query generation, query decomposition, and multi-turn information retrieval [44,28,43,15,33,22]. Despite their effectiveness, these methods often require intricate prompt engineering and impose substantial demands on the model's reasoning capabilities. To improve efficiency and reduce dependency on strong black-box LLMs, subsequent work has proposed supervised fine-tuning strategies for smaller LLMs. For instance, Self-RAG [1] employs a self-reflection mechanism, iteratively refining model outputs through predicted reflection tokens. RetroLLM [24] integrates retrieval and generation by enabling the model to directly generate fine-grained evidence from the corpus via constrained decoding. Recent advances also include test-time scaling techniques [25,14,47,13], notably Monte Carlo Tree Search (MCTS), which dynamically expands the search space during inference. For example, RAG-star [13] integrates retrieved information into a tree-based reasoning process, while AirRAG [5] employs MCTS to activate intrinsic reasoning capabilities and expand the solution space. Despite promising results, these approaches introduce significant computational overhead, limiting their practical applicability.",
            "reference_string": "[278367823 | Sun et al. | 2025 | Citations: 12]"
        },
        {
            "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
            "venue": "ACM Transactions on Knowledge Discovery from Data",
            "year": 2024,
            "reference_count": 90,
            "citation_count": 26,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.08680, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2656889",
                    "name": "Teo Su\u0161njak"
                },
                {
                    "authorId": "2296719088",
                    "name": "Peter Hwang"
                },
                {
                    "authorId": "1783269",
                    "name": "N. Reyes"
                },
                {
                    "authorId": "3312622",
                    "name": "A. Barczak"
                },
                {
                    "authorId": "11430146",
                    "name": "Timothy R. McIntosh"
                },
                {
                    "authorId": "143976433",
                    "name": "Surangika Ranathunga"
                }
            ],
            "abstract": "This research pioneers the use of fine-tuned Large Language Models (LLMs) to automate Systematic Literature Reviews (SLRs), presenting a significant and novel contribution in integrating AI to enhance academic research methodologies. Our study employed advanced fine-tuning methodologies on open sourced LLMs, applying textual data mining techniques to automate the knowledge discovery and synthesis phases of an SLR process, thus demonstrating a practical and efficient approach for extracting and analyzing high-quality information from large academic datasets. The results maintained high fidelity in factual accuracy in LLM responses, and were validated through the replication of an existing PRISMA-conforming SLR. Our research proposed solutions for mitigating LLM hallucination and proposed mechanisms for tracking LLM responses to their sources of information, thus demonstrating how this approach can meet the rigorous demands of scholarly research. The findings ultimately confirmed the potential of fine-tuned LLMs in streamlining various labor-intensive processes of conducting literature reviews. As a scalable proof-of-concept, this study highlights the broad applicability of our approach across multiple research domains. The potential demonstrated here advocates for updates to PRISMA reporting guidelines, incorporating AI-driven processes to ensure methodological transparency and reliability in future SLRs. This study broadens the appeal of AI-enhanced tools across various academic and research fields, demonstrating how to conduct comprehensive and accurate literature reviews with more efficiency in the face of ever-increasing volumes of academic studies while maintaining high standards.",
            "corpus_id": 269149146,
            "sentences": [
                {
                    "corpus_id": "269149146",
                    "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
                    "text": "Retrieval-Augmented Generation (RAG) [68] is a framework that combines the capabilities of large language models (LLMs) with external knowledge sources through a retrieval mechanism.Unlike traditional language models that generate text based solely on the text's internal representation, RAG models retrieve relevant information from a knowledge base (like a database or the internet) and integrate this into the generation process.The core components of RAG include the retrieval mechanism, which fetches relevant documents or data, and the generative model, which synthesizes the retrieved information into coherent and contextually relevant responses.The theoretical principles underpinning RAG stem from the need to enhance language models with the ability to access and utilize external, structured knowledge [69].This is in response to the limitations of traditional LLMs that rely solely on their pre-trained parameters for knowledge, which can be outdated or incomplete.The typical architecture of RAG systems involves a retriever that fetches relevant information from a database and a generator that incorporates this information into the final output, thus, the integration of these components allows the model to produce contextually enriched and factually accurate text [68].The incorporation of RAG into the workflow of LLMs, therefore, presents a sophisticated approach to augmenting the model's knowledge base beyond its pretraining, specifically tailored to the demands of the evolving nature of studies in SLRs.By allowing the model to access an external corpus of domain-specific literature the context within which the LLM operates, it attains the ability to be enriched while a critical countermeasure to the model's propensity for generating plausible yet factually incorrect information -hallucination -is mitigated.In the realm of SLRs, where the precision of synthesized knowledge is paramount, RAG's ability to draws upon relevant information from a targeted corpus helps ensure that the generative outputs of LLMs are anchored in verifiable data.\n\n2.4 Advancing LLMs for Specialized Domains: From Pretraining to Fine-Tuning\n\nIn training LLMs, they initially undergo pretraining on extensive, diverse datasets, acquiring a foundational grasp of both language and knowledge.",
                    "score": 0.5604506830388586,
                    "section_title": "RAG for Enhanced Factual Accuracy in SLRs",
                    "char_start_offset": 18167,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 182,
                            "end": 432
                        },
                        {
                            "start": 432,
                            "end": 654
                        },
                        {
                            "start": 654,
                            "end": 819
                        },
                        {
                            "start": 819,
                            "end": 978
                        },
                        {
                            "start": 978,
                            "end": 1288
                        },
                        {
                            "start": 1288,
                            "end": 1529
                        },
                        {
                            "start": 1529,
                            "end": 1839
                        },
                        {
                            "start": 1839,
                            "end": 2073
                        },
                        {
                            "start": 2075,
                            "end": 2150
                        },
                        {
                            "start": 2152,
                            "end": 2299
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90576171875
                }
            ],
            "relevance_judgement": 0.90576171875,
            "relevance_judgment_input_expanded": "# Title: Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning\n# Venue: ACM Transactions on Knowledge Discovery from Data\n# Authors: Teo Su\u0161njak, Peter Hwang, N. Reyes, A. Barczak, Timothy R. McIntosh, Surangika Ranathunga\n## Abstract\nThis research pioneers the use of fine-tuned Large Language Models (LLMs) to automate Systematic Literature Reviews (SLRs), presenting a significant and novel contribution in integrating AI to enhance academic research methodologies. Our study employed advanced fine-tuning methodologies on open sourced LLMs, applying textual data mining techniques to automate the knowledge discovery and synthesis phases of an SLR process, thus demonstrating a practical and efficient approach for extracting and analyzing high-quality information from large academic datasets. The results maintained high fidelity in factual accuracy in LLM responses, and were validated through the replication of an existing PRISMA-conforming SLR. Our research proposed solutions for mitigating LLM hallucination and proposed mechanisms for tracking LLM responses to their sources of information, thus demonstrating how this approach can meet the rigorous demands of scholarly research. The findings ultimately confirmed the potential of fine-tuned LLMs in streamlining various labor-intensive processes of conducting literature reviews. As a scalable proof-of-concept, this study highlights the broad applicability of our approach across multiple research domains. The potential demonstrated here advocates for updates to PRISMA reporting guidelines, incorporating AI-driven processes to ensure methodological transparency and reliability in future SLRs. This study broadens the appeal of AI-enhanced tools across various academic and research fields, demonstrating how to conduct comprehensive and accurate literature reviews with more efficiency in the face of ever-increasing volumes of academic studies while maintaining high standards.\n## RAG for Enhanced Factual Accuracy in SLRs\nRetrieval-Augmented Generation (RAG) [68] is a framework that combines the capabilities of large language models (LLMs) with external knowledge sources through a retrieval mechanism.Unlike traditional language models that generate text based solely on the text's internal representation, RAG models retrieve relevant information from a knowledge base (like a database or the internet) and integrate this into the generation process.The core components of RAG include the retrieval mechanism, which fetches relevant documents or data, and the generative model, which synthesizes the retrieved information into coherent and contextually relevant responses.The theoretical principles underpinning RAG stem from the need to enhance language models with the ability to access and utilize external, structured knowledge [69].This is in response to the limitations of traditional LLMs that rely solely on their pre-trained parameters for knowledge, which can be outdated or incomplete.The typical architecture of RAG systems involves a retriever that fetches relevant information from a database and a generator that incorporates this information into the final output, thus, the integration of these components allows the model to produce contextually enriched and factually accurate text [68].The incorporation of RAG into the workflow of LLMs, therefore, presents a sophisticated approach to augmenting the model's knowledge base beyond its pretraining, specifically tailored to the demands of the evolving nature of studies in SLRs.By allowing the model to access an external corpus of domain-specific literature the context within which the LLM operates, it attains the ability to be enriched while a critical countermeasure to the model's propensity for generating plausible yet factually incorrect information -hallucination -is mitigated.In the realm of SLRs, where the precision of synthesized knowledge is paramount, RAG's ability to draws upon relevant information from a targeted corpus helps ensure that the generative outputs of LLMs are anchored in verifiable data.\n\n2.4 Advancing LLMs for Specialized Domains: From Pretraining to Fine-Tuning\n\nIn training LLMs, they initially undergo pretraining on extensive, diverse datasets, acquiring a foundational grasp of both language and knowledge.",
            "reference_string": "[269149146 | Susnjak et al. | 2024 | Citations: 26]"
        },
        {
            "title": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented Generation with Reranker KV-Cache Reuse",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 30,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.02921, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2354227117",
                    "name": "Yuwei An"
                },
                {
                    "authorId": "2117232731",
                    "name": "Yihua Cheng"
                },
                {
                    "authorId": "2354220575",
                    "name": "Seo Jin Park"
                },
                {
                    "authorId": "2249947059",
                    "name": "Junchen Jiang"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing the performance of large language models (LLMs) by integrating external knowledge into the generation process. A key component of RAG pipelines is the reranker, which selects the most relevant documents from a pool of retrieved candidates and significantly improves the quality of the generated responses. While rerankers refine the selection of retrieved documents in RAG pipelines, they introduce computational challenges that hinder high throughput and low latency. To address this problem, we propose HyperRAG, a system that optimizes the trade-off between quality and efficiency in RAG pipelines by leveraging KV-cache reuse for efficient reranker inference. By reusing document-side KV-cache, HyperRAG achieves both high-quality generation and system-level efficiency. To fully realize the benefits of KV-cache reuse, HyperRAG incorporates a range of system-level optimizations designed to enhance efficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3 throughput improvement with decoder-only rerankers while also delivering higher downstream performance compared with traditional RAG service.",
            "corpus_id": 277596427,
            "sentences": [
                {
                    "corpus_id": "277596427",
                    "title": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented Generation with Reranker KV-Cache Reuse",
                    "text": "Retrieval-Augmented Generation (RAG) (Ram et al., 2023;Lewis et al., 2021;Asai et al., 2023;Khandelwal et al., 2020;Jin et al., 2024a;Shao et al., 2024) is the process of optimizing the output of a large language model and it references an authoritative knowledge base outside of its training data sources before generating a response. This paradigm has gained traction for tasks requiring factual accuracy and up-to-date information, such as question answering, summarization, and dialogue generation. \n\nFigure 1: Classic RAG Workflow: The query is embedded and used to retrieve top-K documents. Then the reranker selects the most relevant ones which are combined with the query to generate the final response. \n\nTraditional RAG pipelines rely on retrievers which depend on embedding representations and cosine similarity to fetch relevant documents (Robertson & Zaragoza, 2009;Reimers & Gurevych, 2019;Wang et al., 2024). While this approach is straightforward, it often struggles to achieve optimal results in more complex scenarios. To solve the problem, advancements have introduced reranker mechanisms that refine the retrieved documents to improve relevance and contextuality before generation. These rerankers, often transformer-based, significantly boost the quality of generated content by ensuring the most pertinent documents are prioritized. \n\nEarly Rerankers were predominantly trained on encoder-only models such as BERT (Devlin et al., 2019) or XLM-RoBERTa (Conneau et al., 2020), leveraging their strong encoding capabilities to improve retrieval precision. However, recent advancements have demonstrated the growing dominance of decoder-based rerankers which capitalize on the powerful generative language capabilities of modern decoder models (Chen et al., 2024;Ma et al., 2023;Pradeep et al., 2023). By fine-tuning these decoder-based models on tasks originally designed for encoder-only rerankers, they achieve significant gains in performance, benefiting from both their inherent generative power and the fine-grained contextual understanding acquired during fine-tuning.",
                    "score": 0.6106745469901098,
                    "section_title": "Retrieval-Augmented Generation",
                    "char_start_offset": 3985,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 502
                        },
                        {
                            "start": 505,
                            "end": 596
                        },
                        {
                            "start": 597,
                            "end": 711
                        },
                        {
                            "start": 714,
                            "end": 923
                        },
                        {
                            "start": 924,
                            "end": 1036
                        },
                        {
                            "start": 1037,
                            "end": 1201
                        },
                        {
                            "start": 1202,
                            "end": 1354
                        },
                        {
                            "start": 1357,
                            "end": 1574
                        },
                        {
                            "start": 1575,
                            "end": 1819
                        },
                        {
                            "start": 1820,
                            "end": 2093
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 879,
                            "end": 904,
                            "matchedPaperCorpusId": "201646309"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90380859375
                }
            ],
            "relevance_judgement": 0.90380859375,
            "relevance_judgment_input_expanded": "# Title: HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented Generation with Reranker KV-Cache Reuse\n# Venue: arXiv.org\n# Authors: Yuwei An, Yihua Cheng, Seo Jin Park, Junchen Jiang\n## Abstract\nRetrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing the performance of large language models (LLMs) by integrating external knowledge into the generation process. A key component of RAG pipelines is the reranker, which selects the most relevant documents from a pool of retrieved candidates and significantly improves the quality of the generated responses. While rerankers refine the selection of retrieved documents in RAG pipelines, they introduce computational challenges that hinder high throughput and low latency. To address this problem, we propose HyperRAG, a system that optimizes the trade-off between quality and efficiency in RAG pipelines by leveraging KV-cache reuse for efficient reranker inference. By reusing document-side KV-cache, HyperRAG achieves both high-quality generation and system-level efficiency. To fully realize the benefits of KV-cache reuse, HyperRAG incorporates a range of system-level optimizations designed to enhance efficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3 throughput improvement with decoder-only rerankers while also delivering higher downstream performance compared with traditional RAG service.\n## Retrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) (Ram et al., 2023;Lewis et al., 2021;Asai et al., 2023;Khandelwal et al., 2020;Jin et al., 2024a;Shao et al., 2024) is the process of optimizing the output of a large language model and it references an authoritative knowledge base outside of its training data sources before generating a response. This paradigm has gained traction for tasks requiring factual accuracy and up-to-date information, such as question answering, summarization, and dialogue generation. \n\nFigure 1: Classic RAG Workflow: The query is embedded and used to retrieve top-K documents. Then the reranker selects the most relevant ones which are combined with the query to generate the final response. \n\nTraditional RAG pipelines rely on retrievers which depend on embedding representations and cosine similarity to fetch relevant documents (Robertson & Zaragoza, 2009;Reimers & Gurevych, 2019;Wang et al., 2024). While this approach is straightforward, it often struggles to achieve optimal results in more complex scenarios. To solve the problem, advancements have introduced reranker mechanisms that refine the retrieved documents to improve relevance and contextuality before generation. These rerankers, often transformer-based, significantly boost the quality of generated content by ensuring the most pertinent documents are prioritized. \n\nEarly Rerankers were predominantly trained on encoder-only models such as BERT (Devlin et al., 2019) or XLM-RoBERTa (Conneau et al., 2020), leveraging their strong encoding capabilities to improve retrieval precision. However, recent advancements have demonstrated the growing dominance of decoder-based rerankers which capitalize on the powerful generative language capabilities of modern decoder models (Chen et al., 2024;Ma et al., 2023;Pradeep et al., 2023). By fine-tuning these decoder-based models on tasks originally designed for encoder-only rerankers, they achieve significant gains in performance, benefiting from both their inherent generative power and the fine-grained contextual understanding acquired during fine-tuning.",
            "reference_string": "[277596427 | An et al. | 2025 | Citations: 1]"
        },
        {
            "title": "PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.14510, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326445025",
                    "name": "Jiayi Wu"
                },
                {
                    "authorId": "2274199835",
                    "name": "Hengyi Cai"
                },
                {
                    "authorId": "1387839383",
                    "name": "Lingyong Yan"
                },
                {
                    "authorId": "2322418772",
                    "name": "Hao Sun"
                },
                {
                    "authorId": "2320925202",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "2237948548",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "2243455567",
                    "name": "Dawei Yin"
                },
                {
                    "authorId": "2238182172",
                    "name": "Ming Gao"
                }
            ],
            "abstract": "The emergence of Retrieval-augmented generation (RAG) has alleviated the issues of outdated and hallucinatory content in the generation of large language models (LLMs), yet it still reveals numerous limitations. When a general-purpose LLM serves as the RAG generator, it often suffers from inadequate response informativeness, response robustness, and citation quality. Past approaches to tackle these limitations, either by incorporating additional steps beyond generating responses or optimizing the generator through supervised fine-tuning (SFT), still failed to align with the RAG requirement thoroughly. Consequently, optimizing the RAG generator from multiple preference perspectives while maintaining its end-to-end LLM form remains a challenge. To bridge this gap, we propose Multiple Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator of RAG systems to align with RAG requirements comprehensively. Specifically, we construct high-quality instruction fine-tuning data and multi-perspective preference data by sampling varied quality responses from the generator across different prompt documents quality scenarios. Subsequently, we optimize the generator using SFT and Direct Preference Optimization (DPO). Extensive experiments conducted on four question-answer datasets across three LLMs demonstrate that PA-RAG can significantly enhance the performance of RAG generators. Our code and datasets are available at https://github.com/wujwyi/PA-RAG.",
            "corpus_id": 274859726,
            "sentences": [
                {
                    "corpus_id": "274859726",
                    "title": "PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization",
                    "text": "The emergence of Retrieval-augmented generation (RAG) has alleviated the issues of outdated and hallucinatory content in the generation of large language models (LLMs), yet it still reveals numerous limitations. When a general-purpose LLM serves as the RAG generator, it often suffers from inadequate response informativeness, response robustness, and citation quality. Past approaches to tackle these limitations, either by incorporating additional steps beyond generating responses or optimizing the generator through supervised fine-tuning (SFT), still failed to align with the RAG requirement thoroughly. Consequently, optimizing the RAG generator from multiple preference perspectives while maintaining its end-to-end LLM form remains a challenge. To bridge this gap, we propose Multiple Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator of RAG systems to align with RAG requirements comprehensively. Specifically, we construct high-quality instruction fine-tuning data and multi-perspective preference data by sampling varied quality responses from the generator across different prompt documents quality scenarios. Subsequently, we optimize the generator using SFT and Direct Preference Optimization (DPO). Extensive experiments conducted on four question-answer datasets across three LLMs demonstrate that PA-RAG can significantly enhance the performance of RAG generators. Our code and datasets are available at https://github.com/wujwyi/PA-RAG.",
                    "score": 0.6160855547463291,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9033203125
                }
            ],
            "relevance_judgement": 0.9033203125,
            "relevance_judgment_input_expanded": "# Title: PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Jiayi Wu, Hengyi Cai, Lingyong Yan, Hao Sun, Xiang Li, Shuaiqiang Wang, Dawei Yin, Ming Gao\n## Abstract\nThe emergence of Retrieval-augmented generation (RAG) has alleviated the issues of outdated and hallucinatory content in the generation of large language models (LLMs), yet it still reveals numerous limitations. When a general-purpose LLM serves as the RAG generator, it often suffers from inadequate response informativeness, response robustness, and citation quality. Past approaches to tackle these limitations, either by incorporating additional steps beyond generating responses or optimizing the generator through supervised fine-tuning (SFT), still failed to align with the RAG requirement thoroughly. Consequently, optimizing the RAG generator from multiple preference perspectives while maintaining its end-to-end LLM form remains a challenge. To bridge this gap, we propose Multiple Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator of RAG systems to align with RAG requirements comprehensively. Specifically, we construct high-quality instruction fine-tuning data and multi-perspective preference data by sampling varied quality responses from the generator across different prompt documents quality scenarios. Subsequently, we optimize the generator using SFT and Direct Preference Optimization (DPO). Extensive experiments conducted on four question-answer datasets across three LLMs demonstrate that PA-RAG can significantly enhance the performance of RAG generators. Our code and datasets are available at https://github.com/wujwyi/PA-RAG.\n",
            "reference_string": "[274859726 | Wu et al. | 2024 | Citations: 1]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "269149041",
            "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
            "text": "For every x and preceding generation y < t,the model decodes a retrieval token to evaluate the utility of retrieval.If retrieval is not required,the model predicts the next output segment,as it does in a standard LM.If retrieval is needed,the model generates: a critique token to evaluate the retrieved passage's relevance,the next response segment,and a critique token to evaluate if the information in the response segment is supported by the passage.Finally,a new critique token evaluates the overall utility of the response.\n\n\"REPLUG: Retrieval-Augmented Black-Box Language Models [115]:\" This research explores how to optimize retrieval results in black-box language models,such as those that only expose APIs without revealing embeddings,using a retrieval-augmented approach.\n\n\"Atlas: Few-shot Learning with Retrieval Augmented Language Models [45]:\" The paper discusses methods for joint training of retrievers and language models,especially for knowledge-intensive tasks.\n\n\"RA-DIT: RETRIEVAL-AUGMENTED DUAL IN-STRUCTION TUNING [66]:\" A lightweight fine-tuning method that combines RAG and SFT is proposed to enhance the performance of retrieval-augmented language models.\n\n\"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering [118]:\" This paper assesses the domain adaptability of RAG models in open-domain question answering (ODQA) tasks and proposes RAG-end2end,an extension of RAG that can adapt to specific domain knowledge bases by updating all components during training.\n\n\"RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance [8]:\" This article provides a guide on the optimization strategies of RAG,fine-tuning,and their combina-",
            "score": 0.8420221995607047,
            "section_title": "XIII. RAG-MEMORY-FINETUNING",
            "char_start_offset": 145958,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 116,
                    "end": 216
                },
                {
                    "start": 216,
                    "end": 453
                },
                {
                    "start": 453,
                    "end": 528
                },
                {
                    "start": 530,
                    "end": 781
                },
                {
                    "start": 783,
                    "end": 979
                },
                {
                    "start": 981,
                    "end": 1179
                },
                {
                    "start": 1181,
                    "end": 1547
                },
                {
                    "start": 1549,
                    "end": 1721
                }
            ],
            "ref_mentions": [
                {
                    "start": 1296,
                    "end": 1301,
                    "matchedPaperCorpusId": "252735056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9208984375
        },
        {
            "corpus_id": "270870251",
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "text": "Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context [30][31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges.\n\nSeveral surveys have extensively discussed current RAG systems, covering aspects like text generation [7,8], integration with LLMs [6,39], multimodal [40], and AI-generated content [41].While these surveys provide comprehensive overviews of existing RAG methodologies, selecting the appro-  priate algorithm for practical implementation remains challenging.In this paper, we focus on best practices for applying RAG methods, advancing the understanding and application of RAG in LLMs.",
            "score": 0.8231243358922785,
            "section_title": "Retriever and Generator Fine-tuning",
            "char_start_offset": 7670,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 94,
                    "end": 242
                },
                {
                    "start": 242,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 551
                },
                {
                    "start": 553,
                    "end": 739
                },
                {
                    "start": 739,
                    "end": 910
                },
                {
                    "start": 910,
                    "end": 1037
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 185,
                    "matchedPaperCorpusId": "258865283"
                },
                {
                    "start": 658,
                    "end": 660,
                    "matchedPaperCorpusId": "250340214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9775390625
        },
        {
            "corpus_id": "271270644",
            "title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
            "text": "This section gives an overview of RAG for NLP. As shown in Figure 1, RAG typically consists of three modules, the retriever, the generator, and retrieval fusions. Retriever module usually comprises three components: an encoder for encoding inputs into embeddings, an efficient indexing that supports approximate nearest neighbor search, and the datastore for storing external knowledge in the form of key-value pairs. The main challenge in the retriever module is finding the optimal trade-off between retrieval efficiency and retrieval quality. The retrieval efficiency refers to how fast the relevant information can be obtained, which involves accelerating encoding, efficient indexing, batch querying in the datastore, etc. The retrieval quality refers to how relevant the information can be retrieved, which involves chunk representation learning, advanced approximate nearest neighbor search algorithms, etc. \n\nRetrieval Fusions aims to leverage the retrieved information to augment the generation. These fusion techniques can be categorized into three major types: query-based fusion, latent fusion, and logits-based fusion. The query-based fusion augments inputs with retrievals before feeding them into the generators. The logits-based fusion focuses on the output logits of generators and fuses the retrievals logits for more robust logits. The latent fusion refers to introducing retrieval representations into the latent representations of generators, thus implicitly improving the models' performance. \n\nGenerator module can be classified into two branches of generators: default generators and retrieval-augmented (RA) generators. The default generators include most pre-trained/fine-tuned large language models, such as GPT-series models [11,114,118,119], Mistral models [71], and Gemini-series models [4,108,124]. The RA generators refer to the pre-trained/fine-tuned generators that consist of modules for fusing retrievals, such RETRO [10] and Enc-Dec [93]. Those generators generate responses or make predictions. \n\nThe workflow of RAG involves three steps: (1) retrieving the relevant information from external databases based on given inputs; \n\n(2) fusing the retrieved information with inputs or intermediate states based on the fusion techniques; (3) making predictions by generators based on the input and corresponding retrievals.",
            "score": 0.806213815484089,
            "section_title": "OVERVIEW OF RETRIEVAL-AUGMENTED GENERATION",
            "char_start_offset": 3632,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 47,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 914
                },
                {
                    "start": 917,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1514
                },
                {
                    "start": 1517,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2032
                },
                {
                    "start": 2035,
                    "end": 2163
                },
                {
                    "start": 2166,
                    "end": 2355
                }
            ],
            "ref_mentions": [
                {
                    "start": 1761,
                    "end": 1765,
                    "matchedPaperCorpusId": "49313245"
                },
                {
                    "start": 1765,
                    "end": 1769,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1953,
                    "end": 1957,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1970,
                    "end": 1974,
                    "matchedPaperCorpusId": "252846580"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92578125
        },
        {
            "corpus_id": "274982489",
            "title": "Evaluating the Performance of Large Language Models in Scientific Claim Detection and Classification",
            "text": "Future research directions are inspired by the paper on Retrieval Augmented Generation (RAG) [7] and other methodologies: \n\n\u2022 Retrieval Augmented Generation (RAG): While LLMs fall short in knowledge-intensive tasks compared to task-specific architectures, RAG offers a promising solution. It involves augmenting the input query with information retrieved from a Knowledge Base, using vector databases for numerical vector representation and similarity matching techniques like Cosine Similarity or TF-IDF. \u2022 Fine-Tuning on Specific Datasets: To better capture the intricacies and specific requirements of a dataset, feeding it directly into LLMs for fine-tuning is recommended. Detailed guidance on this process can be found at OpenAI's fine-tuning platform.",
            "score": 0.7739813315920698,
            "section_title": "Future Work",
            "char_start_offset": 26467,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 124,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 758
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 96,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89453125
        },
        {
            "corpus_id": "277043297",
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "text": "Generator-Guided Retriever Training. Conversely, generator-guided retriever training focuses on optimizing the retriever based on the generator's performance and requirements. In this paradigm, the generator's ability to produce coherent and accurate text influences the retriever's selection process. DKRR [95] leverages the generator's attention scores to fine-tune the retriever, enhancing its capability to select the most pertinent information. AAR [272] employs smaller language models to generate supervision signals that guide the retriever's training, ensuring that the retrieved documents are optimally aligned with the generator's needs. RA-DIT [140] fine-tunes large language models before training the retriever, fostering better alignment and synergy between the two components. Additionally, UPRISE [33] uses a frozen LLM to guide the fine-tuning of a prompt retriever, thereby improving its effectiveness in retrieving data that the generator can utilize more effectively. This bidirectional influence ensures that the retriever evolves in tandem with the generator, fostering a more integrated and efficient RAG system.",
            "score": 0.744215238402276,
            "section_title": "RAG Training",
            "char_start_offset": 76826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1136
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97412109375
        },
        {
            "corpus_id": "268248396",
            "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
            "text": "Interestingly, larger LMs generally do not benefit from finetuning, while smaller ones do. Therefore, a small fine-tuned LM with RAG can perform on par or better than a large LM; e.g., StableLM2 (1.6B) vs. Llama3 (8B) (Table 3). \u2022 Retrieval model: Comparing retrievers with varying performance in the RAG system, we observe that as the popularity of factual knowledge increases, the performance of the retriever decreases (Figure 7). Moreover, the performance of the RAG system increases by using higher performance retriever (Figures 1 and 8). \u2022 Fine-tuning vs. RAG: Comparing these two knowledge injection methods, RAG substantially outperforms fine-tuning. Fine-tuned LMs combined with RAG either outperform or perform on par with vanilla LMs with RAG in all but one case (Figure 1). \n\nWhile fine-tuning improves accuracy in answering factual questions, both with and without RAG, it demands a considerable amount of effort and resources. This leads us to our second research question: (RQ2): Can we avoid the cost of fine-tuning by developing an advanced RAG approach that surpass the performance of a fine-tuned LM with RAG? To answer this question, we develop Stimulus RAG (SRAG), a new RAG approach that stimulates an LM to generate the correct response based on the provided hint in the prompt. The hint is extracted from the top retrieved documents by the retrieval model. Our results demonstrate that Stimulus RAG outperforms all other combinations of fine-tuning, both with and without retrievethen-generate RAG. \n\nTo summarize, this paper makes the following contributions: \n\n\u2022 We study the effectiveness of fine-tuning and RAG approaches for question answering over less popular factual knowledge and compare the performance of these models across distinct setups: vanilla and fine-tuned models, both with and without RAG, using different data augmentation methods.",
            "score": 0.734106536314751,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1523
                },
                {
                    "start": 1526,
                    "end": 1585
                },
                {
                    "start": 1588,
                    "end": 1878
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.630859375
        },
        {
            "corpus_id": "271571401",
            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
            "text": "When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model. \u2022 RL from LLM/human feedback. Reinforcement learning based on feedback from the final generated answers. In addition to using human evaluations, powerful LLMs can also serve as an evaluative judge. 3) Dual FT: In the RAG system, fine-tuning both the retriever and the generator simultaneously is a unique feature of the RAG system. It is important to note that the emphasis of system fine-tuning is on the coordination between the retriever and the generator. An exemplary implementation is RA-DIT [27], which fine-tunes both the LLM and the retriever. The LM-ft component updates the LLM to maximize the Fig. 16. Generator fine-tuning pattern, The main methods include SFT, distillation and RL from LLM/human feedback. Fig. 17. Dual fine-tuning pattern. In this mode, both the retriever and generator participate in fine-tuning, and their preferences will be aligned. likelihood of the correct answer given the retrieval-augmented instructions while the R-ft component updates the retriever to minimize the KL-Divergence between the retriever score distribution and the LLM preference.",
            "score": 0.7296369220165467,
            "section_title": "E. Tuning Pattern",
            "char_start_offset": 57340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1290
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94970703125
        },
        {
            "corpus_id": "275993994",
            "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects",
            "text": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.",
            "score": 0.7212100433580074,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97265625
        },
        {
            "corpus_id": "270764659",
            "title": "Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation",
            "text": "To address the misalignment between different components of retrieval-augmented generation (RAG) and improve overall generation performance, we propose the DPA-RAG framework, which is illustrated in Figure 2. In general, DPA-RAG improves traditional RAG architecture in two main aspects: (1) we fine-tune a preference-aligned reranker between the retriever and the LLM to selectively filter out knowledge that aligns with LLMs' knowledge preferences ( \u00a73.3); and (2) we design a self-alignment mechanism that fine-tunes LLMs to better recognize and utilize knowledge consistent with their reasoning preferences ( \u00a73.4).To acquire the LLM's preference knowledge, we devise a three-step construction method, motivated by our preliminary analysis of how different types of retrieved documents affect RAG performance ( \u00a73.2).Below, we will first introduce the task definition ( \u00a73.1) and then we delve into the specifics of our approach.",
            "score": 0.7157319517139624,
            "section_title": "Methodology",
            "char_start_offset": 8100,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 619
                },
                {
                    "start": 619,
                    "end": 821
                },
                {
                    "start": 821,
                    "end": 933
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8818359375
        },
        {
            "corpus_id": "271571401",
            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
            "text": "RAG is continuously integrating with more LLM-related technologies. In Modular RAG, many components are composed of trainable language models. Through fine-tuning, the performance of the components and the compatibility with the overall flow can be further optimized. This section will introduce three main patterns of fine-tuning stages, namely retriever fine-tuning, generator fine-tuning, and dual finetuning. 1) Retriever FT: In the RAG flow, common methods for fine-tuning the retriever is shown in Figure 15 ,which include: \n\n\u2022 Direct supervised fine-tuning of the retriever. Constructing a specialized dataset for retrieval and fine-tuning the dense retriever. For example, using open-source retrieval datasets or constructing one based on domain-specific data. \n\n\u2022 Adding trainable adapter modules. Sometimes, direct fine-tuning of the API-base embedding model (e.g., Ope-nAI Ada-002 and Cohere) is not feasible. Incorporating an adapter module can enhance the representation of your data. Additionally, the adapter module facilitates better alignment with downstream tasks, whether for taskspecific (e.g., PRCA [42]) or general purposes (e.g., AAR [58]). \u2022 LM-supervised Retrieval (LSR). Fine-tuning the retriever based on the results generated by LLM. \u2022 LLM Reward RL. Still using the LLM output results as the supervisory signal. Employing reinforcement learning to align the retriever with the generator. The whole retrieval process is disassembled in the form of a generative Markov chain. 2) Generator FT: The primary methods for fine-tuning a generator in RAG flow is shown in Figure 16, which include: \n\n\u2022 Direct supervised fine-tuning. Fine-tuning through an external dataset can supplement the generator with additional knowledge. Another benefit is the ability to customize input and output formats. By setting the Q&A format, LLM can understand specific data formats and output according to instructions. \u2022 Distillation. When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model.",
            "score": 0.7153928392445175,
            "section_title": "E. Tuning Pattern",
            "char_start_offset": 55399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2144
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96435546875
        },
        {
            "corpus_id": "276774736",
            "title": "Explainable Depression Detection in Clinical Interviews with Personalized Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) enhances language models (LMs) by incorporating retrieved text passages into the input, leading to significant improvements in knowledge-intensive tasks (Guu et al., 2020;Lewis et al., 2020)  instruction-tuning LMs with a fixed number of retrieved passages or jointly pre-training a retriever and LM followed by few-shot fine-tuning (Luo et al., 2023;Izacard et al., 2022). Some approaches adaptively retrieve passages during generation (Jiang et al., 2023), while others, like Schick et al. (2023), train LMs to generate API calls for named entities. However, these improvements often come with trade-offs in runtime efficiency, robustness, and contextual relevance (Mallen et al., 2023;Shi et al., 2023). To address these challenges, recent work introduces methods like SELF-RAG, which enables on-demand retrieval and filters out irrelevant passages through self-reflection, enhancing robustness and control (Lin et al., 2024;Yoran et al., 2024). SELF-RAG (Asai et al., 2023) also evaluates the factuality and quality of the generated output without relying on external models during inference, making it more efficient and customizable. Additionally, other concurrent RAG methods, such as LATS (Zhou et al., 2023), explore ways to improve retrieval for specific tasks like question answering through tree search. \n\n3 Method",
            "score": 0.7145402274526105,
            "section_title": "Retrievel Augementated Generation",
            "char_start_offset": 6546,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1363
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 208,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 208,
                    "end": 227,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 474,
                    "end": 494,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 515,
                    "end": 535,
                    "matchedPaperCorpusId": "256697342"
                },
                {
                    "start": 704,
                    "end": 725,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 725,
                    "end": 742,
                    "matchedPaperCorpusId": "256459776"
                },
                {
                    "start": 947,
                    "end": 965,
                    "matchedPaperCorpusId": "263605962"
                },
                {
                    "start": 965,
                    "end": 984,
                    "matchedPaperCorpusId": "263608822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8525390625
        },
        {
            "corpus_id": "277043297",
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "text": "Training RAG models requires balancing the optimization of both retrieval and generation components to achieve optimal performance. Effective training strategies ensure the retriever fetches relevant information while the generator produces coherent and accurate outputs. This section reviews various methods in RAG training, including static training, unidirectional guided training, and collaborative training, shown in Figure 8. Each approach offers distinct benefits and challenges, affecting the effectiveness and adaptability of RAG models across applications. By exploring these paradigms, we can enhance the integration of retrieval and generation processes, ultimately improving RAG performance. This method is particularly advantageous in scenarios with limited computational resources or when rapid deployment is essential. For instance, fixing the retriever while optimizing the generator allows the system to benefit from established retrieval mechanisms, such as traditional BM25 [204] or pre-trained models like BERT [49], without the overhead of simultaneously training the retriever. This can lead to faster training cycles and reduced resource consumption. \n\nHowever, the primary drawback of static optimization is the potential compromise in overall system performance. \n\nSince only one component is being optimized, the synergy between retrieval and generation may not be fully realized, potentially limiting the model's ability to adapt to specific tasks or domains. To mitigate this, careful selection of the fixed component and the optimization process is essential to ensure that the evolving component can effectively leverage the fixed information. to better integrate and utilize external information. For example, RETRO [23] employs a pre-trained BERT model as the retriever to provide relevant context that enhances the generator's output. Similarly, RALMs [264] use a pre-trained COLBERTV2 retriever to fine-tune large language models (LLMs), thereby improving the generator's capacity to incorporate retrieved data effectively. ITER-RTGEN [217] utilizes S-BERT to guide the fine-tuning of a T5 generator, ensuring that the generated text aligns closely with the relevant retrieved information. Moreover, SMALLCAP [201] integrates CLIP as a retriever to direct a GPT-2 model in generating accurate and contextually appropriate captions. \n\nThis guided approach ensures that the generator benefits from a steady stream of relevant information, enhancing the overall quality and relevance of the generated content. \n\nGenerator-Guided Retriever Training.",
            "score": 0.7139034406042973,
            "section_title": "RAG Training",
            "char_start_offset": 74282,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1174
                },
                {
                    "start": 1177,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2224
                },
                {
                    "start": 2225,
                    "end": 2366
                },
                {
                    "start": 2369,
                    "end": 2541
                },
                {
                    "start": 2544,
                    "end": 2580
                }
            ],
            "ref_mentions": [
                {
                    "start": 994,
                    "end": 999,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1748,
                    "end": 1752,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 2244,
                    "end": 2249,
                    "matchedPaperCorpusId": "252668790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96044921875
        },
        {
            "corpus_id": "252992929",
            "title": "Enhanced vectors for top-k document retrieval in Question Answering",
            "text": "In this architecture, the model that in the fine-tuning phase, uses the training to recover passages for each instance using the model that is being fine-tuned in inner-loop retrieval.\n\nMoving towards retriever training which is \"end-to-end\", this is carried out by ORQA, REALM, and RAG. However, for each training batch , as it's impossible to apply forward and backward passes on the complete set, such inner loop retrieval necessitates significant approximations. Here, while fine-tuning for OpenQA, RAG, REALM, ORQA, and freeze the document encoder along with the vectors that are indexed, are freezed, which limits the model's capacity to adapt to this job and/or to new corpora.\n\nAt the time of the document encoder fine-tuning and re-indexing a just few more times, RGS provides efficient and scalable approach in which the trainng samples are self-gathered by the retriever. By using outer-loop retrieval, RGS eliminates these restrictions.",
            "score": 0.711186848144683,
            "section_title": "Inner-loop retrieval",
            "char_start_offset": 28099,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53955078125
        },
        {
            "corpus_id": "273695367",
            "title": "RuleRAG: Rule-guided retrieval-augmented generation with language models for question answering",
            "text": "The overview of our proposed rule-guided retriever and generator fine-tuning in RuleRAG-FT are illustrated in Figure 2 (b) and (c), respectively. For rule-guided retriever fine-tuning (RGFT-retriever), we update the LM encoders in a contrastive learning objective (Chen et al., 2020) and train over supervised fine-tuning data F R provided in our constructed benchmarks, where inputs are the queries plus rules and supervised labels are heuristic oracle documents. Compared with retrievers employed with simple retrieval principles, our fine-tuned retrievers can recall more relevant results, aligned with the preferences of the rules. For rule-guided generator fine-tuning (RGFT-generator), we adopt the supervised instruction-tuning objective (Iyer et al., 2023;Chung et al., 2024) while combining each query q with two components: retrieved documents D q from the retrieval phase and the same set of rules R q consistent with the retrieval phase. The rules introduced in the RGFT-generator train LLMs on how to optimally attribute from the retrieved context into answers by following rules, making RuleRAG leverage the fine-tuned retrievers more rationally. Experiments show our proposed RGFT can further guarantee and boost the retrieval quality and answering accuracy of RuleRAG-FT than RuleRAG-ICL. \n\nRule-guided retriever fine-tuning (RGFT-retriever). We utilize two main types of retrievers: sparse retrievers and dense retrievers. As the sparse retriever, we use Pyserini1 to implement the standard training-free BM25 (Robertson & Zaragoza, 2009), which relies on word-level frequencies. As the dense retrievers, we adopt the dual-encoder based retriever architecture, such as DPR2 and SimCSE3 . We freeze the document encoder and tune the query encoder for high retrieval efficiency (Lewis et al., 2020). Given a ((q, r) , D o ) pair in the fine-tuning data F R where D o serve as the oracle documents, each d + i \u2208 D o is a positive learning example while each in-batch d \u2212 j \u0338 \u2208 D o is a negative example.",
            "score": 0.7073053144796827,
            "section_title": "RULERAG-FT",
            "char_start_offset": 13388,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 2017
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 283,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 1527,
                    "end": 1555,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1793,
                    "end": 1813,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90869140625
        },
        {
            "corpus_id": "269982691",
            "title": "FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research",
            "text": "The experimental results are shown in Table 3. Overall, RAG methods significantly outperform the direct generation baseline, which clearly demonstrates the benefits of incorporating external knowledge into the generation process. We further have the following observations: (1) Standard RAG, with advanced retrievers and generators, is a strong baseline, showing robust performance  2) AAR improves retrievers by fine-tuning the contriever model, achieving results comparable to the E5 baseline on various datasets. (3) All three methods employing refiners exhibit significant improvements, particularly on multi-hop datasets such as HotpotQA and 2WikiMultihopQA. This is potentially because complex problems result in less accurate passage retrieval, introducing more noise and highlighting the necessity for refiner optimization. (4) As for generator optimization method, Ret-Robust fine-tunes the LLaMA-2-13B model via LoRA [110], significantly enhancing generator's capability of understanding retrieved passages and outperforming other training-free approaches. (5) The effectiveness of optimizing the RAG process varies depending on the dataset complexity. On simpler datasets such as NQ and TriviaQA, FLARE and Iter-RetGen perform comparably to, or slightly below, Standard RAG. In contrast, for more complex datasets requiring multi-step reasoning, such as HotpotQA, these methods demonstrate substantial improvements over the baseline. This indicates that adaptive retrieval methods are particularly advantageous for tackling complex problems, but they may introduce higher operational costs with limited benefits for simpler tasks.",
            "score": 0.703032090896004,
            "section_title": "Benchmarking Results",
            "char_start_offset": 15407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1641
                }
            ],
            "ref_mentions": [
                {
                    "start": 927,
                    "end": 932,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1067,
                    "end": 1070,
                    "matchedPaperCorpusId": "261245623"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79443359375
        },
        {
            "corpus_id": "273969615",
            "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
            "text": "Task-specific advancements in RAG systems focus on refining retrieval-augmented models for particular applications, improving their efficiency and effectiveness in complex tasks. Demonstrate-Search-Predict [116] introduces a modular approach that breaks down complex problems into manageable tasks, enhancing performance in multi-hop reasoning and open-domain question answering. Similarly, RA-DIT [117] uses dual instruction tuning to fine-tune the retriever and generative model, optimizing their collaboration for knowledgeintensive benchmarks. These methods highlight the importance of tailoring RAG systems to specific tasks, enabling more effective and accurate solutions across diverse domains.",
            "score": 0.6983662324014837,
            "section_title": "C. Retrieval-Augmented Generation",
            "char_start_offset": 69936,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 701
                }
            ],
            "ref_mentions": [
                {
                    "start": 398,
                    "end": 403,
                    "matchedPaperCorpusId": "263605962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "278171164",
            "title": "ComplexVCoder: An LLM-Driven Framework for Systematic Generation of Complex Verilog Code",
            "text": "To overcome the limitations of LLMs in domain-specific tasks, retrieval-augmented generation (RAG) has emerged as a widely adopted technique in LLM-based generation frameworks. RAG enhances the accuracy and domain knowledge of LLMs by incorporating external knowledge sources into the generation process. Instead of relying solely on the internal parameters of the model, RAG enables the LLM to look up relevant information from a designated knowledge base before generating outputs. The key step of the RAG is the retrieval step, where a retriever module queries a database to fetch domain-specific documents or code snippets. The retrieved contents are then integrated into the model's prompt during generation. For instance, ChipNeMo [2] incorporates RAG by retrieving relevant textual passages to enrich prompts for Verilog code generation, supplemented with further parameter fine-tuning to improve output quality. AutoVCoder [17] enhances the retrieval process by fine-tuning the retriever itself using contrastive learning to boost relevance and diversity in the retrieved content. \n\nHowever, current RAG approaches often place significant emphasis on optimizing the retriever, which introduces additional complexity through separate data preparation and model fine-tuning. This underscores the need for a more lightweight yet effective RAG strategy, particularly in simplifying retriever construction without compromising performance.",
            "score": 0.6964154696545095,
            "section_title": "C. Retrieval-Augmented Generation in LLM for EDA",
            "char_start_offset": 12580,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1442
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85693359375
        },
        {
            "corpus_id": "272524830",
            "title": "LLMs Will Always Hallucinate, and We Need to Live With This",
            "text": "Fine-tuning improves how large language models (LLMs) perform on specific tasks, but it does not always ensure that the information they produce is factually accurate. Retrieval-Augmented Generation (RAG) addresses this by combining the strengths of language models with information retrieval systems, allowing the model to generate content based on accurate, up-to-date information. \n\nIn simple terms, RAG works like this: \n\nHere, y is the output, G is the language model, x is the input, and R(x) is the relevant information retrieved from an external knowledge base.",
            "score": 0.6879866463959645,
            "section_title": "Retrieval-Augmented Generation (RAG): Bridging Knowledge Gaps",
            "char_start_offset": 12455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 383
                },
                {
                    "start": 386,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 569
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7431640625
        },
        {
            "corpus_id": "269214364",
            "title": "Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study",
            "text": "Our experimental results demonstrate several important findings:\n\nGains in accuracy from RAG with fine-tuning component models 1. RAG with fine-tuned retriever, fine-tuned generator, or full fine-tuning outperforms the generic RAG.Specifically, using the FinanceBench dataset, we achieved accuracy improvements of up to 20 percentage points over baseline RAG.\n\n2. Fine-tuning the retriever model results in higher accuracy gains compared to fine-tuned generators.This advantage is significant as fine-tuning embedding models for retrieval is less costly and less laborintensive than fine-tuning LLMs for generation.\n\nGains in accuracy from RAG with OODA Reasoning 1. Integrating iterative reasoning capabilities, such as OODA, with the RAG engine substantially enhances performance.Specifically, the Generic RAG with OODA Reasoning configuration achieves an accuracy increase of up to 50 percentage points across the FinanceBench dataset compared to the generic RAG baseline.\n\nFinanceBench dataset, the generic OODA configuration outperformed the fully fine-tuned RAG by a considerable margin of 20-25 percentage points.",
            "score": 0.6870831936349309,
            "section_title": "Key Findings",
            "char_start_offset": 22603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 66,
                    "end": 231
                },
                {
                    "start": 231,
                    "end": 359
                },
                {
                    "start": 361,
                    "end": 463
                },
                {
                    "start": 463,
                    "end": 615
                },
                {
                    "start": 617,
                    "end": 782
                },
                {
                    "start": 782,
                    "end": 975
                },
                {
                    "start": 977,
                    "end": 1120
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87060546875
        },
        {
            "corpus_id": "278339057",
            "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models",
            "text": "Retrieval Fig. 1. Overview of DRO objective. The selection model directly estimate a document permutation for the generator to predict an answer, with both components trained jointly. \n\n1 Introduction Large language models (LLMs) have shown remarkable text generation abilities; however, they often provide factually incorrect content [4,53,73] due to the hallucination [16] or out-of-date information [9]. To mitigate these limitations, retrieval-augmented generation (RAG) is proposed to integrate external retrievers with LLMs, which enables the model to access extensive corpora and retrieve relevant documents for references, thereby enhancing factuality. By integrating the retriever with LLMs, RAG has shown superior performance in knowledge-intensive tasks such as question answering [49,61] and conversational information seeking [5,24,68]. \n\nFollowing the most widely used architecture [9,11,23], RAG typically includes two components to answer an input query: (i) knowledge selection, where retrieval and re-ranking models select target documents, (ii) answer generation, where an LLM generator generates correct answers conditioned on the selected documents. To enhance coverage and improve answer quality, RAG models often provide multiple retrieved documents as input to the generator. The interrelationships among these documents are crucial for final performance [15,28,32,72]. We refer to a specific selection of retrieved documents as a document permutation. \n\nImproving RAG performance. To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45]. \n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25,51].",
            "score": 0.68585837654152,
            "section_title": "Generative selector",
            "char_start_offset": 138,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 18,
                    "end": 44
                },
                {
                    "start": 45,
                    "end": 183
                },
                {
                    "start": 186,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2044
                },
                {
                    "start": 2047,
                    "end": 2158
                }
            ],
            "ref_mentions": [
                {
                    "start": 338,
                    "end": 341,
                    "matchedPaperCorpusId": "271162228"
                },
                {
                    "start": 370,
                    "end": 374,
                    "matchedPaperCorpusId": "265067168"
                },
                {
                    "start": 402,
                    "end": 405,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 796,
                    "end": 799,
                    "matchedPaperCorpusId": "270370889"
                },
                {
                    "start": 842,
                    "end": 845,
                    "matchedPaperCorpusId": "267406766"
                },
                {
                    "start": 845,
                    "end": 848,
                    "matchedPaperCorpusId": "269983269"
                },
                {
                    "start": 896,
                    "end": 899,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 902,
                    "end": 905,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1379,
                    "end": 1383,
                    "matchedPaperCorpusId": "252568176"
                },
                {
                    "start": 1389,
                    "end": 1392,
                    "matchedPaperCorpusId": "269605438"
                },
                {
                    "start": 1645,
                    "end": 1649,
                    "matchedPaperCorpusId": "10986612"
                },
                {
                    "start": 1753,
                    "end": 1757,
                    "matchedPaperCorpusId": "270199429"
                },
                {
                    "start": 2040,
                    "end": 2043,
                    "matchedPaperCorpusId": "269293655"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96044921875
        },
        {
            "corpus_id": "264288947",
            "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
            "text": "Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) augments the input space of LMs with retrieved text passages (Guu et al., 2020;Lewis et al., 2020), leading to large improvements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram et al., 2023). A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by fewshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only once at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation on top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named entities. Yet, the improved task performance of such approaches often comes at the expense of runtime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of attributions (Liu et al., 2023a;Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve generation quality and attributions.",
            "score": 0.6821702588445524,
            "section_title": "RELATED WORK",
            "char_start_offset": 4683,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1265
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 148,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 148,
                    "end": 167,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 881,
                    "end": 902,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 937,
                    "end": 955,
                    "matchedPaperCorpusId": "256459776"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82666015625
        },
        {
            "corpus_id": "273502291",
            "title": "Developing Retrieval Augmented Generation (RAG) based LLM Systems from PDFs: An Experience Report",
            "text": "Choosing between fine-tuning, using Retrieval Augmented Generation (RAG), or base models can be a challenging decision for practitioners. Each approach offers distinct advantages depending on the context and constraints of the use case. This section aims to outline the scenarios in which each method is most effective, providing a decision framework to guide practitioners in selecting the appropriate strategy.",
            "score": 0.6802559411485128,
            "section_title": "When to Use RAG: Considerations for Practitioners",
            "char_start_offset": 3141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 412
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53271484375
        },
        {
            "corpus_id": "254246471",
            "title": "Retrieval as Attention: End-to-end Learning of Retrieval and Reading within a Single Transformer",
            "text": "Starting from the initial model trained on NQ (#1), we first fine-tune the reader while fixing the query encoder (#2), and as expected QA performance improves. However fine-tuning both query encoder and reader (end-to-end #3) makes the retriever collapse with zero relevant documents returned, indicating end-to-end fine-tuning does not work for RAG on new domains. In order to improve both retrieval and QA, we need to fine-tune RAG in a pipeline manner: first fine-tune the re-triever (both query and doc encoder) similarly to DPR using retrieval annotations (#4), then finetune the reader (#5). With the DPR-like fine-tuned retriever, end-to-end fine-tuning of query encoder and reader still fails (#6), although the retriever does not completely collapse. \n\nEnd-to-end fine-tuning of ReAtt improves retrieval and QA simultaneously. Fine-tuning starting from ReAtt trained on NQ is better than starting from T5, indicating the capability learned in NQ could be transferred to BioASQ. Comparing RAG and ReAtt, we identify several keys that enable endto-end adaptation. (1) ReAtt relying on token-level attention has a strong initial performance, (2) crossdocument adjustment over both close and random documents in ReAtt provides a better gradient estimation than only using retrieved documents in RAG, (3) distillation-based loss in ReAtt might be more effective than multiplying the retrieval probability into the final generation probability. \n\nLeveraging Retrieval Annotations As shown on the right-hand side of Tab. 4, ReAtt is able to consume retrieval supervision in a generative format and achieve competitive performance as other supervised dense retrievers. \n\nUnsupervised Adaptation with SSM As shown in Tab. 6, adaptation by simply masking salient entities from sentences as input and generating masked entities using ReAtt improves the retrieval performance on 4 datasets, some by a large margin, achieving comparable or superior performance than strong retrieval adaptation methods such as TSDAE+GPL that relies on query generation.",
            "score": 0.6796512918465704,
            "section_title": "Experimental Results",
            "char_start_offset": 28126,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1447
                },
                {
                    "start": 1450,
                    "end": 1669
                },
                {
                    "start": 1672,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 2048
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83349609375
        },
        {
            "corpus_id": "276287820",
            "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG",
            "text": "Retrieval Augmented Generation: RAG enhances Large Language Models (LLMs) by integrating external data sources, such as knowledge bases, to improve relevance and accuracy (Lewis et al., 2020;Guu et al., 2020;Karpukhin et al., 2020). Recent advancements have extended its applicability across domains (Asai et al., 2024;Kim et al., 2024;Yan et al., 2024;Liu et al., 2024), but RAG systems still face key challenges: hallucinations due to mismatches between retrieved data and the LLM's pre-existing knowledge (Setty et al., 2024;Jin et al., 2024), difficulty with complex multi-document reasoning (Setty et al., 2024), and an inability to fully leverage fixed-domain settings where all domain-specific documents are available beforehand because typically neither the retriever nor the generator LLM are trained on the domain data. Domain-Aware Fine-Tuning for RAG: Joint training of the retriever and LLM has been proposed as a way to improve RAG's domain-specific performance (Guu et al., 2020;Sachan et al., 2021;Siriwardhana et al., 2023;Shi et al., 2024). By jointly training the retriever and LLM, the system can better adapt to domain-specific contexts. However, this approach introduces complexities, including the need for specialized loss functions and frequent retriever updates. \n\nAnother line of work (Mecklenburg et al., 2024;Zhang et al., 2024b) focuses solely on adding domain knowledge to LLMs as an alternative to RAG. These approaches fine-tune LLMs using questionanswer (QA) pairs derived from domain data and aim to answer any new test query without retrieving any document. As a result, they fail to leverage access to the domain documents during inference. \n\nRecently, Zhang et al. introduced Retrieval-Augmented Fine-Tuning (RAFT), a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts.",
            "score": 0.6795535119413307,
            "section_title": "Related Work",
            "char_start_offset": 5971,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1677
                },
                {
                    "start": 1680,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1980
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94189453125
        },
        {
            "corpus_id": "271956620",
            "title": "Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models",
            "text": "By integrating external information, RAG methods enhance reasoning and generation process (Gao et al. 2024;Zhao et al. 2024). Early work primarily focuses on improving retrieval model performance to obtain relevant documents for subsequent generation (Qu et al. 2021;Wang et al. 2023;Zheng et al. 2024). Recent research has expanded RAG framework to real-world noisy scenarios, aiming to build robust RAG systems by enhancing the generator (Fang et al. 2024;Xiang et al. 2024). For instance, Self-RAG (Asai et al. 2024) employs four specialized tokens and GPT-4-generated instruction-tuning data to fine-tune the Llama2 model. RetRobust (Yoran et al. 2024) introduces an automated data generation method to fine-tune the generator to utilize retrieved passages against noise effectively. Ro-bustRAG (Xiang et al. 2024) proposes a defense framework that enhances RAG model robustness against retrieval corruption attacks through an isolate-then-aggregate strategy, achieving certifiable robustness via secure text aggregation techniques. However, these investigations are constrained by their narrow focus on specific noise types and the inherent assumption that noise is harmful, potentially hindering method generalization. This paper aims to present a systematic analysis of RAG noise and reveal its role.",
            "score": 0.6752571992826202,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 4828,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1307
                }
            ],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 302,
                    "matchedPaperCorpusId": "269330213"
                },
                {
                    "start": 440,
                    "end": 458,
                    "matchedPaperCorpusId": "32339717"
                },
                {
                    "start": 501,
                    "end": 518,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 637,
                    "end": 655,
                    "matchedPaperCorpusId": "263608822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62744140625
        },
        {
            "corpus_id": "270560505",
            "title": "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability",
            "text": "The rapid advancements in natural language processing (NLP) have led to the development of sophisticated large language models (LLMs) that excel in tasks such as text generation, summarization, and question answering.Among these advancements, Retrieval-Augmented Generation (RAG) has emerged as a promising approach for the retrieval-based systems with generative models to produce highly accurate and contextually relevant outputs.The concept of Retrieval-Augmented Generation (RAG) was introduced by Lewis et al.In their seminar 2020 paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\" [20].In their research, Lewis et al. present a method that combines retrieval-based and generative models to enhance the performance of knowledge-intensive tasks.By integrating non-parametric memory (retrieved documents) with parametric memory (the generative model's internal parameters), RAG models achieve superior accuracy and flexibility in tasks such as open-domain question answering and abstract question answering.Karpukhin et al. (2020) developed dense passage retrieval for open-domain question answering, which significantly boosts retrieval accuracy by using dense vector representations and a neural retriever [18].More recent work further advances the field by introducing novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments [24].There has been efficient ways to improve the retrieval process such as the Keyword Augmented Retrieval (KAR), which integrates keyword generation using transformer models with document metadata to identify the right context quickly and cost-effectively [23].Also, approach to handle sparse information where classical RAG using hybrid retriever fails to generate correct answers have been reported [17].More recent work by Tay et al. (2023) on the UL2 model and studies on ColBERT by Khattab and Zaharia (2020) have further pushed the boundaries of retrieval and generation synergies in RAG frameworks [19] [25].",
            "score": 0.6742359174084658,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 217,
                    "end": 514
                },
                {
                    "start": 514,
                    "end": 615
                },
                {
                    "start": 615,
                    "end": 779
                },
                {
                    "start": 779,
                    "end": 1040
                },
                {
                    "start": 1040,
                    "end": 1414
                },
                {
                    "start": 1414,
                    "end": 1672
                },
                {
                    "start": 1672,
                    "end": 1817
                },
                {
                    "start": 1817,
                    "end": 2026
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85888671875
        },
        {
            "corpus_id": "270870632",
            "title": "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation",
            "text": "We provide comprehensive ablations on the impact of retrieval quality on generation.We study modern SoTA retrievers -including models from the MTEB benchmark which have been fine-tuned on datasets like NQ.Table 6 lists all the models we consider, and Table 7 present the retrieval performance alongside the generation quality (with and without re-ranking respectively).Overall, we observe that SoTA models from MTEB achieve better performance in both aspects.These results are somewhat expected, as fine-tuning ranking models on the target collection improves ranking quality and therefore the relevance of input contexts.However, it does not measure the \"zero-shot\" performance of the RAG pipeline -especially given the inability of learned retrievers to generalize to out-of-domain collections [77].In the meantime, re-ranking closes the gap between approaches.",
            "score": 0.6740720841742012,
            "section_title": "D. Retrieval Analysis",
            "char_start_offset": 27235,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 84,
                    "end": 205
                },
                {
                    "start": 205,
                    "end": 369
                },
                {
                    "start": 369,
                    "end": 459
                },
                {
                    "start": 459,
                    "end": 622
                },
                {
                    "start": 622,
                    "end": 801
                },
                {
                    "start": 801,
                    "end": 863
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70703125
        },
        {
            "corpus_id": "274965531",
            "title": "XRAG: eXamining the Core - Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) Jiang et al. [2023], Guu et al. [2020], Gao et al. [2023b], Borgeaud et al. [2022], Asai et al. [2024], Singh et al. [2025], Han et al. [2025], Mao et al. [2025] represents a pivotal strategy in Q&A tasks, demonstrating enhanced performance by delivering more informative and accurate answers compared to relying solely on large language models (LLMs). The efficacy of basic RAG systems Du et al. [2024], Lu et al. [2024] is contingent upon the seamless operation of four core components: pre-retrieval, retrieval, post-retrieval, and generation. The preretrieval stage indexes the corpus and reforms queries for efficient retrieval. The retrieval stage focuses on identifying and extracting documents relevant to a given query. The post-retrieval stage refines, summarizes, or compacts information to ensure contextual clarity. Finally, the generation stage employs the LLM to produce responses. These sequential stages critically influence output quality, highlighting the RAG framework's interdependence. Advanced RAG modules (e.g., reranker, refiner) offer sophisticated algorithms for tailored search solutions, surpassing standardized methodologies. \n\nToolkits like LangChain Chase [2022] and LlamaIndex Liu [2022], modularize the RAG process, increasing adaptability and broadening its applications. However, they are typically cumbersome, making adaptation to new data challenging and validating or optimizing innovative methods inconve-  Zhang et al. [2024] (Fair.Comp) indicates evaluation by aligning key components like seeds, generators, retrievers, and instructions. Unified Datasets (Unif.Data) ensures unified dataset formats for retrieval and generation. Modular Evaluation (Mod.Eva) assesses RAG modular differences. Failure Management (Fail.Mgmt) systematically implements strategies for identifying and mitigating RAG failure points. ConR uses token-matching for evaluating retrieval, ConG uses token-matching for evaluating generation, and CogL is based on LLM-based instructions for retrieval and generation evaluation.",
            "score": 0.673754578990642,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1192
                },
                {
                    "start": 1195,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 56,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 97,
                    "end": 119,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 121,
                    "end": 139,
                    "matchedPaperCorpusId": "264288947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64306640625
        },
        {
            "corpus_id": "277781278",
            "title": "UltraRAG: A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) is an effective method for mitigating hallucination and factual inaccuracy issues of large language models (LLMs) (Jiang et al., 2023;Xu et al., 2023;Luo et al., 2023;Hu et al., 2023). It has been widely adopted in various natural language processing (NLP) tasks, such as open-domain QA (Trivedi et al., 2023), language modeling (He et al., 2021), and dialogue (Cai et al., 2019). A typical RAG system consists of two key components, a retriever and a generator (Shi et al., 2023;Yu et al., 2023). The retriever retrieves relevant documents from an external corpus based on the user's query (Karpukhin et al., 2020;Xiong et al., 2021) and the generator utilizes these documents as context of inputs to augment the generation process (Ram et al., 2023;Xu et al., 2023). \n\nWith the continuous advancement of research on RAG systems, recent studies have introduced additional modules and explored various training methods specifically tailored for RAG systems (Yan et al., 2024;Lin et al., 2023;Wang et al., 2023;Wei et al., 2024). For example, Yan et al. (2024) propose an additional retrieval evaluator to refine the quality of retrieved documents. Li et al. (2024) utilize a rollout method to obtain rewards from the entire RAG system for each module and optimize them based on the reward. The success of these approaches highlights the growing need for a general-purpose RAG toolkit, which can streamline development and evaluation across diverse RAG frameworks. \n\nRAG Toolkits. Various RAG toolkits have been developed to assist users in building customized RAG systems, such as LangChain (Chase, 2022) and LlamaIndex (Liu, 2022).",
            "score": 0.6720973344379766,
            "section_title": "Related Work",
            "char_start_offset": 4812,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 837
                },
                {
                    "start": 840,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1532
                },
                {
                    "start": 1535,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1701
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 203,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 356,
                    "end": 378,
                    "matchedPaperCorpusId": "254877499"
                },
                {
                    "start": 398,
                    "end": 415,
                    "matchedPaperCorpusId": "237452184"
                },
                {
                    "start": 430,
                    "end": 448,
                    "matchedPaperCorpusId": "52281331"
                },
                {
                    "start": 660,
                    "end": 684,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 684,
                    "end": 703,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87646484375
        },
        {
            "corpus_id": "278339057",
            "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models",
            "text": "End-to-end training. These methods train RAG components with an end-to-end objective. We benchmark: \n\n(i) Atlas [17], a pre-trained retrieval-augmented LLM; \n\n(ii) RA-DIT [27], which initially trains a generator and subsequently fine-tune a dual-encoder retriever; and (iii) DDR-RAG [25], which employs DPO [39] (Direct Preference Optimization) to jointly train a point-wise ranker and an answer generator. \n\nTo ensure fairness, we set the size of retrieval documents as 20 for all the baselines and the  = 5, aligning with the implementation of DRO. Since the code or model checkpoints are unavailable for some baselines, we mark their incomplete results as \"-\". In such cases, we report results from the original papers but only for reference.",
            "score": 0.669866081257827,
            "section_title": "Baselines",
            "char_start_offset": 29124,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 99
                },
                {
                    "start": 102,
                    "end": 156
                },
                {
                    "start": 159,
                    "end": 406
                },
                {
                    "start": 409,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 745
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 116,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 307,
                    "end": 311,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.857421875
        },
        {
            "corpus_id": "273962778",
            "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
            "text": "In this section, we analyze the efficacy of the two-stage fine-tuning in the Invar-RAG architecture, including the retrieval stage (LLM-aligned Retrieval with Invariance Loss) and the generation stage. We design three variants: (1)w/o representation learning: this variant uses the coarse text representation mapped by small language model (MiniLM-v26 ) to calculate the relevance score and adopt the same generation fine-tuning method in Sec. \u2022 With the representation learning method, LLM-based retrieval contributes to improving the retrieval and corresponding generation performance. \u2022 Invariance loss significantly boosts our designed Invar-RAG by making the prediction rely more on invariant patterns. \u2022 Generative fine-tuning is crucial for enhancing LLM's capability of giving predictions based on retrieved information. Moreover, it shows the effectiveness of the two-stage fine-tuning for a single LLM. \n\nFor ablation results on other two datasets (NQ and PopQA), please found them in Appendix E.",
            "score": 0.6682938977493181,
            "section_title": "Ablation Study",
            "char_start_offset": 19303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 912
                },
                {
                    "start": 915,
                    "end": 1006
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77099609375
        },
        {
            "corpus_id": "263605962",
            "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
            "text": "In comparison, RA-DIT conducts parallel retrieval-augmentation by generating distinct prompts for each retrieved passage and subsequently aggregating the outcomes; SAIL, on the other hand, concatenates the top retrieved passages in the augmentation. Furthermore, RA-DIT adopts a holistic view of the RALM architecture, employing a learnable neural retriever and proposing a dual optimization framework. SAIL, in comparison, leans on commercial search engines and BM25 and focuses on the LM-side enhancement (e.g. it proposes an in-context retrieval selection technique to guide the model focus towards informative content). \n\nAnother family of RALMs incorporate retrieval in the output distribution of the LM (Khandelwal et al., 2020;Zhong et al., 2022). Such models retrieve a set of k nearest-neighbor tokens using the LM context representation, and interpolate this distribution of retrieved tokens with the LM output distribution to generate the next token at inference time. Alternatively, the retrieved token distribution can be used alone to make a non-parametric LM (Min et al., 2023). \n\nInstruction Tuning Instruction fine-tuning has been proposed to align pre-trained LLMs to follow natural language instructions and avoid extensive prompt engineering (Ouyang et al., 2022;Wei et al., 2022;Chung et al., 2022a;Wang et al., 2022;Iyer et al., 2022). We propose retrievalaugmented instruction tuning (RA-IT) as part of our dual instruction tuning framework to improve the LM's ability to leverage retrieved information. \n\nInformation Retrieval Retrieval methods include sparse retrievers that does matching over a sparse bag-of-words representation (Robertson & Zaragoza, 2009;Formal et al., 2021), dense retrievers that embed queries and documents into a fixed-size dense vector for nearest-neighbor search (Karpukhin et al., 2020;Xiong et al., 2021), and multi-vector retrievers which uses multiple vectors as the representation and more complex search algorithms for increased accuracy (Khattab & Zaharia, 2020;Li et al., 2023).",
            "score": 0.6679175597315636,
            "section_title": "RELATED WORK",
            "char_start_offset": 24257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1093
                },
                {
                    "start": 1096,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1526
                },
                {
                    "start": 1529,
                    "end": 2038
                }
            ],
            "ref_mentions": [
                {
                    "start": 709,
                    "end": 734,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 734,
                    "end": 753,
                    "matchedPaperCorpusId": "249062699"
                },
                {
                    "start": 1074,
                    "end": 1092,
                    "matchedPaperCorpusId": "15641339"
                },
                {
                    "start": 1262,
                    "end": 1283,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1283,
                    "end": 1300,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1320,
                    "end": 1338,
                    "matchedPaperCorpusId": "253098274"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86279296875
        },
        {
            "corpus_id": "278033562",
            "title": "MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation",
            "text": "Recent advancements in Retrieval-Augmented Generation (RAG) have spurred the development of various evaluation tools and benchmarks (Gao et al., 2023). However, existing solutions often suffer from limitations, either lacking comprehensive datasets or failing to sufficiently assess retriever performance. Several tools have emerged to evaluate RAG systems, focusing on metrics such as context relevance, answer faithfulness, and answer relevance. For example, RAGAS (Es et al., 2023) provides a framework for evaluating these dimensions of RAG performance. Similarly, ARES (Saad-Falcon et al., 2023) offers an automated evaluation system, utilizing lightweight language model judges fine-tuned on synthetic data to assess both retrieval and generation components. Additionally, RAGCHECKER (Ru et al., 2024) enables detailed analysis of retrieval and generation within RAG systems. While these tools offer valuable insights through diverse evaluation metrics, they often lack dedicated datasets tailored for benchmarking RAG performance comprehensively.",
            "score": 0.6675304310439557,
            "section_title": "RAG Framework",
            "char_start_offset": 5911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1053
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8037109375
        },
        {
            "corpus_id": "273798073",
            "title": "PrefRAG: Preference-Driven Multi-Source Retrieval Augmented Generation",
            "text": "In traditional RAG, finetuning methods are widely employed to enhance the retriever and generator (Lin et al., 2024;Ke et al., 2024). Beyond this, modular RAG systems integrate a series of LLM-based components (Gao et al., 2023). Fine-tuning helps models better follow complex instructions within these components (He et al., 2024), improving RAG systems' performance and task adaptability (Asai et al., 2024;Zhang et al., 2024;Jeong et al., 2024). Classic supervised fine-tuning strategy (SFT) trains only on positive samples. While DPO as a more direct reinforcement learning fine-tuning (RLFT) method, leverages positive-negative sample pairs to effectively and efficiently strengthen LLMs' ability to follow complex instructions. Under the multisource setting, our work thus employs DPO to enhance the model's ability to follow the retrieval selection instruction to select the optimal retrieval source during adaptive retrieval.",
            "score": 0.6665972041104171,
            "section_title": "Related Work",
            "char_start_offset": 6869,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 933
                }
            ],
            "ref_mentions": [
                {
                    "start": 428,
                    "end": 447,
                    "matchedPaperCorpusId": "268553748"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8857421875
        },
        {
            "corpus_id": "271571401",
            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
            "text": "The development of RAG technology can be summarized in three stages. Initially, retrieval-augmented techniques were introduced to improve the performance of pre-trained language models on knowledge-intensive tasks [19], [20]. In specific implementations, Retro [21] optimized pre-trained autoregressive models through retrieval augmentation, while Atlas [22] utilized a retrieval-augmented few-shot fine-tuning method, enabling language models to adapt to diverse tasks. IRCOT [23] further enriched the reasoning process during the inference phase by combining chain-of-thought and multistep retrieval processes. Entering the second stage, as the language processing capabilities of LLMs significantly improved, retrieval-augmented techniques began to serve as a means of supplementing additional knowledge and providing references, aiming to reduce the hallucination. For instance, RRR [24] improved the rewriting phase, and LLMlingua [25] removed redundant tokens in retrieved document chunks. With the continuous progress of RAG technology, research has become more refined and focused, while also achieving innovative integration with other technologies such as graph neural networks [26] and fine-tuning techniques [27]. The overall pipeline has also become more flexible, such as using LLMs to proactively determine the timing of retrieval and generation [14], [28]. \n\nThe development of RAG technology has been accelerated by LLM technology and practical application needs. Researchers are examining and organizing the RAG framework and development pathways from different perspectives. Building upon the enhanced stages of RAG, Gao et al., [2] subdivided RAG into enhancement during pre-training, inference, and fine-tuning stages. Based on the main processes of RAG, relevant works on RAG were organized from the perspectives of retrieval, generation, and augmentation methods. Huang et al., [29] categorize RAG methods into four main classes: pre-retrieval, retrieval, post-retrieval, generation, and provide a detailed discussion of the methods and techniques within each class. Hu et al., [30] discuss Retrieval-Augmented Language Models (RALMs) form three key components, including retrievers, language models, augmentations, and how their interactions lead to different model structures and applications.",
            "score": 0.6664765390576984,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 8318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2318
                }
            ],
            "ref_mentions": [
                {
                    "start": 220,
                    "end": 224,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 261,
                    "end": 265,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 936,
                    "end": 940,
                    "matchedPaperCorpusId": "252186384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81982421875
        },
        {
            "corpus_id": "273229050",
            "title": "Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG",
            "text": "This paper investigates the impact of increasing the number of retrieved passages on the performance of long-context LLMs in retrieval-augmented generation (RAG) systems. Contrary to expectations, we observe that performance initially improve but then degrade as more passages are included. This phenomenon is attributed to the detrimental influence of retrieved \"hard negatives\". To mitigate this issue, we propose and evaluate three solutions: training-free retrieval reordering, RAG-specific implicit LLM fine-tuning, and RAG-oriented LLM fine-tuning with intermediate reasoning. A systematic analysis of the training-based methods explores the effects of data distribution, retriever for training, and training context length. Interesting future directions include exploring (automated) position optimization with more advanced retrieval ordering methods, and fine-tuning the LLMs for RAG with more fine-grained and multi-step reasoning chains. (2) Contriever is more similar to BM25, while bge is more similar to e5 (since their curves are closer respectively).",
            "score": 0.6638367322479122,
            "section_title": "Conclusions",
            "char_start_offset": 29954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1066
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83154296875
        },
        {
            "corpus_id": "268032903",
            "title": "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)",
            "text": "2.1 Retrieval-Augmented Generation (RAG) Retrieval-augmented generation (RAG), first introduced by Lewis et al. (2020), has emerged as one of the most popular approaches to enhance the generation ability of LLMs (Liu, 2022;Chase, 2022;Van Veen et al., 2023;Ram et al., 2023;Shi et al., 2023). This synergy markedly boosts the output's accuracy and relevance (Gao et al., 2023), mitigating essential issues commonly referred to as \"hal-lucinations\" of LLMs (Shuster et al., 2021). One of RAG's distinctive features is its flexible architecture, allowing for the seamless interchange or update of its three core components: the dataset, the retriever, and the LLM. This flexibility means that adjustments to any of these elements can be made without necessitating re-training or fine-tuning of the entire system (Shao et al., 2023;Cheng et al., 2023). These unique advantages have positioned RAG as a favored approach for a range of practical applications, including personal chatbots and specialized domain experts like medical diagnostic assistants (Panagoulias et al., 2024).",
            "score": 0.6631725772632608,
            "section_title": "Related Work",
            "char_start_offset": 4018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1076
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 118,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1049,
                    "end": 1075,
                    "matchedPaperCorpusId": "267112617"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63037109375
        },
        {
            "corpus_id": "270062508",
            "title": "M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions",
            "text": "Retrieval-Augmented Generation.We review the literature of Retrieval-Augmented Generation (RAG) in terms of (1) Naive RAG, (2) Advanced RAG, and (3) Modular RAG.For (1), Naive RAG follows a standard process including indexing, retrieval, and generation (Ma et al., 2023).However, its quality faces significant challenges such as low precision, hallucination, and redundancy during the process.For (2), Advanced RAG is further developed to overcome the shortcomings of Naive RAG.Specifically, during the indexing stage, the objective is to enhance the quality of the indexed content by optimizing data embedding (Li et al., 2023).During the retrieval stage, the focus is on identifying the appropriate context by calculating the similarity between the query and chunks, where the techniques involve fine-tuning embedding models (Xiao et al., 2023), or learning dynamic embeddings for different context (Karpukhin et al., 2020).During the generation stage, it merges the retrieved context with the query as an input into large language models (LLMs), where it addresses challenges posed by context window limits with re-ranking the most relevant content (Jiang et al., 2023b;Zhuang et al., 2023), or compressing prompts (Litman et al., 2020;Xu et al., 2023).In addition, Self-RAG (Asai et al., 2023) is proposed to identify whether retrieval is necessary, or the retrieved context is relevant, which helps language models to produce meaningful generation (Asai et al., 2023).For (3), Modular RAG diverges from the traditional Naive RAG structure by incorporating external modules to further enhance the performance, including search module (Wang et al., 2023a), memory module (Wang et al., 2022;Cheng et al., 2023b), tuning module (Lin et al., 2023), and task adapter (Cheng et al., 2023a;Dai et al., 2023).",
            "score": 0.6628874045253198,
            "section_title": "Related Work",
            "char_start_offset": 6288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 31,
                    "end": 161
                },
                {
                    "start": 161,
                    "end": 271
                },
                {
                    "start": 271,
                    "end": 393
                },
                {
                    "start": 393,
                    "end": 478
                },
                {
                    "start": 478,
                    "end": 629
                },
                {
                    "start": 629,
                    "end": 926
                },
                {
                    "start": 926,
                    "end": 1256
                },
                {
                    "start": 1256,
                    "end": 1473
                },
                {
                    "start": 1473,
                    "end": 1805
                }
            ],
            "ref_mentions": [
                {
                    "start": 611,
                    "end": 628,
                    "matchedPaperCorpusId": "258987900"
                },
                {
                    "start": 901,
                    "end": 925,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1152,
                    "end": 1173,
                    "matchedPaperCorpusId": "263830701"
                },
                {
                    "start": 1173,
                    "end": 1193,
                    "matchedPaperCorpusId": "264406035"
                },
                {
                    "start": 1218,
                    "end": 1239,
                    "matchedPaperCorpusId": "214641123"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.787109375
        },
        {
            "corpus_id": "269292881",
            "title": "Retrieval-Augmented Generation-based Relation Extraction",
            "text": "Retrieval-Augmented Generation (RAG) for large language models can be classified into two categories: i) naive RAG and ii) advanced RAG.Naive RAG has basic steps: retrieve, augmentation, and generation, while the advanced version includes a post-processing step before sending the retrieved information to a user [24].The concept of RAG has been suggested as a way to minimize the undesired alterations in Language Models (LLMs) when conversational systems built on LLMs generate arbitrary responses to a query [7].RAG is an example of open-book exams which are applied to the usage of LLMs.The retriever mechanism in RAG finds an example of the user query (prompt), and then the user query is regenerated along with the example by the data-augmentation module in RAG.Ovadia et al. [25] evaluates the knowledge injection capacities of both fine-tuning and the RAG approach and found that LLMs dealt with performance problems through unsupervised fine-tuning while RAG outperformed the fine-tuning approach in unsupervised learning.\n\nIn this work, we introduce a Retrieval-Augmented Generation-based Relation Extraction (RAG4RE) approach to identify the relationship between a pair of entities in a sentence.",
            "score": 0.6626388562952918,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 8393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 318
                },
                {
                    "start": 318,
                    "end": 515
                },
                {
                    "start": 515,
                    "end": 591
                },
                {
                    "start": 591,
                    "end": 768
                },
                {
                    "start": 768,
                    "end": 1031
                },
                {
                    "start": 1033,
                    "end": 1207
                }
            ],
            "ref_mentions": [
                {
                    "start": 511,
                    "end": 514,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74560546875
        },
        {
            "corpus_id": "270357334",
            "title": "RAG Does Not Work for Enterprises",
            "text": "Retrieval-Augmented Generation (RAG) is an emerging architecture that combines the strengths of pre-trained language models with external knowledge retrieval to enhance the accuracy, consistency, and contextual relevance of generated outputs [ Lewis et al., 2020 ].A typical RAG system consists of three main components: a retriever, a generator, and a knowledge base.\n\nThe retriever is responsible for finding the most relevant documents or passages from the knowledge base given an input query.It uses techniques from information retrieval and semantic search to efficiently search through large collections of text and rank the results based on their similarity to the query [ Karpukhin et al., 2020 ].Advanced retrieval methods may employ dense vector representations, sparse encodings, or a combination of both to capture semantic meaning beyond simple keyword matching [ Zhang et al., 2023 ].\n\nThe generator is a large pre-trained language model, such as GPT-4, Claude Opus or T5, that takes the input query and the retrieved documents as context to generate a final output.The generator is trained to condition its output on both the query and the retrieved knowledge, allowing it to incorporate relevant information and produce more accurate, consistent, and contextually appropriate responses [ Lewis et al., 2020 ].The generator may use techniques like attention, copying, or content selection to effectively fuse the retrieved knowledge with its own learned patterns.\n\nThe knowledge base is a structured or unstructured collection of documents that the RAG system can retrieve from.It can include a wide range of sources, such as web pages, books, articles, databases, or proprietary enterprise data [ Guu et al. 2020, Khandelwal et al., 2020 ].The knowledge base is typically pre-processed and indexed in a way that enables efficient retrieval based on semantic similarity.The quality, coverage, and freshness of the knowledge base are critical factors in the overall performance of the RAG system.\n\nDuring inference, a RAG system works as follows [ Lewis et al., 2020, Lewis et al., 2020, Karpukhin et al., 2020, Izacard and Grave, 2021 ]:\n\n1.The input query is passed to the retriever, which searches the knowledge base and returns a ranked list of relevant documents.",
            "score": 0.6591620654334875,
            "section_title": "Detailed explanation of RAG architecture and components",
            "char_start_offset": 5982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 265,
                    "end": 368
                },
                {
                    "start": 370,
                    "end": 496
                },
                {
                    "start": 496,
                    "end": 705
                },
                {
                    "start": 705,
                    "end": 898
                },
                {
                    "start": 900,
                    "end": 1080
                },
                {
                    "start": 1080,
                    "end": 1325
                },
                {
                    "start": 1325,
                    "end": 1478
                },
                {
                    "start": 1480,
                    "end": 1593
                },
                {
                    "start": 1593,
                    "end": 1756
                },
                {
                    "start": 1756,
                    "end": 1885
                },
                {
                    "start": 1885,
                    "end": 2010
                },
                {
                    "start": 2012,
                    "end": 2152
                },
                {
                    "start": 2154,
                    "end": 2156
                },
                {
                    "start": 2156,
                    "end": 2282
                }
            ],
            "ref_mentions": [
                {
                    "start": 1728,
                    "end": 1755,
                    "matchedPaperCorpusId": "253428554"
                },
                {
                    "start": 2124,
                    "end": 2151,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81787109375
        },
        {
            "corpus_id": "270870251",
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "text": "Ensuring the accuracy of responses generated by Large Language Models (LLMs) such as Chat-GPT [13] and LLaMA [14] is essential.However, simply enlarging model size does not fundamentally address the issue of hallucinations [15,16], especially in knowledge-intensive tasks and specialized domains.Retrieval-augmented generation (RAG) addresses these challenges by retrieving relevant documents from external knowledge bases, providing accurate, real-time, domain-specific context to LLMs [6].Previous works have optimized the RAG pipeline through query and retrieval transformations, enhancing retriever performance, and fine-tuning both the retriever and generator.These optimizations improve the interaction between input queries, retrieval mechanisms, and generation processes, ensuring the accuracy and relevance of responses.",
            "score": 0.6588955747570535,
            "section_title": "Related Work",
            "char_start_offset": 4844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 127,
                    "end": 296
                },
                {
                    "start": 296,
                    "end": 491
                },
                {
                    "start": 491,
                    "end": 665
                },
                {
                    "start": 665,
                    "end": 829
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 230,
                    "matchedPaperCorpusId": "266164171"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9296875
        },
        {
            "corpus_id": "272911196",
            "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
            "text": "We release all models and generated datasets to facilitate further study on HuggingFace1 . In Section 2, we present some related work on RAG, fine tuning methods for RAG including RAFT, and PEFT techniques such as LoRA. In Section 3, we introduce our compute-efficient RAFT method, CRAFT, in detail. In Section 4, we present the experiment setup. In Section 5 we report the results, and in Section 6 we conclude the paper. \n\nRetrieval Augmented Generation (RAG) RAG (Lewis et al., 2021) enhances LLMs by retrieving relevant document chunks from external knowledge bases through semantic similarity calculations. This method mitigates the generation of factually incorrect content by referencing external knowledge rather than relying solely on knowledge the model learned during training, thereby improving the relevancy of the generated text while reducing \"hallucinations\". Despite its advantages, RAG faces challenges, particularly with domain-specific or knowledge-intensive tasks, particularly when handling queries beyond the scope of its retrieved data (Zhang et al., 2023), though at a lesser extent when compared to non-retrieval-augmented LLMs. Other major challenges with RAG includes requiring a high-performing retriever model to produce representative embeddings from the document chunks and retrieval system that balances scale and accuracy. Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024). RAG excels in dynamic environments by offering real-time knowledge updates and effective utilization of external knowledge sources with high interpretability. However, it comes with higher latency and the possibility of added noise from extraneous contexts. \n\nFine tuning for RAG Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever. RAFT (Zhang et al., 2024b) includes a fine tuning strategy that generates training data from the QA target domain data for instruction fine tuning.",
            "score": 0.6580173737316082,
            "section_title": "Introduction",
            "char_start_offset": 4339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 422
                },
                {
                    "start": 425,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2235
                },
                {
                    "start": 2236,
                    "end": 2383
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89990234375
        },
        {
            "corpus_id": "271270644",
            "title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
            "text": "As introduced in Section 6, RAG training includes two branch of works, RAG with/without datastore update. For RAG without datastore update, the main challenge is how to jointly optimize all parameters in RAG. This may involves new loss functions with multiple objectives, new optimizations for efficient tuning parameters in retriever and generator, or other training strategies. \n\nFor RAG with datastore update, one challenge is how to align the retrieval representations with the generator's representations. Although the time cost of the update operation in datastore cannot be ignored, some works [14] reduce the update frequency by asychronously updating, thus achieving the alignment of knowledge representation and model's representation. Another challenge is when to retrain/fine-tune the generator in RAG when new corpus is added. Due to the in-context learning capability of exisitng LLM-based generators and high training overhead, retraining/finetuning the generator or directly inferring the generator becomes a challenging choice for different scenarios. Recently, some efficient training strategies [28,57] have been proposed to accelerate the fine-tuning process, which can be taken into considerations.",
            "score": 0.656839288322697,
            "section_title": "RAG Training",
            "char_start_offset": 59907,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1219
                }
            ],
            "ref_mentions": [
                {
                    "start": 601,
                    "end": 605,
                    "matchedPaperCorpusId": "249191271"
                },
                {
                    "start": 1114,
                    "end": 1118,
                    "matchedPaperCorpusId": "258841328"
                },
                {
                    "start": 1118,
                    "end": 1121,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9140625
        },
        {
            "corpus_id": "270045808",
            "title": "RE-Adapt: Reverse Engineered Adaptation of Large Language Models",
            "text": "Retrieval-augmented generation (RAG) Lewis et al. ( 2020) is a popular alternative for utilizing new data with instruction-tuned models.Instead of altering the model directly, RAG maintains a database of all text and retrieves relevant documents to include in the prompt as context.This begs the question, is RE-Adapt still beneficial if the new data is already available via RAG?\n\nTable 3: QA performance when using RAG with BM25 and (Oracle) retrievers.\n\nretriever is unrealistic in practice, it allows us to further isolate the benefit of combining RAG with fine-tuning by eliminating any impact from imperfect retrieval.\n\nThe RAG results are shown in Table 3. Again we see significant improvements when using RE-Adapt and LoRE-Adapt even in this RAG setting where the model should already have access to the relevant information needed to answer the questions.The BM-25 search retrieved the correct document with approximately 73% accuracy across models.Using RE-Adapt to incorporate the data outside of RAG alleviates the shortcomings of the retriever.However, RE-Adapt also improved results when using the oracle, suggesting that adding domain knowledge with an adapter also reduces incorrect interpretations of the context retrieved via RAG.",
            "score": 0.6563212587476102,
            "section_title": "RE-Adapt with RAG",
            "char_start_offset": 23044,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 282
                },
                {
                    "start": 282,
                    "end": 380
                },
                {
                    "start": 382,
                    "end": 455
                },
                {
                    "start": 457,
                    "end": 624
                },
                {
                    "start": 626,
                    "end": 864
                },
                {
                    "start": 864,
                    "end": 958
                },
                {
                    "start": 958,
                    "end": 1057
                },
                {
                    "start": 1057,
                    "end": 1248
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6416015625
        },
        {
            "corpus_id": "269043117",
            "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers",
            "text": "Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the \u2018Blended RAG\u2019 method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a \u2018Blended Retriever\u2019 to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance.",
            "score": 0.6562538468973445,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8828125
        },
        {
            "corpus_id": "263605962",
            "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
            "text": "In this paper, we propose RA-DIT, a lightweight Retrieval-Augmented Dual Instruction Tuning framework that can effectively retrofit any pre-trained LLM with retrieval capabilities. RA-DIT updates the LLM with retrieval-augmented instruction tuning to make better use of retrieved knowledge and ignore irrelevant or distracting information. It also fine-tunes the retriever with supervision from the LLM to retrieve texts that can better help the LLM generate correct outputs. RA-DIT achieves state-of-the-art performance in zero-and few-shot evaluations on knowledge intensive benchmarks, surpassing un-tuned in-context RALM approaches such as REPLUG and compete effectively against methods that require extensive pre-training such as ATLAS.",
            "score": 0.6560944389264464,
            "section_title": "CONCLUSION",
            "char_start_offset": 26531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 741
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "273695367",
            "title": "RuleRAG: Rule-guided retrieval-augmented generation with language models for question answering",
            "text": "For our proposed RuleRAG-ICL, in addition to adding rule guidance to both retrievers and generators (RG-retriever + RG-generator), we also add rule guidance only to the retrieval stage (RG-retriever + generator), trying to prove that introducing rules in two stages can both contribute to the performance. For our proposed RuleRAG-FT, the complete method involves retrievers and generators with RGFT. The ablation study shows both of them are individually beneficial to the results. To emphasize the contribution of rules, we introduce several variants of RuleRAG-FT. The SSFT in Table 2 represents the standard supervised fine-tuning following the vanilla manner, where the fine-tuning instruction consists only of the queries and retrieved documents without rules. Note that whether or not the inputs are added with rules during inference is consistent with how the models are fine-tuned during training.",
            "score": 0.6558882415072265,
            "section_title": "SETUP OF RULERAG",
            "char_start_offset": 17591,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 906
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87890625
        },
        {
            "corpus_id": "276580741",
            "title": "RankCoT: Refining Knowledge for Retrieval-Augmented Generation through Ranking Chain-of-Thoughts",
            "text": "Modular RAG systems (Gao et al., 2024;Xu et al., 2024c) focus on refining external knowledge through different modules implemented by LLMs, which have become a key trend in the RAG area. For instance, Self-RAG (Asai et al., 2024a) uses different tags for adaptive retrieval (Jiang et al., 2023) and self-reflection to refine knowledge. Some approaches also focus on reformulating queries to identify more useful documents for answering ques-tions (Yan et al., 2024;Trivedi et al., 2023). Yan et al. (2024) introduce a retrieval evaluator that acts as a judge to trigger query reformulation, search, and knowledge refinement actions to supply more accurate evidence for generation. \n\nTo further improve the performance of modular RAG systems, these models focus on fine-tuning various components of the RAG framework. Some efforts aim to align the information needs between the retriever and the generator by optimizing the retrievers based on feedback from the generation models (Yu et al., 2023;Shi et al., 2024;Izacard and Grave, 2021). Lin et al. (2024) adapt LLMs within the RAG setting by constructing instructiontuning data for Supervised Fine-Tuning (SFT), enabling the models to better leverage the retrieved documents. Additionally, Li et al. (2024) use Direct Preference Optimization (DPO) (Rafailov et al., 2024) to jointly optimize the modules in a RAG system, aligning their data preferences.",
            "score": 0.6555963627496948,
            "section_title": "Related Work",
            "char_start_offset": 5892,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 680
                },
                {
                    "start": 683,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1405
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 230,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 274,
                    "end": 294,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 465,
                    "end": 486,
                    "matchedPaperCorpusId": "254877499"
                },
                {
                    "start": 979,
                    "end": 996,
                    "matchedPaperCorpusId": "258960666"
                },
                {
                    "start": 996,
                    "end": 1013,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 1013,
                    "end": 1037,
                    "matchedPaperCorpusId": "227746078"
                },
                {
                    "start": 1300,
                    "end": 1323,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93212890625
        },
        {
            "corpus_id": "271915646",
            "title": "Xinyu: An Efficient LLM-based System for Commentary Generation",
            "text": "The domain of Natural Language Processing (NLP) has witnessed substantial progress [14,21,30,31,42], especially through the advent of Large Language Models (LLMs) [2,17,18,27,35]. These models show exceptional text generation proficiency, yielding high fluency and readability outputs [32,39]. Their ability to adapt to downstream tasks with minimal in-context examples is particularly noteworthy. To further augment the efficacy of LLMs in downstream tasks, two main methods have been identified: supervised fine-Tuning (SFT) and retrieval augmented generation (RAG). \n\nSupervised Fine-Tuning (SFT) entails the adaptation of an LLM to a specific downstream task. This process refines the model's parameters to align with the data distribution and task requirements, ensuring the model's behavior mirrors human behavior within the given domain. The topic of SFT has been extensively explored in numerous research. Ouyang et al. [18] pioneered the introduction of supervised fine-tuning and reinforcement learning to align language models with human intent. Zhou et al. [41] compiled a dataset of merely 1K examples for SFT, demonstrating that the success of SFT depends on the quality and diversity of data. \n\nRetrieval Augmented Generation (RAG) amalgamates LLMs with content retrieved from external databases. This approach offers a promising solution to the challenges encountered by LLMs, such as hallucination, outdated knowledge, and untraceable reasoning processes. The conventional RAG process encompasses indexing, retrieval, and generation [9,15]. RAG has been further enhanced by a range of innovative techniques: fine-tuning retrieval models to obtain precise semantic representations [11,28,33], reformulating queries to align with the semantic space of queries and documents [8,20,29], fine-tuning LLMs to harmonize the output of the retriever with the LLM's preference [10,22,34]. \n\nIn our work, we leverage the advances of both SFT and RAG to enhance the performance of the Xinyu.",
            "score": 0.6551537299517024,
            "section_title": "RELATED WORK 2.1 Large Language Models",
            "char_start_offset": 5295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1895
                },
                {
                    "start": 1898,
                    "end": 1996
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 87,
                    "matchedPaperCorpusId": "259950027"
                },
                {
                    "start": 90,
                    "end": 93,
                    "matchedPaperCorpusId": "226262321"
                },
                {
                    "start": 93,
                    "end": 96,
                    "matchedPaperCorpusId": "259858754"
                },
                {
                    "start": 96,
                    "end": 99,
                    "matchedPaperCorpusId": "254998782"
                },
                {
                    "start": 169,
                    "end": 172,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 928,
                    "end": 932,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87939453125
        },
        {
            "corpus_id": "275357908",
            "title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG",
            "text": "Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying Large Language Models (LLMs), as it can address typical limitations such as generating hallucinated or outdated information. However, when building real-world RAG applications, practical issues arise. First, the retrieved information is generally domain-specific. Since it is computationally expensive to fine-tune LLMs, it is more feasible to fine-tune the retriever to improve the quality of the data included in the LLM input. Second, as more applications are deployed in the same real-world system, one cannot afford to deploy separate retrievers. Moreover, these RAG applications normally retrieve different kinds of data. Our solution is to instruction fine-tune a small retriever encoder on a variety of domain-specific tasks to allow us to deploy one encoder that can serve many use cases, thereby achieving low-cost, scalability, and speed. We show how this encoder generalizes to out-of-domain settings as well as to an unseen retrieval task on real-world enterprise use cases.",
            "score": 0.6547075719841893,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9130859375
        },
        {
            "corpus_id": "273452788",
            "title": "Towards an In-Context LLM-Based Approach for Automating the Definition of Model Views",
            "text": "Fine-tuning and RAG. The performance of off-theshelf LLMs on a given task is strongly dependent on how much the task is covered by their training dataset [18]. To extend the application of LLMs to tasks that require additional task-specific knowledge, the two most common techniques are fine tuning and retrieval-augmented generation (RAG). \n\nFine-tuning enhances an LLM, already pre-trained on a vast and diverse corpus of text, by additional training on new task-specific content. It refines the LLM model with specialized datasets relevant to the targeted task [41]. Retrieval Augmented Generation (RAG) enhances the standard LLM response for specific contextual data. It allows the injection of such data for the targeted task by indexing it in a vector database, and making it directly accessible by the LLM [31]. \n\nBoth techniques show promising results. Still, fine-tuning demands a large dataset of examples and high computational resources [31]. While more accessible, RAG applications still need a fairly large dataset and an infrastructure for the retrieval process [27]. The availability of public dataset is a well-known problem in MDE and, especially for view definition, not many examples are publicly available. Thus, in this paper we do not use any of these techniques, and we study a solution that works directly on off-the-shelf LLMS.",
            "score": 0.6512591622493908,
            "section_title": "2.2.2",
            "char_start_offset": 8057,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 818
                },
                {
                    "start": 821,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1353
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 564,
                    "end": 568,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 813,
                    "end": 817,
                    "matchedPaperCorpusId": "269762693"
                },
                {
                    "start": 949,
                    "end": 953,
                    "matchedPaperCorpusId": "269762693"
                },
                {
                    "start": 1077,
                    "end": 1081,
                    "matchedPaperCorpusId": "265308606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66943359375
        },
        {
            "corpus_id": "269983737",
            "title": "DuetRAG: Collaborative Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Language Models Augmenting language models with relevant information obtained from various external knowledge bases has been shown to significantly improve the performance of various NLP tasks, including language modeling (Guu et al., 2020;Borgeaud et al., 2022;Shi et al., 2023;Lin et al., 2023) and open domain question answering (Izacard et al., 2022;Zhang et al., 2024).RAG mainly adopts the \"retrieve then read\" paradigm.Specifically, the input question is first used as the query, then the retrieval module retrieves relevant documents from the external knowledge base, and finally the retrieved documents and questions are merged into a complete input to generate final output.For example, RETRO (Borgeaud et al., 2022) modifies the autoregressive LM to focus on relevant documents through chunked cross-attention, thereby introducing new parameters to the model.REPLUG (Shi et al., 2023) assumes black-box access to LM and optimizes it by fine-tuning the retriever.RAFT (Zhang et al., 2024) proposes a fine-tuned data that additionally contains relevant documents and answers with reasoning chains to train language models for domain-specific open-book settings.Finetuning for RAG Recently, related work has studied how to improve the overall performance by fine-tuning the LLM or retriever in the RAG framework.For example, RADIT (Lin et al., 2023) proposes a dual-instruction fine-tuning framework to fine-tune both the LLM and the retriever simultaneously.InstructRetro (Wang et al., 2023) pre-trains a larger autoregressive large-scale language model with retrieval function and performs instruction fine-tuning based on it.ChatQA (Liu et al., 2024) additionally proposes a context-enhanced instruction fine-tuning stage, specifically to enhance the model's ability to perform context awareness in conversational QA.RAFT (Zhang et al., 2024) proposes a kind of fine-tuning data that additionally contains related documents and answers with reasoning chains to fine-tune LLM and improve LLM's ability to understand the retrieved documents under the RAG framework.",
            "score": 0.6507983151436671,
            "section_title": "Related Work",
            "char_start_offset": 2461,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 704
                },
                {
                    "start": 704,
                    "end": 890
                },
                {
                    "start": 890,
                    "end": 993
                },
                {
                    "start": 993,
                    "end": 1190
                },
                {
                    "start": 1190,
                    "end": 1340
                },
                {
                    "start": 1340,
                    "end": 1487
                },
                {
                    "start": 1487,
                    "end": 1656
                },
                {
                    "start": 1656,
                    "end": 1848
                },
                {
                    "start": 1848,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 260,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 260,
                    "end": 282,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 723,
                    "end": 746,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89697265625
        },
        {
            "corpus_id": "273812370",
            "title": "Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors",
            "text": "Retrieval-augmented generation (RAG) integrates traditional language modeling with dynamic external data retrieval to address limitations such as data recency and relevance in response generation (Lewis et al., 2020;Izacard & Grave, 2021;Cheng et al., 2024;Zhang et al., 2024a). A typical RAG system consists of three main components: a retriever, a language model generator, and an extensive knowledge database. \n\nRetriever: This component employs an encoder to encode input queries and retrieve the top-k relevant documents from the knowledge base. The retriever function f r maps a query Q to a subset of documents D k \u2282 D, where D denotes the entire knowledge database. More formally, this can be expressed as: \n\nLLM generator: Post retrieval, the generator, often a pre-trained model, synthesizes the final text output based on the contextual information extracted from the retrieved documents (Lewis et al., 2020). The generator's function f g integrates the context from the retrieved documents to enhance the relevance and accuracy of the generated content, given by: \n\nKnowledge database: Serving as the repository of information, the knowledge base contains diverse sources such as Wikipedia, news articles, and domain-specific literature, providing the factual backbone for retrieval operations. The integration of these components allows RAG systems to produce contextually enriched responses, effectively minimizing issues like hallucinations often seen in standalone language models. Moreover, the ability to dynamically pull information from updated sources ensures that the responses generated are not only relevant but also accurate (Guu et al., 2020).",
            "score": 0.6497889508841175,
            "section_title": "Retrieval-augmented generation",
            "char_start_offset": 7939,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1669
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 216,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 216,
                    "end": 238,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 238,
                    "end": 257,
                    "matchedPaperCorpusId": "258479968"
                },
                {
                    "start": 899,
                    "end": 919,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1650,
                    "end": 1668,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82421875
        },
        {
            "corpus_id": "267750557",
            "title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning",
            "text": "In this work, we have introduced Mafin, a novel methodology for fine-tuning black-box embedding models, thereby addressing a significant gap in the field of Retrieval Augmented Generation (RAG). \n\nRecognizing the need for enhanced performance in black-box embedding models, especially when applied to new documents or within specific domains, Mafin effectively meets this challenge by augmenting a black-box model with a small tunable embedding model thus significantly boosting its performance while only requiring a minimal fine-tuning cost. This method leverages both the powerful language representation provided by large pre-trained models and the benefits of fine-tuning with a small embedding model. The low fine-tuning cost of the small models enables its use for large-scale, customized online fine-tuning tailored to each company and individual, promising to be a performance-effective and cost-efficient framework for the RAG infrastructure. \n\nWe have demonstrated Mafin's excellence in fine-tuning embedding models for text retrieval tasks within the RAG framework. Future work will explore Mafin's potential across a broader range of fields. We aim to test and validate our fine-tuning methodology's effectiveness in tasks such as classification and clustering, thereby further expanding the applicability and impact of our approach. \n\nhttp://arxiv.org/ps/2402.12177v4",
            "score": 0.6467826558671823,
            "section_title": "Conclusion",
            "char_start_offset": 23095,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 197,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1381
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "276408682",
            "title": "Improving Retrieval-Augmented Deep Assertion Generation via Joint Training",
            "text": "Experimental Design. In this section, we investigate how the proposed joint training strategy module affects performance in the retrieval-augmented generation setting. We first consider a \"No retriever\" baseline that directly finetunes the generator with input focal-tests and their assertions in the training datasets. We then compare our jointtraining retriever in AG-RAG with different retrievers: (1) a \"Random Retriever\" baseline that utilizes random sampling as the retriever; (2) an \"IR Retriever\" baseline that utilizes IR as the retriever, following Yu et al. [16] and Sun et al. [17]; \n\n(3) a \"Pre-trained Retriever\" baseline that utilizes a pretrained Codet5 without any fine-tuning as the retriever; (4) a \"Fine-tuned Retriever\" baseline that first fine-tunes a  pre-trained CodeT5 using the training set, and utilizes the trained CodeT5 as the retriever. \n\nResults. Table 4 presents the comparison results of our default retriever and baselines. Overall, we find that the default joint retriever achieves the best performance on all metrics and datasets. Particularly, IR Retriever achieves 57.26% for accuracy, 72.54% for CodeBLEU, and 73.57% for BLEU on average, outperforming No Retriever by 7.48%, 5.66%, and 5.30%, demonstrating the benefits of retrieving similar TAPs in guiding the assertion generation process and motivating AG-RAG to explore more powerful PLMbased retriever. Meanwhile, the downgraded performance of Random Retriever implies that randomly retrieved TAPs cannot provide helpful guiding signals due to the inherent noise in the randomly retrieved data, which lacks relevance. Besides, fine-tuning CodeT5 is able to retrieve more useful TAPs for the generator than the default CodeT5 model, with a prediction accuracy of 64.19% and 53.40% on both datasets. The possible reason lies in that, compared with the default CodeT5, fine-tuning CodeT5 incorporates knowledge of assertion generation, which improves its ability to generate more effective embeddings for retrieval.",
            "score": 0.6460188216932207,
            "section_title": "RQ2: Analysis of Joint Training",
            "char_start_offset": 35524,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 569,
                    "end": 573,
                    "matchedPaperCorpusId": "248347700"
                },
                {
                    "start": 589,
                    "end": 593,
                    "matchedPaperCorpusId": "261945147"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.896484375
        },
        {
            "corpus_id": "271329405",
            "title": "MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) combines traditional language models with external databases to improve natural language processing (NLP) tasks [33], [34], [35].RAG models use a retriever to retrieve relevant information and a generator to generate answers based on the retrieved information.This improves accuracy and relevance, especially for domainspecific queries [36], [16].RAG's strengths lie in its ability to update knowledge bases without retraining and customise components for specific tasks such as cybersecurity [33], [34].However, RAG struggles with latency and scalability issues, especially when processing concurrent queries [33].Despite these limitations, RAG remains a versatile tool for a range of NLP applications, from chatbots to content creation.Ongoing research focuses on optimizing retrieval mechanisms and computational power [36], [16].",
            "score": 0.6423407474857168,
            "section_title": "B. Retrieval Augmented Generation",
            "char_start_offset": 7134,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 166,
                    "end": 297
                },
                {
                    "start": 297,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 652
                },
                {
                    "start": 652,
                    "end": 775
                },
                {
                    "start": 775,
                    "end": 870
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 153,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 155,
                    "end": 159,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 373,
                    "end": 377,
                    "matchedPaperCorpusId": "3541996"
                },
                {
                    "start": 379,
                    "end": 383,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 536,
                    "end": 540,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 647,
                    "end": 651,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 859,
                    "end": 863,
                    "matchedPaperCorpusId": "3541996"
                },
                {
                    "start": 865,
                    "end": 869,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.693359375
        },
        {
            "corpus_id": "270878612",
            "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs",
            "text": "Large language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG). In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data. For generation, we compare our model with many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks. Specifically, our Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In addition, it also performs comparably to GPT-4 on five RAG benchmarks in the biomedical domain without instruction fine-tuning on biomedical data, demonstrating its superb capability for generalization to new domains.",
            "score": 0.641627344701692,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9140625
        },
        {
            "corpus_id": "268248396",
            "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
            "text": "The non-parametric knowledge injection is performed using RAG, which consists of two components: the Retriever and the Generator [6,15]. Retriever. The first key component in a RAG system is a retriever , which builds an index for a document corpus . During inference, given an input sequence , the retriever identifies and ranks relevant documents   = (, ). In our retrieval process, we employ both sparse and dense retrievers. We utilize BM25 [44] as a sparse retriever due to its popularity and effectiveness. For dense retrievers, we employ DPR [28] and Contriever [22] methods. Both models convert textual data into vector representations using a transformer network. The similarity between the query  and document  is defined as  (, ) = \u00ec  \u2022 \u00ec , which computes the dot product between embedding vectors \u00ec  and \u00ec . DPR employs two independent BERT models, trained discriminatively using querydocuments pairs with negative samples from BM25. Contriever, on the other hand, is trained using a shared BERT model for query and document encoding, optimized using a contrastive loss. We also employ a two-step retrieval pipeline, which includes first-stage retrieval using BM25 and reranking using DPR [1, 7,34]. Generator. The second step involves a generator component responsible for synthesizing an answer, typically implemented via LMs. Generative LMs operate by predicting the probability distribution of the next token, given the previous tokens. In RAG, the generative LM takes a query  and top- ranked documents from   , denoted as    = [ 1 , ...,   ], and generates a response by sequentially predicting the next token. Our RAG prompt prepends the documents before the query, following [15,39]. \n\nIn this paper, we define and assess four distinct configurations of injecting knowledge with fine tuning and RAG: (1) -FT-RAG: the vanilla LM without retrieved documents, (2) -FT+RAG: the vanilla LM with retrieved documents, (3) +FT-RAG: the fine-tuned LM without retrieved documents, (4) +FT+RAG: the fine-tuned LM with retrieved documents.",
            "score": 0.639326167541662,
            "section_title": "Knowledge Injection with RAG",
            "char_start_offset": 16008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1703
                },
                {
                    "start": 1706,
                    "end": 2047
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 135,
                    "matchedPaperCorpusId": "267301416"
                },
                {
                    "start": 445,
                    "end": 449,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 549,
                    "end": 553,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 569,
                    "end": 573,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 1205,
                    "end": 1207,
                    "matchedPaperCorpusId": "264350276"
                },
                {
                    "start": 1695,
                    "end": 1699,
                    "matchedPaperCorpusId": "267301416"
                },
                {
                    "start": 1699,
                    "end": 1702,
                    "matchedPaperCorpusId": "254877603"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86865234375
        },
        {
            "corpus_id": "270199806",
            "title": "Phantom: General Trigger Attacks on Retrieval Augmented Language Generation",
            "text": "Retrieval Augmented Generation (RAG). RAG is a technique used to ground the responses of an LLM generator to a textual corpus which may help minimize hallucinations [46] and help ensure response freshness, without requiring expensive fine-tuning or re-training operations. RAG systems use two main components: a retriever and a generator. RAG Retriever. A knowledge base is a set of documents collected either from the user's local file system or from external sources such as Wikipedia and news articles. The retriever is a separately trained embedding model that produces document embeddings in a vector space [12; 21; 24]. The retriever model operates over passages, which are contiguous, fixed-size sequences of tokens in a document. Given a user's query , the retriever generates encodings of the query E  and encodings E  of all documents passages  in the knowledge base. The top- most similar passages, as identified by the similar score sim(E  , E  ), are selected. These document passages are then aggregated in a prompt that is forwarded, together with the user query, to the generator. LLM Generator. This is an LLM typically trained with the autoregressive next-token prediction objective. We will consider instruction trained models that are subsequently fine-tuned with safety alignment objectives -Harmlessness, Helpfulness, and Honesty (HHH) -, such as GPT-4 [1] or Llama 3 [49]. The LLM is given as input the system prompt (examples in Figure 1), a user's query  and the top- retrieved passages-this enables personalization and grounding. The main advantage of RAG over other personalization methods (e.g, fine-tuning the LLM on users' data) is the relatively low computation cost. This is because several pre-trained retriever models are publicly available and computing similarity scores with the knowledge database is in general inexpensive. Attacks on RAG. Here we summarize the emergent research thread concerning attacks against RAG systems. We direct the reader to Appendix A.1 for a broader discussion of related work. [58] introduce corpus poisoning attacks on RAG systems, although these are focused towards the retriever, and have no adversarial objective on the generator.",
            "score": 0.6384301019930555,
            "section_title": "Background and Related Work",
            "char_start_offset": 4882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2201
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66357421875
        },
        {
            "corpus_id": "276421958",
            "title": "Agentic Medical Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge",
            "text": "for QA \n\nRetrieval Augmented Generation (RAG) is a framework designed to enhance QA by integrating relevant external knowledge into the generation process. The framework combines retriever and generator components to ensure responses are grounded in evidence. Below, we outline various approaches within the RAG paradigm:",
            "score": 0.6367201988906995,
            "section_title": "Retrieval Augmented Generation (RAG)",
            "char_start_offset": 7608,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 6
                },
                {
                    "start": 9,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 321
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71728515625
        },
        {
            "corpus_id": "269757277",
            "title": "ERAGent: Enhancing Retrieval-Augmented Language Models with Improved Accuracy, Efficiency, and Personalization",
            "text": "Retrieval Augmented Generation (RAG) (Lewis et al., 2020) leverages a retriever that provides substantial external information for improving the generated output of LLMs.This strategy utilizes knowledge in a parameter-free manner, circumvents the high training costs of LLMs' parameterized knowledge.Furthermore, it alleviates the hallucination issues in LLMs, significantly enhancing the factual accuracy and relevance of the generated content.The concept of RAG is rooted in the DrQA framework (Chen et al., 2017), which marked the initial phase of integrating retrieval mechanisms with Language Models (LMs) through heuristic retrievers like TF-IDF for sourcing evidence.Subsequently, RAG underwent evolution with the introduction of Dense Passage Retrieval (Karpukhin et al., 2020), and further advancements in RAG (Lewis et al., 2020) and REALM (Ram et al., 2023).These methods utilize pre-trained transformers and are characterized by the joint optimization of retrieval and generation components.Recent advancements have extended RAG's capabilities by integrating Large Language Models (LLMs).Exemplary developments such as RE-PLUG (Shi et al., 2023) and IC-RALM (Ram et al., 2023) demonstrate the potent generalization abilities of LLMs in zero-shot or few-shot scenarios.These models are capable of following complex instructions, understanding retrieved information, and utilizing limited demonstrations to generate high-quality responses.",
            "score": 0.6337552204483254,
            "section_title": "Retrieval Augmented Generation",
            "char_start_offset": 6872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 170,
                    "end": 300
                },
                {
                    "start": 300,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 674
                },
                {
                    "start": 674,
                    "end": 869
                },
                {
                    "start": 869,
                    "end": 1003
                },
                {
                    "start": 1003,
                    "end": 1100
                },
                {
                    "start": 1100,
                    "end": 1280
                },
                {
                    "start": 1280,
                    "end": 1449
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 819,
                    "end": 839,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8125
        },
        {
            "corpus_id": "270878612",
            "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs",
            "text": "Effect of Designed Components.Table 3 shows the ablations of RankRAG with Llama3 8B as the backbone on nine general-domain datasets.Overall, we observe all of the proposed components contribute to the final performance.Removing context ranking hurts performance on all tasks, justifying its efficacy in selecting the most relevant contexts for the target question.Besides, the retrieval-augmented QA (RQA) and retrieval-augmented ranking (RAR) designed for instruction finetuning improve outcomes on most tasks by helping the model explicitly pinpoint relevant contexts.On the contrary, the RAFT method used in (Lin et al., 2024)   Performance with Different Retrievers. Figure 3 exhibits the performance of RankRAG and ChatQA-1.5 with different dense retrievers on three representative tasks, where we consider DPR (Karpukhin et al., 2020) and Contriever-MS MARCO (Izacard et al., 2022) as two variants.We note that although the initial retrieved result is not good enough, RankRAG still surpasses ChatQA-1.5 by more than 10% for both retrievers on average.To summarize, RankRAG is robust to the choice of retrievers.To demonstrate that RankRAG can adapt to specialized domains, we conduct experiments on Mirage (Xiong et al., 2024), a recently introduced RAG benchmark for the biomedical field.We follow Xiong et al. (2024) to employ MedCPT (Jin et al., 2023) as the retriever R with MedCorp3 as the corpus D.",
            "score": 0.6320796847345851,
            "section_title": "Ablation Studies",
            "char_start_offset": 24434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 30,
                    "end": 132
                },
                {
                    "start": 132,
                    "end": 219
                },
                {
                    "start": 219,
                    "end": 364
                },
                {
                    "start": 364,
                    "end": 570
                },
                {
                    "start": 570,
                    "end": 904
                },
                {
                    "start": 904,
                    "end": 1058
                },
                {
                    "start": 1058,
                    "end": 1118
                },
                {
                    "start": 1118,
                    "end": 1296
                },
                {
                    "start": 1296,
                    "end": 1411
                }
            ],
            "ref_mentions": [
                {
                    "start": 611,
                    "end": 629,
                    "matchedPaperCorpusId": "263605962"
                },
                {
                    "start": 816,
                    "end": 840,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60693359375
        },
        {
            "corpus_id": "269502216",
            "title": "On the Evaluation of Machine-Generated Reports",
            "text": "Retrieval-Augmented Generation.Early retrieval augmented generation systems have been evaluated using task-specific metrics on end-to-end tasks.For example, in the context of question answering, exact match and  1 metrics have been used [30,41].For summarization, ROUGE and BERTScore on reference summaries are common [26].These approaches have two limitations: they only measure ability to complete end tasks, and thus cannot assess intermediate stages or evaluate generation across multiple dimensions; and they are not well-suited to capture failures that can be introduced by current generative models [27].\n\nMore recently, techniques have proposed to more holistically evaluate RAG systems.Gienapp et al. [25] introduce a theoretical framework for evaluating ad hoc generative retrieval.Chen et al. [11] focus on robustness of RAG systems against various perturbations.Thakur et al. [82] benchmark hallucinations and the ability of RAG systems to identify relevant information for 18 languages.Others have introduced benchmarks to measure the ability of RAG systems to provide citations [6,23,53,90].While not specifically  designed for RAG applications, metrics designed to evaluate factuality (e.g., FactScore [58]) or faithful manipulation of long inputs (e.g., BooookScore [10]) can complement application-specific evaluation frameworks.\n\nMost approaches to automated evaluation aim to estimate the effectiveness of RAG systems across desirable dimensions (e.g., faithfulness, answer relevance, and context relevance).Techniques include prompting LLMs to evaluate generated summaries [76], and fine-tuning lightweight models on synthetic data [73].Downstream applications, such as question answering, can also be used to evaluate the effectiveness of RAG systems [74].",
            "score": 0.6309402424111245,
            "section_title": "3.2.4",
            "char_start_offset": 19611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 31,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 245
                },
                {
                    "start": 245,
                    "end": 323
                },
                {
                    "start": 323,
                    "end": 611
                },
                {
                    "start": 613,
                    "end": 695
                },
                {
                    "start": 695,
                    "end": 792
                },
                {
                    "start": 792,
                    "end": 874
                },
                {
                    "start": 874,
                    "end": 999
                },
                {
                    "start": 999,
                    "end": 1105
                },
                {
                    "start": 1105,
                    "end": 1346
                },
                {
                    "start": 1348,
                    "end": 1527
                },
                {
                    "start": 1527,
                    "end": 1657
                },
                {
                    "start": 1657,
                    "end": 1777
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "258865156"
                },
                {
                    "start": 1101,
                    "end": 1104,
                    "matchedPaperCorpusId": "258587884"
                },
                {
                    "start": 1772,
                    "end": 1776,
                    "matchedPaperCorpusId": "238207962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6962890625
        },
        {
            "corpus_id": "259064131",
            "title": "TimelineQA: A Benchmark for Question Answering over Timelines",
            "text": "Furthermore, by fine-tuning the retrievers on the episodic data, we get a significant boost in performance for both extractive and rag setups, indicating that the QA systems do not generalize well to episodic data, and that improving retrieval is crucial to getting good performance from these models, particularly for RAG. After fine-tuning, the generative model performance still lags behind the extractive setup.",
            "score": 0.6303225617605936,
            "section_title": "Atomic QA",
            "char_start_offset": 27036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 415
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "267320876",
            "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
            "text": "LLMs excel in text generation but also confront challenges such as outdated knowledge and the generation of hallucinatory content [6,19,43]. In response to these challenges, RAG, also referred to as RALM (Retrieval-Augmented Language Models), incorporates external knowledge to generate responses characterized by enhanced accuracy and realism [47]. This is particularly critical in domains that heavily depend on precision and reliability, including but not limited to the legal, medical, and financial sectors. Retrieval models have been promoting the development of language models [15,33,59]. \n\nConventional RAG systems adhere to a standardized workflow encompassing indexing, retrieval, and generation phases [28,36]. The indexing phase encompasses data cleansing, extraction, transformation into plain text, segmentation, and indexing, utilizing embedding models to transform text fragments into vector representations [2,18]. In the retrieval phase, the system computes similarity scores based on the user's query to select the most pertinent text fragments. In the generation phase, the query and selected documents are amalgamated into prompts, facilitating the LLMs in generating a response. While this method is straightforward, it encounters challenges related to retrieval quality, generation quality, and enhancement processes [21,23]. \n\nIn response to these challenges, researchers concentrate on the enhancement of the retriever, a task that can be categorized into three key aspects: pre-retrieval processing, retrieval model optimization, and post-retrieval processing [20]. Pre-retrieval processing encompasses data transformer to enhance text standardization, ensuring factual accuracy, optimizing index structures, adjusting block sizes, and rewriting query [4,16,50,52]. Retrieval model optimization entails the fine-tuning of domain-specific embedding models and the application of dynamic embedding techniques [11,60]. Post-retrieval processing minimizes context length through reranking and compression operations, aiming to emphasize critical information, diminish noise interference, and enhance integration and utilization by the generator [37,53,55]. Furthermore, to enhance the precision and efficiency of the generator when handling retrieval content, scholars have undertaken a series of optimization measures. As an illustration, researchers have devised methods such as Chain-of-Note (CON) for the generator [58].",
            "score": 0.6297886226223467,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 7293,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 596
                },
                {
                    "start": 599,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1349
                },
                {
                    "start": 1352,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2179
                },
                {
                    "start": 2180,
                    "end": 2342
                },
                {
                    "start": 2343,
                    "end": 2447
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 133,
                    "matchedPaperCorpusId": "224706057"
                },
                {
                    "start": 136,
                    "end": 139,
                    "matchedPaperCorpusId": "233231373"
                },
                {
                    "start": 585,
                    "end": 589,
                    "matchedPaperCorpusId": "18739283"
                },
                {
                    "start": 589,
                    "end": 592,
                    "matchedPaperCorpusId": "267377589"
                },
                {
                    "start": 592,
                    "end": 595,
                    "matchedPaperCorpusId": "207670589"
                },
                {
                    "start": 714,
                    "end": 718,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 925,
                    "end": 928,
                    "matchedPaperCorpusId": "263866951"
                },
                {
                    "start": 1345,
                    "end": 1348,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 1782,
                    "end": 1785,
                    "matchedPaperCorpusId": "254877046"
                },
                {
                    "start": 1785,
                    "end": 1788,
                    "matchedPaperCorpusId": "257505063"
                },
                {
                    "start": 1934,
                    "end": 1938,
                    "matchedPaperCorpusId": "252519173"
                },
                {
                    "start": 2168,
                    "end": 2172,
                    "matchedPaperCorpusId": "257532405"
                },
                {
                    "start": 2175,
                    "end": 2178,
                    "matchedPaperCorpusId": "264590451"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8359375
        },
        {
            "corpus_id": "273695367",
            "title": "RuleRAG: Rule-guided retrieval-augmented generation with language models for question answering",
            "text": "The Standard RAG framework has better performance than the above non-retrieval or self-verifying methods, highlighting the importance of retrieved documents for knowledge-intensive queries. However, their low performance is still unsatisfactory, suggesting that their principles of retrieval and generation are weak and leave much to be desired. In the experiments, we illustrate that the performance can be further improved under the guidance of rules from two perspectives: through in-context learning (ICL) in RuleRAG-ICL and through RGFT in RuleRAG-FT. \n\nFor RuleRAG-ICL (RG-DPR + LLAMA2_7B), introducing rules in the retrieval stage alone enhances the recall performance of the retriever and further improves the answer accuracy of the original LLAMA2_7B. RuleRAG-ICL (RG-DPR + RG-LLAMA2_7B) consistently surpasses Standard RAG across various metrics (+9.3 in R@10, +5.9 in EM and +3.2 in T-F1 on average absolute performance over all five benchmarks), achieving the improved performance. This confirms the sub-optimal ability of the current RAG and the effectiveness of our proposed dual rule-guided retriever and generator. \n\nFor RuleRAG-FT, our proposed RGFT can amazingly improve performance by a significant margin (+45.7 in R@10, +24.2 in EM and +15.3 in T-F1 compared to the best performance of RuleRAG-ICL). To further corroborate that these gains are due to the introduced rules, we first isolate the key component, rules, from fine-tuning data F R for RGFT, to form the standard supervised fine-tuning (SSFT) (Rule Ablation in Table 2) and then isolate the impact of the fine-tuned generator from the fine-tuned retriever in RuleRAG-FT (RGFT Ablation in Table 2). RGFT Ablation shows both RGFT-DPR and RGFT-LLAMA2_7B are beneficial when used individually, implicitly suggesting that the two phases do not depend on each other.",
            "score": 0.6295988080557207,
            "section_title": "MAIN RESULTS",
            "char_start_offset": 21869,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 556
                },
                {
                    "start": 559,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1841
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.861328125
        },
        {
            "corpus_id": "275336185",
            "title": "Foundations of GenIR",
            "text": "In addition, since retrieval results are fed as a part of the LLM inputs, whether the LLMs can generate the response based on the retrieved documents instead of their internal knowledge could be seen as a special type of instruction-following ability. Studies have been conducted to teach LLMs to utilize retrieval results faithfully and constantly in RAG systems [125] On the other hand, factors such as irrelevant results and ranking perturbations are well acknowledged to be harmful for the performance of generators in RAG, so there are also studies that try to improve the robustness of LLMs from the perspective of RAG. For example, Zhang et al. [126] proposes to fine tune LLMs with the presence of retrieval results (i.e., retrieval augmented fine tuning) so that LLMs can learn the domain-specific knowledge introduced by the retriever and improve their robustness against potential distracting information from retrieval. \n\nFrom the perspective of augmentation methods, existing research mostly focuses on the joint optimization of RAG system as a whole. In other words, the loss functions of RAG optimization should be built from the performance metrics of downstream tasks directly. While this paradigm is appealing, it often has strict requirements on the design of RAG systems. Particularly, it's difficult to apply such joint optimization algorithms on a RAG system in which retrievers and generators are loosely connected through prompts constructed from discrete retrieval results. While reinforcement learning could solve the problem in theory, its empirical performance when being used as the solo optimization algorithms for ranking systems is still not satisfying at this point [127]. If you already have a good retriever and only conduct fine-tuning with a fixed LLM, then it may work [128], but this still doesn't look like a perfect solution because reinforcement learning usually subject to large variance in practice. To the best of our knowledge, how to directly connect the training of retrievers with the auto-regressive loss of the generators in RAG is still an open question. Answering this question requires us to go deep into the structure of generative AI models and retrieval models, and develop new model structures that can take advantages from studies on both sides.",
            "score": 0.6292419307555146,
            "section_title": "Optimization of Retrieval and Generation",
            "char_start_offset": 42008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2304
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8759765625
        },
        {
            "corpus_id": "268091298",
            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
            "text": "Lastly, we wish to outline several potential directions for future RAG research and applications. \n\n1) Novel Design of Augmentation Methodologies: Existing research has explored various interaction patterns between retrievers and generators. However, due to distinct objectives in these two components, the practical augmentation process has a significant impact on the final generation results. Investigation of more advanced foundations for augmentation holds promise for fully unleashing the potential of RAG. \n\n2) Flexible RAG Pipelines: RAG systems are progressively embracing flexible pipelines, such as recursive, adaptive, and iterative RAG. With precise tuning and meticulous engineering, the unique blend of retrieval sources, retrievers, generators, and RAG subsystems promises to tackle complex tasks and boost overall performance. We eagerly anticipate pioneering exploration that will drive the evolution of even more innovative RAG systems. \n\n3) Broader Applications: RAG is a general technique applied in various applications. However, some generative tasks have not yet explored RAG, and in many domains, RAG is applied naively without considering the domain's unique characteristics. We believe designing domain-specific RAG techniques will significantly benefit broader applications. \n\n4) Efficient Deployment and Processing: There exist several deployment solutions for query-based RAG with LLMs, such as LangChain [350], LLAMA-Index [135], and PipeRAG [351]. However, for other RAG foundations and/or generation tasks, there lacks a plug-and-play solution. Besides, due to retrieval overhead and increasing complexities in retrievers and generators, achieving efficient RAG is still challenging and necessitates further system-level optimizations. \n\n5) Incorporating Long-tail and Real-time Knowledge: While a key motivation of RAG is to harness real-time and long-tail knowledge, few studies have explored the pipeline for knowledge updating and expansion. Many existing works use merely the generators' training data as retrieval sources, neglecting the dynamic and flexible information that retrieval could offer. As a consequence, there is a growing research on designing RAG systems with continuously updated knowledge and flexible sources. We also expect RAG to step further, adapting to personalized information in today's web service.",
            "score": 0.6274104801635265,
            "section_title": "B. Potential Future Directions",
            "char_start_offset": 79468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 100,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 512
                },
                {
                    "start": 515,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1302
                },
                {
                    "start": 1305,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1768
                },
                {
                    "start": 1771,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2137
                },
                {
                    "start": 2138,
                    "end": 2266
                },
                {
                    "start": 2267,
                    "end": 2363
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86474609375
        },
        {
            "corpus_id": "276617922",
            "title": "Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems",
            "text": "Retrieval Augmented Generation (RAG) has emerged as a popular technique for improving the performance of Large Language Models (LLMs) on question-answering tasks over specific datasets. A benefit of using RAG pipelines is that they can often achieve high performance on specific tasks without the need for extensive alignment and fine-tuning (Gupta et al., 2024), a costly and timeconsuming process. However, the end-to-end pipeline of a RAG system is dependent on many parameters that span different components (or modules) of the system, such as the choice of LLM, the embedding model used in retrieval, the number of chunks retrieved and hyperparameters governing a reranking model. Examples of choices, parameters, and hyperparameters that are often made or tuned when implementing a RAG pipeline are listed in Table 1. Importantly, the performance of a RAG pipeline is dependent on these choices (Fu et al., 2024), many of which can be difficult to tune manually. While those building RAG pipelines might avoid fine-tuning costs, they often spend time and resources on hyperparameter optimization (HO). \n\nDespite this, there is little research exploring methods for collectively optimizing all the hyperparameters in a given LLM and RAG pipeline (Fu et al., 2024). Further, to the best of our knowledge, there is no work that addresses this challenge in multi-objective settings, where the RAG pipeline must achieve high performance across a range of objectives, like minimizing a system's inference time while maximizing its helpfulness. In this work, we aim to fill this gap by introducing an approach for collectively optimizing the hyperparameters of a RAG system in a multi-objective setting. \n\nFigure 1: A high-level overview of our approach. First, we source the datasets that we will use to optimize our RAG pipeline, define the choices, parameters and hyperparameters that will be optimized over (see Table 1), and select the objectives for optimization (e.g., cost, latency, safety, and alignment). Second, we introduce a train-test paradigm for evaluating the performance of RAG pipelines, and use Bayesian optimization (BO) to find the optimal parameter configurations.",
            "score": 0.6270289729349465,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2186
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82568359375
        },
        {
            "corpus_id": "273798011",
            "title": "Unified Generative and Discriminative Training for Multi-modal Large Language Models",
            "text": "Early models for retrieval primarily focused on word representations [16,64,74], with minimal generative capabilities. Some recent works have endeavored to fine-tune generative pre-trained LLMs to generate discriminative embeddings, albeit at the expense of compromising the model's original generative capabilities [44,70,65,63,24,71]. GRIT [66] integrates generative and discriminative tasks in NLP and demonstrates mutual benefits between them. However, its training cost is prohibitively high compared to individual tasks. Moreover, due to its specialized attention mechanism, the model can only be trained from scratch. \n\nRetrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) [25,69], which harnesses the advanced inference capabilities of LLMs along with external knowledge, has the potential to significantly mitigate issues related to long-tail entities and reduce the occurrence of hallucina-tory responses [29,36,101,77,90,92,97]. Recently, there have also been related studies in the multimodal domain attempting to utilize retrieval augmentation [93,96]. These methods typically require an additional retrieval module (e.g., CLIP), leading to component optimization challenges where the overall model performance is affected by the performance of the retrieval model, as well as concerns regarding the compatibility between the retrieval model and the MLLMs. Furthermore, retrieval modules like CLIP struggle to handle compositional or fine-grained scenarios, posing certain challenges for retrieval.",
            "score": 0.6269043074964757,
            "section_title": "Related Work",
            "char_start_offset": 8945,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 624
                },
                {
                    "start": 627,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1527
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 76,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 931,
                    "end": 935,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 935,
                    "end": 938,
                    "matchedPaperCorpusId": "261033863"
                },
                {
                    "start": 951,
                    "end": 954,
                    "matchedPaperCorpusId": "265445443"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6064453125
        },
        {
            "corpus_id": "269740933",
            "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
            "text": "With effective fine-tuning, bi-encoder retrievers are also applied widely in ICL-based RAG [82,93,101,111,126,176]. Specifically, they have been more often used for sentence embedding similarity-based retrieval, as well as for some special requirement in ICL, such as diverse example retrieval [176].\n\nAnother stream of dense retrievers having been widely applied in RA-LLMs uses one encoder only, which may be based on Transformer, BERT or other off-the-shelf sequence modeling backbones.These one-encoder retrievers are generally pre-trained on largescale unaligned documents by contrastive learning [122], which may therefore excel for their versatility, meaning that they can transfer and generalize better to new domains or tasks.Such generalpurpose pre-trained retrievers, e.g., Contriever [42] and Spider [118], would be more flexible to use in LLMs targeting on various tasks and have demonstrated their effectiveness in many RA-LLM methods, such as In-Context RALM [117], Atlas [55] and Self-RAG [5].According to experimental results in existing studies [182], for opendomain QA tasks, when cooperated with InstructGPT [107], applying general-purpose pre-trained retriever (Contriever) without fine-tuning achieves comparable performance to sparse retriever (BM25).However, they are both worse than the DPR model finetuned on target datasets, showing the effectiveness of fine-tuning on targeted tasks and data.\n\n3.1.2Retrieval Granularity.Retrieval granularity denotes the retrieval unit in which the corpus is indexed, e.g., document, passage, token, or other levels like entity.For RAG, the choice of retrieval  granularity can significantly impact the overall performance of the model in terms of effectiveness and efficiency as they determine the saving space for the database as well as the computational cost for searching [4].",
            "score": 0.6267963285138218,
            "section_title": "Retrieval",
            "char_start_offset": 12890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 300
                },
                {
                    "start": 302,
                    "end": 489
                },
                {
                    "start": 489,
                    "end": 735
                },
                {
                    "start": 735,
                    "end": 1009
                },
                {
                    "start": 1009,
                    "end": 1274
                },
                {
                    "start": 1274,
                    "end": 1420
                },
                {
                    "start": 1422,
                    "end": 1427
                },
                {
                    "start": 1427,
                    "end": 1449
                },
                {
                    "start": 1449,
                    "end": 1590
                },
                {
                    "start": 1590,
                    "end": 1843
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 95,
                    "matchedPaperCorpusId": "258564230"
                },
                {
                    "start": 98,
                    "end": 102,
                    "matchedPaperCorpusId": "262063582"
                },
                {
                    "start": 106,
                    "end": 110,
                    "matchedPaperCorpusId": "245218561"
                },
                {
                    "start": 110,
                    "end": 114,
                    "matchedPaperCorpusId": "256826793"
                },
                {
                    "start": 294,
                    "end": 299,
                    "matchedPaperCorpusId": "256826793"
                },
                {
                    "start": 796,
                    "end": 800,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 812,
                    "end": 817,
                    "matchedPaperCorpusId": "245144844"
                },
                {
                    "start": 974,
                    "end": 979,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 987,
                    "end": 991,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1005,
                    "end": 1008,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 1128,
                    "end": 1133,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70263671875
        },
        {
            "corpus_id": "268513072",
            "title": "Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI",
            "text": "This method modifies only a small portion of the model's weights, reducing the number of parameters that need to be updated during fine-tuning.By focusing on these adaptable components, LoRA efficiently updates the model, maintaining performance while significantly lowering computational demands and memory usage.\n\nImproving the performance of Retrieval Augmented Generation (RAG) involves several strategic enhancements across data preparation, indexing, and query handling.To reduce computational time, we can explore various index types for better context retrieval.Additionally, we can also transform queries to better match the retrieval context.Each of these tactics aims at refining the interaction between the LLM and the data, ensuring more accurate, relevant, and efficient generation outcomes [20].",
            "score": 0.62662587312533,
            "section_title": "B. Challenges in the Generative Models",
            "char_start_offset": 7772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 143,
                    "end": 314
                },
                {
                    "start": 316,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 570
                },
                {
                    "start": 570,
                    "end": 652
                },
                {
                    "start": 652,
                    "end": 810
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58935546875
        },
        {
            "corpus_id": "273185619",
            "title": "Contrastive Learning to Improve Retrieval for Real-World Fact Checking",
            "text": "Retrieval-augmented generation (RAG) relies on two key modules: a retriever and a reader/generation model. For many RAG systems, noisy retrieval hurts downstream performance by providing irrelevant or misleading documents (Yoran et al., 2024). Sauchuk et al. (2022) found that adding distractors can cause a 27% drop on veracity classification accuracy on FEVER. Therefore, it's important for retrievers to find relevant documents and simultaneously avoid damaging ones. Shi et al. (2023) attempts to solve this problem by finetuning the retrieval component while fixing the reader LM, similar to our work. Other approaches like Ke et al. (2024) create a more complex system with a \"bridging\" model between the retriever and reader. Nevertheless, noisy retrieval remains a failure point in RAG systems (Barnett et al., 2024), and tangible downstream gains can be realized by further finetuning.",
            "score": 0.6264539890680402,
            "section_title": "Retrieval Augmented Generation Systems",
            "char_start_offset": 5329,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 894
                }
            ],
            "ref_mentions": [
                {
                    "start": 244,
                    "end": 265,
                    "matchedPaperCorpusId": "250340232"
                },
                {
                    "start": 802,
                    "end": 824,
                    "matchedPaperCorpusId": "266933076"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91357421875
        },
        {
            "corpus_id": "273963552",
            "title": "Clustering Algorithms and RAG Enhancing Semi-Supervised Text Classification with Large LLMs",
            "text": "Retrieval Augmented Generation (RAG) [9], a method first introduced in 2020, gained popularity after the emergence of LLM. RAG offers benefits such as associating answers with professional knowledge without undergoing training, thereby enhancing the answer quality and reducing hallucinations. This is achieved by first retrieving relevant information from external knowledge, mainly through embedding models, and subsequently combining the retrieved information into the input prompt of LLMs to generate answers. There are a number of works aiming at improving RAG pipeline from various aspects such as query rewrite, document rerank, and post-retrieval processing [10]. The RAG framework proved to be highly suitable for text classification tasks. In this regard, [11] utilized retrieval-enhanced LLMs to generate text classification datasetes under zero-shot condition. Moreover, [12] proposed a Retrieval-Augmented framework to alleviate poor generalization issues existed in text classification taskes. \n\nHowever, for methods like RAG or CoT that do not entail fine-tuning, the outputs might not always comply with the instructions in the prompt, while fine-tuning enables adjustments of the behaviors of LLMs, such as adhering to specific writing styles [10]. At the same time, adding content to the prompt and generating reasoning prior to answering leads to increased computation costs. For instance, as of July 2024, for GPT-4-o, it is $5 per 1M input and $15 per 1M output, and the cost for text-embedding-3-large is $0.13 .This can be depleted rapidly when we have a large volume of data and if we aim to apply CoT and RAG to every data point. In production scenarios where there is a massive amount of data and the answer is required as soon as possible, a smaller yet quicker specialized model that directly outputs the answer is much preferred. While [13] reveals that LLMs can improve themselves through the utilization of CoT prompting and self-consistency to generate high-quality answers from unlabeled data and fine-tuning based on these generated answers. Studies such as [14] have further demonstrated that fine-tuning a model with fewer parameters through knowledge distillation from the CoT outputs of larger LLMs results in improved task performance on datasets like GSM8K.",
            "score": 0.6263821353725556,
            "section_title": "Introduction",
            "char_start_offset": 2349,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 1865,
                    "end": 1869,
                    "matchedPaperCorpusId": "253080328"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64990234375
        },
        {
            "corpus_id": "268091298",
            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
            "text": "Despite significant advancements in generative models, AIGC still grapples with challenges like outdated knowledge, lack of long-tail knowledge [27], and risks of leaking private training data [28]. Retrieval-Augmented Generation (RAG) aims to mitigate these issues with its flexible data repository [29]. The retrievable knowledge acts as non-parametric memory, which is easily updatable, accommodates extensive long-tail knowledge, and can encode confidential data. Moreover, retrieval can lower generation costs. RAG can reduce the size of large models [30], support long contexts [31], and eliminate certain generation steps [32]. \n\nA typical RAG process is depicted in Fig. 1. Given an input query, the retriever identifies relevant data sources, and the retrieved information interacts with the generator to improve the generation process. There are several foundational paradigms (foundations in short) according to how the retrieved results augment the generation: they can serve as augmented input to the generator [33], [34]; they can join at the middle stage of generation as latent representations [35], [36]; they can contribute to the final generation results in the form of logits [37], [38]; they can even influence or omit certain generation steps [32], [39]. Additionally, researchers have proposed various enhancements to improve the foundational RAG process. These methods encompass specific optimizations for individual components as well as holistic enhancements aimed Fig. 1: A generic RAG architecture. The user queries, spanning different modalities, serve as input to both the retriever and the generator. The retriever extracts relevant information from data sources. The generator interacts with the retrieval results and ultimately produces outcomes of various modalities. at the entire pipeline. \n\nIn addition, while the concept of RAG initially emerged in text-to-text generation [34], this technique has also found applications across various domains, including codes [40]- [42], audios [43], [44], images [45]- [47], videos [48], [49], 3D [50], [51], knowledge [52]- [54], and AI for science [55], [56].",
            "score": 0.6252009056352348,
            "section_title": "I. INTRODUCTION A. Background",
            "char_start_offset": 1995,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1825
                },
                {
                    "start": 1828,
                    "end": 2136
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 148,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 193,
                    "end": 197,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1024,
                    "end": 1028,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1110,
                    "end": 1114,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 1116,
                    "end": 1120,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1202,
                    "end": 1206,
                    "matchedPaperCorpusId": "237452184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89306640625
        },
        {
            "corpus_id": "268248396",
            "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
            "text": "In this paper, we aimed to determine the most suitable approach for customizing language models (LMs) for less-resourced domains. We examined the effectiveness of retrieval augmented generation (RAG) and fine-tuning (FT) methods, focusing on four key aspects: (i) Table 7: SRAG performance. By adding the extracted hint to the top of the input prompt, SRAG's performance surpasses other settings. Statistically significant differences in the SRAG(S) and SRAG(D) columns are compared with the -FT+RAG and +FT+RAG columns. Superscripts (a) and (b) denote statistically significant differences (better or worse) compared to -FT+RAG and +FT+RAG, respectively, as determined by the Wilcoxon test (p-value < 0.01). fine-tuning methods, specifically full fine-tuning versus parameterefficient fine-tuning (PEFT), (ii) data augmentation techniques, (iii) the type and size of LMs, including decoder-only versus encoderdecoder models ranging from 80 million to 11 billion parameters, and (iv) the performance of retrieval models. Our findings reveal several key points. First, PEFT enhances downstream task performance and preserves the reasoning abilities of LMs while incorporating new knowledge. Second, prompt-based QA generation exhibits superior performance in factual QA tasks. Third, a small fine-tuned LM with RAG can perform on par with or even surpass a larger LM model. Additionally, RAG's performance improves with higher-performing retrievers. Notably, when comparing knowledge injection methods, RAG significantly outperforms FT. We addressed the cost of fine-tuning by developing Stimulus RAG (SRAG), a novel RAG approach that prompts an LM to generate correct responses based on hints provided in the prompt. This method eliminates the need for extensive fine-tuning, making it a cost-effective solution for enhancing LM performance in less-resourced domains.",
            "score": 0.6242984850210156,
            "section_title": "DISCUSSION AND CONCLUSIONS",
            "char_start_offset": 33941,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1867
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5205078125
        },
        {
            "corpus_id": "271534349",
            "title": "RLCoder: Reinforcement Learning for Repository-Level Code Completion",
            "text": "Retrieval-augmented generation (RAG) [23] is an approach that enhances the quality of generation by retrieving from external knowledge bases. This method includes three key components [24]: retriever, generator, and augmentation techniques. The retriever is used to find relevant information from a large-scale dataset or knowledge base, including pertinent documents, facts, or text snippets that are relevant to the input query or prompt. The retrieved information is fed into the generator, which integrates this external knowledge into the generation stage. Augmentation techniques focus on how retrieved information is integrated into the generation process. To formalize the RAG process, consider a scenario where we want to generate code based on a query q and a set of retrieved candidates {c 1 , c 2 , ..., c n }. The process can be described by the following formula: \n\nwhere the Retrieve(\u2022) function selects the most relevant candidates based on the query q from the candidate set {c 1 , c 2 , ..., c n }, and the Generate(\u2022) function then takes the query and the retrieved candidates to generate the target code. \n\nIn recent years, researchers have conducted a substantial amount of research related to RAG, highlighting its promising potential for future applications [23], [25]- [28]. Many studies have utilized RAG for code-related research [29]- [42]. In repository-level code completion, due to the massive amount of code in the repository and limited context of generator [12], it is impractical to use the entire repository as the context for generation. Therefore, most current methods employ the RAG method to retrieve suitable candidates from the repository for generation [12], [13], [16], [17].",
            "score": 0.6240433422260382,
            "section_title": "A. Retrieval-Augmented Generation",
            "char_start_offset": 6053,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 1124
                },
                {
                    "start": 1127,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1718
                }
            ],
            "ref_mentions": [
                {
                    "start": 1362,
                    "end": 1366,
                    "matchedPaperCorpusId": "258170012"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82861328125
        },
        {
            "corpus_id": "276543876",
            "title": "Geographic Named Entity Matching and Evaluation Recommendation Using Multi-Objective Tasks: A Study Integrating a Large Language Model (LLM) and Retrieval-Augmented Generation (RAG)",
            "text": "Common deep learning models include LSTM and GRU [44,45]. With the development of large language model [46][47][48] technology, the Retrieval-Augmented Generation (RAG) framework was proposed initially by Patrick Lewis et al. [49] to address the limitations of large language models in handling domainspecific issues. Gao et al. reintroduced the RAG approach to mitigate the 'hallucination' phenomenon of large language models. Various RAG styles have since been proposed, generally categorized into Naive RAG, Advanced RAG, and Modular RAG, each focusing on different retrieval, generation, and enhancement techniques [50]. This framework enhances prediction capabilities by retrieving relevant information from a vast number of documents during the question-answering or text-generation stage and incorporating this information into the response. For example, Lin et al. [51] used the Retrieval-Augmented Dual Instruction Tuning (RA-DIT) method to enable pre-trained large language models with retrieval capabilities, significantly improving their performance in knowledge-intensive tasks. Wang-Chiew Tan et al. [52] innovated the RAG model architecture to improve the efficiency and accuracy of query answering tasks, while Lin et al. [53] enhanced retrieval performance and answer quality in visual question-answering tasks through fine-grained multimodal retrieval. Inspired by these developments, we propose an RAG-based address recommendation framework that uses new addresses and related information as an external knowledge base, combined with matching addresses from Geographical Named Entity Matching Models, to conduct address recommendation tasks.",
            "score": 0.6238020662490171,
            "section_title": "Geographical Text Recommendation Based on Retrieval-Augmented Generation",
            "char_start_offset": 10342,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 58,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1660
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 53,
                    "matchedPaperCorpusId": "199510402"
                },
                {
                    "start": 53,
                    "end": 56,
                    "matchedPaperCorpusId": "233699286"
                },
                {
                    "start": 226,
                    "end": 230,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1238,
                    "end": 1242,
                    "matchedPaperCorpusId": "263310932"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8203125
        },
        {
            "corpus_id": "271039066",
            "title": "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning",
            "text": "This study explores Parameter-Efficient Fine-Tuning (PEFT) methods applied to Retrieval-Augmented Generation (RAG) models, comparing GPT and RETRO architectures. RETRO generally outperforms GPT in zero-shot settings due to their pre-training process that integrates external retrieval, enhancing contextual understanding. However, GPT models show a higher performance potential with PEFT, indicating more room for improvement during fine-tuning. Both RETRO and GPT models perform optimally around the 8B parameter mark, balancing cost and performance. While P-tuning is effective in larger models, it lags behind other methods in smaller models, particularly for RETRO. Applying PEFT to Instructiontuned RETRO yields limited improvement compared to base RETRO, suggesting a saturation point in leveraging pre-training and fine-tuning benefits. Our comprehensive analysis offers valuable insights for optimizing large language models with PEFT and RAG to the community.",
            "score": 0.6231194339163172,
            "section_title": "Conclusion",
            "char_start_offset": 13407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 968
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85205078125
        },
        {
            "corpus_id": "273502262",
            "title": "Customized FinGPT Search Agents Using Foundation Models",
            "text": "Fine-tuning adapts generic LLMs to specific tasks or domains. Recent techniques like Low-Rank Adaptation (LoRA) [14] and (QLoRA) [9] improves efficiency and reduces memory usage of tuning. As shown in these works [14] [9] [20], these methods indeed specializes LLMs in financial applications. However, fine-tuning alone is insufficient for addressing the time-sensitiveness [19] of financial data as this data constant updates. To overcome this, we combine fine-tuning with Retrieval Augmented Generation (RAG).",
            "score": 0.6218255052739109,
            "section_title": "LLM Finetuning Methods",
            "char_start_offset": 5767,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 511
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 116,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 129,
                    "end": 132,
                    "matchedPaperCorpusId": "258841328"
                },
                {
                    "start": 213,
                    "end": 217,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 374,
                    "end": 378,
                    "matchedPaperCorpusId": "258331934"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86376953125
        },
        {
            "corpus_id": "268554288",
            "title": "FIT-RAG: Black-Box RAG with Factual Information and Token Reduction",
            "text": "This joint training helps the generation model better utilize the retrieved information, and improves model synergy and generalization performance.However, this approach becomes impractical when the generation module is a large language model, which can have billions of parameters.On one hand, fine-tuning the full LLM is often infeasible due to the massive computational resources required; on the other hand, many existing LLMs are only accessible via APIs [30,31] and cannot be fine-tuned.\n\nTo overcome the infeasibility of fine-tuning LLMs in RAG, black-box RAG, which alternatively regards a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it without fine-tuning, has achieved success in knowledgeintensive tasks and gained much attention.Existing black-box RAG methods [36,44,53,54] typically fine-tune the retriever only based on LLMs' preferences (e.g., whether LLMs can give correct answer with the retrieved documents) and concatenate all the retrieved documents as the input, which suffers both effectiveness and efficiency issues.Only considering LLMs' preferences in retrieval causes ignorance of factual information, which can degenerate the effectiveness of RAG for it may mislead the retriever.As demonstrated in Figure 1, the LLM can answer correctly with the retrieved documents, but the documents themselves do not actually contain relevant factual information for the given question.For example, Q1 asks the location of the State Hermitage Museum; however, the retrieved document Fig. 2. The overview of FIT-RAG provides information about the Museum of Moscow.Although the LLM can give the correct answer, the retrieved document is actually unnecessary.If these unnecessary documents is used to reward the retriever, they can mislead the retriever.Besides, concatenating all the retrieved documents as the input causes waste of tokens, which can introduce excessive unnecessary tokens and hurts the efficiency of RAG.",
            "score": 0.6203805404660758,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2033,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 147,
                    "end": 282
                },
                {
                    "start": 282,
                    "end": 493
                },
                {
                    "start": 495,
                    "end": 769
                },
                {
                    "start": 769,
                    "end": 1067
                },
                {
                    "start": 1067,
                    "end": 1235
                },
                {
                    "start": 1235,
                    "end": 1428
                },
                {
                    "start": 1428,
                    "end": 1605
                },
                {
                    "start": 1605,
                    "end": 1698
                },
                {
                    "start": 1698,
                    "end": 1793
                },
                {
                    "start": 1793,
                    "end": 1962
                }
            ],
            "ref_mentions": [
                {
                    "start": 804,
                    "end": 807,
                    "matchedPaperCorpusId": "263828724"
                },
                {
                    "start": 810,
                    "end": 813,
                    "matchedPaperCorpusId": "258960666"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4130859375
        },
        {
            "corpus_id": "273969615",
            "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
            "text": "Retrieval-Augmented Generation (RAG) addresses the limitations of traditional generative models in handling specialized or long-tail knowledge. Early models like GPT, trained on vast corpora, excel at general queries but struggle with domain-specific or rare information, often generating hallucinations [95]. RAG, introduced by Facebook AI Research in 2020 [96], enhances generative models by integrating realtime document retrieval, improving accuracy and contextual grounding. Gao et al. [97] categorize RAG into Naive, Advanced, and Modular paradigms, detailing key components like retrievers, generators, and augmentation methods. A comparative study by Ovadia et al. [98] shows that RAG outperforms unsupervised fine-tuning, particularly in scenarios involving new or unseen knowledge, underscoring its superiority in knowledge injection and model adaptation. \n\nThe effectiveness of RAG systems heavily depends on the quality and relevance of the retrieved knowledge, which directly influences the accuracy and factual grounding of generated content. To enhance retrieval efficiency and overcome the limitations of traditional methods, several advancements have been proposed, particularly for zero-shot and few-shot retrieval tasks. Techniques such as HyDE [99] and REINA [100] utilize LLMs to generate hypothetical documents, improving retrieval performance without requiring labeled data. The Rewrite-Retrieve-Read [101] framework introduces a query rewriting step, allowing the input query to be better aligned with retrieval modules. By using reinforcement learning to adapt queries, R3 enhances retrieval quality, improving performance in open-domain and multiple-choice question answering tasks. Promptagator [102] demonstrates the effectiveness of few-shot learning in dense retrieval, utilizing LLMs to generate synthetic training data from minimal examples, surpassing models trained on large-scale datasets like MS MARCO. This underscores the viability of few-shot learning and LLM-generated synthetic data in resource-constrained settings. To bridge the preference gap between retrievers and LLMs, Zixuan Ke et al. [103] introduce the BGM framework, which employs a sequence-to-sequence model to align retrieved information with LLM preferences.",
            "score": 0.6196400090700381,
            "section_title": "C. Retrieval-Augmented Generation",
            "char_start_offset": 65622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2263
                }
            ],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 362,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1264,
                    "end": 1268,
                    "matchedPaperCorpusId": "254877046"
                },
                {
                    "start": 1424,
                    "end": 1429,
                    "matchedPaperCorpusId": "258841283"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81591796875
        },
        {
            "corpus_id": "269982691",
            "title": "FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research",
            "text": "To validate the effectiveness of FlashRAG, we conduct a series of experiments for providing reproducible benchmarking results and facilitating further exploration. In our main experiment, by default, we employ the latest LLaMA-3-8B-instruct [109] model as the generator and E5-base-v2 as the retriever. Some methods may fine-tune their own models in different RAG components, and we provide the details of them in Appendix A. \n\nMethods. We evaluate the performance of all supported RAG methods. These methods are categorized based on the RAG component they primarily aim to optimize: AAR [101] aims at optimizing the retriever; LongLLMLingua [49], RECOMP [52], Selective-Context [50] and Trace [102] focus on refining and compressing the input; Ret-Robust [104], Spring [103] and REPLUG [97] aim to enhance the generator and its decoding approaches; SKR [37], Adaptive-RAG [38] introduce the judger to decide the necessity of retrieval for a query; SuRe [98], Self-RAG [11], FLARE [12], Iter-RetGen [99], RQRAG [106] and ITRG [100] optimize the entire RAG flow, including using multi-round retrieval and generation processes.",
            "score": 0.6196224125823485,
            "section_title": "Textual Experimental Result and Discussion",
            "char_start_offset": 14257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 1125
                }
            ],
            "ref_mentions": [
                {
                    "start": 588,
                    "end": 593,
                    "matchedPaperCorpusId": "258960666"
                },
                {
                    "start": 787,
                    "end": 791,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 854,
                    "end": 858,
                    "matchedPaperCorpusId": "263828724"
                },
                {
                    "start": 873,
                    "end": 877,
                    "matchedPaperCorpusId": "268553748"
                },
                {
                    "start": 954,
                    "end": 958,
                    "matchedPaperCorpusId": "269293435"
                },
                {
                    "start": 981,
                    "end": 985,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 999,
                    "end": 1003,
                    "matchedPaperCorpusId": "258866037"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8544921875
        },
        {
            "corpus_id": "270285886",
            "title": "XL-HeadTags: Leveraging Multimodal Retrieval Augmentation for the Multilingual Generation of News Headlines and Tags",
            "text": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020a) involves two key phases: first, retrieving contextually relevant information, and sec-ond, using this information to guide the generation process (Zhao et al., 2023).RAG has been applied to various tasks such as machine translation (He et al., 2021), dialogue generation (Cai et al., 2019), and abstractive summarization (Peng et al., 2019).Inspired by these applications, we leverage multimodal information like images and captions to select salient content from news articles, introducing MultiRAGen (Multimodal Retrieval Augmented Generation).MultiRAGen comprises two main components: (1) Multimodal Retrievers ( \u00a73.1) and (2) Instruction Tuning ( \u00a73.2).",
            "score": 0.6194881357560933,
            "section_title": "MultiRAGen",
            "char_start_offset": 8401,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 225,
                    "end": 400
                },
                {
                    "start": 400,
                    "end": 605
                },
                {
                    "start": 605,
                    "end": 716
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 224,
                    "matchedPaperCorpusId": "257632157"
                },
                {
                    "start": 380,
                    "end": 399,
                    "matchedPaperCorpusId": "202540017"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67919921875
        },
        {
            "corpus_id": "273346467",
            "title": "FunnelRAG: A Coarse-to-Fine Progressive Retrieval Paradigm for RAG",
            "text": "Retrieval-Augmented Generation (RAG) prevails in Large Language Models. It mainly consists of retrieval and generation. The retrieval modules (a.k.a. retrievers) aim to find useful information used to facilitate the generation modules (a.k.a. generators). As such, generators' performance largely depends on the effectiveness and efficiency of retrievers. However, the widely used retrieval paradigm remains flat. It treats retrieval procedures as a one-off deal with constant granularity. Despite effectiveness, we argue that they suffer from two limitations: (1) flat retrieval exerts a significant burden on one retriever; (2) constant granularity limits the ceiling of retrieval performance. In this work, we propose a progressive retrieval paradigm with coarse-to-fine granularity for RAG, termed FunnelRAG, so as to balance effectiveness and efficiency. Specifically, FunnelRAG establishes a progressive retrieval pipeline by collaborating coarse-to-fine granularity, large-to-small quantity, and low-to-high capacity, which can relieve the burden on one retriever and also promote the ceiling of retrieval performance. Extensive experiments manifest that FunnelRAG achieves comparable retrieval performance while the time overhead is reduced by nearly 40 percent.",
            "score": 0.6189344114673185,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.828125
        },
        {
            "corpus_id": "263605962",
            "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
            "text": "Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average.",
            "score": 0.6183732558070992,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96435546875
        },
        {
            "corpus_id": "270560495",
            "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models",
            "text": "Large Language Models (LLMs) have the unique capability to understand and generate human-like text from input queries. When fine-tuned, these models show enhanced performance on domain-specific queries. OpenAI highlights the process of fine-tuning, stating:\"To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples, but the right number varies greatly based on the exact use case.\"This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI. This study highlights the need for vigorous investigation and validation of fine-tuned models for domain-specific tasks.",
            "score": 0.6182779256008579,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81005859375
        },
        {
            "corpus_id": "263605962",
            "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
            "text": "We report the main results in Table 2. In particular, RA-DIT is compared to LLAMA (Touvron et al., 2023a) as well as REPLUG (Shi et al., 2023b), in both 0-shot and 5-shot settings. We first observe that REPLUG works much better than the base LLAMA 65B, confirming the benefits of RALMs on knowledge-intensive tasks. Furthermore, RA-DIT significantly outperforms REPLUG (+8.9% in 0-shot and +1.4% in 5-shot on average over MMLU, NQ, TQA  and ELI5) and achieves the best performance on most datasets. This supports our claim that combining off-the-shelf LLMs and retrievers is sub-optimal, and our dual instruction tuning approach is an effective way of retrofitting LLMs with retrieval capabilities. 6   We also compare with ATLAS, a state-of-the-art encoder-decoder based RALM that jointly pretrains the language model and the retriever. Here we adopt a 64-shot setting similar to Izacard et al. (2022b) with the following differences. While ATLAS conducts 64-shot fine-tuning for each individual task and reports the performance of task-specific models, we continuously fine-tune the RA-DIT checkpoint using the 64-shot examples from all tasks combined, and report the performance of a single model across tasks. As shown in Table 2, despite using a single model, RA-DIT outperforms ATLAS by an average of 4.1 points, achieving higher performance on 6 out of the 8 datasets. \n\nCommonsense Reasoning We benchmark RA-DIT 65B on a set of commonsense reasoning tasks to evaluate the impact of retrieval-augmented instruction tuning on the LLM's parametric knowledge and reasoning capabilities. We do not perform retrieval augmentation in this experiment. As shown in Table 3, RA-DIT demonstrates improvements over the base LLAMA models on 7 out of 8 evaluation datasets, indicating that the parametric knowledge and reasoning capabilities of the LLM component are in general preserved.",
            "score": 0.6181695835688292,
            "section_title": "Knowledge-Intensive Tasks",
            "char_start_offset": 14989,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 38
                },
                {
                    "start": 39,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1375
                },
                {
                    "start": 1378,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1882
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86279296875
        },
        {
            "corpus_id": "273695367",
            "title": "RuleRAG: Rule-guided retrieval-augmented generation with language models for question answering",
            "text": "In Figure 3, we initialize RuleRAG-FT with more retrievers: dense retrievers DPR (Karpukhin et al., 2020), SimCSE (Gao et al., 2021) and training-free sparse retriever BM25 (Robertson & Zaragoza, 2009), and we use several retrieval configurations: retrievers without fine-tuning or with SSFT/RGFT while recalling different numbers of top-scored documents. Before fine-tuning, the Recall@k and EM performance of the three retrievers are comparable and each has its own performance, with no Preprint Table 4: The performance of RuleRAG-ICL with a powerful retriever, Contriever, under three LLMs. obvious advantages or disadvantages. For instance, comparing the three retrievers side-by-side, DPR has the best Recall@10 and SimCSE has the best EM under top-10 documents before fine-tuning. \n\nAfter fine-tuning, DPR consistently outperforms SimCSE and RGFT consistently outperforms SSFT. Specifically, under considering top-scored documents with the same k, for the two trainable dense retrievers, the RGFT version recalls more relevant information (Recall@k) than the SSFT version by a large margin, demonstrating the generality of the proposed RGFT across different retrievers. As a result, the EM scores of the generated answers are better when higher-quality documents from retrievers are provided. Moreover, when the retrievers and generators are applied with RGFT, RuleRAG-FT shows substantial performance gains, even with the retrieval number limited to top-1. For DPR and SimCSE, as we include more documents, the Recall@k and EM scores increasingly improve. This shows that leveraging rules to guide the retrieval and generation processes builds a bridge between queries and answers since rules provide retrieval directions and attributable mechanisms. For BM25, although Recall@k keeps increasing, EM experiences a drop, probably due to the introduced noise.",
            "score": 0.6171704881758159,
            "section_title": "MORE RETRIEVERS FOR RULERAG-FT",
            "char_start_offset": 26031,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1865
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 132,
                    "matchedPaperCorpusId": "233296292"
                },
                {
                    "start": 173,
                    "end": 201,
                    "matchedPaperCorpusId": "207178704"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7373046875
        },
        {
            "corpus_id": "274859726",
            "title": "PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization",
            "text": "These approaches attempt to bridge the gap between off-the-shelf LLMs and the specific demands of RAG, but both face significant limitations. End-to-end architectures primarily rely on supervised fine-tuning (SFT) of general LLMs (Yoran et al., 2024a;Fang et al., 2024). These approaches focus on constructing high-quality responses in RAG scenarios. However, through further analysis, we found that the requirements for the generators in RAG tasks are highly context-dependent and often interwoven. These varying and intertwined demands make it challenging for a model to meet all RAG objectives through standard SFT alone, as it does not incorporate the necessary preference information required for adapting to different retrieval scenarios. On the other hand, pipeline architectures introduce additional steps beyond generation, such as re-ranking retrieved documents, filtering irrelevant information, or employing post-hoc verification to ensure that citations support the claims (Dong et al., 2024a;Yu et al., 2024;Wang et al., 2024;Sun et al., 2023). However, these additional steps can only satisfy certain RAG requirements and still have a considerable gap in aligning with the global RAG requirements. Moreover, these additional steps introduce extra computational costs and time consumption, making it less practical for large-scale deployment. This further inspires us to think: Is it possible to fully align the generator with the diverse RAG requirements while retaining the simplicity and efficiency of an end-to-end architecture? \n\nTo this end, in this paper, we propose Multi-Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), aiming to optimize the generator of RAG systems to align comprehensively with specific RAG requirements. As illustrated in Figure 1, PA-RAG maintains the endto-end architecture of the generator while enabling it to learn multi-perspective preference information. The training of PA-RAG is divided into two phases. The first phase is foundational capability training, where the generator acquires the basic ability to utilize and cite documents through instruction fine-tuning. To construct high-quality instruction fine-tuning data, we utilize ChatGPT to generate complete and correct answers and employ a citation rewrite mechanism to ensure citation quality.",
            "score": 0.6161945407174335,
            "section_title": "Introduction",
            "char_start_offset": 2236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1546
                },
                {
                    "start": 1549,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2148
                },
                {
                    "start": 2149,
                    "end": 2332
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "274859726",
            "title": "PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization",
            "text": "The emergence of Retrieval-augmented generation (RAG) has alleviated the issues of outdated and hallucinatory content in the generation of large language models (LLMs), yet it still reveals numerous limitations. When a general-purpose LLM serves as the RAG generator, it often suffers from inadequate response informativeness, response robustness, and citation quality. Past approaches to tackle these limitations, either by incorporating additional steps beyond generating responses or optimizing the generator through supervised fine-tuning (SFT), still failed to align with the RAG requirement thoroughly. Consequently, optimizing the RAG generator from multiple preference perspectives while maintaining its end-to-end LLM form remains a challenge. To bridge this gap, we propose Multiple Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator of RAG systems to align with RAG requirements comprehensively. Specifically, we construct high-quality instruction fine-tuning data and multi-perspective preference data by sampling varied quality responses from the generator across different prompt documents quality scenarios. Subsequently, we optimize the generator using SFT and Direct Preference Optimization (DPO). Extensive experiments conducted on four question-answer datasets across three LLMs demonstrate that PA-RAG can significantly enhance the performance of RAG generators. Our code and datasets are available at https://github.com/wujwyi/PA-RAG.",
            "score": 0.6160855547463291,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9033203125
        },
        {
            "corpus_id": "273501949",
            "title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services",
            "text": "Retrieval-Augmented Generation (RAG) leverages both parametric and non-parametric memory, significantly enhancing the performance of Large Language Models (LLMs) in translation and question-answering tasks, as highlighted by Lewis et al. [16]. These models, equipped with extensive training datasets and substantial parameters, have led advancements in these fields. However, they face persistent challenges such as the potential for outdated or incorrect information and difficulties with real-time updates [8], [10], [12], [17]. The RAG approach improves LLM performance through pretraining, combining various memory types to generate factbased, varied, and accurate language representations. This method employs a dynamic updating mechanism to refresh the knowledge base without retraining the entire model, thereby enhancing reliability and clarity [18]. Nonetheless, RAG faces issues like noise or conflicting information during the retrieval phase, necessitating improvements for response accuracy and reliability [19]. Lin et al. [20] suggest integrating RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches. \n\nSELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation. It also incorporates selfreflection tokens that allow the model to assess the quality and integrity of its responses, thereby increasing the connectedness and correctness of its outputs [21]. Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]. Among PEFT methods, Low-Rank Adaptation (LoRA) and its advanced versions, such as LoRA+ and DoRA, have shown significant improvements. LoRA introduces trainable low-rank matrices within the Transformer architecture, enhancing the efficiency of parameter updates without altering the model architecture significantly [17], [23].",
            "score": 0.6147044427271824,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 3739,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1163
                },
                {
                    "start": 1166,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2358
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91357421875
        },
        {
            "corpus_id": "273962778",
            "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
            "text": "Retrieval-augmented generation (RAG) has shown impressive capability in providing reliable answer predictions and addressing hallucination problems. A typical RAG implementation uses powerful retrieval models to extract external information and large language models (LLMs) to generate answers. In contrast, recent LLM-based retrieval has gained attention for its substantial improvements in information retrieval (IR) due to the LLMs' semantic understanding capability. However, directly applying LLM to RAG systems presents challenges. This may cause feature locality problems as massive parametric knowledge can hinder effective usage of global information across the corpus; for example, an LLM-based retriever often inputs document summaries instead of full documents. Moreover, various pre-trained tasks in LLMs introduce variance, further weakening performance as a retriever. To address these issues, we propose a novel two-stage fine-tuning architecture called Invar-RAG. In the retrieval stage, an LLM-based retriever is constructed by integrating LoRA-based representation learning to tackle feature locality issues. To enhance retrieval performance, we develop two patterns (invariant and variant patterns) and an invariance loss to reduce LLM variance. In the generation stage, a refined fine-tuning method is employed to improve LLM accuracy in generating answers based on retrieved information. Experimental results show that Invar-RAG significantly outperforms existing baselines across three open-domain question answering (ODQA) datasets. Code is available in the Supplementary Material for reproducibility.",
            "score": 0.6145959835810823,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87158203125
        },
        {
            "corpus_id": "268554288",
            "title": "FIT-RAG: Black-Box RAG with Factual Information and Token Reduction",
            "text": "This paper focuses on Retrieval-Augmented Generation (RAG) system for black-box Large Language Models (LLMs), namely black-box RAG.In this section, we first give the definition of RAG and subsequently introduce the black-box RAG.\n\nRetrieval-Augmented Generation (RAG).Given a natural language question , an external knowledge corpus W and a generative language model M, a RAG system aims to help M generate more accurate and informative responses to  using a retrieval model R, which effectively retrieves relevant documents D = ( 1 ,  2 ,  3 , ...) from W. The form of introducing external knowledge to the language model varies, including modifying attention weights during generation, incorporating it into input prompts, or using it in post-calibration of the model output.Moreover, existing RAG methods typically require joint fine-tuning of the retriever and the language model (e.g.Atlas [15], REALM [9]).However, joint fine-tuning is unaffordable in amount of practical scenarios due to the extremely large parameter scale of LLMs.In these scenarios, we can alternatively treat an LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a RAG system, namely black-box RAG.Next, we introduce the definition of the black-box RAG.\n\nRetrieval-Augmented Generation System for Black-box LLM (Black-box RAG).A RAG system for blackbox LLM aims to enhance the generation capability of the black-box LLM M  by retrieving external knowledge without updating the LLM parameters.While the parameters of the black-box LLM M  are frozen, the parameters of the retrieval model R are learnable.Thus, the RAG system for black-box LLM only optimizes R to improve overall system performance, without modifying M  .Moreover, existing black-box RAG systems typically inject the retrieved documents D into M  by constructing an input prompt that concatenates the question  and documents D, which leverages the powerful in-context learning capabilities of the LLM.",
            "score": 0.6136082203977394,
            "section_title": "Problem Formulation",
            "char_start_offset": 14673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 229
                },
                {
                    "start": 231,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 777
                },
                {
                    "start": 777,
                    "end": 889
                },
                {
                    "start": 889,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 1039
                },
                {
                    "start": 1039,
                    "end": 1204
                },
                {
                    "start": 1204,
                    "end": 1259
                },
                {
                    "start": 1261,
                    "end": 1333
                },
                {
                    "start": 1333,
                    "end": 1498
                },
                {
                    "start": 1498,
                    "end": 1609
                },
                {
                    "start": 1609,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1972
                }
            ],
            "ref_mentions": [
                {
                    "start": 895,
                    "end": 899,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 907,
                    "end": 910,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7333984375
        },
        {
            "corpus_id": "270123034",
            "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
            "text": "Retrieval-Augmented Generation (RAG) Compared to standard text generation, RAG incorporates a retrieval module that accesses external knowledge to enhance generation quality (Lewis et al., 2020b;Guu et al., 2020;Zhu et al., 2023).The mainstream RAG follows a \"retrieve-then-read\" paradigm, where the retrieval module provides external knowledge as additional context, which is then read by generation models to produce the final output (Izacard et al., 2023;Shi et al., 2023;Ram et al., 2023;Borgeaud et al., 2022;Lin et al., 2023;Zhu et al., 2024).To optimize the use of external knowledge, some methods focus on crafting effective prompts that guide the utilization of retrieved information (Shi et al., 2023;Ram et al., 2023).These prompt-based methods are applicable to any LLM without tuning its parameters.However, they depend heavily on skillful prompt writing and the LLMs' ability to understand instructions.In contrast, other studies attempts to directly train the model to better use the retrieved knowledge.For example, REALM (Guu et al., 2020) and RETRO (Borgeaud et al., 2022) incorporate retrieval in end-to-end retrieval-augmented pretraining.RA-DIT (Lin et al., 2023) employs finetuning to enhance LLMs' retrieval understanding.These tuning-based methods often yield better performance than prompt-based methods by optimizing model parameters for RAG.However, they may compromise the LLMs' general capabilities, particularly in non-retrieval scenarios.Different from existing methods, we design a new lightweight tuning method for RAG.It is a plug-and-play module that enhances RAG performance using trainable virtual tokens, which can be removed in non-RAG scenarios to preserve the LLMs' general generation abilities.",
            "score": 0.6134771925378251,
            "section_title": "Related Work",
            "char_start_offset": 5432,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 549
                },
                {
                    "start": 549,
                    "end": 729
                },
                {
                    "start": 729,
                    "end": 812
                },
                {
                    "start": 812,
                    "end": 917
                },
                {
                    "start": 917,
                    "end": 1019
                },
                {
                    "start": 1019,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1245
                },
                {
                    "start": 1245,
                    "end": 1368
                },
                {
                    "start": 1368,
                    "end": 1469
                },
                {
                    "start": 1469,
                    "end": 1552
                },
                {
                    "start": 1552,
                    "end": 1736
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 195,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 436,
                    "end": 458,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 492,
                    "end": 514,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1067,
                    "end": 1090,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7646484375
        },
        {
            "corpus_id": "270870251",
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "text": "We have evaluated the impact of various methods for fine-tuning LLM generators.Previous studies have demonstrated the feasibility of training both the retriever and generator jointly.We would like to explore this possibility in the future.In this study, we embraced the principle of modular design to simplify the search for optimal RAG implementations, thereby reducing complexity.Due to the daunting costs associated with constructing vector databases and conducting experiments, our evaluation was limited to investigating the effectiveness and influence of representative chunking techniques within the chunking module.It would be intriguing to further explore the impact of different chunking techniques on the entire RAG systems.While we have discussed the application of RAG in the domain of NLP and extended its scope to image generation, an enticing avenue for future exploration would involve expanding this research to other modalities such as speech and video.increase in performance across all metrics.Approximately equal performance is achieved by monoT5 and monoBERT, and RankLLaMA performs best, each ascending in latency.TILDEv2 is the fastest, taking approximately 10 to 20 milliseconds per query at the cost of performance.Additionally, TILDEv2 requires that the passages reranked be identically included in the previously indexed collection.Preprocessing must be redone at inference for new unseen passages, negating the efficiency advantages.",
            "score": 0.6119439772651664,
            "section_title": "Limitations",
            "char_start_offset": 32817,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 79,
                    "end": 183
                },
                {
                    "start": 183,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 382
                },
                {
                    "start": 382,
                    "end": 623
                },
                {
                    "start": 623,
                    "end": 735
                },
                {
                    "start": 735,
                    "end": 972
                },
                {
                    "start": 972,
                    "end": 1015
                },
                {
                    "start": 1015,
                    "end": 1138
                },
                {
                    "start": 1138,
                    "end": 1242
                },
                {
                    "start": 1242,
                    "end": 1361
                },
                {
                    "start": 1361,
                    "end": 1463
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.783203125
        },
        {
            "corpus_id": "271571401",
            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
            "text": "The retrieval process is pivotal in RAG systems. By leveraging powerful embedding models, queries and text can be efficiently represented in latent spaces, which facilitates the establishment of semantic similarity between questions and documents, thereby enhancing retrieval. Three main considerations that need to be addressed include retrieval efficiency, quality, and the alignment of tasks, data and models. \n\n1) Retriever Selection: With the widespread adoption of RAG technology, the development of embedding models has been in full swing. In addition to traditional models based on statistics and pre-trained models based on the encoder structure, embedding models fine-tuned on LLMs have also demonstrated powerful capabilities [39]. However, they often come with more parameters, leading to weaker inference and retrieval efficiency. Therefore, it is crucial to select the appropriate retriever based on different task scenarios. \n\nSparse Retriever uses statistical methods to convert queries and documents into sparse vectors. Its advantage lies in its efficiency in handling large datasets, focusing only on non-zero elements. However, it may be less effective than dense vectors in capturing complex semantics. Common methods include TF-IDF and BM25. \n\nDense Retriever employs pre-trained language models (PLMs) to provide dense representations of queries and documents. Despite higher computational and storage costs, it offers more complex semantic representations. Typical models include BERT structure PLMs, like ColBERT, and multi-task fine-tuned models like BGE [40] and GTE [41]. \n\nHybrid Retriever is to use both sparse and dense retrievers simultaneously. Two embedding techniques complement each other to enhance retrieval effectiveness. Sparse retriever can provide initial screening results. Additionally, sparse models enhance the zero-shot retrieval capabilities of dense models, particularly in handling queries with rare entities, thereby increasing system robustness. \n\n2) Retriever Fine-tuning: In cases where the context may diverge from pre-trained corpus, particularly in highly specialized fields like healthcare, law, and other domains abundant in proprietary terminology. While this adjustment demands additional effort, it can substantially enhance retrieval efficiency and domain alignment. \n\nSupervised Fine-Tuning (SFT). Fine-tuning a retrieval model based on labeled domain data is typically done using contrastive learning. This involves reducing the distance between positive samples while increasing the distance between negative samples.",
            "score": 0.6118894132939461,
            "section_title": "C. Retrieval",
            "char_start_offset": 21512,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 49,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1599
                },
                {
                    "start": 1602,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1997
                },
                {
                    "start": 2000,
                    "end": 2208
                },
                {
                    "start": 2209,
                    "end": 2329
                },
                {
                    "start": 2332,
                    "end": 2361
                },
                {
                    "start": 2362,
                    "end": 2466
                },
                {
                    "start": 2467,
                    "end": 2583
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.919921875
        },
        {
            "corpus_id": "277596427",
            "title": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented Generation with Reranker KV-Cache Reuse",
            "text": "Retrieval-Augmented Generation (RAG) (Ram et al., 2023;Lewis et al., 2021;Asai et al., 2023;Khandelwal et al., 2020;Jin et al., 2024a;Shao et al., 2024) is the process of optimizing the output of a large language model and it references an authoritative knowledge base outside of its training data sources before generating a response. This paradigm has gained traction for tasks requiring factual accuracy and up-to-date information, such as question answering, summarization, and dialogue generation. \n\nFigure 1: Classic RAG Workflow: The query is embedded and used to retrieve top-K documents. Then the reranker selects the most relevant ones which are combined with the query to generate the final response. \n\nTraditional RAG pipelines rely on retrievers which depend on embedding representations and cosine similarity to fetch relevant documents (Robertson & Zaragoza, 2009;Reimers & Gurevych, 2019;Wang et al., 2024). While this approach is straightforward, it often struggles to achieve optimal results in more complex scenarios. To solve the problem, advancements have introduced reranker mechanisms that refine the retrieved documents to improve relevance and contextuality before generation. These rerankers, often transformer-based, significantly boost the quality of generated content by ensuring the most pertinent documents are prioritized. \n\nEarly Rerankers were predominantly trained on encoder-only models such as BERT (Devlin et al., 2019) or XLM-RoBERTa (Conneau et al., 2020), leveraging their strong encoding capabilities to improve retrieval precision. However, recent advancements have demonstrated the growing dominance of decoder-based rerankers which capitalize on the powerful generative language capabilities of modern decoder models (Chen et al., 2024;Ma et al., 2023;Pradeep et al., 2023). By fine-tuning these decoder-based models on tasks originally designed for encoder-only rerankers, they achieve significant gains in performance, benefiting from both their inherent generative power and the fine-grained contextual understanding acquired during fine-tuning.",
            "score": 0.6106745469901098,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 3985,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 2093
                }
            ],
            "ref_mentions": [
                {
                    "start": 879,
                    "end": 904,
                    "matchedPaperCorpusId": "201646309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90380859375
        },
        {
            "corpus_id": "271571401",
            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
            "text": "Hu et al., [30] discuss Retrieval-Augmented Language Models (RALMs) form three key components, including retrievers, language models, augmentations, and how their interactions lead to different model structures and applications. They emphasize the importance of considering robustness, accuracy, and relevance when evaluating RALMs and propose several evaluation methods. Ding et al., [31] provide a comprehensive review from the perspectives of architecture, training strategies, and applications. They specifically discuss four training methods of RALMs: training-free methods, independent training methods, sequence training methods, and joint training methods, and compare their advantages and disadvantages. Zhao et al., [32]analyze the applications of RAG technology in various fields such as text generation, code generation, image generation, and video generation from the perspective of augmented intelligence with generative capabilities. \n\nThe current collation of RAG systems primarily focuses on methods with a fixed process, mainly concerned with optimizing the retrieval and generation stages. However, it has not turned its attention to the new characteristics that RAG research is continuously evolving, namely the characteristics of process scheduling and functional componentization. There is currently a lack of comprehensive analysis of the overall RAG system, which has led to research on paradigms lagging behind the development of RAG technology.",
            "score": 0.6104282901511617,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 10408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1470
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74169921875
        },
        {
            "corpus_id": "266359151",
            "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
            "text": "2) New Patterns: Modular RAG offers remarkable adaptability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple \"Retrieve\" and \"Read\" mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks. \n\nInnovations such as the Rewrite-Retrieve-Read [7]model leverage the LLM's capabilities to refine retrieval queries through a rewriting module and a LM-feedback mechanism to update rewriting model., improving task performance. Similarly, approaches like Generate-Read [13] replace traditional retrieval with LLM-generated content, while Recite-Read [22] emphasizes retrieval from model weights, enhancing the model's ability to handle knowledge-intensive tasks. Hybrid retrieval strategies integrate keyword, semantic, and vector searches to cater to diverse queries. Additionally, employing sub-queries and hypothetical document embeddings (HyDE) [11] seeks to improve retrieval relevance by focusing on embedding similarities between generated answers and real documents. \n\nAdjustments in module arrangement and interaction, such as the Demonstrate-Search-Predict (DSP) [23] framework and the iterative Retrieve-Read-Retrieve-Read flow of ITER-RETGEN [14], showcase the dynamic use of module outputs to bolster another module's functionality, illustrating a sophisticated understanding of enhancing module synergy. The flexible orchestration of Modular RAG Flow showcases the benefits of adaptive retrieval through techniques such as FLARE [24] and Self-RAG [25]. This approach transcends the fixed RAG retrieval process by evaluating the necessity of retrieval based on different scenarios. Another benefit of a flexible architecture is that the RAG system can more easily integrate with other technologies (such as fine-tuning or reinforcement learning) [26]. For example, this can involve fine-tuning the retriever for better retrieval results, fine-tuning the generator for more personalized outputs, or engaging in collaborative fine-tuning [27].",
            "score": 0.6096042122869251,
            "section_title": "C. Modular RAG",
            "char_start_offset": 13909,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 443
                },
                {
                    "start": 446,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2198
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9228515625
        },
        {
            "corpus_id": "274437238",
            "title": "Generating a Low-code Complete Workflow via Task Decomposition and RAG",
            "text": "The greatest impact of Task Decomposition and RAG is on model training, or model fine-tuning in our case, as how models are trained determine how they will interface with the rest of the system (i.e., model inputs and outputs). Using samples such as the one shown in Figure 4, we shipped a first version of the application that could generate only outlines, with low rates of hallucination thanks to a simple RAG implementation. But the real challenge was to generate the complete workflow. \n\nThe first step was to find a good enough retriever. As our use case is domain-specific, available open-source retriever models, such as GTR-T5 [38], did not provide satisfactory results. For scalability and latency reasons, we chose a small retriever of about 100 million parameters that encoded text in tens of milliseconds. Using our labeled data, we fine-tuned this small retriever via a common technique called contrastive learning [39], [40], thereby obtaining good retrieval quality. \n\nThe next step was to fine-tune the FM to perform the sub-tasks. Given that we were not prompting an off-the-shelf FM, we had flexibility on how to train the FM to do RAG. To decrease latency, it is desirable to minimize the number of retrievals. This is accomplished with a method known as adaptive retrieval [21], where the FM requests additional data, when needed, to continue the generation (Figure 2b). This can result in higher output quality as the FM is trained to rely on the data it has memorized during training for input that is easy or very frequent. For input that is more complex or infrequent in the data distribution, the FM relies on RAG. Figure 6 shows an example of how the labeled samples for createFlow were transformed to suggest choices to the FM coming from the environment. We used choices: as a special token to stop generation and call the retriever using the step annotation, in this case: add work notes to them. At inference time, when this token is generated, the retriever module is invoked. The samples for populateInputs were similarly transformed to give choices to the FM for table names, column names, and column values.",
            "score": 0.6093273039144302,
            "section_title": "C. Model Training",
            "char_start_offset": 28711,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 490
                },
                {
                    "start": 493,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 982
                },
                {
                    "start": 985,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2142
                }
            ],
            "ref_mentions": [
                {
                    "start": 636,
                    "end": 640,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 929,
                    "end": 933,
                    "matchedPaperCorpusId": "8281592"
                },
                {
                    "start": 935,
                    "end": 939,
                    "matchedPaperCorpusId": "233296292"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68212890625
        },
        {
            "corpus_id": "278170975",
            "title": "Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets",
            "text": "Retrieval-Augmented Generation (RAG) has advanced significantly in recent years. The complexity of RAG systems, which involve multiple components-such as indexing, retrieval, and generation-along with numerous other parameters, poses substantial challenges for systematic evaluation and quality enhancement. Previous research highlights that evaluating RAG systems is essential for documenting advancements, comparing configurations, and identifying effective approaches for domain-specific applications. This study systematically reviews 63 academic articles to provide a comprehensive overview of state-of-the-art RAG evaluation methodologies, focusing on four key areas: datasets, retrievers, indexing and databases, and the generator component. We observe the feasibility of an automated evaluation approach for each component of a RAG system, leveraging an LLM capable of both generating evaluation datasets and conducting evaluations. In addition, we found that further practical research is essential to provide companies with clear guidance on the do's and don'ts of implementing and evaluating RAG systems. By synthesizing evaluation approaches for key RAG components and emphasizing the creation and adaptation of domain-specific datasets for benchmarking, we contribute to the advancement of systematic evaluation methods and the improvement of evaluation rigor for RAG systems. Furthermore, by examining the interplay between automated approaches leveraging LLMs and human judgment, we contribute to the ongoing discourse on balancing automation and human input, clarifying their respective contributions, limitations, and challenges in achieving robust and reliable evaluations.",
            "score": 0.6090123562943459,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7939453125
        },
        {
            "corpus_id": "278635834",
            "title": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning",
            "text": "Using documents retrieved from extended knowledge bases to enhance the capabilities of large language models (LLMs) has been proven effective in NLP tasks, including language modeling (Borgeaud et al., 2022;Ram et al., 2023;Zhang et al., 2024) and question answering (Izacard et al., 2023;Shi et al., 2023;Yoran et al., 2023;Lin et al., 2023;Fang et al., 2024). Specifically, a Retrieval-Augmented Generator (RAG) system takes a query as input and uses a retriever to retrieve relevant documents from an external knowledge base. Then, it combines the documents with the query and feeds them into the LLM to make up for the LLM's own lack of knowledge. Optimization of the RAG system focuses on two main areas: improving the retriever and enhancing the generator (LLM) to the RALM. Replug (Shi et al., 2023) uses KL divergence to align retriever results with LLM preferences. LLM-Embedder (Zhang et al., 2024) employs a distillation objective based on LLM rankings. These methods train the retriever to better match LLM preferences. For generator, FiD (Izacard and Grave, 2020) finetunes LLM to handle retrieved documents and queries, addressing irrelevant information. Other studies introduce noise to improve LLM robustness (Yoran et al., 2023;Fang et al., 2024). Combining the strengths of both approaches, RA-DIT (Lin et al., 2023) uses modular training to optimize the retriever and LLM separately, enhancing overall RAG system performance.",
            "score": 0.6083343866543466,
            "section_title": "Retrieval-augmented Generation",
            "char_start_offset": 5200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1444
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 207,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 207,
                    "end": 224,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 224,
                    "end": 243,
                    "matchedPaperCorpusId": "271915498"
                },
                {
                    "start": 267,
                    "end": 289,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 306,
                    "end": 325,
                    "matchedPaperCorpusId": "263608822"
                },
                {
                    "start": 325,
                    "end": 342,
                    "matchedPaperCorpusId": "263605962"
                },
                {
                    "start": 888,
                    "end": 908,
                    "matchedPaperCorpusId": "271915498"
                },
                {
                    "start": 1225,
                    "end": 1245,
                    "matchedPaperCorpusId": "263608822"
                },
                {
                    "start": 1316,
                    "end": 1334,
                    "matchedPaperCorpusId": "263605962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9697265625
        },
        {
            "corpus_id": "273350575",
            "title": "Self-adaptive Multimodal Retrieval-Augmented Generation",
            "text": "Table 2 (top) demonstrates that fine-tuning the retrieval model R or the VLM M improves multimodal RAG performance. However, finetuning M provides more substantial gains. This suggests that the baseline retrieval model R is already sufficiently optimized, leading to smaller performance improvements from finetuning. In contrast, optimizing M directly enhances the model's ability to process multimodal data. Notably, the fine-tuned RAG(R * + M * ) shows slightly lower performance than MuRAG, despite differences in the volume of training data. \n\n3. SAM-RAG vs. Conventional RAG: As shown in Table 2 (middle), SAM-RAG consistently outperforms conventional RAG methods. By dynamically retrieving documents until relevant information is found, SAM-RAG avoids the limitations of fixed retrieval strategies. Fine-tuning the retrieval model R speeds up document retrieval but does not significantly impact overall performance, as the strength of SAM-RAG lies in leveraging the fine-tuned M. The results indicate that fine-tuning M leads to the most notable performance improvements in SAM-RAG, especially for multimodal tasks. \n\n4. Effect of GPT Integration: Table 2 (bottom) highlights the substantial performance gains from integration of G into the SAM-RAG and conventional RAG frameworks. Models incorporating G outperform all other configurations, underscoring G's advanced reasoning and understanding abilities. SAM-RAG combined with G shows the most significant improvements, particularly in visual tasks, where it exceeds textual performance. This suggests that the SAM-RAG framework, when paired with G, achieves a deeper understanding of visual content than other approaches. To validate the effectiveness of each component of SAM-RAG, a series of ablation studies are performed that check the performance of the framework after removing each component, as shown in Table 3. The results indicate that, compared to conventional RAG, the introduction of distinct verification, particularly isRel and isSup , leads to notable improvements in both F1 and EM scores. Specifically, although introducing isRel only brings minimal improvement, the subsequent introduction of isUse and isSup makes a greater impact. When all verifications are combined (labeled \"with all\"), the EM score rises to the maximum value, reflecting an improvement of more than 20% compared to the performance of MuRAG.",
            "score": 0.6080349123317135,
            "section_title": "Performance Comparison with Baselines:",
            "char_start_offset": 17677,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 545
                },
                {
                    "start": 548,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 1122
                },
                {
                    "start": 1125,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2212
                },
                {
                    "start": 2213,
                    "end": 2392
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85546875
        },
        {
            "corpus_id": "276885086",
            "title": "Personalized Text Generation with Contrastive Activation Steering",
            "text": "Retrieval-Augmented Generation (RAG) RAG-based approaches achieve personalization through context-aware retrieval. Given input x, the system retrieves k most relevant historical responses from P u using retriever R, then generates personalized responses by combining retrieved documents R(x, P u , k) with the input prompt: \n\nParameter-Efficient Fine-Tuning (PEFT) PEFT methods customize LLMs by training lightweight adapters (e.g., LoRA (Hu et al., 2021)) on user-specific data while keeping base model parameters frozen (Tan et al., 2024b). For each user u, a distinct adapter \u03b8 u is trained via: \n\nwhere L(\u2022) denotes the sequence-to-sequence cross-entropy loss. During inference:",
            "score": 0.607981934827396,
            "section_title": "Base Solutions",
            "char_start_offset": 5184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 323
                },
                {
                    "start": 326,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 682
                }
            ],
            "ref_mentions": [
                {
                    "start": 522,
                    "end": 541,
                    "matchedPaperCorpusId": "267523232"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84375
        },
        {
            "corpus_id": "273403870",
            "title": "REFINE on Scarce Data: Retrieval Enhancement through Fine-Tuning via Model Fusion of Embedding Models",
            "text": "Retrieval augmented generation (RAG) pipelines are commonly used in tasks such as question-answering (QA), relying on retrieving relevant documents from a vector store computed using a pretrained embedding model. However, if the retrieved context is inaccurate, the answers generated using the large language model (LLM) may contain errors or hallucinations. Although pretrained embedding models have advanced, adapting them to new domains remains challenging. Fine-tuning is a potential solution, but industry settings often lack the necessary fine-tuning data. To address these challenges, we propose REFINE, a novel technique that generates synthetic data from available documents and then uses a model fusion approach to fine-tune embeddings for improved retrieval performance in new domains, while preserving out-of-domain capability. We conducted experiments on the two public datasets: SQUAD and RAG-12000 and a proprietary TOURISM dataset. Results demonstrate that even the standard fine-tuning with the proposed data augmentation technique outperforms the vanilla pretrained model. Furthermore, when combined with model fusion, the proposed approach achieves superior performance, with a 5.76% improvement in recall on the TOURISM dataset, and 6.58 % and 0.32% enhancement on SQUAD and RAG-12000 respectively.",
            "score": 0.6079299794841261,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81201171875
        },
        {
            "corpus_id": "270688152",
            "title": "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation",
            "text": "Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; where plan and answer tasks use the same example data, different loss functions, and train two task-specific prompts simultaneously. The right shows the inference process, where the plan-answer process is repeated iteratively until completion. Guu et al., 2020) enhances LLMs by retrieving relevant passages, thereby improving both the quality and accuracy of generated content, particularly in knowledge-intensive tasks (Shen et al., 2023;Chen et al., 2023). Early works (Es et al., 2023;Lyu et al., 2024) chose to retrieve once, incorporating a fixed number of retrieved passages with a query into LLMs to generate a response. Recent research indicates that adaptive retrieval, tailored to the demands of LLMs, can further enhance generation. FLARE (Jiang et al., 2023b) uses the generated sentence with a low confidence score as the query to retrieve external knowledge adaptively and then regenerates the current sentence, while Self-RAG (Asai et al., 2023) introduces special tokens allowing the model to adaptively retrieve and reflect the quality of generated content. SuRe (Kim et al., 2024) generates conditional summarizations of retrieval and evaluating them with carefully designed prompts. However, existing approaches may not take full advantage of the planning capabilities of LLMs. Additionally, these methods may struggle to extract relevant content from retrieved passages and are easily influenced by irrelevant information. \n\nParameter-Efficient Fine-Tuning. Despite the powerful generative capabilities of LLMs, fine-tuning them requires substantial computational resources (Lester et al., 2021;Ding et al., 2022;Liu et al., 2023). To achieve more efficient fine-tuning, parameter-efficient tuning methods have emerged. These methods either fine-tune a small portion of the model parameters or introduce additional learnable parameters without fine-tuning the model itself (Hu et al., 2021;Liu et al., 2021;Ding et al., 2022;Wang et al., 2023).",
            "score": 0.6078635891674844,
            "section_title": "Related Work",
            "char_start_offset": 7176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1533
                },
                {
                    "start": 1536,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 2055
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 88,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 334,
                    "end": 351,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 530,
                    "end": 548,
                    "matchedPaperCorpusId": "264350686"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76318359375
        },
        {
            "corpus_id": "268248396",
            "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
            "text": "Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LMs in handling low-frequency entities on question answering tasks. We conduct extensive experiments on twelve LMs of varying size and type and different FT methods, data augmentation, and retrieval models. Our findings indicate that while FT boosts the performance across entities of varying popularity, RAG surpasses FT by a large margin particularly for least popular factual knowledge. Additionally, the success of both RAG and FT approaches is amplified by improving retrieval and data augmentation techniques. Fine tuning, while beneficial for small LMs, requires extensive resources. To address this issue, we propose the new Stimulus RAG approach that surpasses the effectiveness of fine tuning based approaches, thereby eliminating the need for the costly data augmentation and fine tuning step for enriching LMs with less popular factual knowledge.",
            "score": 0.6071567837421993,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62548828125
        },
        {
            "corpus_id": "266359151",
            "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
            "text": "Targeted fine-tuning based on the scenario and data characteristics on LLMs can yield better results. This is also one of the greatest advantages of using on-premise LLMs. When LLMs lack data in a specific domain, additional knowledge can be provided to the LLM through fine-tuning. Huggingface's fine-tuning data can also be used as an initial step. \n\nAnother benefit of fine-tuning is the ability to adjust the model's input and output. For example, it can enable LLM to adapt to specific data formats and generate responses in a particular style as instructed [37]. For retrieval tasks that engage with structured data, the SANTA framework [76] implements a tripartite training regimen to effectively encapsulate both structural and semantic nuances. The initial phase focuses on the retriever, where contrastive learning is harnessed to refine the query and document embeddings. \n\nAligning LLM outputs with human or retriever preferences through reinforcement learning is a potential approach. For instance, manually annotating the final generated answers and then providing feedback through reinforcement learning. In addition to aligning with human preferences, it is also possible to align with the preferences of fine-tuned models and retrievers [79]. When circumstances prevent access to powerful proprietary models or larger parameter open-source models, a simple and effective method is to distill the more powerful models(e.g. GPT-4). Fine-tuning of LLM can also be coordinated with fine-tuning of the retriever to align preferences. A typical approach, such as RA-DIT [27], aligns the scoring functions between Retriever and Generator using KL divergence.",
            "score": 0.6070750554025748,
            "section_title": "B. LLM Fine-tuning",
            "char_start_offset": 40474,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1668
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91796875
        },
        {
            "corpus_id": "272120100",
            "title": "IDAS: Intelligent Driving Assistance System Using RAG",
            "text": "Retrieval Augmented Generation (RAG) is a technique that combines the strengths of information retrieval and natural language generation to improve the quality and relevance of generated text. It was introduced in 2020 by Lewis et al. at Facebook Artificial Intelligence Research (FAIR) [25]. The main idea is to enhance the generation process by retrieving relevant documents or pieces of information from a large corpus that cannot fit inside the working memory of a large language model. These relevant documents can then be used as context information for the LLM to inform and guide the generation of the final output. \n\nRAG models can produce more accurate, informative, and contextually relevant responses compared to traditional generation models that rely solely on the input query without additional context. \n\nThe RAG process typically involves three main components: \n\n1) Vector database generation: In this component, we create a database of the documents that later can be used to search for relevant information efficiently. 2) Retriever: This component is responsible for searching and retrieving relevant documents or passages from a vector database based on the input query. 3) Generator: Once the relevant information is retrieved, the generator component uses this information to produce a coherent and contextually appropriate response. \n\nThe generator is a language model that can leverage the retrieved documents to generate high-quality text.",
            "score": 0.6063756180954254,
            "section_title": "B. RAG PIPELINE FOR SPECIFIC CAR KNOWLEDGE",
            "char_start_offset": 12565,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 818
                },
                {
                    "start": 821,
                    "end": 878
                },
                {
                    "start": 881,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1357
                },
                {
                    "start": 1360,
                    "end": 1466
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 291,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7802734375
        },
        {
            "corpus_id": "270870251",
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "text": "In this section, we focus on fine-tuning the generator while leaving retriever fine-tuning for future exploration.We aim to investigate the impact of fine-tuning, particularly the influence of relevant or irrelevant contexts on the generator's performance.\n\nFormally, we denote x as the query fed into the RAG system, and D as the contexts for this input.\n\nThe fine-tuning loss of the generator is the negative log-likelihood of the ground-truth output y.\n\nTo explore the impact of fine-tuning, especially relevant and irrelevant contexts, we define d gold as a context relevant to the query, and d random as a randomly retrieved context.We train the model by varying the composition of D as follows:\n\n\u2022 D g : The augmented context consists of query-relevant documents, denoted as D g = {d gold }.\n\n\u2022 D r : The context contains one randomly sampled document, denoted as D r = {d random }.\n\n\u2022 D gr : The augmented context comprises a relevant document and a randomly-selected one, denoted as D gr = {d gold , d random }. \u2022 D gg : The augmented context consists of two copies of a query-relevant document, denoted as\n\nWe denote the base LM generator not fine-tuned as M b , and the model fine-tuned under the corresponding D as M g , M r , M gr , M gg .We fine-tuned our model on several QA and reading comprehension datasets.Ground-truth coverage is used as our evaluation metric since QA task answers are relatively short.We select Llama-2-7B [50] as the base model.Similar to training, we evaluate all trained models on validation sets with D g , D r , D gr , and D \u2205 , where D \u2205 indicates inference without retrieval.Figure 3 presents our main results.Models trained with a mix of relevant and random documents (M gr ) perform best when provided with either gold or mixed contexts.This suggests that mixing relevant and random contexts during training can enhance the generator's robustness to irrelevant information while ensuring effective utilization of relevant contexts.Therefore, we identify the practice of augmenting with a few relevant and randomly-selected documents during training as the best approach.Detailed dataset information, hyperparameters and experimental results can be found in Appendix A.5.",
            "score": 0.6060274527124764,
            "section_title": "Generator Fine-tuning",
            "char_start_offset": 22546,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 114,
                    "end": 256
                },
                {
                    "start": 258,
                    "end": 355
                },
                {
                    "start": 357,
                    "end": 455
                },
                {
                    "start": 457,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 700
                },
                {
                    "start": 702,
                    "end": 797
                },
                {
                    "start": 799,
                    "end": 888
                },
                {
                    "start": 890,
                    "end": 1114
                },
                {
                    "start": 1116,
                    "end": 1251
                },
                {
                    "start": 1251,
                    "end": 1324
                },
                {
                    "start": 1324,
                    "end": 1422
                },
                {
                    "start": 1422,
                    "end": 1466
                },
                {
                    "start": 1466,
                    "end": 1619
                },
                {
                    "start": 1619,
                    "end": 1654
                },
                {
                    "start": 1654,
                    "end": 1783
                },
                {
                    "start": 1783,
                    "end": 1977
                },
                {
                    "start": 1977,
                    "end": 2116
                },
                {
                    "start": 2116,
                    "end": 2216
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8623046875
        },
        {
            "corpus_id": "271218596",
            "title": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems",
            "text": "Despite remarkable reasoning and conversational abilities, out-of-the-box pre-trained Large Language Models (LLMs) struggle to reason about out-of-domain, knowledge-intensive queries [21,14].In response, Retriever-Augmented Generation (RAG) systems [21,20] are becoming increasingly popular in user-facing dialogue applications [35].Generally, RAG systems comprise a retriever component that queries relevant documents from an in-domain corpus and a downstream LLM generator model that incorporates the retrieved documents along with the original user query to output an informed response.The additional context helps ground the LLM in factual information and has been shown to boost performance on knowledge-intensive tasks [21].\n\nStill, when used in production settings, RAG systems are prone to hallucinations as the generator model struggles to retrieve relevant information from the context [1,31,7].In the absence of a one-fits-all approach, application-specific RAG systems must be fine-tuned for optimal performance on domain-specific tasks.However, the choice of retriever and generator models for each application is complex and has serious implications on overall system quality and costs.With numerous commercial and open-source generative LLMs readily available 1 and many variable parameters in the RAG system design (Figure 1), tuning an optimal system for a particular RAG application involves iterative evaluation of multiple configurations.This motivates the need for automated RAG evaluation solutions.\n\nIn response, automated RAG evaluation systems like RAGAS [9] and TruLens [37] have emerged.These systems adopt a zero-shot LLM prompt-based approach to predict a set of curated RAG evaluation metrics.However, the lack of unified RAG benchmarks makes it difficult to compare approaches against each other.Each new study designs a new dataset, often employing LLMs as generators and labelers [9,33,4], which renders them irreproducible.A few benchmarks like RGB [4], AttributionBench [22] and RAGTruth [41] have been proposed recently, but they are small in size and target a disjoint set of labels.The exact RAG evaluation criteria also vary from study to study.",
            "score": 0.605690286736519,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 191,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 589
                },
                {
                    "start": 589,
                    "end": 730
                },
                {
                    "start": 732,
                    "end": 905
                },
                {
                    "start": 905,
                    "end": 1049
                },
                {
                    "start": 1049,
                    "end": 1200
                },
                {
                    "start": 1200,
                    "end": 1458
                },
                {
                    "start": 1458,
                    "end": 1521
                },
                {
                    "start": 1523,
                    "end": 1614
                },
                {
                    "start": 1614,
                    "end": 1723
                },
                {
                    "start": 1723,
                    "end": 1827
                },
                {
                    "start": 1827,
                    "end": 1957
                },
                {
                    "start": 1957,
                    "end": 2120
                },
                {
                    "start": 2120,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 187,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 249,
                    "end": 253,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 328,
                    "end": 332,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 725,
                    "end": 729,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 899,
                    "end": 902,
                    "matchedPaperCorpusId": "235898896"
                },
                {
                    "start": 902,
                    "end": 904,
                    "matchedPaperCorpusId": "258947803"
                },
                {
                    "start": 1580,
                    "end": 1583,
                    "matchedPaperCorpusId": "263152733"
                },
                {
                    "start": 1913,
                    "end": 1916,
                    "matchedPaperCorpusId": "263152733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "274423377",
            "title": "Zero-Indexing Internet Search Augmented Generation for Large Language Models",
            "text": "To enhance the quality of outputs generated by LLMs, various learning approaches and learnable components have been explored within the RAG paradigm to integrate retrieved information effectively [59,60,61]. For example, WebGPT [4] fine-tuned the generative LLM, i.e., GPT-3, to answer long-form questions by using a textbased web-browsing environment, enhancing both retrieval and synthesis in an end-to-end manner; recently, RankRAG [62] instruction-tunes an end-to-end generative LLM for the dual purpose of context ranking and answer generation in RAG. Instead of re-training the generative LLM, DRAGIN [40] introduces a module dynamically determining when and what to retrieve based on the LLM's information needs during the text generation process; similarly, an advanced continual knowledge learning approach has been explored to selectively decide whether to access the Internet or not dynamically [7]. Query re-write module [39,41] has also been studied to eliminate ambiguity in the original input prompt; for example, RQ-RAG [41] tuned a 7B plugin model in RAG paradigm to dynamically refine search queries through explicit rewriting, decomposition, and disambiguation.",
            "score": 0.6053770683304971,
            "section_title": "Model Tuning in RAG",
            "char_start_offset": 51717,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1180
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 200,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 203,
                    "end": 206,
                    "matchedPaperCorpusId": "263608822"
                },
                {
                    "start": 933,
                    "end": 937,
                    "matchedPaperCorpusId": "258841283"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8779296875
        },
        {
            "corpus_id": "268091298",
            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
            "text": "Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.",
            "score": 0.6049474199354308,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8759765625
        },
        {
            "corpus_id": "269982691",
            "title": "FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research",
            "text": "In RAG, the choice of retrievers and generators plays a crucial role in determining the final performance. Therefore, we conduct an experiment to explore their impact. Note that we do not include methods requiring specific retrievers or generators (e.g., Self-RAG requires trained models). \n\nAs shown in the left part of Figure 2, most methods are sensitive to retrieval quality. The performance gap between using the BM25 and E5 retriever can approach nearly 10%. This gap is likely due to the presence of more noise in the retrieved passages of BM25, thereby disturbing the generation process with irrelevant information. In contrast, compression methods such as RECOMP show better robustness across various retrievers, suggesting that compression effectively mitigates the noise. Moreover, this robustness can be further enhanced by fine-tuning the generator. For example, Ret-Robust introduces a generator-specific training strategy that effectively minimizes the impact of irrelevant passages. \n\nThe influence of generators is also explored by using two models in different sizes (Qwen-1.5-14B and LLaMA-3-8B). Intriguingly, the larger model cannot consistently outperform the smaller one. For example, in methods such as FLARE and RECOMP, the smaller model yields better performance. Given that LLaMA-3-8B outperforms Qwen-1.5-14B in many public benchmarks, it suggests that the LLMs' RAG performance may be highly relevant to their general generation capabilities rather than their size. This observation highlights the complexity of LLMs' performance evaluation, suggesting that factors other than size, such as model architecture or training data quality, can also play significant roles.",
            "score": 0.6049033856507425,
            "section_title": "Impact of Retrievers and Generators",
            "char_start_offset": 17088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 289
                },
                {
                    "start": 292,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1697
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7529296875
        },
        {
            "corpus_id": "271903170",
            "title": "Graph Retrieval-Augmented Generation: A Survey",
            "text": "Jointly training retrievers and generators simultaneously enhances performance on downstream tasks by leveraging their complementary strengths. Some approaches unify retrievers and generators into a single model, typically LLMs, and train them with both retrieval and generation objectives simultaneously [112]. This method capitalizes on the cohesive capabilities of a unified architecture, enabling the model to seamlessly retrieve relevant information and generate coherent responses within a single framework. \n\nOther methodologies involve initially training retrievers and generators separately, followed by joint training techniques to fine-tune both components. For instance, Subgraph Retriever [196] adopts an alternating training paradigm, where the retriever's parameters are fixed to use the graph data for training the generator. Subsequently, the generator's parameters are fixed, and feedback from the generator is used to guide the retriever's training. This iterative process helps both components refine their performance in a coordinated manner.",
            "score": 0.6046818482864046,
            "section_title": "Joint Training",
            "char_start_offset": 76071,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1063
                }
            ],
            "ref_mentions": [
                {
                    "start": 702,
                    "end": 707,
                    "matchedPaperCorpusId": "247158305"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96044921875
        },
        {
            "corpus_id": "276107364",
            "title": "Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) enhances language models by integrating retrieval mechanisms, allowing them to generate responses based on dynamically retrieved documents rather than relying solely on pre-trained knowledge. This approach is particularly effective for knowledge-intensive tasks such as open-domain question answering, fact verification, and knowledge-based text generation. Rankify provides a modular and extensible interface for applying multiple RAG methods, including zero-shot generation, Fusion-in-Decoder (FiD) [41], and in-context learning [89]. \n\nIn Rankify, the Generator module enables seamless integration of RAG techniques, allowing users to experiment different generative approaches. Users can specify the desired RAG method and model, applying generation strategies across retrieved documents. \n\nUsers can apply these methods to generate responses based on retrieved documents. Listing 7 demonstrates how to use Rankify's RAG module with an in-context learning approach. \n\nListing 7: Applying Retrieval-Augmented Generation (RAG) in Rankify. \n\nRankify allows users to leverage large-scale language models such as LLaMA [108], GPT-4 [8], and T5-based models [87] for retrieval-augmented generation. By supporting both encoderdecoder architectures (FiD [41]) and decoder-only models (e.g., GPT, LLaMA), Rankify provides flexibility for optimizing generation quality based on task-specific requirements.",
            "score": 0.6046561838475726,
            "section_title": "Retrieval-Augmented Generation (RAG) Models",
            "char_start_offset": 21549,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1077
                },
                {
                    "start": 1080,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1436
                }
            ],
            "ref_mentions": [
                {
                    "start": 568,
                    "end": 572,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 1193,
                    "end": 1197,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6923828125
        },
        {
            "corpus_id": "275357908",
            "title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG",
            "text": "As more and more Generative AI (GenAI) applications are integrated into real-world production systems, Retrieval-Augmented Generation (RAG) has been adopted in industry as a common technique to improve the output of Large Language Models (LLMs). RAG alleviates inherent LLM pitfalls such as propensity to hallucinate, generating outdated knowledge, and lack of traceability to data sources (Fan et al., 2024;Gao et al., 2024). \n\nIntroducing a retrieval step into the generation process introduces, however, several practical challenges. While an LLM with a large number of parameters, such as GPT-4 (OpenAI et al., 2024), can be prompted to work with any kind of input and generate any kind of textual output, the retriever needs to be small, fast, and perform well with data sources that tend to be domain-specific. : Given an ecosystem of RAG applications, how do we build a retriever that can adapt to a specific domain and to a variety of retrieval tasks? \n\nOff-the-shelf retrievers of different sizes are available to AI practitioners. Embedding services such as Voyage1 perform well on open-source benchmarks but they do not necessarily generalize to the kind of data seen in real-world settings, especially when this data is structured and comes from existing databases. \n\nAnother practical challenge is achieving scalability and generalization across different GenAI use cases that depend on retrieval. A crucial advantage of LLMs compared to traditional machine learning models is that they can generalize to a myriad of tasks due to vast amounts of pretraining data and instruction fine-tuning (Wei et al., 2022;Zhang et al., 2024a;Ouyang et al., 2022). But if the retriever does not perform well and fast across many retrieval tasks, the downstream generation will be negatively affected. \n\nThe problem we are trying to solve is then: how to adapt the retrieval step to a specific domain and to a variety of retrieval tasks? In this work, we are not interested in the choice of LLM, assuming that improvements in the retrieved results translate into improvements in the downstream generation task.",
            "score": 0.6038642778255638,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 959
                },
                {
                    "start": 962,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1799
                },
                {
                    "start": 1802,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2108
                }
            ],
            "ref_mentions": [
                {
                    "start": 390,
                    "end": 408,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 1604,
                    "end": 1622,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1642,
                    "end": 1662,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8798828125
        },
        {
            "corpus_id": "277113527",
            "title": "Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems",
            "text": "Retrieval-Augmented Generation (RAG) has emerged as a promising framework to mitigate hallucinations in Large Language Models (LLMs), yet its overall performance is dependent on the underlying retrieval system. In the finance domain, documents such as 10-K reports pose distinct challenges due to domain-specific vocabulary and multi-hierarchical tabular data. In this work, we introduce an efficient, end-to-end RAG pipeline that enhances retrieval for financial documents through a three-phase approach: pre-retrieval, retrieval, and post-retrieval. In the pre-retrieval phase, various query and corpus preprocessing techniques are employed to enrich input data. During the retrieval phase, we fine-tuned state-of-the-art (SOTA) embedding models with domain-specific knowledge and implemented a hybrid retrieval strategy that combines dense and sparse representations. Finally, the post-retrieval phase leverages Direct Preference Optimization (DPO) training and document selection methods to further refine the results. Evaluations on seven financial question answering datasets-FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA, and MultiHiertt-demonstrate substantial improvements in retrieval performance, leading to more accurate and contextually appropriate generation. These findings highlight the critical role of tailored retrieval techniques in advancing the effectiveness of RAG systems for financial applications. A fully replicable pipeline is available on GitHub: https://github.com/seohyunwoo-0407/GAR.",
            "score": 0.6033075763405278,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84716796875
        },
        {
            "corpus_id": "268091298",
            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
            "text": "Despite the widespread adoption of RAG, it suffers from several limitations by nature. \n\n1) Noises in Retrieval Results: Information retrieval is inherently flawed due to information loss in item representations and ANN search. The inevitable noise, manifesting as irrelevant content or misleading information, can create failure points in RAG systems [343]. However, although improving retrieval accuracy seems intuitive for RAG effectiveness, recent research surprisingly finds that noisy retrieval results might enhance generation quality [344]. A possible explanation is that diverse retrieval outcomes could contribute to prompt construction [345]. Thus, the impact of retrieval noise remains unclear, leading to confusion about metric selection and retriever-generator interaction in practical uses. \n\n2) Extra Overhead: While retrieval can reduce generation costs in certain cases [30]- [32], it incurs non-negligible overhead in most cases. In other words, the retrieval and interaction processes increase latency inevitably. This is amplified when RAG is combined with complex enhancement methods, such as recursive retrieval [346] and iterative RAG [186]. Furthermore, as the scale of retrieval sources expands, the storage and access complexity will also increase [347]. Such overhead hampers the practicality of RAG in real-time services that are sensitive to latency. \n\n3) The Gap between Retrievers and Generators: Since the objectives of retrievers and generators may not align, and their latent spaces might differ, designing their interaction requires meticulous design and optimization. Current approaches either disentangle retrieval and generation or integrate them at an intermediate stage. While the former is more modular, the latter could benefit from joint training but hamper generality. Selecting a cost-effective interaction method to bridge the gap poses a challenge and necessities deliberation in practice. \n\n4) Increased System Complexity: The introduction of retrieval unavoidably increases the system complexity and the number of hyper-parameters to tune. For instance, a recent study found that using top-k rather than a single retrieval improves attribution but harms fluency in query-based RAG [348], while other aspects such as metric selection are still under explored. Thus, it requires more expertise to tune the generation service when RAG is involved.",
            "score": 0.602453661036788,
            "section_title": "A. Limitations",
            "char_start_offset": 76563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 89,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1937
                },
                {
                    "start": 1940,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2308
                },
                {
                    "start": 2309,
                    "end": 2394
                }
            ],
            "ref_mentions": [
                {
                    "start": 1159,
                    "end": 1164,
                    "matchedPaperCorpusId": "257663528"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6455078125
        },
        {
            "corpus_id": "267412954",
            "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
            "text": "While Retrieval Augmented Generation provides a method of injecting new knowledge sources into the LLM, the model itself remains unchanged which is why RAG alone has subpar results on very domain specific data. LLMs like GPT or LLaMA are not trained with a specific purpose in mind, yet many users would like to use LLMs for a specific purpose; in our case, we wanted to achieve domain-specific question answering from documents. Fine-tuning enables us to customize the output of the model based on specific use cases without having to go through the entire training process again. Actual fine-tuning alters the weights of the model based on the additional fine-tuning data, while RAG edits the query without making any changes to the model. So, a combination of these two methods could optimize the model for a specific use-case. There are various methods for fine-tuning large language models, such as Supervised Fine-tuning (SFT) via Parameter Efficient Fine Tuning (PEFT) Supervised Fine-tuning is when the training data consists of paired input and outputs of various examples that demonstrate how you want the model to respond to certain queries. By fine-tuning the model, we aim to get the model to mimic the style of the training data when asked questions similar to those in Figure 1: Retrieval-Augmented Generation 3 the data. Since our goal for the fine-tuned model was to be good at answering questions on financial documents, our training data consisted of a set of questions and answers curated by human financial experts. The datasets we employed were meant for supervised fine-tuning as they consisted of a question, context, and answer column. \n\nParameter Efficient Fine Tuning is a method that significantly reduces the amount of compute and memory required for Fine Tuning LLMs. Instead of retraining and adjusting all the weights, PEFT freezes all the weights of the pre-trained model and then augments it with additional parameters during the fine tuning process. This differs from full fine tuning which retrains all the parameter weights, and transfer learning which only retrains the head of the model. PEFT can result in comparable performance to fully fine tuned models while having significantly less trainable parameters.",
            "score": 0.6011580870886439,
            "section_title": "Fine Tuning",
            "char_start_offset": 5729,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1660
                },
                {
                    "start": 1663,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2126
                },
                {
                    "start": 2127,
                    "end": 2249
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "267770557",
            "title": "ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling",
            "text": "RAG have been widely used for language modeling (Borgeaud et al., 2022;Ram et al., 2023), question answering (Lewis et al., 2020;Izacard et al., 2022b;Shi et al., 2023b), and domain adaptation (Xu et al., 2023a,b;Shi et al., 2023c). To align retrievers with LLMs, most RAG methods integrate a pre-trained retriever with a generator and subsequently undergo an end-to-end fine-tuning process to effectively capture knowledge (Lewis et al., 2020). Among them, Atlas (Izacard et al., 2022b) leverages retrieved documents as latent variables and fine-tunes retrieval models with four designed loss. AAR (Yu et al., 2023d) identifies the LLM's preferred documents through FiD cross-attention scores (Izacard and Grave, 2021), and fine-tuning the retriever with hard negative sampling. However, these methods are inapplicable to black-box LLM as they require accessing LLM parameters. The only exception is RePlug (Shi et al., 2023b), which conducts supervised training by evaluating the KL divergence between the probability distributions of the retrieved documents and LLM's likelihood.",
            "score": 0.60107157417372,
            "section_title": "Related Work",
            "char_start_offset": 6650,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1082
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 71,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 71,
                    "end": 87,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 151,
                    "end": 169,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 213,
                    "end": 231,
                    "matchedPaperCorpusId": "263621862"
                },
                {
                    "start": 599,
                    "end": 617,
                    "matchedPaperCorpusId": "258960666"
                },
                {
                    "start": 694,
                    "end": 719,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 908,
                    "end": 927,
                    "matchedPaperCorpusId": "256389797"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82080078125
        },
        {
            "corpus_id": "269188036",
            "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
            "text": "To provide clarity and structure, this paper is organized as follows: Section 2 outlines the overall RAG workflow, dividing the methodologies into pre-retrieval, retrieval, post-retrieval, and generation phases. Sections 3 through 6 explore the core techniques within each phase. Section 7 focuses on the evaluation methodologies for RAG. Section 8 summarizes the reviewed studies, detailing the retrievers and generators used, while Section 9 discusses challenges and future research directions, extending beyond text-based studies to include multimodal data applications. The paper concludes with Section 10. \n\nOther related surveys provide valuable insights into the evolving RAG landscape from different angles. Gao et al. [38] identified three key stages in RAG development: pre-training enhancement, inference, and fine-tuning. Zhao et al. [162] focused on the diverse applications of RAG, including text, code, image, and video generation, emphasizing augmented intelligence in generative tasks. Meanwhile, Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how interactions between retrievers, language models, and augmentations influence model architectures and applications. \n\nIn this paper, we aim to offer a comprehensive and unified framework for understanding RAG from an information retrieval (IR) perspective, identifying key challenges and areas for improvement. We delve into the core technologies that drive RAG, assessing their effectiveness in addressing retrieval and generation tasks. Additionally, this survey introduces the evaluation methods employed in RAG research, highlights current limitations, and proposes promising avenues for future exploration.",
            "score": 0.6004689206835169,
            "section_title": "Introduction",
            "char_start_offset": 2330,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 610
                },
                {
                    "start": 613,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1212
                },
                {
                    "start": 1215,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1708
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8671875
        },
        {
            "corpus_id": "270869499",
            "title": "First Place Solution of 2023 Global Artificial Intelligence Technology Innovation Competition Track 1",
            "text": "Retrieval Augmented Generation (RAG) technology [2,6,7,9,15] in the field of natural language processing represents an innovative breakthrough.Traditional NLP techniques primarily rely on large language models, but their accuracy and depth may be limited when dealing with complex queries that require extensive background knowledge.To overcome this limitation, RAG combines conventional information retrieval methods with modern generative language models, aiming to enhance the model's text generation capabilities by incorporating external knowledge sources.The core principle is to integrate retrieval and generation techniques, allowing the model to access and utilize a vast amount of external information before generating text.RAG excels in addressing knowledge-intensive NLP tasks such as question answering, fact verification, and more.In recent years, RAG systems have evolved from a primary stage to an advanced stage, and then to a modular stage, to improve performance, cost-effectiveness, and   crease the difficulty of the pre-training task, gradually increasing the proportion of masking as the number of epochs increased.Specifically, we set an initial mask proportion of 0.3, and after every 10 epochs of pre-training, we perform fine-tuning of the downstream task.If the performance of the fine-tuning is lower than the previous one, we increase the mask proportion by 0.05 and continue with pre-training.Ultimately, we increase the number of pre-training epochs to 140, which significantly improves the text generation performance of the downstream task.For the construction of the training set with retrieval knowledge, we use D(Description) as the query and calculate the similarity with the key of each key-value pair in the knowledge base (e.g., vector inner product, L2 distance, or cosine similarity).If the similarity is larger than the threshold k, we call it an effective retrieval.We retrieve this key-value pair and concatenate the value to the end of the query as the new training sample corresponding to the query.For the val set and test set, we use the same retrieval method to construct the val set and test set with retrieval knowledge.Retrieval Iterations.For the first retrieval augmentation, the embeddings of key-value pairs are computed using a model trained on a training set without a knowledge base.",
            "score": 0.6002740804483746,
            "section_title": "Retrieval Augmentation in NLP",
            "char_start_offset": 3769,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 143,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 561
                },
                {
                    "start": 561,
                    "end": 735
                },
                {
                    "start": 735,
                    "end": 846
                },
                {
                    "start": 846,
                    "end": 1139
                },
                {
                    "start": 1139,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1425
                },
                {
                    "start": 1425,
                    "end": 1575
                },
                {
                    "start": 1575,
                    "end": 1828
                },
                {
                    "start": 1828,
                    "end": 1912
                },
                {
                    "start": 1912,
                    "end": 2048
                },
                {
                    "start": 2048,
                    "end": 2174
                },
                {
                    "start": 2174,
                    "end": 2195
                },
                {
                    "start": 2195,
                    "end": 2345
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 51,
                    "matchedPaperCorpusId": "252735160"
                },
                {
                    "start": 51,
                    "end": 53,
                    "matchedPaperCorpusId": "264426178"
                },
                {
                    "start": 53,
                    "end": 55,
                    "matchedPaperCorpusId": "252568176"
                },
                {
                    "start": 55,
                    "end": 57,
                    "matchedPaperCorpusId": "267053546"
                },
                {
                    "start": 57,
                    "end": 60,
                    "matchedPaperCorpusId": "247058346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72314453125
        },
        {
            "corpus_id": "276408784",
            "title": "Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA",
            "text": "Recent advancements in Generative AI have significantly improved the efficiency and adaptability of natural language processing (NLP) systems, particularly through Retrieval-Augmented Generation (RAG), Low-Rank Adaptation (LoRA), and Weight-Decomposed Low-Rank Adaptation (DoRA). RAG integrates external knowledge to enhance factual consistency in generative outputs, while LoRA enables parameter-efficient fine-tuning of large language models (LLMs). DoRA further refines this process by optimizing fine-tuning through adaptive parameter ranking and domain-aware weight adjustments, improving learning efficiency while maintaining inference performance. This paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications. Furthermore, this study examines the trade-offs between fine-tuning efficiency, computational cost, and real-time adaptability across different models. Findings highlight RAG's effectiveness in knowledge grounding, LoRA's cost-efficient domain adaptation, and DoRA's ability to balance fine-tuning efficiency with model precision. These insights provide practical guidance for deploying AI-driven generative systems in accuracy-critical domains such as healthcare, finance, and legal services, ensuring scalability, reliability, and optimal performance in dynamic environments.",
            "score": 0.5998827076559501,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "256630726",
            "title": "Lexicon-Enhanced Self-Supervised Training for Multilingual Dense Retrieval",
            "text": "In our method, we have incorporated two data augmentation modules, namely lexicon-enhanced retrieval, and query generation. Here, we would like to check how each module contributes to the final performance. We construct the ablation experiments on the Mr. TYDI data. We prepare four variants of our method that try all combinations: \n\n\u2022 w/o LR denotes that the retriever does not be fine-tuned with data from the lexicon-enhanced retrieval module. But the generator also is finetuned with data from the lexicon-enhanced retrieval module. \n\n\u2022 w/o LR + denotes that both the retriever and the generator do not be fine-tuned with data from the lexicon-enhanced retrieval module. \n\n\u2022 w/o QG denotes that the retriever does not be fine-tuned with data from the query generation module. \n\n\u2022 w/o ALL denotes without both the two modules, a.k.a., zero-shot multilingual retrieval. \n\nTable 4 presents all comparison results of the four variants. Due to the limited space, we present results for each language in Appendix E. As we can see, the performance rank can be given as follows w/o ALL < w/o QG < LeSTM. These results indicate that both the two augmentation modules are essential to improve performance. And we can find that the lexicon-enhanced retrieval module is more effective than the query generation module, because of w/o LR < w/o QG. In addition, we find that w/o LR > w/o LR + , it denotes the zero-shot multilingual query generation suffers from lots of problems and it also can demonstrate the effectiveness of the lexicon-enhanced retrieval module.",
            "score": 0.5998138423123007,
            "section_title": "Ablation Study",
            "char_start_offset": 22250,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 332
                },
                {
                    "start": 335,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 537
                },
                {
                    "start": 540,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 780
                },
                {
                    "start": 783,
                    "end": 872
                },
                {
                    "start": 875,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1558
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56494140625
        },
        {
            "corpus_id": "273346467",
            "title": "FunnelRAG: A Coarse-to-Fine Progressive Retrieval Paradigm for RAG",
            "text": "Retrieval-Augmented Generation (RAG) has been shown highly effective in enhancing Large Language Models (LLMs) (Gao et al., 2023;Shi et al., 2023) and has been widely adopted in the industry, such as Microsoft's GraphRAG (Edge et al., 2024), Google's REALM (Guu et al., 2020), and Meta's RA-DIT (Lin et al., 2024). Its effectiveness mainly comes from retrieving external nonparametric knowledge into LLMs to remedy their incomplete, incorrect, or outdated internal parametric knowledge (Karpukhin et al., 2020;Min et al., Figure 1: Comparison between (a) the flat retrieval and (b) the progressive retrieval paradigm, where is the segmentation operation. FUNNELRAG performs progressive retrieval from large to small quantity, from coarse to fine granularity, and with simple to complex retrievers, which balances effectiveness and efficiency. 2019). The de facto RAG framework usually segments documents into short retrieval units, such as 100-word passages (Jiang et al., 2024), resulting in a massive corpus with tens of millions of candidate units. Then, the retriever is tasked to find the \"needle\" (i.e., the golden retrieval units) from the \"haystack\" (i.e., the enormous candidate corpus) (Lee et al., 2024;Kamradt, 2023). Finally, the retrieved units serve as the input context to the generator to facilitate generation. Its working flow is shown in Figure 1(a). Wikipedia dump is used as the non-parametric knowledge source (Lewis et al., 2020), where each document is segmented into disjoint 100-word chunks, resulting in a total of 21M short passages. Then, the retriever seeks through a vast number of 21M candidates to get several potentially valuable passages, such as four. \n\nDespite effectiveness, the existing retrieval paradigms still suffer from two major limitations: \n\n\u2022 Flat Retrieval. Most RAG frameworks approach the retrieval stage as a one-off deal, where the retriever is requested to take tens of millions of candidates as input and to find the golden retrieval units at a sitting.",
            "score": 0.5995764866590958,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1688
                },
                {
                    "start": 1691,
                    "end": 1787
                },
                {
                    "start": 1790,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 2009
                }
            ],
            "ref_mentions": [
                {
                    "start": 486,
                    "end": 510,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.529296875
        },
        {
            "corpus_id": "274234701",
            "title": "RAMIE: retrieval-augmented multi-task information extraction with large language models on dietary supplements",
            "text": "sacrificed little performance to achieve multi-head BERT to tackle multiple tasks in one model. On the other hand, RAG 49 offers a promising avenue to enhance LLMs for DSs. By incorporating retrieval mechanisms, RAG allows models to access additional relevant examples during generation 20,50 . For instance, RAG has been successfully applied to improve question-answering systems 51 . \n\nOur contributions are summarized as follows, \n\n\u2022 To the best of our knowledge, it is the first work to explore the potential of LLMs for information extraction multitasks in the DS domain, including NER, RE, TE, UC tasks. \n\n\u2022 We proposed the RAMIE (Retrieval-Augmented Multi-task Information Extraction) framework which demonstrated a high performance via RAG and instruction fine-tuning, and achieve efficiency via MTL. \n\n\u2022 We conducted comprehensive experiments on 8 state-of-the-art LLMs through 1) single-task instruction fine-tuning, 2) our RAMIE framework, and 3) multi-task instruction fine-tuning.",
            "score": 0.5994945253032715,
            "section_title": "Introduction",
            "char_start_offset": 4229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 385
                },
                {
                    "start": 388,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 808
                },
                {
                    "start": 811,
                    "end": 993
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 121,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7021484375
        },
        {
            "corpus_id": "269293655",
            "title": "Evaluating Retrieval Quality in Retrieval-Augmented Generation",
            "text": "Retrieval-augmented generation (RAG) has emerged as a prominent approach in natural language processing, combining the strengths of retrieval and generation models [35], with use cases in decreasing hallucination [1,29], knowledge-grounding [9,16,34], and personalization [25,26].Evaluating RAG systems is important as it ensures the effectiveness of integrating retrieval-based methods with generative models [10,23].Traditionally, RAG evaluation has primarily relied on end-to-end assessment, which entails comparing the generated output with one or more ground truth references [20].While this is crucial, it presents several limitations, especially, for evaluating retrieval models in RAG systems.\n\nFirst, end-to-end evaluation lacks transparency regarding which retrieved document contributed to the generated output, hindering interpretability of the system's behavior.Secondly, it is resourceintensive, consuming significant time and computational power, particularly when dealing with a large set of retrieval results consumed by the LLM.To process long input sequences resulting from the utilization of all retrieved documents by the LLM, GPUs with substantial memory capacities are essential for end-to-end evaluation.Moreover, many ranking systems rely on interleaving (i.e., replacing one or more documents in the result list) for evaluation and optimization, which further complicates the evaluation, as slight variations in retrieval results necessitate re-computation of the RAG pipeline.Finally, optimizing ranking models often requires document-level feedback, such as user clicks [3,6].However, endto-end evaluation only provides list-level feedback for the retrieval results.That said, this paper studies retrieval evaluation in RAG.\n\nHuman annotations can be a potential solution for evaluating retrieval models in RAG, however, accurate annotations are often challenging and costly to obtain.More recently, with the emergence of large language models (LLMs) and their advanced capabilities in reasoning and text comprehension, they have been utilized to annotate documents for retrieval evaluation [10,23].Nevertheless, these approaches predominantly evaluate the retriever in RAG systems based on human preferences, whereas the primary objective of the retrieval model in RAG is to serve the LLM that leverages the retrieved results [35].",
            "score": 0.5991184604898318,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 418
                },
                {
                    "start": 418,
                    "end": 586
                },
                {
                    "start": 586,
                    "end": 701
                },
                {
                    "start": 703,
                    "end": 875
                },
                {
                    "start": 875,
                    "end": 1046
                },
                {
                    "start": 1046,
                    "end": 1228
                },
                {
                    "start": 1228,
                    "end": 1503
                },
                {
                    "start": 1503,
                    "end": 1604
                },
                {
                    "start": 1604,
                    "end": 1694
                },
                {
                    "start": 1694,
                    "end": 1752
                },
                {
                    "start": 1754,
                    "end": 1913
                },
                {
                    "start": 1913,
                    "end": 2127
                },
                {
                    "start": 2127,
                    "end": 2360
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 216,
                    "end": 219,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 244,
                    "end": 247,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "269605438"
                },
                {
                    "start": 272,
                    "end": 276,
                    "matchedPaperCorpusId": "269009728"
                },
                {
                    "start": 581,
                    "end": 585,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 1598,
                    "end": 1601,
                    "matchedPaperCorpusId": "258212955"
                },
                {
                    "start": 1601,
                    "end": 1603,
                    "matchedPaperCorpusId": "2979013"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7900390625
        },
        {
            "corpus_id": "268554288",
            "title": "FIT-RAG: Black-Box RAG with Factual Information and Token Reduction",
            "text": "Retrieval-Augmented Generation (RAG) is a technique that augments natural language generation models with relevant content retrieved from knowledge sources, aiming at improving the quality and relevance of text generation.Previous works have demonstrated its strong performance in knowledge-intensive tasks such as question answering, fact checking, and content recommendation [12,24,32,41].\n\nRetrievers interact with external corpus to acquire relevant information.For open-domain question answering, the Wikipedia corpus [6] is commonly used.As for retrieval methods, it can broadly be categorized into two types: sparse retrievers and dense retrievers.Sparse retrievers, such as TF-IDF [37] and BM25 [35], predominantly rely on keyword matching for document retrieval.These methods determine the relevance between queries and documents by analyzing the occurrence and distribution of keywords within the documents.Dense retrievers employ dual-encoders to generate dense vector representations of text for more accurate semantic matching.Consequently, dense retrievers are considered more suitable for retrieval-augmented applications.Some techniques like vector quantization [25,47] and embedding optimization [48] also improves the efficiency of dense retrievers.Common dense retrievers include DPR [20], ANCE [49] and Contriever [13].Specifically, DPR [20] is trained with supervised learning on question-answer pairs, and focuses on extracting relevant passages by analyzing the semantic content of both questions and answers.ANCE [49] leverages approximate nearest neighbor search and contrastive learning to enhance the model's ability to discern between relevant and non-relevant documents in a dense vector space.Contriever [13] employs unsupervised contrastive learning to adapt to inherent data structure, especially beneficial when the annotated training data is scarce.To enhance the quality of retrieved documents, some work conduct further reranking to these documents for personalization [3,40,56] and diversification [27,38].\n\nRecent work has explored different ways for language models to leverage retrieved or generated text as external knowledge.One approach is to integrate retrieval into language model pre-training or fine-tuning.For instance, REALM [9] integrates external document retrieval into pre-training, enhancing performance in downstream tasks by retrieving relevant information.",
            "score": 0.5975290700705944,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 8684,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 222,
                    "end": 391
                },
                {
                    "start": 393,
                    "end": 466
                },
                {
                    "start": 466,
                    "end": 544
                },
                {
                    "start": 544,
                    "end": 655
                },
                {
                    "start": 655,
                    "end": 771
                },
                {
                    "start": 771,
                    "end": 917
                },
                {
                    "start": 917,
                    "end": 1040
                },
                {
                    "start": 1040,
                    "end": 1137
                },
                {
                    "start": 1137,
                    "end": 1267
                },
                {
                    "start": 1267,
                    "end": 1339
                },
                {
                    "start": 1339,
                    "end": 1532
                },
                {
                    "start": 1532,
                    "end": 1723
                },
                {
                    "start": 1723,
                    "end": 1883
                },
                {
                    "start": 1883,
                    "end": 2043
                },
                {
                    "start": 2045,
                    "end": 2167
                },
                {
                    "start": 2167,
                    "end": 2254
                },
                {
                    "start": 2254,
                    "end": 2413
                }
            ],
            "ref_mentions": [
                {
                    "start": 377,
                    "end": 381,
                    "matchedPaperCorpusId": "257637217"
                },
                {
                    "start": 384,
                    "end": 387,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 387,
                    "end": 390,
                    "matchedPaperCorpusId": "4711425"
                },
                {
                    "start": 689,
                    "end": 693,
                    "matchedPaperCorpusId": "2996187"
                },
                {
                    "start": 703,
                    "end": 707,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 1178,
                    "end": 1182,
                    "matchedPaperCorpusId": "259949894"
                },
                {
                    "start": 1182,
                    "end": 1185,
                    "matchedPaperCorpusId": "247922621"
                },
                {
                    "start": 1213,
                    "end": 1217,
                    "matchedPaperCorpusId": "245986472"
                },
                {
                    "start": 1303,
                    "end": 1307,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1314,
                    "end": 1318,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1334,
                    "end": 1338,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 1357,
                    "end": 1361,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1537,
                    "end": 1541,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1734,
                    "end": 1738,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 2005,
                    "end": 2008,
                    "matchedPaperCorpusId": "1292249"
                },
                {
                    "start": 2008,
                    "end": 2011,
                    "matchedPaperCorpusId": "316030"
                },
                {
                    "start": 2011,
                    "end": 2014,
                    "matchedPaperCorpusId": "240230721"
                },
                {
                    "start": 2035,
                    "end": 2039,
                    "matchedPaperCorpusId": "220730230"
                },
                {
                    "start": 2039,
                    "end": 2042,
                    "matchedPaperCorpusId": "235792531"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78466796875
        },
        {
            "corpus_id": "272753366",
            "title": "Retrieval-Augmented Test Generation: How Far Are We?",
            "text": "Retrieval Augmented Generation (RAG) is a prompting strategy that enhances the responses of Large Language Models (LLMs) by incorporating additional knowledge sources beyond the pretrained knowledge base [2]. In RAG, the query intended for the LLM is used to search and retrieve relevant documents from a database [2,5]. This process leverages external knowledge, such as documents from various sources, to produce more contextually relevant and accurate responses compared to traditional generation-only models [5]. \n\nOne advantage of RAG is that it eliminates the need for retraining, as the LLM can search and access the latest information to generate more reliable output through retrieval-based generation [42]. RAG techniques have been successful in numerous software-related tasks, including code generation [32], code search [11], commit message generation [43], code suggestions [4], assertion generation [25], and automated program repair [25,36]. However, the effectiveness of RAG relies heavily on the quality of the retrieved knowledge [19]. Therefore, it is essential to assess the impact of various knowledge resources on the performance of RAG-based techniques.",
            "score": 0.5970132730270366,
            "section_title": "Retrieval Augmented Generation (RAG)",
            "char_start_offset": 5054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1177
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "270738768"
                },
                {
                    "start": 314,
                    "end": 317,
                    "matchedPaperCorpusId": "270738768"
                },
                {
                    "start": 317,
                    "end": 319,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 512,
                    "end": 515,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 833,
                    "end": 837,
                    "matchedPaperCorpusId": "265509385"
                },
                {
                    "start": 888,
                    "end": 891,
                    "matchedPaperCorpusId": "269130676"
                },
                {
                    "start": 914,
                    "end": 918,
                    "matchedPaperCorpusId": "259860357"
                },
                {
                    "start": 949,
                    "end": 953,
                    "matchedPaperCorpusId": "259860357"
                },
                {
                    "start": 953,
                    "end": 956,
                    "matchedPaperCorpusId": "261697451"
                },
                {
                    "start": 1049,
                    "end": 1053,
                    "matchedPaperCorpusId": "265466391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66162109375
        },
        {
            "corpus_id": "271097348",
            "title": "Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting",
            "text": "Retrieval Augmented Generation Retrieval Augmented Generation (RAG) enhances LLMs by retrieving relevant documents from external databases and incorporating them into the generation process (Gao et al., 2023b;Lewis et al., 2020;Khandelwal et al., 2020;Izacard & Grave, 2021;Luo et al., 2023a;Xia et al., 2024b;Wang et al., 2024). Recent work has primarily focused on enabling LLMs to understand when and what to retrieve (Ma et al., 2023;Chen et al., 2023b;Jiang et al., 2023b;Schick et al., 2024), or designing approaches to better utilize contexts (Yu et al., 2023;Yoran et al., 2023;Wang et al., 2023b;Sarthi et al., 2024;Baek et al., 2023;Xu et al., 2023;Kim et al., 2024). Among them, SAIL (Luo et al., 2023a) fine-tunes a pre-trained LLM on web search data to filter irrelevant contents. Self-Reflective RAG (Asai et al., 2023) introduces reflection tokens to guide retrieval and annotation in instruction-tuning datasets. However, both approaches require additional instruction-tuning of generic LLMs, which is resource-intensive and may lead to forgetting or over-fitting (Luo et al., 2023b). Furthermore, long context with retrieved documents can suffer from computational inefficiency and position bias (Liu et al., 2024). Corrective RAG (Yan et al., 2024) on the other hand proposes a lightweight retrieval evaluator, but it lacks the capability for high-level reasoning. In contrast, our proposed SPECULATIVE RAG addresses these limitations by leveraging a smaller RAG drafter model to efficiently understand diverse perspectives in retrieval results and generate drafts for the generalist LMs to verify and integrate.",
            "score": 0.5951456597545876,
            "section_title": "RELATED WORKS",
            "char_start_offset": 4900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1630
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 228,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 228,
                    "end": 252,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 457,
                    "end": 477,
                    "matchedPaperCorpusId": "15641339"
                },
                {
                    "start": 477,
                    "end": 497,
                    "matchedPaperCorpusId": "256697342"
                },
                {
                    "start": 625,
                    "end": 643,
                    "matchedPaperCorpusId": "264306280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53662109375
        },
        {
            "corpus_id": "271270644",
            "title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
            "text": "Training the generator involves updating its parameters or those in the retrieval fusion modules. Since the generator is generally an LLM, training the LLM is a resource-and time-consuming process. Fortunately, several parameter-efficient fine-tuning techniques, such as LoRA [57], are proposed to address the fine-tuning problem of LLMs. Although the parameters in the retrieval fusion modules are less than those in the generator, only fine-tuning those parameters may encounter some training problems, such as low convergence and overfitting. Jointly tuning the parameters in the generator and the retrieval fusion modules is a better way to train the generator and the retrieval fusion modules if there are sufficient and powerful resources.",
            "score": 0.5939689111540323,
            "section_title": "Training generator.",
            "char_start_offset": 32926,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 745
                }
            ],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 280,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95166015625
        },
        {
            "corpus_id": "273963542",
            "title": "Leveraging Retrieval-Augmented Generation for Persian University Knowledge Retrieval",
            "text": "Recent advancements in RAG have focused on innovative techniques and methodologies to optimize retrieval and generation processes. Lewis et al. (2020) highlight the power of RAG in knowledge-intensive NLP tasks, demonstrating its potential to solve complex information retrieval challenges [1]. Shahul et al. (2023) introduced RAGAS, a framework for automated evaluation of RAG pipelines, emphasizing the importance of reference-free evaluation metrics to enhance the evaluation process of RAG systems. Siriwardhana et al. (2022) developed RAG-end2end, which optimizes RAG for domain-specific knowledge bases, significantly improving performance in specialized domains such as healthcare and news [4]. Yu (2022) explored the use of retrieval-augmented generation across heterogeneous knowledge, addressing the challenges of retrieving information from diverse sources [6]. Nakhod (2023) proposed applying RAG to elevate low-code developer skills by integrating domain-specific knowledge into large language models, thereby improving their practical utility [9]. Melz (2023) introduced ARM-RAG, a system that enhances large language models' intelligence through storing and retrieving reasoning chains, demonstrating significant improvements in problem-solving tasks [10]. Chen et al. (2023) provided a comprehensive evaluation of the impact of RAG on large language models, highlighting the potential bottlenecks and challenges in applying RAG across different tasks [7]. Heydari et al. (2024) proposed the Context Awareness Gate (CAG) architecture, a novel mechanism that dynamically adjusts the LLM's input prompt based on whether the user query necessitates external context retrieval, thereby enhancing the efficiency and accuracy of RAG systems [11].",
            "score": 0.5930877064998845,
            "section_title": "B. Recent Advances and Techniques in RAG",
            "char_start_offset": 3454,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1755
                }
            ],
            "ref_mentions": [
                {
                    "start": 702,
                    "end": 711,
                    "matchedPaperCorpusId": "250391000"
                },
                {
                    "start": 868,
                    "end": 871,
                    "matchedPaperCorpusId": "250391000"
                },
                {
                    "start": 873,
                    "end": 886,
                    "matchedPaperCorpusId": "266204857"
                },
                {
                    "start": 1057,
                    "end": 1060,
                    "matchedPaperCorpusId": "266204857"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.845703125
        },
        {
            "corpus_id": "276249796",
            "title": "C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation",
            "text": "Recent advances in retrieval-augmented generation (RAG) for large language models (LLMs) have demonstrated remarkable capabilities in various tasks (Anthropic, 2024; Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). Hurst et al., 2024;Dubey et al., 2024;Yang et al., 2024a;Mesnard et al., 2024;Asai et al., 2024;Chen et al., 2024a;b;Wei et al., 2025;Sun et al., 2025b;a), empowering LLMs to acquire up-to-date or domain-specific knowledge while mitigating hallucinations (Gao et al., 2023;Fan et al., 2024;Qiao et al., 2024). The effectiveness of RAG systems, however, hinges on the alignment1 between the retriever and the LLM-an inherently challenging goal as these components are typically developed independently without co-training. This lack of co-training can result in semantic mismatch and suboptimal interactions: retrievers may fail to provide information tailored to the LLM's needs, while LLMs may struggle to generate effective queries or seamlessly incorporate retrieved content. Existing approaches address this misalignment through three main strategies: (1) fine-tuning retrievers to align with LLM preferences, (2) optimizing LLMs to adapt to retriever behavior, and (3) introducing intermediate modules to bridge the gap between them (Ma et al., 2023;Shi et al., 2024;Asai et al., 2024;Wei et al., 2025;Yu et al., 2024a;b). Despite progress, these methods face notable challenges: fine-tuning retrievers often requires carefully curated data and may not be feasible for commercial search engines (Schmidt, 2014;Nakano et al., 2021), while optimizing LLMs is resource-intensive and risks compromising their original capabilities (Zhou et al., 2024).",
            "score": 0.5928531881735628,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1757
                }
            ],
            "ref_mentions": [
                {
                    "start": 383,
                    "end": 401,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 401,
                    "end": 420,
                    "matchedPaperCorpusId": "270559546"
                },
                {
                    "start": 422,
                    "end": 439,
                    "matchedPaperCorpusId": "271909559"
                },
                {
                    "start": 439,
                    "end": 457,
                    "matchedPaperCorpusId": "270225999"
                },
                {
                    "start": 578,
                    "end": 595,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 1360,
                    "end": 1377,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 1377,
                    "end": 1395,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 1395,
                    "end": 1412,
                    "matchedPaperCorpusId": "271909559"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80517578125
        },
        {
            "corpus_id": "273374817",
            "title": "Evaluation of Attribution Bias in Retrieval-Augmented Large Language Models",
            "text": "Retrieval Augmented Generation. Given a question q and a set of top-k retrieved documents D ={d 1 , d 2 , . . . , d k } from a collection C, the goal of retrieval augmented generation (RAG) is to generate an answer for q using D as context. LLMs are currently an important component of RAG pipelines, acting as the generator. The generator is given q, D, and an instruction prompt on how to generate the answer (Jeong et al., 2024;Lee et al., 2024;Li et al., 2024a). Using top-k retrieved documents helps LLMs to be exposed to information that it might not have been trained/fine-tuned with during development. These documents are commonly retrieved using an effective sparse and/or dense retriever (Lewis et al., 2020). \n\nAttributive RAG. LLMs are prone to generate hallucinated (and even factually incorrect) answers (Ji et al., 2023;Rawte et al., 2023;Yue et al., 2024). \n\nAttributing answers in RAG with LLMs is an approach taken as a step towards ensuring the veracity of the output of these models (Bohnet et al., 2022;Hu et al., 2024;Kamalloo et al., 2023;Khalifa et al.;Li et al., 2024b). Menick et al. (2022) teach language models to support answers with verified quotes. Ye et al. (2024) propose a learning-based framework in which they fine-tune LLMs to generate citations, as opposed to prompting or relying on post-hoc attribution. Stolfo (2024) analyzes whether every generated sentence in the output of LLMs is grounded in the retrieved documents or the LLM's pre-training data.",
            "score": 0.5924679184980113,
            "section_title": "Background",
            "char_start_offset": 4962,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1493
                }
            ],
            "ref_mentions": [
                {
                    "start": 411,
                    "end": 431,
                    "matchedPaperCorpusId": "268553748"
                },
                {
                    "start": 431,
                    "end": 448,
                    "matchedPaperCorpusId": "270514562"
                },
                {
                    "start": 448,
                    "end": 465,
                    "matchedPaperCorpusId": "259501744"
                },
                {
                    "start": 699,
                    "end": 719,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 819,
                    "end": 836,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 836,
                    "end": 855,
                    "matchedPaperCorpusId": "263831293"
                },
                {
                    "start": 855,
                    "end": 872,
                    "matchedPaperCorpusId": "268667523"
                },
                {
                    "start": 1063,
                    "end": 1078,
                    "matchedPaperCorpusId": "268819100"
                },
                {
                    "start": 1078,
                    "end": 1095,
                    "matchedPaperCorpusId": "263830219"
                },
                {
                    "start": 1181,
                    "end": 1197,
                    "matchedPaperCorpusId": "265220884"
                },
                {
                    "start": 1345,
                    "end": 1358,
                    "matchedPaperCorpusId": "269033410"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6494140625
        },
        {
            "corpus_id": "272827955",
            "title": "Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely",
            "text": "Generating responses requires determining if the retrieved information is sufficient or if additional external data is needed. Handling conflicts between retrieved knowledge and the model's internal prior knowledge is also essential [84,85,86]. Supervised fine-tuning is an effective method to enhance the generation performance in RAG systems. When faced with irrelevant or erroneous information as the retrieved context, pre-trained large language models are often easily misled, resulting in incorrect responses. Many studies have shown that by subtly designing training data for RAG systems, fine-tuning or pretraining can effectively mitigate this issue [87,88,89]. Through experimental analysis, RAAT [89], demonstrated that the detrimental effects of irrelevant retrieval noise, relevant retrieval noise, and counterfactual retrieval noise on RAG models increase progressively. By incorporating with these training process, these methods enables the LLM to internally recognize noisy contexts, leading to significant improvements in response generation quality even in the presence of noisy retrievals. Furthermore, to ensure more consistent performance between the retriever and generator within the RAG system, some studies employ joint training of both retriever and generator during the training phase [90,91,92]. \n\n4 Implicit Fact Queries (L2)",
            "score": 0.5920132829526401,
            "section_title": "Response Generation Enhancement",
            "char_start_offset": 25340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1324
                },
                {
                    "start": 1327,
                    "end": 1355
                }
            ],
            "ref_mentions": [
                {
                    "start": 1317,
                    "end": 1320,
                    "matchedPaperCorpusId": "252568176"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92041015625
        },
        {
            "corpus_id": "258170263",
            "title": "Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study",
            "text": "Retrieval has been applied in various NLP tasks for years, including question answering (QA) (e.g., Bilotti et al., 2007), machine translation (e.g., Zhang et al., 2018), and conversation (Shuster et al., 2021;Thoppilan et al., 2022;Komeili et al., 2021). In particular, language models have been augmented with retrieval at different stages, including inference time (Khandelwal et al., 2020;Yogatama et al., 2021), fine-tuning stage (Karpukhin et al., 2020;Lewis et al., 2020b;Guu et al., 2020), and pretraining stage (Borgeaud et al., 2022;Izacard et al., 2022). \n\nLMs have been augmented with retrieval at the fine-tuning stage for downstream tasks, primarily for open-domain QA. DPR (Karpukhin et al., 2020) finetunes one BERT to encode questions and the other BERT to encode answers within a dual encoder framework, using a contrastive loss to align the hidden representations of question and corresponding answer. RAG (Lewis et al., 2020b) studies the fine-tuning recipe for retrieval-augmented generation models, especially on open-domain QA tasks. FiD (Izacard and Grave, 2021) improves RAG with a better LM backbone T5, and fuses multiple retrieved passages to the decoder during fine-tuning to further improve QA accuracy. We-bGPT (Nakano et al., 2021) leverages web search engine and fine-tunes GPT using reinforcement learning with human feedback (RLHF) for reference generation and factuality improvement, which is orthogonal to our work that focuses on pretraining with retrieval. The proposed RLHF can be applied to RETRO as well. \n\nREALM (Guu et al., 2020) performs both unsupervised pretraining and supervised fine-tuning strategies for retrieval-augmented BERT model in open-domain QA.",
            "score": 0.5911680160244668,
            "section_title": "Related Work",
            "char_start_offset": 7050,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1546
                },
                {
                    "start": 1549,
                    "end": 1704
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 121,
                    "matchedPaperCorpusId": "6265033"
                },
                {
                    "start": 150,
                    "end": 169,
                    "matchedPaperCorpusId": "4698173"
                },
                {
                    "start": 393,
                    "end": 415,
                    "matchedPaperCorpusId": "231717977"
                },
                {
                    "start": 435,
                    "end": 459,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 479,
                    "end": 496,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 520,
                    "end": 543,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 688,
                    "end": 712,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1061,
                    "end": 1086,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83056640625
        },
        {
            "corpus_id": "270357334",
            "title": "RAG Does Not Work for Enterprises",
            "text": "Retrieval-Augmented Generation (RAG) is an emerging paradigm that combines the strengths of pre-trained language models with external knowledge retrieval to enhance the accuracy, consistency, and contextual relevance of generated outputs [ Lewis et al., 2020 ].In a typical RAG architecture, a retriever component first selects the most relevant documents or passages based on the input query, and then a generator component conditions on both the query and the retrieved content to produce a final output [ Izacard and Grave, 2021 ].RAG has shown significant promise in improving the factual accuracy, consistency, and contextual awareness of generative models across a wide range of applications, such as question answering, dialogue systems, and content creation [ Zhao et al., 2024 ].However, implementing RAG effectively in real-world, enterprise settings poses several challenges, which this paper aims to address.",
            "score": 0.5910527326289035,
            "section_title": "Background",
            "char_start_offset": 5003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 261,
                    "end": 534
                },
                {
                    "start": 534,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 920
                }
            ],
            "ref_mentions": [
                {
                    "start": 506,
                    "end": 533,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 766,
                    "end": 787,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81982421875
        },
        {
            "corpus_id": "274763296",
            "title": "MST-R: Multi-Stage Tuning for Retrieval Systems and Metric Evaluation",
            "text": "Regulatory documents are rich in nuanced terminology and specialized semantics. FRAG systems: Frozen retrieval-augmented generators utilizing pre-trained (or, frozen) components face consequent challenges with both retriever and answering performance. We present a system that adapts the retriever performance to the target domain using a multi-stage tuning (MST) strategy. Our retrieval approach, called MST-R (a) first fine-tunes encoders used in vector stores using hard negative mining, (b) then uses a hybrid retriever, combining sparse and dense retrievers using reciprocal rank fusion, and then (c) adapts the cross-attention encoder by fine-tuning only the top-k retrieved results. We benchmark the system performance on the dataset released for the RIRAG challenge (as part of the RegNLP workshop at COLING 2025). We achieve significant performance gains obtaining a top rank on the RegNLP challenge leaderboard. We also show that a trivial answering approach games the RePASs metric outscoring all baselines and a pre-trained Llama model. Analyzing this anomaly, we present important takeaways for future research.",
            "score": 0.589681845057481,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.818359375
        },
        {
            "corpus_id": "273229050",
            "title": "Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG",
            "text": "Retrieval-augmented generation (RAG) empowers large language models (LLMs) to utilize external knowledge sources. The increasing capacity of LLMs to process longer input sequences opens up avenues for providing more retrieved information, to potentially enhance the quality of generated outputs. It is plausible to assume that a larger retrieval set would contain more relevant information (higher recall), that might result in improved performance. However, our empirical findings demonstrate that for many long-context LLMs, the quality of generated output initially improves first, but then subsequently declines as the number of retrieved passages increases. This paper investigates this phenomenon, identifying the detrimental impact of retrieved\"hard negatives\"as a key contributor. To mitigate this and enhance the robustness of long-context LLM-based RAG, we propose both training-free and training-based approaches. We first showcase the effectiveness of retrieval reordering as a simple yet powerful training-free optimization. Furthermore, we explore training-based methods, specifically RAG-specific implicit LLM fine-tuning and RAG-oriented fine-tuning with intermediate reasoning, demonstrating their capacity for substantial performance gains. Finally, we conduct a systematic analysis of design choices for these training-based methods, including data distribution, retriever selection, and training context length.",
            "score": 0.5895243227922115,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8623046875
        },
        {
            "corpus_id": "276939617",
            "title": "Enhancing the Precision and Interpretability of Retrieval-Augmented Generation (RAG) in Legal Technology: A Survey",
            "text": "Fine-tuning retrieval models is essential for aligning embeddings with legal-domain-specific data, particularly when the context diverges considerably from the For example, HyPA-RAG fine-tunes its distilBERT model on legal corpora and CASEGPT adopts a fine-tuned version of Legal-BERT to achieve enhanced retrieval performance [39], [49]. In addition, CamemBERT is fully fine-tuned on the long-form LQA (LLeQA) dataset to improve its ability to handle complex legal queries [21]. Table 4 shows the best Transformers-based retrievers, of which four models are enhanced by fine-tuning processes.",
            "score": 0.589481376550888,
            "section_title": "3) FINE-TUNING RETRIEVAL MODELS",
            "char_start_offset": 27999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 593
                }
            ],
            "ref_mentions": [
                {
                    "start": 474,
                    "end": 478,
                    "matchedPaperCorpusId": "263310713"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7900390625
        },
        {
            "corpus_id": "273186155",
            "title": "Enhancing Retrieval in QA Systems with Derived Feature Association",
            "text": "First introduced by [8], retrieval augmented generation (RAG) allows LLMs to pull relevant information into context from a cache of documents. The system allows these models to access up-to-date information, rely less on their parameterized-memory, and leverage a large corpus of documents during generation, despite their limited context window [17]. RAG extends LLMs with a retrieval mechanism that takes a query, selects the most relevant texts from a given corpus, and hands them to the generator to inform its answer. Early approaches, were optimized end-to-end, using a jointly learned retriever and generator that communicated through a shared embedding space [8]. However, the requirement that such a system must be trained from scratch for each choice of generator architecture makes this approach expensive and cumbersome given the rapid pace with which new LLMs are developed. However, this paradigm was subverted by [14], which assumes the generator to be black-box, training a generator-agnostic retriever that simply prepends the retrieved text to the generator's input. In practice, the retriever is often further simplified to score documents based on their cosine similarity in a pretrained embedding space (dense retrieval); also popular is the use of BM25, a simple term-frequency based similarity metric (sparse retrieval) [17].",
            "score": 0.5889691532155026,
            "section_title": "Introduction 1.Preliminaries",
            "char_start_offset": 31,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1348
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8388671875
        },
        {
            "corpus_id": "270737712",
            "title": "Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need",
            "text": "Since the launch of ChatGPT in November 2022, Large Language Models (LLMs) have become increasingly popular integrated components for organizations seeking to enhance productivity and enrich their product portfolio offerings. However, it is well known that GPT-4 Turbo's training corpora cut-off date is April 2023, rendering the model lacking in current events knowledge. Furthermore, as LLMs are pre-trained on public domain text and do not possess proprietary information, their capabilities are limited when it comes to our company's knowledge-intensive applications. \n\nFine-tuning (Radford et al., 2018;Dodge et al., 2020) is a technique that can be used to inject new knowledge into pre-trained LLMs by adjusting their gradient parameters through fitting specific datasets. However, OpenAI's fine-tuning API is currently only available through an experimental access program, and GPT-4 fine-tuning requires more effort to achieve significant improvements, as noted by OpenAI (2023). \n\nRetrieval-augmented generation (RAG) is proposed initially by Lewis et al. (2021). This method involves storing extra knowledge in a non-parametric dense vector index and using a pre-trained neural retriever to search relevant context, followed by generating content with a pre-trained sequence-tosequence (seq2seq) model. Ovadia et al. (2024) argue that RAG consistently outperforms unsupervised fine-tuning on a wide range of knowledgeintensive tasks. \n\nA RAG application leveraging LLMs enhanced with the company's proprietary knowledge has become one of the pivotal factors advancing the adoption of GPT-4-based applications at our enterprise, a global payment technology company. This phenomenon highlights the need to apply viable approaches to evaluate RAG applications, as lacking performance metrics poses risks to the enterprise' business and may have negative consequences. \n\nOne approach to evaluate the quality of RAGs focuses on their unique characteristic: that they consist of a Retriever model and a Generator model. Therefore, studies have used metrics such as context relevance, and answer relevance to evaluate the two components separately then assess the answer faithfulness between them (Saad-Falcon et al., 2024;Es et al., 2023).",
            "score": 0.5887782325130655,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 988
                },
                {
                    "start": 991,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1444
                },
                {
                    "start": 1447,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1875
                },
                {
                    "start": 1878,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2244
                }
            ],
            "ref_mentions": [
                {
                    "start": 586,
                    "end": 608,
                    "matchedPaperCorpusId": "49313245"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6962890625
        },
        {
            "corpus_id": "275906690",
            "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
            "text": "Retrieval-augmented generation (RAG) is a hybrid approach that integrates LLMs with a retrieval mechanism to fetch relevant information from external documents or knowledge in real-time during the text generation process. This technique allows LLMs to dynamically access vast external corpora (like knowledge graphs, databases or search engines) to retrieve the most relevant information [41,90,105,107]. RAG enables language models to retrieve factual information and generate more accurate and contextually aware outputs, especially in cases where a language model's training data may be outdated or incomplete [99]. This approach provides the LLM with an ''external memory'' to supplement its internal knowledge base, thus enhancing LLM quality and accuracy. \n\nRAG comprise of retrievers, generators, and knowledge bases [41]. The retriever dynamically fetches relevant information from external corpora, the generator uses this retrieved information to generate a response, and the knowledge base is a collection of text such as scientific articles, news articles, or Knowledge Graphs [95,119,120]. This architecture allows the LLM to access up-to-date knowledge beyond its static training data, alleviating hallucination issues by grounding the generated response in factual data. \n\nRAG enhances retrieval and generation through several key techniques which may include document chunking, embedding models, retrieval techniques, querying, knowledge graph integration, iterative retrieval and generation and self-reflection. Chunking breaks down large text into manageable sizes using a mix of static and semantic methods to maintain context [119,121]. Helps embedding models to represent information and queries in a way that maintains semantic meaning, improving retrieval accuracy [25]. RAG employs retrieval methods such as dense retrieval, which uses vector representations (cosine similarity) for semantic matching, and sparse encoding for keyword matches between queries and documents; with hybrid approaches combining the strengths of both [95,119]. Dense Passage Retrieval (DPR) technique utilizes dense embeddings to match semantically relevant document chunks to queries and the combination of DPR with traditional sparse retrieval (i.e., BM25) has been shown to further enhance retrieval precision in complex or high-precision tasks [82,109].",
            "score": 0.5885655634074616,
            "section_title": "Retrieval augmented generation",
            "char_start_offset": 48408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 761
                },
                {
                    "start": 764,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1285
                },
                {
                    "start": 1288,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2358
                }
            ],
            "ref_mentions": [
                {
                    "start": 392,
                    "end": 395,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 1097,
                    "end": 1101,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7802734375
        },
        {
            "corpus_id": "268032903",
            "title": "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)",
            "text": "In our research, we concentrated primarily on the application of retrieval augmentation during the inference stage, without delving into its integration during pre-training or fine-tuning phases. Future work will aim to explore these compelling areas. Moreover, while our study has highlighted the privacy risks associated with commonly employed retrieval-augmented generation (RAG) systems, other retrieval-based language models (LMs) feature distinct components and architectures (Huang et al., 2023;Borgeaud et al., 2022) that warrant further investigation. In addition, developing effective strategies to protect retrieval data and leveraging RAG systems for the safeguarding of training data represent open research questions that we intend to pursue.",
            "score": 0.5885092841619601,
            "section_title": "Limitations",
            "char_start_offset": 29021,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 756
                }
            ],
            "ref_mentions": [
                {
                    "start": 502,
                    "end": 524,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54052734375
        },
        {
            "corpus_id": "270688549",
            "title": "RE-AdaptIR: Improving Information Retrieval through Reverse Engineered Adaptation",
            "text": "Information retrieval (IR) is a fundamental component of various modern applications, powering search engines, recommender systems, and various data analytics pipelines.Recently, large language models (LLMs) have achieved state-of-the art results on dense text retrieval, identifying and ranking the most relevant text for a given query by comparing learned vector representations of the text (Reimers and Gurevych, 2019;Khattab and Zaharia, 2020;Karpukhin et al., 2020;Izacard et al., 2022;Ma et al., 2023;Jiang et al., 2023;Weller et al., 2024).The effectiveness of text retrieval systems have a direct impact on numerous domains, including healthcare, finance, and social media, where accurate and timely access to information is critical.Retrieval is also critical in the context of retrieval augmented generation (RAG), enabling LLMs access to external resources when constructing a response (Lewis et al., 2020).For these reasons, we seek a practical and efficient approach for improving existing text retrieval models.Supervised fine-tuning of LLMs for text retrieval tasks has become a widely adopted approach, leveraging their pretrained language understanding capabilities to achieve state-of-the-art results on various benchmarks (Ma et al., 2023;Jiang et al., 2023;Weller et al., 2024).However, adapting an LLM for text retrieval requires labeled datasets, with numerous example queries and documents both related and unrelated to forming a helpful response.This poses a significant challenge to improving these systems, as data annotation or synthetic generation can be too expensive, difficult, and errorprone (Fredriksson et al., 2020;Desmond et al., 2021).Making matters worse, fine-tuning an existing LLM on new domains can cause forgetting, a decreased performance on previously capable tasks (McCloskey and Cohen, 1989;Kotha et al., 2024).Fleshman and Van Durme (2024) recently proposed reverse engineered adaptation (RE-ADAPT), an approach for solving similar dilemmas faced when fine-tuning existing instruction-tuned models.",
            "score": 0.588063353435694,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 169,
                    "end": 547
                },
                {
                    "start": 547,
                    "end": 742
                },
                {
                    "start": 742,
                    "end": 918
                },
                {
                    "start": 918,
                    "end": 1025
                },
                {
                    "start": 1025,
                    "end": 1298
                },
                {
                    "start": 1298,
                    "end": 1470
                },
                {
                    "start": 1470,
                    "end": 1672
                },
                {
                    "start": 1672,
                    "end": 1858
                },
                {
                    "start": 1858,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 393,
                    "end": 421,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 421,
                    "end": 447,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 447,
                    "end": 470,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 897,
                    "end": 917,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1624,
                    "end": 1650,
                    "matchedPaperCorpusId": "227130095"
                },
                {
                    "start": 1811,
                    "end": 1838,
                    "matchedPaperCorpusId": "61019113"
                },
                {
                    "start": 1838,
                    "end": 1857,
                    "matchedPaperCorpusId": "262054014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70751953125
        },
        {
            "corpus_id": "270123034",
            "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
            "text": "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs' original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs' performance but also preserves their general generation capabilities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across 12 question-answering tasks demonstrate the superiority of our approach.",
            "score": 0.5877741370455536,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89501953125
        },
        {
            "corpus_id": "267061013",
            "title": "Dynamic Q&A of Clinical Documents with Large Language Models",
            "text": "Retrieval Augmented Generation (RAG) has emerged as a transformative technique across diverse domains, revolutionizing information processing and decision-making processes. \n\nIn a work by Datta et al. [16], the authors propose two models, MAKG (Medical Appropriateness Knowledge Graph) and RAG-GPT (Retrieval Augmented Generation -Generative Pretrained Transformer). MAKG functions as an autonomous coarse-grained medical-inappropriateness vigilance model for payers and regulators, while RAG-GPT operates as a fine-grained LLM with human-in-the-loop for assessing medical appropriateness and inappropriateness. \n\nAnother work by Lewis et al. [34] introduces RAG models that combine parametric and non-parametric memory components to enhance sequence-to-sequence (seq2seq) models. By endowing pre-trained, parametric-memory generation models with a non-parametric memory, RAG models achieve state-of-the-art results in open-domain extractive question answering and knowledge-intensive generation tasks. \n\nIn the same vein of the previous works, our research focuses on RAG for medical question answering and exploring how RAG in this context is more efficient than traditional models fine-tuning.",
            "score": 0.5877428988248021,
            "section_title": "Retrieval Augmented Generation (RAG) in Various Contexts",
            "char_start_offset": 11665,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 175,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 1002
                },
                {
                    "start": 1005,
                    "end": 1196
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "266126075"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7080078125
        },
        {
            "corpus_id": "270870796",
            "title": "From Efficient Multimodal Models to World Models: A Survey",
            "text": "Fine-tuning techniques involve further training pre-trained models on specific task datasets to enhance model performance in that task.Below are common fine-tuning techniques and their recent advancements:\n\nLoRA (Low-Rank Adaptation) [59] is a low-rank adaptation technique that adds low-rank matrices to pre-trained models for fine-tuning, reducing computational and storage costs while maintaining performance.QLoRA [60] is an improved version that further optimizes the fine-tuning process through quantization techniques.Retrieval-Augmented Generation (RAG) [61] combines information retrieval and generative models, enhancing generative model performance by retrieving relevant information from external data sources.The LangChain [62] library provides various tools allowing large models to access real-time information from sources like Google Search, vector databases, or knowledge graphs, further improving RAG effectiveness.LlamaIndex (GPT Index) [63], [64] is an integrated data framework designed to enhance large language models (LLMs) by enabling the use of private or custom data.LlamaIndex provides data connectors, indexing and graph-building mechanisms, and advanced retrieval and query interfaces, simplifying data integration and information retrieval processes.\n\nBy applying these fine-tuning techniques appropriately, pretrained model knowledge can be fully utilized, improving performance in new tasks while reducing training time and computational resource consumption.",
            "score": 0.5874777786508157,
            "section_title": "G. Fine-Tuning Techniques for Model Architectures",
            "char_start_offset": 32883,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 135,
                    "end": 205
                },
                {
                    "start": 207,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 525
                },
                {
                    "start": 525,
                    "end": 722
                },
                {
                    "start": 722,
                    "end": 934
                },
                {
                    "start": 934,
                    "end": 1095
                },
                {
                    "start": 1095,
                    "end": 1282
                },
                {
                    "start": 1284,
                    "end": 1493
                }
            ],
            "ref_mentions": [
                {
                    "start": 418,
                    "end": 422,
                    "matchedPaperCorpusId": "258841328"
                },
                {
                    "start": 562,
                    "end": 566,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 736,
                    "end": 740,
                    "matchedPaperCorpusId": "260223847"
                },
                {
                    "start": 963,
                    "end": 967,
                    "matchedPaperCorpusId": "73651944"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9267578125
        },
        {
            "corpus_id": "269149146",
            "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
            "text": "We experimented with five different combinations of finetuning and RAG in order to determine what the most effective approaches could be for the SLR-automation process.Our experimental setup investigated the following methodologies:\n\n1. Baseline: Evaluate the Mistral-7B-Instruct on its knowledge and ability to answer SLR-related test dataset questions.\n\n2. Fine-tuning LLMs using LoRA: Leveraging Low-Rank Adaptation for fast and efficient parameter adjustment.\n\n3. Fine-tuning LLMs using NEFTune: Introducing noise into embedding vectors to investigate effects on generalization improvements.\n\n4. Instruct LLM + RAG with Raw Articles: Combining LLMs with Retrieval-Augmented Generation, using unprocessed article text as the retrieval corpus.\n\n5. Instruct LLM + RAG with Auto-Extracted Data: Employing RAG with a knowledge base of automatically extracted data comprising the finetuning dataset for focused information retrieval.\n\n6. Best Finetuned LLMs + Best RAG Solution: Integrating the top-performing fine-tuning and RAG methods to optimize SLR automation.\n\nEach method also summarised in Table 3, was evaluated for factually correct answers with respect to the SLR dataset.",
            "score": 0.587317214905158,
            "section_title": "Experimental Design",
            "char_start_offset": 44331,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 232
                },
                {
                    "start": 234,
                    "end": 354
                },
                {
                    "start": 356,
                    "end": 463
                },
                {
                    "start": 465,
                    "end": 595
                },
                {
                    "start": 597,
                    "end": 745
                },
                {
                    "start": 747,
                    "end": 931
                },
                {
                    "start": 933,
                    "end": 1063
                },
                {
                    "start": 1065,
                    "end": 1181
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85107421875
        },
        {
            "corpus_id": "272689561",
            "title": "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey",
            "text": "These enhanced systems, known as Advanced RAG, introduce specialized modules at different stages of the retrieval and generation pipeline, which can be categorized as pre-retrieval and post-retrieval components. In the pre-retrieval stage, a common issue is that the original query may be too short or vague, resulting in irrelevant retrieval results. To address this, a rewriter is introduced to clarify or expand the query. Rewriting methods include directly prompting the LLM [34,35] or training a rewriter model using feedback from the generator [36]. In the post-retrieval stage, the generator often faces challenges due to the length or noise of the retrieved content, which can affect the generation quality [33,37]. To mitigate this, a reranker is used to reorder the retrieval results [38]. Rerankers, often using cross-encoder architectures, better measure the similarity between the query and retrieved documents, pushing more relevant documents forward and removing less relevant ones. Another optimization component is the refiner, which summarizes or compresses retrieved content using techniques like prompting the LLM to summarize [39,40], or training a summarizer through supervised fine-tuning or reinforcement learning [41][42][43]. Despite the flexibility of Advanced RAG, its sequential structure limits adaptability in complex scenarios, such as queries requiring step-by-step reasoning. \n\nModular RAG. As RAG research evolves, it has entered the modular RAG stage, where components are treated as flexible modules that can be combined to create customized pipelines for different scenarios, offering greater flexibility and adaptability. Research now focuses on optimizing these pipelines, which come in four main types: Sequential, Conditional, Branching, and Loop. Sequential Pipelines process queries linearly, similar to advanced RAG, with pre-retrieval and post-retrieval stages. Conditional Pipelines route queries along different execution paths based on their type. For instance, SKR [44] identifies queries that the LLM can answer without retrieval, while Adaptive-RAG [45] classifies queries as simple or complex, using multi-round retrieval for complex ones. Branching Pipelines execute multiple paths simultaneously for a query, combining the results to form the final output.",
            "score": 0.5867210051639979,
            "section_title": "Retrieval-augmented Generation System",
            "char_start_offset": 8637,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1409
                },
                {
                    "start": 1412,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2192
                },
                {
                    "start": 2193,
                    "end": 2311
                }
            ],
            "ref_mentions": [
                {
                    "start": 479,
                    "end": 483,
                    "matchedPaperCorpusId": "263830368"
                },
                {
                    "start": 483,
                    "end": 486,
                    "matchedPaperCorpusId": "252519173"
                },
                {
                    "start": 550,
                    "end": 554,
                    "matchedPaperCorpusId": "258841283"
                },
                {
                    "start": 715,
                    "end": 719,
                    "matchedPaperCorpusId": "256459776"
                },
                {
                    "start": 794,
                    "end": 798,
                    "matchedPaperCorpusId": "250391085"
                },
                {
                    "start": 1147,
                    "end": 1151,
                    "matchedPaperCorpusId": "263831502"
                },
                {
                    "start": 1151,
                    "end": 1154,
                    "matchedPaperCorpusId": "269293435"
                },
                {
                    "start": 1238,
                    "end": 1242,
                    "matchedPaperCorpusId": "271745607"
                },
                {
                    "start": 1242,
                    "end": 1246,
                    "matchedPaperCorpusId": "267751485"
                },
                {
                    "start": 1246,
                    "end": 1250,
                    "matchedPaperCorpusId": "264590451"
                },
                {
                    "start": 2015,
                    "end": 2019,
                    "matchedPaperCorpusId": "263828724"
                },
                {
                    "start": 2101,
                    "end": 2105,
                    "matchedPaperCorpusId": "268553748"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.939453125
        },
        {
            "corpus_id": "273532677",
            "title": "SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains",
            "text": "Retrieval-augmented generation (RAG) (Lewis et al., 2020;Gao et al., 2023;Guti\u00e9rrez et al., 2024;Asai et al., 2024) is a powerful technique that enhances large language models (LLMs) for various knowledge-intensive tasks such as question answering (QA) by incorporating external knowledge sources. This method not only customizes responses to handle long-tail knowledge but also avoids the need for costly model retraining (Ovadia et al., 2023). Additionally, RAG helps reduce the issue of LLM hallucination by ensuring responses are grounded in relevant evidence (Shuster et al., 2021), thereby improving the overall accuracy and reliability of LLM outputs. \n\n* Work done during an internship at Amazon. \n\nWhile extensive research has focused on developing effective (Asai et al., 2024;Lin et al., 2024;Liu et al., 2024) and efficient (Xu et al., 2024a) RAG systems for general-domain QA tasks, adapting RAG to specialized domains for LLMs poses significant challenges. These models often struggle with distribution shifts and fail to accurately extract information from domain-specific contexts (Miller et al., 2020;Liu et al., 2022). Moreover, directly using black-box LLMs (OpenAI, 2023;Anthropic, 2023;Wang et al., 2023b) in specialized domains raises concerns about privacy when dealing with sensitive proprietary data. It is essential to finetune LLMs on domain-relevant QA tasks to unlock the full potential of LLM-based RAG systems in specialized domains. \n\nDespite the critical need for domain-specific finetuning, the primary challenge lies in the acquisition of high-quality fine-tuning data towards RAG applications. Prior works rely on continuous pretraining (Chen et al., 2023;Zhang et al., 2024a) on specialized corpora or fine-tuning on domain-specific instruction-tuning data (Wu et al., 2024;Wadden et al., 2024). However, the mismatch between these general-purpose tasks and domain-specific QA hinders their effectiveness.",
            "score": 0.5857765657580918,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 704
                },
                {
                    "start": 707,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1464
                },
                {
                    "start": 1467,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1942
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 97,
                    "end": 115,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 564,
                    "end": 586,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 768,
                    "end": 787,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 1097,
                    "end": 1118,
                    "matchedPaperCorpusId": "216867120"
                },
                {
                    "start": 1118,
                    "end": 1135,
                    "matchedPaperCorpusId": "237417170"
                },
                {
                    "start": 1794,
                    "end": 1811,
                    "matchedPaperCorpusId": "269136910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81396484375
        },
        {
            "corpus_id": "269043117",
            "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers",
            "text": "From the retriever evaluation experiments, we know the best retriever, i.e., the best combination of indices + query. In this section, we extend this knowledge to evaluate the RAG pipeline. To avoid the effect of LLM size or type, we perform all experiments using FLAN-T5-XXL. SqUAD is a commonly bench-marked dataset for RAG systems or Generative Q&A using LLMs. Our study juxtaposes three variations of the RAG pipeline from prior work using the evaluation metrics of Exact Match (EM) and F1 scores to gauge the accuracy of answer generation, as well as Top-5 and Top-10 for retrieval accuracy. \n\n\u2022 RAG-original [12]: This variant, a model fine-tuned on the Natural Questions dataset, has been appraised without domain-specific adaptation. \u2022 RAG-end2end [12]: As an extension of RAG-original, this model undergoes additional fine-tuning, tailored for domain adaptation to the SQuAD. \u2022 Blended RAG: Distinctively, our Blended RAG variant has not undergone training on the SQuAD dataset or any related corpora. It harnesses an optimized amalgamation of field selections and hybrid query formulations with semantic indices to feed LLMs to render the most precise responses possible. \n\nConsequently, as shown in Table IV, our Blended RAG showcases enhanced performance for Generative Q&A with F1 scores higher by 50%, even without dataset-specific finetuning. This characteristic is particularly advantageous for large enterprise datasets, where fine-tuning may be impractical or unfeasible, underscoring this research's principal application.",
            "score": 0.5857238818305788,
            "section_title": "V. RAG EXPERIMENTATION",
            "char_start_offset": 13629,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 596
                },
                {
                    "start": 599,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1541
                }
            ],
            "ref_mentions": [
                {
                    "start": 614,
                    "end": 618,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 756,
                    "end": 760,
                    "matchedPaperCorpusId": "252735056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79296875
        },
        {
            "corpus_id": "278129706",
            "title": "DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering",
            "text": "The earliest Retrieval-Augmented Generation (RAG) methods adopted the retrieve-then-read paradigm. Initially, a retriever fetches relevant documents from a corpus, and then a generative model produces an answer based on these documents. Common retrieval methods include sparse retrieval (e.g., BM25 (Robertson and Zaragoza, 2009)), dense retrieval (e.g., E5 (Wang et al., 2022), DPR (Karpukhin et al., 2020)), and search engines like Bing and Google. \n\nTo enhance retrieval accuracy, researchers have proposed various optimization strategies. D2LLM (Liao et al., 2024) transfers the computationally expensive cross-encoder capabilities to a more efficient bi-encoder model. MRAG (Besta et al., 2024) introduces multi-head attention mech- anisms to encode documents into multiple vectors, capturing semantic information more comprehensively. Additionally, some studies utilize reranking techniques to filter retrieval results, ensuring that only the most relevant knowledge is retained (Chen et al., 2024a;Yu et al., 2024b). LongLLM-Lingua (Jiang et al., 2024) further optimizes the document ranking order. \n\nMeanwhile, some research has explored leveraging the inherent knowledge capabilities of large models to enhance the adaptability of retrieval strategies, thereby reducing unnecessary external queries. For instance, SKR (Wang et al., 2023b) assesses the complexity of the the given question by comparing it with similar past questions. FLARE (Jiang et al., 2023b) and DRAGIN (Su et al., 2024) trigger external retrieval when the model's output logits indicate uncertainty. \n\nGiven that the effectiveness of RAG systems heavily depends on query quality, many studies focus on optimizing query formulation to enhance retrieval recall. Methods like HyDE (Gao et al., 2023) and Query2doc (Wang et al., 2023a) generate a pseudo-document based on the question, which is then used for retrieval. RRR (Ma et al., 2023) introduces a rewrite-retrieve-read paradigm and fine-tunes the rewrite model using PPO.",
            "score": 0.585622736376014,
            "section_title": "RAG",
            "char_start_offset": 5683,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 450
                },
                {
                    "start": 453,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1579
                },
                {
                    "start": 1582,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2005
                }
            ],
            "ref_mentions": [
                {
                    "start": 299,
                    "end": 329,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 383,
                    "end": 407,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 549,
                    "end": 568,
                    "matchedPaperCorpusId": "270710849"
                },
                {
                    "start": 1039,
                    "end": 1059,
                    "matchedPaperCorpusId": "270619595"
                },
                {
                    "start": 1327,
                    "end": 1347,
                    "matchedPaperCorpusId": "263828724"
                },
                {
                    "start": 1449,
                    "end": 1470,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 1482,
                    "end": 1499,
                    "matchedPaperCorpusId": "268509926"
                },
                {
                    "start": 1758,
                    "end": 1776,
                    "matchedPaperCorpusId": "254877046"
                },
                {
                    "start": 1791,
                    "end": 1811,
                    "matchedPaperCorpusId": "257505063"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86181640625
        },
        {
            "corpus_id": "276408622",
            "title": "RAG vs. GraphRAG: A Systematic Evaluation and Key Insights",
            "text": "Retrieval-Augmented Generation (RAG) has been widely applied to enhance the performance of Large Language Models (LLMs) by retrieving relevant information from external sources, addressing the limitation of LLMs' restricted context windows, improving factual accuracy, and mitigating hallucinations (Fan et al., 2024;Gao et al., 2023). Most RAG systems primarily process text data by first splitting it into chunks (Finardi et al., 2024). When a query is received, RAG retrieves relevant chunks either through lexical search (Ram et al., 2023) or by computing semantic similarity (Karpukhin et al., 2020), embeddings both the query and text chunks into a shared vector space. Advanced techniques, such as pre-retrieval processing (Ma et al., 2023;Zheng et al., 2023a) and post-retrieval processing (Dong et al., 2024;Xu et al., 2023), as well as fine-tuning strategies (Li et al., 2023), have further enhanced RAG's effectiveness across various domains, including QA) (Yan et al., 2024), dialogue generation (Izacard et al., 2023), and text summarization (Jiang et al., 2023). \n\nSeveral studies have evaluated the effectiveness of RAG systems across various tasks (Yu et al., 2024;Chen et al., 2024;Es et al., 2023), such as multi-hop question answering (Tang and Yang, 2024), biomedical question answering (Xiong et al., 2024), and text generation (Liu et al., 2023). However, no existing study has simultaneously and systematically evaluated and compared RAG and GraphRAG on these general text-based tasks.",
            "score": 0.5853890388596206,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 4780,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1508
                }
            ],
            "ref_mentions": [
                {
                    "start": 299,
                    "end": 317,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 525,
                    "end": 543,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 1008,
                    "end": 1030,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1181,
                    "end": 1199,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 1254,
                    "end": 1275,
                    "matchedPaperCorpusId": "263152125"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7119140625
        },
        {
            "corpus_id": "277043297",
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "text": "refining both the retriever and the generator to achieve optimal overall system performance. This holistic approach ensures that improvements in one component reinforce enhancements in the other, fostering a synergistic relationship that maximizes the capabilities of the entire RAG system. By jointly training both components, the system can better align the retrieval of relevant documents with the generation of accurate and coherent text. Notable implementations of this approach include RAG [126] and the work by [81], which utilize a joint training paradigm in conjunction with Maximum Inner Product Search (MIPS) [53,200] to optimize the retrieval process effectively. This integrated training methodology allows the retriever to progressively improve its relevance scoring in response to the generator's feedback, while the generator concurrently learns to better utilize the retrieved information. The result is a more robust and question-answering by retrieving both images and text, while RA-CM3 [262] extends this capability by enabling simultaneous retrieval and generation of text and images. Recent developments like Transfusion [286] demonstrate the potential of unifying language modeling with diffusion to train a single transformer that can both understand and generate across modalities. Similarly, Show-o [251] presents a unified transformer that combines autoregressive and discrete diffusion modeling to flexibly support a wide range of vision-language tasks including visual question-answering, text-to-image generation, and text-guided inpainting/extrapolation. Other advances like VisRAG [268] showcase the potential of multimodal RAG by leveraging both visual and textual information to achieve more comprehensive document understanding, while LA-RAG [135] advances speech processing by incorporating fine-grained token-level speech retrieval.",
            "score": 0.5853689848114858,
            "section_title": "Collaborative Training. Collaborative optimization training adopts a co-optimization strategy, simultaneously",
            "char_start_offset": 78076,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1870
                }
            ],
            "ref_mentions": [
                {
                    "start": 496,
                    "end": 501,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 518,
                    "end": 522,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 620,
                    "end": 624,
                    "matchedPaperCorpusId": "57573732"
                },
                {
                    "start": 624,
                    "end": 628,
                    "matchedPaperCorpusId": "514516"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9697265625
        },
        {
            "corpus_id": "273532207",
            "title": "Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination",
            "text": "The results presented in Table 5 highlight the domain adaptation performance of the various Retrieval Augmented Generation based models on our hotel customer service dataset. The RAG-Original model serves as a baseline, achieving an EM score of 4.00 and an F1 score of 10.92, reflecting the challenges of applying generic models to specialized domains without further adaptation. However, domain adaptation significantly boosts performance, as seen in the RAG-Finetuned-QA model, which shows a marked increase of 8.50 points in EM and 19.37 points in F1, underscoring the importance of domain-specific fine-tuning for improving response accuracy in specialized domains. The RAG-DPR-adapted model showed further improvements even over models directly fine-tuned on QA datasets, resulted in a more effective architecture for specialized tasks  The RAG-Original model demonstrates relatively low performance whereas with the effect of domain adaptation the other models exhibit notable improvements in both response accuracy and retrieval performance across all evaluation metrics. For RAG-end2end and RAG-DPR-adapted models, left column scores are based on models initialized with rag-token-base and not being finetuned on downstream QA, whereas right column score represents models initialized with rag-token-nq and further finetuend on downstream QA tasks respectively. \n\nFigure 2: The comparison graph showcases the improvements in retrieval accuracy across all models after being fine-tuned with the hotel domain dataset, as evidenced by notable gains in both Top-5 and Top-20 precision. It also indicates that increasing the number of retrieved documents leads to consistent improvements in retrieval performance which is evident by the parallel rise of Top-5 and Top-20 scores across all variants. \n\nhigher in EM and 1.25 points in F1 compared to the RAG-FinetuendQA model, particularly after fine-tuning for downstream QA tasks. However, without this fine-tuning, its performance dropped significantly, which further reflects the necessity of tailored QA finetuning for optimal results in specialized domains.",
            "score": 0.5853641349627985,
            "section_title": "Overall Accuracy on Domain Adaptation",
            "char_start_offset": 36250,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1369
                },
                {
                    "start": 1372,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1801
                },
                {
                    "start": 1804,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2114
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.697265625
        },
        {
            "corpus_id": "269283058",
            "title": "RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) represents a significant advancement in the field of natural language processing (NLP) and machine learning, combining LLMs with the vast information accessible in external knowledge databases.Specifically, RAG is employed to enhance the generative models' ability to produce more accurate, relevant, and contextually rich responses by dynamically retrieving information from a corpus during the generation process.This hybrid approach combines the strengths of two major strands: the deep contextual understanding of LLMs and the precision of knowledge database retrieval.Recent work [1,8,22,27,37,42] has demonstrated that RAG can significantly improve the generation quality across various benchmarks compared to solely generative models.The RAG framework has since been applied across various tasks, including question answering [39], content creation [24], and even code generation [33,43], showcasing its versatility and promise.As shown in Figure 1, RAG operates on a two-step workflow: retrieval and generation, integrating offline preparation with real-time processing for enhanced performance.Initially, in its offline phase, RAG transforms the external knowledge sources, such as documents, into high-dimensional vectors using advanced embedding models.RAG then indexes these vectors into a specialized vector database designed for efficient retrieval.Upon receiving a user request, RAG first accesses this vector database to conduct a vector similarity search, retrieving the documents that best match the request based on their semantic content.Following this, RAG combines the content of these retrieved documents with the original user request, creating an augmented request.This augmented request is then provided to an LLM, which leverages the combined information to generate a response that is more informed and contextually rich.\n\nIn an RAG workflow, the retrieval step is mainly performed on CPUs, while the generation step is executed on GPUs.From a system perspective, the end-to-end performance of RAG is affected by both the retrieval or generation steps.The retrieval time is mainly determined by the vector database's scale, and the generation time is decided by the model size and the sequence length.Our subsequent characterization will identify RAG's performance bottleneck and highlight potential areas for optimization.",
            "score": 0.5845560917876254,
            "section_title": "Background",
            "char_start_offset": 6804,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 452
                },
                {
                    "start": 452,
                    "end": 610
                },
                {
                    "start": 610,
                    "end": 778
                },
                {
                    "start": 778,
                    "end": 972
                },
                {
                    "start": 972,
                    "end": 1140
                },
                {
                    "start": 1140,
                    "end": 1301
                },
                {
                    "start": 1301,
                    "end": 1400
                },
                {
                    "start": 1400,
                    "end": 1595
                },
                {
                    "start": 1595,
                    "end": 1727
                },
                {
                    "start": 1727,
                    "end": 1886
                },
                {
                    "start": 1888,
                    "end": 2002
                },
                {
                    "start": 2002,
                    "end": 2117
                },
                {
                    "start": 2117,
                    "end": 2266
                },
                {
                    "start": 2266,
                    "end": 2388
                }
            ],
            "ref_mentions": [
                {
                    "start": 625,
                    "end": 627,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 630,
                    "end": 633,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 928,
                    "end": 931,
                    "matchedPaperCorpusId": "247255943"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8515625
        },
        {
            "corpus_id": "270357306",
            "title": "Corpus Poisoning via Approximate Greedy Gradient Descent",
            "text": "We experimented with the following retrievers: \n\n\u2022 Dense Passage Retriever(DPR) (Karpukhin et al., 2020) is a two-tower bi-encoder trained with a single BM25 hard negative and in-batch negatives. It has been used as the retrieval component of many Retrieval-Augmented Generation (RAG) models (Lewis et al., 2020). In our paper, we use both the open-sourced Multi model (DPR-mul), which is a bert-base-uncased model trained on four QA datasets (NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), WebQuestions (Berant et al., 2013) and CuratedTREC (Baudi\u0161 and \u0160ediv\u1ef3, 2015)) and the single NQ model (DPR-nq). \n\n\u2022 ANCE (Xiong et al., 2020) is a bi-encoder that generates hard negatives using an approximate Nearest Neighbor (ANN) index of the corpus. The index is continuously updated in parallel to identify challenging negative examples for the model during fine-tuning. \n\n\u2022 Contriever (Gautier et al., 2022) is an unsupervised dense retriever using contrastive learning. It leverages the BERT architecture to encode both queries and documents. Contriever-MS (Contriever fine-tuned on MS MARCO) is a version of the Contriever model that has been fine-tuned using the MS MARCO dataset, which provides large-scale, supervised training data.",
            "score": 0.5844550229111909,
            "section_title": "C.2 Retrievers",
            "char_start_offset": 30930,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 49,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1248
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 103,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 292,
                    "end": 312,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 447,
                    "end": 473,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 484,
                    "end": 504,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 519,
                    "end": 540,
                    "matchedPaperCorpusId": "6401679"
                },
                {
                    "start": 557,
                    "end": 582,
                    "matchedPaperCorpusId": "17723267"
                },
                {
                    "start": 627,
                    "end": 646,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 896,
                    "end": 918,
                    "matchedPaperCorpusId": "249097975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "277621629",
            "title": "KnowsLM: A framework for evaluation of small language models for knowledge augmentation and humanised conversations",
            "text": "Large language models have become better and better in conversational style and their knowledge with time by incorporating every growing information through every cycle of pre-training and launch of new version. Recent research explores how LLMs can be effectively used for dialogue evaluation by leveraging chain-of-thought reasoning, in-context examples, and fine-tuning [8] [15]. However, small and medium size language models viz. LLama 3, Mistral have not achieved similar performance leaving it to fine tuning or RAG based augmentation with sophisticated multi-shot prompts to have a natural conversation with real world awareness of the facts. Small Language Models (SLMs) are increasingly favored over LLMs for their efficiency, lower cost, and domain-specific adaptability, particularly in resource-constrained environments [16]. However, a comprehensive analysis of fine tuning vs RAG to compare their performance on small/medium language models is required to understand the benefits and limitations of both approaches. The current study proposes a framework for the same and also demonstrates the implementation of this framework on LLama 3.3. Besides, the paper also provides a deep dive into how to generate data for effective fine tuning of language models. \n\nFine-tuned medium language models(MLMs), like llama 3.3 70B, are highly effective for downstream task due to their efficient resource consumption. This paper employs Low-Rank Adaptation (LoRA), a fine-tuning technique that minimizes the number of trainable parameters by incorporating trainable rank decomposition matrices. This approach allows for efficient and lightweight adaptation of large language models while preserving their performance capabilities [7]. \n\nComprehensive overview of Retrieval-Augmented Generation (RAG), its evolution, and enterprise applicability, serves as the foundation for understanding the retrieval pipeline used in this study. [4]. Advancements in retrieval frameworks, such as Keyword Augmented Retrieval, demonstrate the potential of leveraging smaller language models for efficient context discovery, thereby reducing inference costs and response times [14]. Hybrid retrieval techniques, such as COS-Mix, which integrate cosine similarity and distance measures, have demonstrated improved retrieval performance, particularly in sparse data scenarios [10]. Research on hyperparameters in Retrieval-Augmented Generation (RAG) systems highlights the impact of Context Window Utilization on retrieval effectiveness and response quality [9].",
            "score": 0.5836073793892924,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1272
                },
                {
                    "start": 1275,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1738
                },
                {
                    "start": 1741,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2170
                },
                {
                    "start": 2171,
                    "end": 2367
                },
                {
                    "start": 2368,
                    "end": 2548
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72119140625
        },
        {
            "corpus_id": "272911034",
            "title": "On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains",
            "text": "Retrieval-Augmented Generation (RAG) RAG, popularized by [11], is a widely adopted approach that integrates retrieval-based mechanisms into generative models to improve performance across various language tasks [21]. The basic flow of RAG follows a 'retrieve and read' fashion [10][11][12][13][22][23][24][25][26]. In this fashion, given an input query, one first retrieves relevant data from the external data corpus and then employs generative models as 'readers', for example, by directly appending the retrieved documents to the queries as context and then feeding them as a whole to LLMs for making predictions. Approaches for enhancing the efficient use of retrieved documents to improve the LLMs' reading capability include using chunked cross-attention during generation [12], prompt-tuning [23], and in-context learning [13]. \n\nDense retrieval systems There are two main categories of retrievers: sparse retrieval systems, which match through lexical patterns (e.g., BM25) [27,28], and deep neural network-based dense retrievers, which match through semantic meanings [29][30][31]. Due to the near-exact token-level matching pattern, sparse retrievers' performance has been shown to be worse than that of dense retrievers in several domains [32], such as healthcare [14]. There are two popular approaches for training dense retrievers: supervised [33,34] and self-supervised [29][30][31]. In the supervised regime, given a paired query and document, the goal is to maximize the similarity, i.e., inner product, between their embeddings. Motivated by advances in unsupervised learning, recent methods have started to apply contrastive learning [35] for training, where they observed improved performance across multiple benchmarks. \n\nIn our work, in addition to popular retrievers, we also consider a retriever trained with contrastive learning on medical datasets: MedCPT [31].",
            "score": 0.5834041879367438,
            "section_title": "Related Work",
            "char_start_offset": 3001,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 834
                },
                {
                    "start": 837,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1739
                },
                {
                    "start": 1742,
                    "end": 1886
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 61,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 281,
                    "end": 285,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 285,
                    "end": 289,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 289,
                    "end": 293,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 305,
                    "end": 309,
                    "matchedPaperCorpusId": "246431219"
                },
                {
                    "start": 779,
                    "end": 783,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 829,
                    "end": 833,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 982,
                    "end": 986,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 986,
                    "end": 989,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1085,
                    "end": 1089,
                    "matchedPaperCorpusId": "259316759"
                },
                {
                    "start": 1250,
                    "end": 1254,
                    "matchedPaperCorpusId": "254044526"
                },
                {
                    "start": 1360,
                    "end": 1363,
                    "matchedPaperCorpusId": "8384258"
                },
                {
                    "start": 1392,
                    "end": 1396,
                    "matchedPaperCorpusId": "259316759"
                },
                {
                    "start": 1652,
                    "end": 1656,
                    "matchedPaperCorpusId": "211096730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85595703125
        },
        {
            "corpus_id": "270845515",
            "title": "Glue pizza and eat rocks - Exploiting Vulnerabilities in Retrieval-Augmented Generative Models",
            "text": "Retrival Augmented Generation (RAG).As shown in Figure 2, RAG systems (Chen et al., 2024;Gao et al., 2023;Lewis et al., 2020;Li et al., 2022Li et al., , 2024) ) are comprised of three fundamental components: knowledge base, retriever, and LLM generator.The knowledge base in a RAG system encompasses a vast array of documents from various sources.For simplicity, we denote the knowledge base as K, comprising n documents, i.e., K = {D 1 , D 2 , . . ., D n }, where D i denotes the ith document.This knowledge base can be significantly large, often containing millions of documents from sources like Wikipedia (Thakur et al., 2021b).When a user submits a query, the retriever R identifies the top-m documents from the knowledge base that are most relevant to the query.This selection serves as the external knowledge to assist the LLM Generator G in providing an accurate response.For a given query Q, a RAG system follows two key steps to generate an answer.\n\n\u2776 Step 1-Knowledge Retrieval: The retriever employs two encoders: a query encoder h Q and a document encoder h D .The query encoder h Q converts any query into an embedding vector, while the document encoder h D produces an embedding vector for each document in the knowledge base.Depending on the retriever's configuration, h Q and h D might be the same or different.For a given query Q, the RAG system retrieves m documents (termed as retrieved documents) from the knowledge base K that exhibit the highest semantic similarity with Q.Specifically, for each document D j \u2208 K, the similarity score between D j and the query Q is computed by their inner product as\n\nFor simplicity, we omit h Q and h D and denote the set of m retrieved documents as R(Q; K), representing the documents from the knowledge base K with the highest similarity scores to the query Q.",
            "score": 0.5829586595551063,
            "section_title": "Background",
            "char_start_offset": 4403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 36,
                    "end": 253
                },
                {
                    "start": 253,
                    "end": 347
                },
                {
                    "start": 347,
                    "end": 450
                },
                {
                    "start": 450,
                    "end": 494
                },
                {
                    "start": 494,
                    "end": 632
                },
                {
                    "start": 632,
                    "end": 768
                },
                {
                    "start": 768,
                    "end": 880
                },
                {
                    "start": 880,
                    "end": 958
                },
                {
                    "start": 960,
                    "end": 1074
                },
                {
                    "start": 1074,
                    "end": 1241
                },
                {
                    "start": 1241,
                    "end": 1328
                },
                {
                    "start": 1328,
                    "end": 1496
                },
                {
                    "start": 1496,
                    "end": 1623
                },
                {
                    "start": 1625,
                    "end": 1820
                }
            ],
            "ref_mentions": [
                {
                    "start": 70,
                    "end": 89,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 106,
                    "end": 125,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75244140625
        },
        {
            "corpus_id": "267069204",
            "title": "Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented Generation in Niche Domains, Exemplified by Korean Medicine",
            "text": "Retrieval-Augmented Generation (RAG) models combine a generative model with an information retrieval function, designed to overcome the inherent constraints of generative models.(1) They integrate the robustness of a large language model (LLM) with the relevance and up-to-dateness of external information sources, resulting in responses that are not only natural and human-like but also the latest, accurate, and contextually relevant to the query. (1)(2)(3)(4) The interaction of the two modules (retrieval and generation) enables responses that would not be achievable with either module alone, making RAG more than just the sum of its components. This approach represents a significant milestone in the field of generative models by enabling the induction of high-quality responses in less-explored domains at a low expense. (5,6) In the conventional RAG operation, the initial step involves converting input queries into vector embeddings, which are then used to retrieve relevant data from the vectorized database. Following this, the generative part of RAG utilizes the retrieved external data for producing contextually rich responses. (7) Thus, both the embedding and generative models are considered crucial factors in the performance of RAG, directly affecting the retrieval process.(8) However, in niche domains, the performance of generic LLM-based embedding models appears suboptimal compared to their effectiveness in more general fields. The lack of specialized training data in these domains results in embeddings that do not adequately capture the nuances and specificity of the domain (9), leading to less accurate and contextually relevant information retrieval. Despite the evident presence of these functional limitations, they have not been much identified through experiments, therefore the optimality of the conventional LLM-based vector embedding RAG methods for niche domains has remained in obscurity. Researchers have been aware of these shortcomings of LLMs and have explored supplementary processes such as fine-tuning to improve the performance. (8,(10)(11)(12) However, the cost of fine-tuning, especially when it involves adjusting the entire or majority of parameters in LLM, has rapidly become expensive, thereby increasing the demand for alternative solutions.",
            "score": 0.5828394053056845,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 1604,
                    "end": 1607,
                    "matchedPaperCorpusId": "227277273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75439453125
        },
        {
            "corpus_id": "278714608",
            "title": "mmRAG: A Modular Benchmark for Retrieval-Augmented Generation over Text, Tables, and Knowledge Graphs",
            "text": "Retrievers We evaluate a diverse set of retrieval models, from classic lexical methods to modern neural models. \n\nClassic retrievers include three widely used IR baselines: \n\n-BM25, a lexical ranking function known for its efficiency and robustness, -Contriever [11], a dense retriever trained with contrastive learning to produce semantically rich embeddings, and -DPR [13], a bi-encoder trained on query-passage pairs. We use public checkpoints of Contriever6 and DPR7 without further fine-tuning them on mmRAG. \n\nModern retrievers include \n\n-BGE [28], i.e. bge-large-en-v1.5, a generative encoder selected for its leading results on the MTEB leaderboard,8 -GTE [15,36], i.e. gte-large-en-v1.5, a Transformer-based generative encoder also ranked among the best on MTEB, and -Fine-tuned BGE and Fine-tuned GTE, both fine-tuned on the train and valid sets of mmRAG for 1 epoch with hard negatives. \n\nWe also set up an Oracle retriever that always outputs an optimal ranking and achieves perfect retrieval accuracy. We use it as a reference when measuring the quality of downstream generation. \n\nGenerators We combine the above retrievers with two popular LLMs of different sizes as generation models: \n\n-GLM [8], i.e. glm-4-plus, a large, online-accessible LLM, and -Qwen [25,30], i.e. Qwen-7B-Instruct, a 7-billion-parameter, locally deployable LLM, representing a resource-constrained setting. \n\nWe prompt them with top-3 retrieved chunks to augment generation. \n\nEvaluation Metrics We measure retrieval accuracy and generation quality. \n\n-For retrieval accuracy, we use three standard IR metrics reported at cut-offs k = 1, 3, 5: Normalized Discounted Cumulative Gain (NDCG@k), Mean Average Precision (MAP@k), and Hits@k (i.e., the proportion of queries that have at least one relevant chunk in the top-k).",
            "score": 0.582559402710944,
            "section_title": "Evaluation Setup",
            "char_start_offset": 17461,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 114,
                    "end": 172
                },
                {
                    "start": 175,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 541
                },
                {
                    "start": 544,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1200
                },
                {
                    "start": 1203,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1395
                },
                {
                    "start": 1398,
                    "end": 1463
                },
                {
                    "start": 1466,
                    "end": 1538
                },
                {
                    "start": 1541,
                    "end": 1809
                }
            ],
            "ref_mentions": [
                {
                    "start": 370,
                    "end": 374,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76513671875
        },
        {
            "corpus_id": "272988123",
            "title": "Towards Automated Patent Workflows: AI-Orchestrated Multi-Agent Framework for Intellectual Property Management and Analysis",
            "text": "Retrieval-Augmented Generation (RAG [6]) provides LLMs with relevant external information from document databases, enabling them to generate outputs that are more contextually accurate, detailed, and grounded for ODQA tasks. This approach helps overcome the limitations of static, pre-trained knowledge in LLMs. In traditional RAG, documents are parsed and processed to extract text, then divided into smaller chunks using fixed-size chunking strategies to facilitate more precise retrieval of relevant content. Each chunk is embedded into a low-dimensional dense vector space that captures its semantic content, allowing for efficient indexing and retrieval of relevant chunks in response to queries. This method enhances generation by conditioning the language model on retrieved, contextually relevant chunks, leading to more accurate and grounded outputs. However, traditional RAG methods face several challenges that limit their effectiveness. These methods primarily rely on small text chunks, requiring the retriever to search through a large database to find relevant information. This can be inefficient, as the retriever often needs to recall numerous text chunks, sometimes necessitating re-ranking to optimize performance. Moreover, small text chunks can lead to semantic incompleteness and the loss of critical details due to document truncation. Dividing crucial context or concepts into multiple segments can impair coherence. Choosing an optimal chunk size is challenging: if chunks are too small, context is lost; if they are too large, retrieval becomes less precise-clearly, one size does not fit all. General-purpose LLMs are typically pre-trained on large text corpora using self-supervised learning techniques, such as predicting the next word in a sentence (autoregressive models) or filling in masked tokens (masked language models). To adapt LLMs for specific tasks, they undergo fine-tuning on task-specific datasets, enhancing their ability to follow instructions, improve contextual understanding, and solve complex problems. Despite these advancements, LLMs are generally not pre-trained or fine-tuned to inherently incorporate external retrieved context from databases, which is crucial for generating more accurate answers in ODQA. To address these limitations, the Retrieval-Augmented Fine-Tuning (RAFT [15]) methodology optimizes LLMs to integrate retrieved content from external databases during fine-tuning.",
            "score": 0.5824479940768318,
            "section_title": "A.1.1 Synthetic Data Generation",
            "char_start_offset": 31288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2262
                },
                {
                    "start": 2263,
                    "end": 2442
                }
            ],
            "ref_mentions": [
                {
                    "start": 36,
                    "end": 39,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77099609375
        },
        {
            "corpus_id": "269740933",
            "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
            "text": "Dense retrieval methods are usually trainable, therefore holding more flexibility and potential in adaptation.As the key component of dense retriever, the embedding models have delicately different designs in existing RAG models.A simple design [62,72,165] is to directly use a part of the generation model as the embedding layer of the retriever, which might be able to enhance the alignment between the retrieval and generation processes.BERT-based backbone [25] is widely applied in retrieval models.One common retriever design in RAG is to construct two-stream encoders with the BERT structure (one encoder for the query and the other for the documents), which is also called bi-encoder [135,164].Early-stage RAG methods tend to freeze [6,117] or partially freeze [74] the parameters of the retriever to perform general-level relevant knowledge extraction and pay more attention to the knowledge leveraging and generator finetuning.Large-scale specialized pre-training further enhances RAG models to excel in more knowledge-intensive tasks.One typical success is Dense Passage Retriever (DPR) [61], which uses a BERTbased backbone and is pre-trained specifically for the OpenQA task with question-answer pair data.DPR has shown strong capacity as a pre-trained retriever, facilitating many RAG models to succeed in various downstream tasks [54,74,135,139,141].It has also been regarded as the first step in the RAG paradigm for improving the performance of LLMs, which may further enhance the alignment of the embeddings between queries and relevant textual data through fine-tuning [16].A recent study [122] has also discovered that DPR training decentralizes how knowledge is stored in the network, creating multiple access pathways to the same information.With effective fine-tuning, bi-encoder retrievers are also applied widely in ICL-based RAG [82,93,101,111,126,176]. Specifically, they have been more often used for sentence embedding similarity-based retrieval, as well as for some special requirement in ICL, such as diverse example retrieval [176].",
            "score": 0.5824131359094855,
            "section_title": "Retrieval",
            "char_start_offset": 11127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 110,
                    "end": 229
                },
                {
                    "start": 229,
                    "end": 440
                },
                {
                    "start": 440,
                    "end": 503
                },
                {
                    "start": 503,
                    "end": 701
                },
                {
                    "start": 701,
                    "end": 936
                },
                {
                    "start": 936,
                    "end": 1044
                },
                {
                    "start": 1044,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1364
                },
                {
                    "start": 1364,
                    "end": 1592
                },
                {
                    "start": 1592,
                    "end": 1763
                },
                {
                    "start": 1763,
                    "end": 2063
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 249,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 249,
                    "end": 252,
                    "matchedPaperCorpusId": "220128068"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 696,
                    "end": 700,
                    "matchedPaperCorpusId": "263877300"
                },
                {
                    "start": 740,
                    "end": 743,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 743,
                    "end": 747,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 768,
                    "end": 772,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1097,
                    "end": 1101,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1344,
                    "end": 1348,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 1348,
                    "end": 1351,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1355,
                    "end": 1359,
                    "matchedPaperCorpusId": "235390519"
                },
                {
                    "start": 1359,
                    "end": 1363,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 1587,
                    "end": 1591,
                    "matchedPaperCorpusId": "257532394"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77099609375
        },
        {
            "corpus_id": "252568176",
            "title": "FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation",
            "text": "The retrieval module is the backbone for all retrieval-augmented generation. The generation quality is to a large extent bound by the retrieval quality, especially if the retrieved information is not memorized by the generator. To answer RQ1 What impact does training the retrieval module have on FiD-Light SP downstream results? we have to be careful to acknowledge the uncertainty of sparse ranking annotations (Hofst\u00e4tter et al., 2022). \n\nTo accurately quantify the retriever's contribution, we compare the downstream effect of a zero-shot, a fine-tuned (methodology described in detail in Appendix B), and two oracle retrievers in Table 1. \n\nIn the first section (rows 1-3) retrievers are evaluated without access to relevance judgements (a real-world environment), whereas in the second section (rows 4 & 5) we infuse relevance information during the evaluation (oracle environment). We find that training the retriever with in-domain training data (row 2) consistently improves results over a zero-shot retriever (row 1) as used by (Hofst\u00e4tter et al., 2022). While always ingesting all known relevant passages during training (row 3) does not significantly change the downstream performance. \n\nTo account for annotation uncertainty in our retriever as oracle experiments, we study two scenarios: 1) infusing all known relevant passages into the retrieved candidate list (row 4) and 2) setting the candidates to be only the known relevant passages (row 5). Commonly, the community compares their results only against the second oracle scenario, showing a large headroom for future improvements for the retriever. However, we argue, due to the sparsity of the annotations, we should compare the results to our more realistic first oracle scenario (row 4). It still shows a significant opportunity for improvement, albeit the total headroom is roughly halfed across the board. Future work may explore more fine-tuning aspects, but we decide to select the simple fine-tuned retriever (row 2).",
            "score": 0.5823395449459327,
            "section_title": "INFLUENCE OF THE RETRIEVER",
            "char_start_offset": 17779,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 643
                },
                {
                    "start": 646,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1994
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75927734375
        },
        {
            "corpus_id": "277501853",
            "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
            "text": "We present a comprehensive framework for enhancing Retrieval-Augmented Generation (RAG) systems through dynamic retrieval strategies and reinforcement fine-tuning. This approach significantly improves large language models on knowledge-intensive tasks, including opendomain question answering and complex reasoning. Our framework integrates two complementary techniques: Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS), which dynamically determines retrieval timing and content based on contextual needs. Together, these techniques enhance both the utilization and relevance of retrieved content, improving factual accuracy and response quality. Designed as a lightweight solution compatible with any Transformer-based LLM without requiring additional training, our framework excels in knowledge-intensive tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a novel method to selectively compress key-value caches by token importance, mitigating memory bottlenecks in long-context applications. The framework also incorporates test-time scaling techniques to dynamically balance reasoning depth and computational resources, alongside optimized decoding strategies for faster inference. Experiments on benchmark datasets show that our framework reduces hallucinations, strengthens domain-specific reasoning, and achieves significant efficiency and scalability gains over traditional RAG systems. This integrated approach advances the development of robust, efficient, and scalable RAG systems across diverse applications.",
            "score": 0.5822818467111301,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92919921875
        },
        {
            "corpus_id": "277113433",
            "title": "When LLMs Meet API Documentation: Can Retrieval Augmentation Aid Code Generation Just as It Helps Developers?",
            "text": "Retrieval-Augmented Generation (RAG) [29] is proposed to enable LLMs to handle tasks by leveraging external knowledge without fine-tuning LLMs. Specifically, RAG maintains a database of relevant domain-specific or latest information unfamiliar to LLMs. Given a query, RAG first runs retrieval phase to dynamically identify multiple pieces of information (also known as entities) relevant to the task from the database. Then, the Top-k entities are included as the context in the prompt to augment the generation phase. \n\nIn the context of documentation-based API usage recommendation, RAG works for a coding requirement query  on a database of API documents D = { 1 ,  2 , ...,   }, where   represents the document of an API unfamiliar to the LLM. It executes two phases: \n\nRetrieval Phase. Given the query  as the input to a RAG system, the retriever computes similarity scores between  and each document   \u2208 D to rank all documents. The RAG system adopts a parameter  that determines the number of top-ranked documents to use as relevant contextual information. The retrieval process can be formally represented as: \n\nAugmented Generation Phase. A prompt is constructed based on top- relevant documents as the contextual information and  as the query, with an instruction to clarify the logical relationship between them. The prompt is input to the generator LLM to get the recommended API usage, which can be formally represented as: \n\nResult \u2190 GeneratorLLM (\u27e8Retriever (D, , ) , \u27e9)",
            "score": 0.5820515365298926,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 5609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 771
                },
                {
                    "start": 774,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1436
                },
                {
                    "start": 1439,
                    "end": 1485
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50048828125
        },
        {
            "corpus_id": "273403554",
            "title": "Probing-RAG: Self-Probing to Guide Language Models in Selective Document Retrieval",
            "text": "We introduce Probing-RAG, an efficient RAG pipeline that incorporates a prober to determine whether the language model needs to retrieve additional documents. Similar to the conventional retrieval-augmented generation pipeline, our approach comprises a generating language model and a retriever. Different from the general pipeline, the generator of Probing-RAG leverages the output from the prober and adaptively calls the retriever based on the model's internal hidden state.",
            "score": 0.5815455296425378,
            "section_title": "Method",
            "char_start_offset": 7175,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 477
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.890625
        },
        {
            "corpus_id": "273798525",
            "title": "LLM-Ref: Enhancing Reference Handling in Technical Writing with Large Language Models",
            "text": "A.1 Basic Retrieval-Augmented Generation (RAG) \n\nRetrieval-Augmented Generation (RAG) is an advanced technique that combines information retrieval with text generation, making it particularly effective when generating responses that require specific contextual information from an external knowledge base. The process is typically divided into three main stages: Ingestion, retrieval, and response generation. Ingestion: Once an input file is read, the first stage in RAG involves chunking and embedding, where source texts are segmented into smaller, manageable units, which are then converted into embedding vectors for retrieval. Smaller chunks generally enhance query precision and relevance, while larger chunks may introduce noise, reducing accuracy. Effective chunk size management is crucial for balancing comprehensiveness and precision. Embedding transforms both the user's query and knowledge base documents into comparable formats, enabling the retrieval of the most relevant information. \n\nRetrieval: In the next stage, the relevant information is retrieved from a vector knowledge base such as FAISS. The retriever searches this vector store to find the most relevant chunks of information based on the user's query. This stage is crucial for ensuring that the model has access to the necessary context for generating accurate and contextually relevant responses. \n\nResponse Generation: In the final stage, the retrieved context is combined with the user's query and fed into the LLM, such as GPT-4, to generate a coherent and relevant response. The model uses the context provided by the retrieved documents to produce answers that are informed by the most pertinent information available. This step highlights the synergy between retrieval and generation, ensuring that the output is not only accurate but also contextually grounded. \n\nEach stage of the RAG process is designed to leverage the strengths of both retrieval and generation, enabling the creation of responses that are informed by specific and relevant external knowledge. By combining these components, RAG systems can significantly enhance the quality and relevance of generated content, making them a powerful tool for applications requiring precise and contextually aware responses.",
            "score": 0.5804931722951635,
            "section_title": "A Appendix",
            "char_start_offset": 31515,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 49,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1000
                },
                {
                    "start": 1003,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1377
                },
                {
                    "start": 1380,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1849
                },
                {
                    "start": 1852,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2265
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.853515625
        },
        {
            "corpus_id": "271218596",
            "title": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems",
            "text": "In this paper we introduce RAGBench, a large-scale dataset composed of real-world RAG examples intended for training and benchmarking RAG evaluation models.Additionally, we formulate TRACe, a RAG evaluation framework comprising four metrics: uTilization, Relevance, Adherence, and Completeness.TRACe standardizes the evaluation process, offering a consistent and systematic approach to measuring RAG system performance across various dimensions.\n\nWe benchmark existing RAG evaluation framework using RAGBench and demonstrate that LLMjudges struggle to compete with a fine-tuned RAG evaluation expert model.Future work may involve fine-tuning larger expert models to explore the potential for narrowing the performance gap between these models and the ground truth.\n\nOur contributions address the need for standardized benchmarks and methodologies, enabling more precise and actionable insights into the strengths and weaknesses of different RAG systems.This, in turn, will facilitate the iterative improvement of RAG models, driving forward the capabilities of retrieval-augmented generation in real-world applications.",
            "score": 0.5802876829831882,
            "section_title": "Conclusion",
            "char_start_offset": 19795,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 294
                },
                {
                    "start": 294,
                    "end": 445
                },
                {
                    "start": 447,
                    "end": 606
                },
                {
                    "start": 606,
                    "end": 764
                },
                {
                    "start": 766,
                    "end": 953
                },
                {
                    "start": 953,
                    "end": 1119
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80078125
        },
        {
            "corpus_id": "269149041",
            "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
            "text": "RAG-Memory-Finetuning is a technique to optimize the performance of large language models (LLMs) by combining Retrieval Augmented Generation (RAG) with fine-tuning.This approach aims to connect LLMs to external knowledge sources 56 AgentLM-70B:https://huggingface.co/THUDM/agentlm-70B 57 AgentLM-7B:https://huggingface.co/THUDM/agentlm-7b 58 AgentLM-7B:https://huggingface.co/THUDM/agentlm-13B 59 AgentTuning Github:https://github.com/THUDM/AgentTuningNavigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies 2023-12-20 through a retrieval mechanism and combine it with generative capabilities to search and integrate relevant information from knowledge bases.The goal of RAG-Memory-Finetuning is to improve the consistency and reliability of the output and reduce hallucination issues.Specifically,the RAG technique is implemented through the following steps:\n\n1) Chunking an external domain-specific knowledge base into small documents,each about 150 words.2) Creating embeddings using a pre-trained model and storing document vectors in a vector database.3) When an input query is passed to the LLM,the most relevant information is retrieved from the external database using metrics such as cosine similarity and combined with the LLM as additional context.4) This external context and the input prompt are passed together to the text generator to produce the output response.Responses generated by RAG are more factual,specific,and diverse.The parameter knowledge provided by traditional finetuning techniques is static,while RAG allows us to bypass retraining and obtain up-to-date information through retrievalbased generation to produce reliable outputs.M gen predicts IsUse given x, y t 12: end if through self-reflection and also addresses the hallucination issues present in large models.\n\nAlgorithm 2 present an overview of SELF-RAG [4] at inference.For every x and preceding generation y < t,the model decodes a retrieval token to evaluate the utility of retrieval.",
            "score": 0.5802763764776953,
            "section_title": "XIII. RAG-MEMORY-FINETUNING",
            "char_start_offset": 144027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 452
                },
                {
                    "start": 452,
                    "end": 730
                },
                {
                    "start": 730,
                    "end": 856
                },
                {
                    "start": 856,
                    "end": 930
                },
                {
                    "start": 932,
                    "end": 1029
                },
                {
                    "start": 1029,
                    "end": 1128
                },
                {
                    "start": 1128,
                    "end": 1330
                },
                {
                    "start": 1330,
                    "end": 1449
                },
                {
                    "start": 1449,
                    "end": 1514
                },
                {
                    "start": 1514,
                    "end": 1731
                },
                {
                    "start": 1731,
                    "end": 1868
                },
                {
                    "start": 1870,
                    "end": 1931
                },
                {
                    "start": 1931,
                    "end": 2047
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6796875
        },
        {
            "corpus_id": "276422305",
            "title": "Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) aims to augment the capabilities of Large Language Models (LLMs) by retrieving and incorporate external documents or chunks prior to generation. However, even improved retriever relevance can brings erroneous or contextually distracting information, undermining the effectiveness of RAG in downstream tasks. We introduce a compact, efficient, and pluggable module designed to refine retrieved chunks before using them for generation. The module aims to extract and reorganize the most relevant and supportive information into a concise, query-specific format. Through a three-stage training paradigm - comprising supervised fine - tuning, contrastive multi-task learning, and reinforcement learning-based alignment - it prioritizes critical knowledge and aligns it with the generator's preferences. This approach enables LLMs to produce outputs that are more accurate, reliable, and contextually appropriate.",
            "score": 0.5799103969628757,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9111328125
        },
        {
            "corpus_id": "278170975",
            "title": "Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets",
            "text": "In recent years, Large Language Models (LLMs) have made significant progress in research and have grown increasingly popular [1]. However, LLMs face several challenges, including issues with hallucinations caused by insufficient context [2], as well as limitations in their learned content, which prevent them from addressing questions requiring specific or proprietary information [1]. To address these issues, [3] introduced Retrieval-Augmented Generation (RAG), which extends LLMs by integrating external knowledge sources for knowledge-intensive natural language processing (NLP) tasks. By incorporating domain-specific information, RAG systems enable tailored responses for specialized topics, improving accuracy, relevance, and contextual understanding. Since 2022, numerous RAG systems have demonstrated the effectiveness of this approach in overcoming some limitations of LLMs [1]. \n\nThese systems are applied across a wide range of NLP tasks and outperform in domain-specific scenarios by leveraging specialized knowledge to enhance performance and relevance [1]. RAG systems operate through three interconnected components: (1) indexing, which structures and organizes external knowledge bases; (2) retrieval, which identifies and extracts relevant documents from these sources; (3) and generation, which combines retrieved information with the input to produce a coherent, contextually relevant response using an LLM and prompt engineering techniques. These systems offer numerous customization options, including fine-tuning retrieval mechanisms, optimizing prompting techniques, refining generation models, and customizing knowledge base design [4], [5]. This flexibility raises the question of what settings should be used to configure RAG systems, and how these systems can be compared and evaluated to determine the most effective system for specific domains. This particular task of identifying optimal parameters in RAG systems is commonly referred to in the literature as RAG evaluation [6], [7]. During the RAG evaluation, each component of the RAG system is systematically evaluated within a predefined workflow to ensure that the system meets the overall quality standards [6]. The process begins with an input query from a prepared QA evaluation dataset, which is used to compute metrics such as retrieval accuracy and response relevance, providing statistical insight into the performance of the RAG components and thus a comprehensive understanding of their effectiveness [6].",
            "score": 0.5797987739884786,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 889
                },
                {
                    "start": 892,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2199
                },
                {
                    "start": 2200,
                    "end": 2501
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 240,
                    "matchedPaperCorpusId": "261530162"
                },
                {
                    "start": 1663,
                    "end": 1666,
                    "matchedPaperCorpusId": "271646356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84375
        },
        {
            "corpus_id": "265308606",
            "title": "Adapting LLMs for Efficient, Personalized Information Retrieval: Methods and Implications",
            "text": "This characteristic ensures that the responses generated are current and relevant, while also providing a structured way to handle sensitive or private information within the retrieval data. This setup aims at striking a careful balance between dynamism and privacy. \n\nA question that often arises is whether RAG can be replaced by fine-tuning. The distinction between RAG and fine-tuning becomes apparent when considering the dynamic nature of the information involved, the computational resources available for retraining, and the specific requirements of the task at hand. While both methodologies aim at enhancing the performance and applicability of Large Language Models (LLMs), they cater to different aspects and scenarios. Finetuning is a well-suited approach for tailoring a model to a specific domain or set of long-term tasks, especially when the underlying challenges are relatively static. It involves adjusting the model parameters on a new dataset to make the model's behavior more aligned with the desired task or domain. On the other hand, RAG is designed to tackle scenarios where the model needs to stay updated with rapidly evolving or expansive information without requiring continuous retraining. By leveraging an external retrieval database, RAG enables the model to interact dynamically with the latest data, ensuring its responses are current and relevant. While fine-tuning demands substantial computational resources for retraining with new data, RAG offers a cost-effective alternative by minimizing the need for exhaustive retraining. In summary, the choice between RAG and fine-tuning is not a matter of simple replacement, but rather a strategic decision based on the particular demands of the task, the nature of the data, and the available resources. Each approach has its own set of advantages and is suited to different use cases, necessitating a careful consideration of the project's requirements to determine the most appropriate strategy.",
            "score": 0.5794288914616974,
            "section_title": "Enhancing the Information Retrieval Process",
            "char_start_offset": 8563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 266
                },
                {
                    "start": 269,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1978
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5703125
        },
        {
            "corpus_id": "273695367",
            "title": "RuleRAG: Rule-guided retrieval-augmented generation with language models for question answering",
            "text": "In this section, we present details of our proposed novel rule-guided retrieval-augmented generation with LMs (RuleRAG) for solving the task of knowledge-intensive factual queries. Notably, RuleRAG includes training-free RuleRAG-ICL and fine-tuned RuleRAG-FT. First, we prompt RuleRAG-ICL with queries and rules for in-context learning during retrieving and inferring. The rules are aimed to guide retrievers to recall logically supportive documents and guide generators to predict attributable answers. Then, RuleRAG-FT further fine-tunes the retrievers and generators to explicitly enhance their rule-following ability by our introduced rule-guided fine-tuning (RGFT), where we leverage the queries combining rules as fine-tuning data and the ground truth answers as supervision data. \n\nThe inferring process of RuleRAG-FT is the same as RuleRAG-ICL.",
            "score": 0.57940649977859,
            "section_title": "PROPOSED METHOD: RULERAG",
            "char_start_offset": 10390,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 852
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92236328125
        },
        {
            "corpus_id": "267412954",
            "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
            "text": "To enhance the accuracy of question and answer (Q&A) tasks, our approach included supervised finetuning on the zero-shot model for GPT-3.5 Turbo. This is needed because zero shot Retrieval-Augmented Generation (RAG) oftentimes retrieves irrelevant information or irrelevant embeddings. Subsequently, we utilized reprompting on GPT4All and Llama2 for further refinement. This process included feeding in example prompts with given questions, evidence text, and answers to guide the model in learning specific response patterns. Additionally, we benchmarked the fine-tuning process by incorporating the Retrieval-Augmented Generation (RAG) technique for Q&A tasks, thereby enhancing the evaluation and overall performance of the models. \n\nThere is an absence in standardized methodologies for evaluating model performance. We assessed the zero-shot model (LLM without fine-tuning) and subsequently evaluated the model with fine-tuning and reprompting. The approach involved iterative accuracy evaluation, incorporating fine-tuning or reprompting into models such as GPT-3.5 Turbo, GPT4All, Llama2, and Claude. The evaluation metrics used were cosine similarity and Rouge-L, measuring accuracy in each iteration",
            "score": 0.5792720772650968,
            "section_title": "Process and Work",
            "char_start_offset": 9452,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1208
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85302734375
        },
        {
            "corpus_id": "276937772",
            "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
            "text": "To evaluate the effectiveness of SEARCH-R1, we compare it against the following baseline methods: \n\nInference without Retrieval: Direct inference and Chain-of-Thought (CoT) reasoning (Wei et al., 2022). \n\nInference with Retrieval: Retrieval-Augmented Generation (RAG) (Lewis et al., 2020), IRCoT (Trivedi et al., 2022a), and Search-o1 (Li et al., 2025). \n\nFine-Tuning-Based Methods: Supervised fine-tuning (SFT) (Chung et al., 2024) and reinforcement learning-based fine-tuning without a search engine (R1) (Guo et al., 2025). \n\nThese baselines cover a broad spectrum of retrieval-augmented and fine-tuning approaches, allowing for a comprehensive assessment of SEARCH-R1 in both zero-shot and learned retrieval settings. \n\nTo make a fair comparison between different methods, we use the same retriever, knowledge corpus, training data and LLMs. More details can be found in Section 4.3.",
            "score": 0.5792352142968269,
            "section_title": "Baselines",
            "char_start_offset": 17050,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 100,
                    "end": 202
                },
                {
                    "start": 205,
                    "end": 353
                },
                {
                    "start": 356,
                    "end": 526
                },
                {
                    "start": 529,
                    "end": 721
                },
                {
                    "start": 724,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 887
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 201,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 268,
                    "end": 288,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6455078125
        },
        {
            "corpus_id": "271744975",
            "title": "A Comparison of LLM Finetuning Methods & Evaluation Metrics with Travel Chatbot Use Case",
            "text": "Furthermore, it incorporates paged optimizers to handle memory spikes. The second method implemented was a novel method called Retrieval Augmented Fine Tuning (RAFT), a training procedure for domain-specific Retrieval Augmented Generation (RAG), which can adapt pretrained LLMs like LLaMa 2 and Mistral for RAG in specific domains, such as ours in travel. [9] RAG is a text generation method for outsourcing relevant information, from a knowledge base, or a large corpus of relevant, factual and quality information, to supply an LLM with contextual clues for producing factual responses. Then, an RLHF training pipeline will be done for domain-specific LLM curation, aligned with human preferences, with reward model training. [10] II. RELATED WORK Through a comprehensive review of literature and existing research papers, we build an understanding for state-or-theart techniques and approaches aimed to achieve competitive performances. New innovations are expressed in different variations for enhancing large language models. Table II summarizes the selection of models, objective addressed, unique approach, performance results and ultimate findings. A survey of existing solutions show that initiatives have been developed to overcome shortcoming identified with public general-purpose and pretrained LLM. Some of the feature performance issues garnered from literature review are potential risks of hallucination, underdeveloped retrieval approaches, and inefficiencies in the use of computational resources. \n\nZhang et. al introduced Retrieval Augmented fine-tuning (RAFT) as a novel training strategy for fine-tuning LLMs to better perform on RAG tasks. The key concept is data augmentation to generate \"question, answer, document\" triplets before fine-tuning. This is done by generating realistic questions paired with elaborate chain of thought answering scheme and purposefully including relevant and irrelevant context documents. Through a chain of thought with the distractor documents, the model learns to extract the correct information from the entire chunk of context through reasoning, ignoring the distractors. RAFT operates by training the model to disregard any retrieved documents that do not contribute to answering a given question, thereby eliminating distractions. The optimal ratio of oracle to distractor documents during training varies across datasets, but including some distractors improves generalization. Finally, during RAG, RAFT retrieves the top-k documents from the database. With RAFT, Zhang et.",
            "score": 0.5785406609063828,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1516
                },
                {
                    "start": 1519,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2292
                },
                {
                    "start": 2293,
                    "end": 2440
                },
                {
                    "start": 2441,
                    "end": 2515
                },
                {
                    "start": 2516,
                    "end": 2536
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73486328125
        },
        {
            "corpus_id": "275820707",
            "title": "An Empirical Study of Retrieval-Augmented Code Generation: Challenges and Opportunities",
            "text": "A similar workflow has been proposed and developed [8,49] before REALM occurs. To give more powerful ability to process different kinds of tasks, Lewis et al. [39] replaces reader with generator under the workflow of retriever-and-reader, which is also called retriever-and-generator. Indeed, retrieval-augmented methods have been used in code generation and have developed for a long time. Hashimoto et al. [25] proposes a retrieve-and-edit framework that edits the retrieved results to the desired output instead of generating code directly. The workflow of retrieve-and-edit is similar to retriever-and-generator [39], but the details of the two models are different. In specific, retrieval has more powerful components and can combine retriever with generator to fine-tune end-to-end. \n\nAlthough the development of the models is rapid, fine-tuning following pre-training remains an indispensable paradigm for small-sized models. Based on this mode, the utilization of similar retrieved results to improve pre-trained models for code generation has demonstrated effectiveness, attracting considerable attention. However, most of these approaches only use single configuration, focusing on a specific view, and do not systematically summarize the retrieval-augmented framework and its usage from various aspects. Therefore, we abstract three phases of the retrievalaugmented framework for code generation (retrieval phase, fusion phase and generation phase) and conduct a systematic study. For the retrieval phase, previous studies related to code search highlight that deep learning-based models can mitigate the semantic gap between query and code compared with statistics-based algorithms [46]. However, deep learning-based techniques rely on labeled data to learn the parameters of the models, which might bring additional training costs [76]. These models may perform worse than statistics-based approaches under zero-shot settings [63]. For the fusion phase, while many fusion strategies can be used in the retrieval-augmented framework from previous studies [33,41], there is a lack of systematic exploration into the effectiveness and impact of the fusion strategies. For the generation phase, our main concern is the impact of the retrieval-augmented framework on various pre-trained code generation models. \n\nRecently, LLMs have shown impressive results in various downstream tasks across domains [9,12,19].",
            "score": 0.5778298561321936,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 17309,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2177
                },
                {
                    "start": 2178,
                    "end": 2318
                },
                {
                    "start": 2321,
                    "end": 2419
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 54,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 54,
                    "end": 57,
                    "matchedPaperCorpusId": "52913933"
                },
                {
                    "start": 159,
                    "end": 163,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "54446010"
                },
                {
                    "start": 616,
                    "end": 620,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1694,
                    "end": 1698,
                    "matchedPaperCorpusId": "226246208"
                },
                {
                    "start": 1844,
                    "end": 1848,
                    "matchedPaperCorpusId": "254044526"
                },
                {
                    "start": 2067,
                    "end": 2071,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75341796875
        },
        {
            "corpus_id": "268091298",
            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
            "text": "2) Retriever Enhancement: In RAG systems, the quality of retrieved content determines the information fed into the generators. Lower content quality increases the risk of model hallucinations or other degradation. In this section, we introduce efficient ways to enhance retrieval effectiveness. Recursive Retrieval: Recursive retrieval is to perform multiple searches to retrieve richer and higher-quality contents. \n\nReACT [132] uses Chain-of-Thought (CoT) [133] to break queries down for recursive retrieval and provide richer information. RATP [134] uses the Monte-Carlo Tree Search for simulations to select optimal retrieval content, which is then templated and forwarded to the generator for output. Chunk Optimization: Chunk optimization refers to adjusting chunk size for improved retrieval results. \n\nLlamaIndex [135] incorporates a series of chunk optimization methods, one of which operates on a 'small to big' principle. The core concept here is to pinpoint finer-grained content but return richer information. For instance, Sentence-window retrieval fetches small text chunks and returns a window of relevant sentences surrounding the retrieved segment. In automerge retrieval, documents are arranged in a tree structure. The process retrieves the parent node, which encapsulates the content of its child nodes, by fetching the child node first. To address the lack of contextual information, RAPTOR [136] employs recursive embedding, clustering, and summarization of text chunks until further clustering becomes infeasible, thereby constructing a multi-level tree structure. Prompt-RAG [137] enhances retrieval accuracy by pre-generating a table of contents, enabling the model to autonomously select relevant chapters based on the query. Raina et al. [138] break text chunks into finer atomic statements to achieve higher recall and improved results. Retriever Finetuning: The retriever, central to the RAG system, relies on a proficient embedding model [139]- [142] to represent related content and feed the generator, enhancing system performance. \n\nAdditionally, embedding models with strong expressive power can be fine-tuned with domain-specific or task-related data to boost performance in targeted areas. REPLUG [86] treats LM as a black box and update the retriever model based on the final results.",
            "score": 0.5776983522137551,
            "section_title": "B. RAG Enhancements",
            "char_start_offset": 27257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 807
                },
                {
                    "start": 810,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2064
                },
                {
                    "start": 2067,
                    "end": 2226
                },
                {
                    "start": 2227,
                    "end": 2322
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9091796875
        },
        {
            "corpus_id": "273345235",
            "title": "Evaluating the Effectiveness and Efficiency of Demonstration Retrievers in RAG for Coding Tasks",
            "text": "Large language models (LLMs) have achieved remarkable success in natural language processing, but they still encounter significant limitations in the domain of knowledge-intensive tasks. In particular, LLMs are susceptible to \"hallucinations\" when confronted with queries that exceed the scope of their training data or necessitate the utilization of current information [1]. Retrieval-Augmented Generation (RAG) is the leading technique for improving LLMs by providing demonstrations from knowledge bases. By referencing external knowledge sources, RAG effectively mitigates the problem of generating factual inconsistency content and facilitates the continuous updating of knowledge [2]. \n\nRecently, the application of RAG has achieved promising results for various code-specific tasks such as Code Search [3], Program Synthesis [4], [5], and Assertion Generation [6]. In these tasks, LLM learns from contextual prompts consisting of task descriptions, queries, and additional demonstration examples without the need to fine-tune the model parameters. The retrieved demonstrations are typically used as the context to assist the pre-trained LLM in comprehending the task and regulating the generation behavior, which usually has a significant impact on the quality of the generated output. Therefore, it is important to retrieve appropriate demonstrations from a vast knowledge base. \n\nThe retriever plays a core role in retrieving relevant demonstrations from the external knowledge base and significantly affects the performance of RAG [7], [8]. Retrievers are typically classified into sparse and dense retrievers based on representation methods. Sparse retrievers operate at the token level, while dense retrievers operate at the level of latent semantics. The most widely-used sparse retriever, BM25 [9], ranks demonstrations based on term frequency (TF) and inverse document frequency (IDF) of the query. Dense retrievers perform retrieval by encoding the query and demonstrations into dense embedding representations and scoring each demonstration by its similarity with the query embedding. For instance, the popular RAG system Llamaindex [10] supports both BM25 and custom embedding encoders in RAG workflow.",
            "score": 0.5775049494074076,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1385
                },
                {
                    "start": 1388,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 808,
                    "end": 811,
                    "matchedPaperCorpusId": "269130676"
                },
                {
                    "start": 831,
                    "end": 834,
                    "matchedPaperCorpusId": "258180059"
                },
                {
                    "start": 836,
                    "end": 839,
                    "matchedPaperCorpusId": "252734952"
                },
                {
                    "start": 866,
                    "end": 869,
                    "matchedPaperCorpusId": "259860357"
                },
                {
                    "start": 1545,
                    "end": 1548,
                    "matchedPaperCorpusId": "269293655"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84228515625
        },
        {
            "corpus_id": "273969566",
            "title": "Toward Optimal Search and Retrieval for RAG",
            "text": "Retrieval-augmented generation (RAG) is a promising method for addressing some of the memory-related challenges associated with Large Language Models (LLMs). Two separate systems form the RAG pipeline, the retriever and the reader, and the impact of each on downstream task performance is not well-understood. Here, we work towards the goal of understanding how retrievers can be optimized for RAG pipelines for common tasks such as Question Answering (QA). We conduct experiments focused on the relationship between retrieval and RAG performance on QA and attributed QA and unveil a number of insights useful to practitioners developing high-performance RAG pipelines. For example, lowering search accuracy has minor implications for RAG performance while potentially increasing retrieval speed and memory efficiency.",
            "score": 0.577361974119076,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9150390625
        },
        {
            "corpus_id": "270215715",
            "title": "Mix-of-Granularity: Optimize the Chunking Granularity for Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) [30] has emerged as a standard practice to enhance the performance of LLMs, aiming to mitigate their problems of \"hallucinations\" and knowledge cut-off.An RAG system typically includes a Retriever that extracts relevant information from an external knowledge database, and a backbone LLM to generate grounded responses by considering the given relevant information (in-context learning [10]).\n\nTechnological advancements in RAG have addressed fundamental questions such as \"what should be retrieved\", \"when should retrieval occur\", and \"how should the retrieved information be utilized\".In terms of \"what to retrieve\", research has evolved from simple token retrieval [28] and entity retrieval [34] to more complex structures like chunks [37] and knowledge graphs [25].Granularity matters a lot in retrieval, coarse-granularity-retrieval yields more information but with lower precision, while fine-granularity-retrieval offers comprehensive information at the cost of efficiency.The question of \"when to retrieve\" has led to strategies ranging from single [49,42] to adaptive [20,17] and multiple retrieval [19] methods.Regarding \"how to use\" the retrieved data, integration techniques have been developed for various levels of the model architecture, including the input [29], intermediate [3], and output layers [31].Although integration at the intermediate and output layers is more effective, challenges remain concerning training requirements and efficiency limitations.",
            "score": 0.5769924728198379,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 6462,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 189,
                    "end": 429
                },
                {
                    "start": 431,
                    "end": 624
                },
                {
                    "start": 624,
                    "end": 806
                },
                {
                    "start": 806,
                    "end": 1017
                },
                {
                    "start": 1017,
                    "end": 1158
                },
                {
                    "start": 1158,
                    "end": 1357
                },
                {
                    "start": 1357,
                    "end": 1513
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1352,
                    "end": 1356,
                    "matchedPaperCorpusId": "258078950"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77587890625
        },
        {
            "corpus_id": "271710111",
            "title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation",
            "text": "Implementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach. We introduce RAG Foundry, an open-source framework for augmenting large language models for RAG use cases. RAG Foundry integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources. We demonstrate the framework effectiveness by augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive datasets. Code is released as open-source in https://github.com/IntelLabs/RAGFoundry.",
            "score": 0.5763288772231681,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "271891920",
            "title": "W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering",
            "text": "The retrieval results of different fine-tuned retrievers are presented in Table 2. These results are based on 2,000 test questions for each fine-tuned or baseline retriever. On NQ, WebQ, and MSMARCO QnA datasets, all retrievers fine-tuned on W-RAG data outperformed their corresponding baselines, while DPR ReCon consistently outperformed all unsupervised baselines, except for SQuAD where BM25 performed best. As expected, DPR fine-tuned from ReContriever showed better retrieval performance than DPR base , which was fine-tuned from the base BERT model. Similar to the OpenQA task, retrievers fine-tuned with W-RAG data slightly underperform those fine-tuned with ground-truth data. Although there is a general positive correlation between retrieval performance and OpenQA performance, the relationship is noisy and sometimes inconsistent. For instance, on SQuAD, BM25 achieved a much higher Recall@5 (20.11) compared to ColBERT base fine-tuned with W-RAG data (10.29), meaning BM25 ranked the relevant passage among the top 5 for approximately 200 more questions. However, in OpenQA, using the top 5 passages from ColBERT base led to better answer accuracy than using those from BM25. Similarly in NQ, DPR ReCon fine-tuned on W-RAG achieved significantly higher retrieval performance than ColBERT base (Recall@5 of 42.67 vs. 26.72), their OpenQA performances were relatively close. This discrepancy may be partly due to the prompt instructing Llama3.1-8B-Instruct to fall back on its internal knowledge when the retrieved passage cannot answer the question. Nevertheless, these findings suggest that traditional notions of relevance may not directly translate into improved quality of answers in RAG systems. Retrievers trained on W-RAG data select top passages based on their ability to elicit the correct answer from the LLM, while ground-truth passages are typically chosen based on term overlap, semantic similarity, and human judgment. Contrary to expectations, stronger retrieval metrics do not always lead to better OpenQA results. This raises a critical question: Is the relevance definition in existing datasets truly effective for evaluating retrievers in RAG systems?",
            "score": 0.5758828513663725,
            "section_title": "Retrieval Results",
            "char_start_offset": 29554,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2181
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56591796875
        },
        {
            "corpus_id": "269125395",
            "title": "Knowledge Ply Chat",
            "text": "Despite their ability to store information and excel at many NLP tasks with fine-tuning, large language models tend to have issues about accurately accessing and altering knowledge, which leads to performance gaps in knowledge-intensive tasks compared to domain-specific architectures. Additionally, these models face problems when it comes to having transparent decision-making processes or updating their world knowledge. To mitigate these limitations, we propose a Retrieval Augmented Generation (RAG) system by improving the Mistral7B model specifically for RAG tasks. The novel training technique includes Parameter-Efficient Fine-Tuning (PEFT) which enables efficient adaptation of large pre- trained models on-the-fly according to task-specific requirements while reducing computational costs. In addition, this system combines pre-trained embedding models that use pre-trained cross-encoders for effective retrieval and reranking of information. This RAG system will thus leverage these state-of-the-art methodologies towards achieving top performances in a range of NLP tasks such as question answering and summarization.",
            "score": 0.5755013820277988,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87353515625
        },
        {
            "corpus_id": "263609465",
            "title": "Prompting, Retrieval, Training: An exploration of different approaches for task-oriented dialogue generation",
            "text": "Since both fine-tuning and retrieval showed increased scores in the automatic metrics, our next experiment consisted in fine-tuning the generation Table 4: Performance of different approaches using the retrieval model paired with a generation model. We compare indexing the contexts against indexing the answers. The retrieval models are the fine-tuned versions, and the FLAN-T5 models are the pretrained versions. model with the retrieved candidates. Our aim was for the FLAN-T5 model to learn to make better use of the retrieved answers during generation. \n\nTable 5 shows the results of our fine-tuned FLAN-T5 models with and without retrieval. Although combining fine-tuning and retrieval resulted in higher scores in terms of automatic metrics, the small increment suggests that most of the performance gain results from fine-tuning and not much from the additional retrieved information.",
            "score": 0.5754583039817505,
            "section_title": "Fine-tuning generation with retrieval",
            "char_start_offset": 17525,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 557
                },
                {
                    "start": 560,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 892
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7421875
        },
        {
            "corpus_id": "278339615",
            "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing",
            "text": "Table 1 summarizes the performance of various methods on three widely used RAG datasets. SIM-RAG full with GPT-4 consistently performs the best in all three datasets, outperforming all baseline methods by a large margin, including Self-RAG and Auto-RAG, which require extensive full model fine-tuning for LLMs. A groupwise comparison further highlights the strengths of SIM-RAG. \n\nFor multi-hop QA datasets, when using closed-source GPT-4 models, SIM-RAG full delivers the highest performance for both HotPotQA and 2Wiki. When using open-source Llama models, Auto-RAG performs the highest on 2Wiki, while SIM-RAG full is the highest on HotPotQA. Auto-RAG fine-tunes Llama3-8b for retrieval decisions and uses a learned retrieval E5, whereas our approach only fine-tunes a smaller Critic and uses a simpler BM25 retriever. If reducing computational costs or using closed-source LLM models is a priority, SIM-RAG would be the optimal choice. \n\nIn contrast, for single-hop QA, a key observation is that all advanced baselines tailored for multi-round RAG perform poorly on the straightforward TriviaQA dataset. None of them match the performance of Standard RAG, and only Auto-RAG and FLARE outperform Naive Generation. This exposes a critical limitation for these approaches: optimizing for complex multi-round retrieval tasks appears to undermine LLM's capability on simpler tasks. This may be due to the inherent biases of LLMs, particularly their difficulty with effective self-critique, as discussed in Section 2.2. These challenges lead to Over-Confidence or Over-Retrieval, making LLMs less competitive on simpler tasks, where even a standard RAG approach with a fixed number of retrieval steps performs better. In contrast, SIM-RAG uses an external model that specializes in \"when to stop the iterative system. \" This distinction allows SIM-RAG full with Llama3 to outperform Standard RAG with Llama3 by a significant margin (70.7% vs. 58.9%) in the EM metric.",
            "score": 0.5751586542673504,
            "section_title": "Results",
            "char_start_offset": 30658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1965
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47119140625
        },
        {
            "corpus_id": "276408682",
            "title": "Improving Retrieval-Augmented Deep Assertion Generation via Joint Training",
            "text": "The overall framework of AG-RAG is illustrated in Fig. 2. In the assertion retrieval stage, AG-RAG searches for similar TAPs from the external codebase by calculating their semantic similarity by a dense retriever. In the assertion generation stage, AG-RAG fine-tunes a pre-trained encoderdecoder generator with the retrieval-augmented inputs, i.e., the original focal-test and similar TAPs. During the training process, both the retriever and generator are optimized with a novel joint training strategy, which better adapts them as a whole pipeline to our task. In the assertion inference stage, after the retriever and a generator are well trained, given a focal-test input and retrieval codebase, the beam search strategy is leveraged to generate a ranked list of candidate assertions and return the one with the highest probability of being correct.",
            "score": 0.5750553170477086,
            "section_title": "APPROACH",
            "char_start_offset": 15820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 854
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.939453125
        },
        {
            "corpus_id": "277468036",
            "title": "Self-Routing RAG: Binding Selective Retrieval with Knowledge Verbalization",
            "text": "Selective retrieval improves retrieval-augmented generation (RAG) by reducing distractions from low-quality retrievals and improving efficiency. However, existing approaches under-utilize the inherent knowledge of large language models (LLMs), leading to suboptimal retrieval decisions and degraded generation performance. To bridge this gap, we propose Self-Routing RAG (SR-RAG), a novel framework that binds selective retrieval with knowledge verbalization. SR-RAG enables an LLM to dynamically decide between external retrieval and verbalizing its own parametric knowledge. To this end, we design a multi-task objective that jointly optimizes an LLM on knowledge source selection, knowledge verbalization, and response generation. We further introduce dynamic knowledge source inference via nearest neighbor search to improve the accuracy of knowledge source decision under domain shifts. Fine-tuning three LLMs with SR-RAG significantly improves both their response accuracy and inference latency. Compared to the strongest selective retrieval baseline, SR-RAG reduces retrievals by 29% while improving the performance by 5.1%.",
            "score": 0.5750327583338524,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83544921875
        },
        {
            "corpus_id": "273662413",
            "title": "Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges",
            "text": "This technique improves model robustness and efficiency, particularly in data-scarce legal NLP applications [24]. \u2022 Parameter-Efficient Fine-Tuning (PEFT): PEFT is a method for adapting PLMs that involves freezing the majority of the model's parameters and only updating a small subset. This approach significantly reduces the computational resources and time required for fine-tuning, making it particularly effective in resource-limited scenarios, while still achieving competitive performance in tasks, such as text generation [73]. These machine learning paradigms enhance the performance of core NLP methods, particularly PLMs, by tailoring models to specific tasks and optimising their training efficiency. \n\n(3) Text retrieval technique: \n\n\u2022 Retrieval-Augmented Generation (RAG): RAG combines traditional Information Retrieval (IR) methods with generative NLP models, allowing systems to retrieve external knowledge before generating responses. Text retrieval techniques, such as RAG complement the core NLP methods by providing additional context and external information that enhances the generation and refinement of texts.",
            "score": 0.5750292711778754,
            "section_title": "Basic foundations and concepts of NLP.",
            "char_start_offset": 17820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 712
                },
                {
                    "start": 715,
                    "end": 744
                },
                {
                    "start": 747,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1133
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 112,
                    "matchedPaperCorpusId": "237571793"
                },
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "260435365"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80859375
        },
        {
            "corpus_id": "259370723",
            "title": "Generating Hashtags for Short-form Videos with Guided Signals",
            "text": "Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) has been widely used in NLP tasks, such as neural machine transla-  tion (Gu et al., 2018;Hossain et al., 2020), opendomain question answering (Lee et al., 2019;Guu et al., 2020;Lewis et al., 2020b) and knowledgegrounded dialogue generation (Lian et al., 2019). Recently, some works also utilized this framework in multimodal tasks. Zhang et al. (2021) proposed a Retrieve-Copy-Generate (RCG) model for openbook video captioning. To tackle the Outsideknowledge visual question answering task, (Gao et al., 2022) transformed the image into plain text, performed knowledge passage retrieval, and generated answers entirely in the natural language space. This work extends the RAG framework to multimodal hashtag generation. Both hashtag retriever and generator can accept any advanced models as drop-in replacements.",
            "score": 0.5748088781892591,
            "section_title": "Related Work",
            "char_start_offset": 6326,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 883
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 159,
                    "matchedPaperCorpusId": "19206366"
                },
                {
                    "start": 212,
                    "end": 230,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 230,
                    "end": 247,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 402,
                    "end": 421,
                    "matchedPaperCorpusId": "232168769"
                },
                {
                    "start": 562,
                    "end": 580,
                    "matchedPaperCorpusId": "249455925"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6083984375
        },
        {
            "corpus_id": "270878535",
            "title": "Generative Large Language Models in Automated Fact-Checking: A Survey",
            "text": "Moreover, RAG systems can be fine-tuned by jointly training the retriever and the LLM, where the LLM provides supervisory signals for training the retriever (Izacard et al., 2023;Zeng and Gao, 2024). By combining retrieval and generation, RAG enhances the effectiveness of the fact-checking process against false information.",
            "score": 0.5745341352554496,
            "section_title": "Augmentation with External Knowledge",
            "char_start_offset": 27469,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 325
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 179,
                    "matchedPaperCorpusId": "251371732"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8818359375
        },
        {
            "corpus_id": "275133960",
            "title": "A Comprehensive Framework for Reliable Legal AI: Combining Specialized Expert Systems and Adaptive Refinement",
            "text": "This section presents and analyzes the performance outcomes of our proposed system, which integrates Retrieval-Augmented Generation (RAG), Knowledge Graphs (KG), a Mixture of Experts (MoE) framework, and Reinforcement Learning from Human Feedback (RLHF). By evaluating large language models (LLMs) such as GPT-4 [9], LLaMA-3 [37], and Google Flan-T5 [38] across various legal tasks, we demonstrate how different architectures and methodologies impact performance in specialized fields like law. \n\nFigure 4 (a) showcases the comparative performance between baseline models and those finetuned using Supervised Fine-Tuning (SFT) with Low-Rank Adaptation (LoRA). The data indicates a substantial improvement in task scores across all models when employing SFT-LoRA. For instance, LLAMA-3's performance escalates from approximately 40% in the baseline to 60% post fine-tuning. This 20% increase exhibits the efficacy of the SFT-LoRA approach in enhancing model capabilities. \n\nMoreover, the reduction in abstention rates by around 15% suggests that fine-tuning not only boosts accuracy but also equips models to respond more confidently. The advanced models like GPT-4 and LLAMA-3 exhibit better adaptability to fine-tuning, outperforming older architectures such as GPT-2 and Google Flan-T5, which show limited gains. This disparity highlights the importance of modern, adaptable architectures for fine-tuning processes to achieve optimal performance enhancements. \n\nHowever, despite these improvements, fine-tuned models occasionally struggled with handling complex and contextually rich queries. This limitation necessitated the integration of additional strategies, such as RAG and RLHF, to further enhance performance and reliability. \n\nFigure 4 (b)compares the performance and abstention rates between models enhanced with Retrieval-Augmented Generation (RAG) and those integrated with Knowledge Graphs (KG). RAG-enhanced models generally achieve higher scores and lower abstention rates compared to KG-enhanced setups. For instance, GPT-4 and LLAMA-3 show substantial improvements with RAG, achieving scores around 60%, while abstention rates are maintained below 20%.",
            "score": 0.5737625900787751,
            "section_title": "Results and Discussion",
            "char_start_offset": 26631,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1461
                },
                {
                    "start": 1464,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1735
                },
                {
                    "start": 1738,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2171
                }
            ],
            "ref_mentions": [
                {
                    "start": 350,
                    "end": 354,
                    "matchedPaperCorpusId": "253018554"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78955078125
        },
        {
            "corpus_id": "267750557",
            "title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning",
            "text": "Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, illustrating its broad applicability and efficiency.",
            "score": 0.5737529473458227,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.896484375
        },
        {
            "corpus_id": "276741193",
            "title": "Beyond QA Pairs: Assessing Parameter-Efficient Fine-Tuning for Fact Embedding in LLMs",
            "text": "Fine-tuning for domainspecific tasks has been extensively explored. The RAFT approach (Zhang et al. 2024) combines Retrieval-Augmented Generation (RAG) with fine-tuning to enhance LLM performance in specific domains by training models to disregard irrelevant retrieved documents, improving focus and accuracy. Similarly, \"RAG vs Fine-tuning\" (Gupta et al. 2024) compares both approaches across various LLMs, demonstrating how each method can be effectively employed for domain-specific applications, particularly in underexplored sectors like agriculture. Additionally, \"Fine-tuning Language Models for Factuality\" (Tian et al. 2023) leverages recent innovations in factuality judgment and preference optimization algorithms to improve the factual accuracy of LLMs, offering a novel approach to mitigating misinformation. \n\nInstruction Tuning and Data Selection Efficient data selection for instruction tuning is crucial for optimizing LLM performance. \"From Quantity to Quality\" (Li et al. 2023) introduces a self-guided methodology that employs the Instruction-Following Difficulty metric to identify highquality instruction data, enhancing training efficiency. Additionally, \"Rethinking Data Selection for Supervised Fine-Tuning\" (Shen 2024) argues that selecting data reflecting human-like interactions, rather than purely based on quality and diversity, yields better results in aligning models with human expectations. The MoDS approach (Du, Zong, and Zhang 2023) further refines data selection by focusing on quality, coverage, and necessity, demonstrating improved performance with a significantly reduced dataset. Addressing LLM limitations such as hallucinations and weak numerical reasoning, ToolQA (Zhuang et al. 2024) introduces a dataset to evaluate LLMs' ability to use external tools for question answering, providing insights into their strengths and weaknesses.",
            "score": 0.5731462199032218,
            "section_title": "Domain-Specific Adaptation",
            "char_start_offset": 7054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 821
                },
                {
                    "start": 824,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1879
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7666015625
        },
        {
            "corpus_id": "266998785",
            "title": "Bridging the Preference Gap between Retrievers and LLMs",
            "text": "Retrieval-augmented Generation (RAG). Augmenting LLMs with relevant information retrieved from various knowledge sources is proven effective across numerous NLP tasks, including language modeling (Borgeaud et al., 2022;Khandelwal et al., 2020;Shi et al., 2023b), question answering (Lewis et al., 2020;Izacard et al., 2022;de Jong et al., 2023;De Jong et al., 2023;Shi et al., 2023b;Guu et al., 2020;Izacard and Grave, 2020;Xu et al., 2023), fact versification (Lewis et al., 2020) and text generation (Lewis et al., 2020). Specifically, RAG utilizes input as a query and comprises two main components: (1) a retriever retrieves a set of items from a side corpus. Particular items may vary across different tasks, including documents, passages, or even tokens. In this study, we focus on retrieving passages; and (2) a LLM incorporates the retrieved items, as additional information in the input context, and makes final predictions. \n\nA fundamental question in this process arises regarding the disparate preferences between LLMs and retrievers, as LLMs performing optimally only when their preferences are satisfied. Bridging the preference gap is crucial. Depending on which components are subject to updates, this challenge can be categorized into three families. \n\nFinetuning retrievers and LLMs jointly. This is the most widely used setting of RAG (Izacard et al., 2022;Khandelwal et al., 2020;Wu et al., 2022;Guu et al., 2020;Lewis et al., 2020). However, most prior work is based on relative small LMs (< 1B). For example, Altas (Izacard et al., 2022) finetunes LLM (T5 (Raffel et al., 2020a)) and retriever (Contriever (Izacard et al., 2021)) jointly by leveraging the LLM to provide supervisory signal to train the retriever.",
            "score": 0.5728581612512004,
            "section_title": "Related Work",
            "char_start_offset": 7083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 933
                },
                {
                    "start": 936,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1267
                },
                {
                    "start": 1270,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1735
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 219,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 219,
                    "end": 243,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 282,
                    "end": 302,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 323,
                    "end": 344,
                    "matchedPaperCorpusId": "256231037"
                },
                {
                    "start": 344,
                    "end": 365,
                    "matchedPaperCorpusId": "256231037"
                },
                {
                    "start": 383,
                    "end": 400,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 461,
                    "end": 480,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 502,
                    "end": 522,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1376,
                    "end": 1400,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 1416,
                    "end": 1433,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1433,
                    "end": 1452,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84130859375
        },
        {
            "corpus_id": "271891920",
            "title": "W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering",
            "text": "In knowledge-intensive tasks such as open-domain question answering (OpenQA), large language models (LLMs) often struggle to generate factual answers, relying solely on their internal (parametric) knowledge. To address this limitation, Retrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving relevant information from external sources, thereby positioning the retriever as a pivotal component. Although dense retrieval demonstrates state-of-the-art performance, its training poses challenges due to the scarcity of ground-truth evidence, largely attributed to the high costs of human annotation. In this paper, we propose W-RAG, a method that draws weak training signals from the downstream task (such as OpenQA) of an LLM, and fine-tunes the retriever to prioritize passages that most benefit the task. Specifically, we rerank the top-$k$ passages retrieved via BM25 by assessing the probability that the LLM will generate the correct answer for a question given each passage. The highest-ranking passages are then used as positive fine-tuning examples for dense retrieval. We conduct comprehensive experiments across four publicly available OpenQA datasets to demonstrate that our approach enhances both retrieval and OpenQA performance compared to baseline models, achieving results comparable to models fine-tuned with human-labeled data.",
            "score": 0.572705640332384,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93994140625
        },
        {
            "corpus_id": "266162497",
            "title": "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs",
            "text": "For each task and model, we compared four approaches: using just the base model, RAG, FT, and finally combining FT and RAG by using the fine-tuned model as the generator. Furthermore, we tested the MMLU tasks using both 0-shot and 5-shot scenarios. The full results are shown in Table 1. An aggregation of the relative accuracy gain, i.e., \n\nwhere M is the base model and M \u2032 is the knowledgeinjected model, is shown in Figure 2. \n\nIn all cases, RAG performed significantly better compared to the base models. Furthermore, using RAG with the base model as the generator was consistently better than only finetuning. In some cases, using the fine-tuned model instead of the base model as the generator in the RAG pipeline improved results even further. However, this is not consistent and thus demonstrates the inherent instability of fine-tuning. Additionally, we found that the 5-shot approach boosts the results by a small margin in most cases, with a similar trend being observed in all of the different approaches.",
            "score": 0.5724100862863057,
            "section_title": "MMLU Results",
            "char_start_offset": 20612,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 339
                },
                {
                    "start": 342,
                    "end": 429
                },
                {
                    "start": 432,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1018
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52880859375
        },
        {
            "corpus_id": "275336185",
            "title": "Foundations of GenIR",
            "text": "As discussed in several RAG surveys [84,90], the optimization of RAG systems usually involves the optimization of three components, i.e., the retriever, the generator, and the augmentation method. If we further step back and look at the high-level goals of RAG optimization, we could also categorize it based on how we evaluate the RAG system, namely the evaluation from the perspectives of retrievers, generators, or the joint systems. The evaluation from the retriever perspectives is not particularly different from existing studies on ranking evaluation. The underlining assumption of this is that, once the LLMs are fed with the passages or documents that contain the correct information, they should be able to produce the correct answers directly. Therefore, the evaluation and optimization of a RAG system could downgrade to the evaluation and optimization of a classic retrieval/ranking systems, to where most existing works on dense retrieval and LTR could be applied [123,124]. Yet, there are still differences between RAG and traditional retrieval tasks as the queries are no long issued by users. How to formulate queries efficiently and effectively from LLMs for the retriever is worthy research question, and studies on this direction has already shown potentials in improving the overall quality of RAG systems [111]. \n\nFrom the perspective of generators, RAG evaluation and optimization focus more on improving the robustness and effectiveness of LLM generation based on a fixed set of retrieval results [108]. This often means extra training or fine-tuning on LLMs to improve their fundamental ability in information processing. For example, retrieved documents could be lengthy, and LLMs are usually not good at processing long input context [101]. Therefore, how to design efficient LLMs that can take long context inputs efficiently and effectively has been a popular research problem that have been widely studied by researchers from both academia and industry [100]. We have seen many companies show off their models based on how many input tokens they can process in one request. In addition, since retrieval results are fed as a part of the LLM inputs, whether the LLMs can generate the response based on the retrieved documents instead of their internal knowledge could be seen as a special type of instruction-following ability.",
            "score": 0.571175441360212,
            "section_title": "Optimization of Retrieval and Generation",
            "char_start_offset": 39904,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1333
                },
                {
                    "start": 1336,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1989
                },
                {
                    "start": 1990,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2355
                }
            ],
            "ref_mentions": [
                {
                    "start": 978,
                    "end": 983,
                    "matchedPaperCorpusId": "28826624"
                },
                {
                    "start": 1327,
                    "end": 1332,
                    "matchedPaperCorpusId": "268509926"
                },
                {
                    "start": 1521,
                    "end": 1526,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1761,
                    "end": 1766,
                    "matchedPaperCorpusId": "259360665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86181640625
        },
        {
            "corpus_id": "270620574",
            "title": "R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation",
            "text": "Retrieval augmented generation (RAG) (Lewis et al., 2020) significantly enhances the capabilities of large language models (LLMs) by integrating external, non-parametric knowledge provided by retrievers. In RAG framework, the retriever locates and looks up useful documents based on a given query, and then the LLM interacts with these retrieved results to generate a response. The coordination of retrieval and generation achieves impressive performance without additional training. Especially in domain-specific and knowledge-intensive Figure 1: A comparison between RAG and R 2 AG. R 2 AG employs a trainable R 2 -Former to bridge the semantic gap between retrievers and LLMs. Optionally, LLMs can be fine-tuned to understand the retrieval information further. \n\ntasks, RAG offers real-time knowledge with high interpretability to LLMs, effectively mitigating the hallucination problem (Mallen et al., 2023). \n\nHowever, there exists a semantic gap between retrievers and LLMs due to their vastly different training objectives and architectures (BehnamGhader et al., 2022). Specifically, retrievers, typically encoder architecture, are designed to retrieve the most relevant documents for a query (Zhu et al., 2023b). Conversely, LLMs, generally decoder architecture, are expected to answer questions based on their inherent knowledge or given documents. However, the interaction between retrievers and LLMs in RAG primarily relies on simple text concatenation (BehnamGhader et al., 2022). This poor communication strategy will lead to several challenges for LLMs. Externally, it is hard for LLMs to utilize more information from retrievers in separate processes. In RAG, the retrieved documents that only preserve sequential relationships are unidirectionally delivered to LLMs, and LLMs do not fully understand why retrievers provide the documents. \n\nParticularly, low-quality documents inevitably appear in retrieved results (Barnett et al., 2024), but LLMs have to accept this noise passively. Internally, it is hard for LLMs to handle all of the retrieved documents with their inherent knowledge. LLMs must process all the results and assess which documents are important, impacting their ability to generate accurate answers (Wu et al., 2024).",
            "score": 0.5705261449111467,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 889,
                    "end": 910,
                    "matchedPaperCorpusId": "254877603"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61865234375
        },
        {
            "corpus_id": "271039137",
            "title": "TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models",
            "text": "Retrieval-Augmented Generation (RAG) is a technique that enhances generation models by incorporating relevant content retrieved from knowledge sources, which has been proven to be effective for mitigating hallucinations in LLMs (Guu et al., 2020;Lewis et al., 2020). RAG system generally adheres to a workflow encompassing three components: indexing, retrieval, and generation. Recent advancements have focused on enhancing the retrieval component. RETRO (Borgeaud et al., 2022) amalgamates large-scale corpora with pre-trained frozen BERT embeddings. Atlas (Izacard et al., 2023) conducts joint training of a retriever and a sequence-to-sequence model to attain a language model with robust few-shot learning capabilities. Self-RAG (Asai et al., 2024) selectively retrieves knowledge and generates critique tokens to criticize its own output. \n\nApart from prior works, our proposed CCU-RAG specifically targets knowledge-intensive tasks within classical Chinese question-answering. It centers on endowing TongGu to discern both the timing and content of retrieval, while enhancing the synergistic efficiency between the model and the retrieval system it relies upon.",
            "score": 0.5703502596046299,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 4503,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1167
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 246,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 455,
                    "end": 478,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 558,
                    "end": 580,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 733,
                    "end": 752,
                    "matchedPaperCorpusId": "264288947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87939453125
        },
        {
            "corpus_id": "265594594",
            "title": "Semantic Embeddings for Arabic Retrieval Augmented Generation (ARAG)",
            "text": "Retrieval Augmented Generation (RAG), introduced by Facebook Researchers in 2020 [1], is a pivotal AI framework facilitating information retrieval for Generative AI models, thereby enhancing their accuracy and capabilities. RAG empowers Large Language Models (LLMs) by granting them access to external knowledge sources, augmenting the content generation process. This dual functionality entails retrieval, wherein RAG meticulously selects pertinent information from provided sources and generation, whereby LLMs craft contextually relevant responses based on user input. \n\nThe advantages of RAG are multi-fold. Firstly, it bolsters the performance by grounding LLMs with factual, up-todate information from external knowledge repositories. Furthermore, RAG maintains contextual relevance in responses, contributing to a more engaging user experience in conversational AI applications. Its scalability is noteworthy, as RAG models seamlessly handle copious volumes of information, proving invaluable for data-intensive tasks. Additionally, the adaptability of RAG models allows fine-tuning for specific applications [2], rendering them versatile across diverse data and use cases. Customizability is another hallmark, permitting RAG models to specialize in particular domains or subjects through customization and fine-tuning on specific knowledge bases. Due to the importance of such a framework for enterprises, extensive research is currently being pursued to discover new algorithms and techniques to enhance the performance of such models bounded by the context-window limitations of LLMs. Although there is ongoing research to expand the window size for LLM to be able to ingest more data in the prompt, the use of techniques like RAG is still of great practical importance, not only on homogeneous unstructured data but also on heterogeneous data [3]. \n\nIn principle, at the heart of the information retrieval module is the semantic embedding module which converts a piece of text, whether a query or a context text chunk to a numeric feature vector that embodies all semantic features of the text. The development of word and sentence embeddings is a relatively recent area of research in natural language processing (NLP) and information retrieval. \n\nMost of the semantic models are English language-centred; however, in recent years, Multilingual embedding models were released [4].",
            "score": 0.5702723411672346,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2257
                },
                {
                    "start": 2260,
                    "end": 2392
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77783203125
        },
        {
            "corpus_id": "272689735",
            "title": "LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology Learning Challenge",
            "text": "Through experimentation on SubTask A.1 -WordNet dataset, they achieved an F1-score of 0.9264 with GPT-4, but significantly improved results when they combined BERT with rule-based strategies, leading to an F1-score of 0.9938 and ranked first place in WordNet dataset. Their findings showed the importance of incorporating rules into LLMs for enhanced accuracy in OL. However, they highlight the challenge of identifying appropriate rules, suggesting that future work should focus on automating rule detection and integrating them seamlessly into LLMs. The WordNet dataset is being considered as a low number of types and having a higher number of types makes it challenging to obtain highly accurate rules. SKH-NLP [14]. Team SKH-NLP participated in SubTask B.1 -GeoNames, where they developed a fine-tuning approach using the LLaMA-3-70B and BERT-Large [10]. This team obtained the first place in SubTask B.1 -GeoNames with an F1-score of 0.6557. Their comprehensive analysis demonstrates that BERT- Large, when fine-tuned, achieves performance close to the larger LLaMA-3-70B model. \n\nPhoenixes [29]. The Phoenixes team explored the application of a Retrieval Augmented Generation (RAG) approach within the 12 subtaks of the challenge. They introduced a promising RAG-specific formulation over all three tasks of OL, where a RAG system with minor changes was developed for both tasks A and B, later can be used as a two-step approach for task C. Task C consists of the following steps: Step 1 -runs the Task B approach for finding child-parent pairs and step 2 -applying the Task A approach for assigning the relations to the pairs. They incorporated Mistral-7B [15] as LLM and Dense Passage Retrieval (DPR) [17] model as the retriever model in the RAG framework. However, their results in both zero-shot and few-shot fall shorter than the fine-tuned models and this suggests that still fine-tuning is the key to obtain a high performance within OL.",
            "score": 0.5702558868566002,
            "section_title": "Participants Contributions",
            "char_start_offset": 14025,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1951
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.609375
        },
        {
            "corpus_id": "271769508",
            "title": "Overview of the BioLaySumm 2024 Shared Task on the Lay Summarization of Biomedical Research Articles",
            "text": "Finally, they developed a regressionbased ranking model that improved system performance by selecting the most promising from a set of generated summaries. \n\nRAG-RLRC-LaySum (Ji et al., 2024) This team developed a Retrieval-Augmented Generation (RAG) Lay Summarisation approach, utilizing multiple knowledge sources (including both source documents and Wikipedia). They experiment with LLMs (Gemini and ChatGPT) for both summary generation and paraphrasing, in addition to a Longformer baseline. Lastly, the team also develop a Reinforcement Learning strategy to fine-tune the readability of generated summaries. SINAI (Chizhikova et al., 2024) Focusing largely on a few-shot setting, this team compared the performance of several popular LLMs including GPT-3.5, GPT-4, and LLAMA3. Further experimentation surrounded the fine-tuning of a smaller LLAMA model (LLAMA3-8B) using both parameter-efficient LoRA techniques and Direct Preference Optimization (Rafailov et al., 2023). XYZ (Zhou et al., 2024) This team performed a thorough comparison of several state-of-the-art LLMs, focusing largely on comparing the readability of generated summaries. Further experimentation surrounds Summary rewriting, Title infusing, K-shot prompting, and LoRA-based fine-tuning, with their best-performing submission utilizing a combination of these methods and obtaining the best overall Readability scores. \n\nHGP_NLP (Malik et al., 2024) This team finetune and evaluate multiple T5 model variants, also experimenting with LoRA-based fine-tuning.",
            "score": 0.56998724946443,
            "section_title": "Submissions",
            "char_start_offset": 18665,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 158,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1391
                },
                {
                    "start": 1394,
                    "end": 1530
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 191,
                    "matchedPaperCorpusId": "269983201"
                },
                {
                    "start": 619,
                    "end": 644,
                    "matchedPaperCorpusId": "271769620"
                },
                {
                    "start": 952,
                    "end": 975,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1402,
                    "end": 1422,
                    "matchedPaperCorpusId": "271769383"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70556640625
        },
        {
            "corpus_id": "273811179",
            "title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language Model",
            "text": "Table 5: The performance on retrieval-augmented generation. For baseline, we use BGE-FT as the retriever and a fine-tuned LLM as reader. Results are reported using Exact Match (EM) scores.",
            "score": 0.5699276441728988,
            "section_title": "Document retrieval",
            "char_start_offset": 24021,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 188
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64306640625
        },
        {
            "corpus_id": "271212697",
            "title": "Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis",
            "text": "The generation module serves as the primary functional component within this system, receiving outputs from the data retrieval module and invoking the LLM module to summarize and condense information, ultimately generating conclusions.\n\nRetrieval-augmented generation (RAG) represents an innovative approach amalgamating retrieval-based methods with generative models, typically Language Models (LMs), to enrich the quality and relevance of generated text (Ciuc\u0103 et al., 2023;Gao et al., 2024).This article's expert question and answer system embodies the concept of RAG.In this paradigm, LLMs retrieve input questions (segmented into subqueries via CoT) from the data retrieval module to acquire information for enhancing generation.Subsequently, LLMs reconsider both the acquired information and original questions.During the reconsideration phase, prompt engineering, leveraging task description, input data, contextual information, and prompt style, enriches and standardizes LLM output (Zhao et al., 2023), enabling LLM to produce answers meeting predefined standards, akin to those of domain experts or proficient students.\n\nIn the realm of RAG research, advanced and modular RAG methodologies are evolving from the original or naive RAG approach (Gao et al., 2024).Advanced RAG incorporates additional processing stages pre-and post-retrieval.Pre-retrieval processing concentrates on optimizing data indexing via various methods, including data granularity refinement, index structure optimization, and metadata incorporation to enhance retrieval content quality.Postretrieval processing involves reranking and prompt compression, with embedding playing a crucial role.Optimization strategies encompass fine-tuning embedding or employing dynamic embedding methods.Embracing the design principles of Advanced RAG, we enhance the pre-retrieval stage with query planning and expansion, while bolstering post-retrieval stages with reranking and summarization techniques.",
            "score": 0.569714057006037,
            "section_title": "Generation module",
            "char_start_offset": 18381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 237,
                    "end": 494
                },
                {
                    "start": 494,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 734
                },
                {
                    "start": 734,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 1129
                },
                {
                    "start": 1131,
                    "end": 1272
                },
                {
                    "start": 1272,
                    "end": 1350
                },
                {
                    "start": 1350,
                    "end": 1570
                },
                {
                    "start": 1570,
                    "end": 1676
                },
                {
                    "start": 1676,
                    "end": 1771
                },
                {
                    "start": 1771,
                    "end": 1973
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8466796875
        },
        {
            "corpus_id": "273403693",
            "title": "Optimizing and Evaluating Enterprise Retrieval-Augmented Generation (RAG): A Content Design Perspective",
            "text": "Our retriever and generative components are closed boxes, accessed through APIs, with a limited ability to fine-tune. \u2022 Knowledge base content -We are able to improve results by optimizing the knowledge base content itself. We developed content strategy and writing guidelines for RAG. \u2022 Evaluating results -We test our RAG solutions with real user questions before making the solutions available to external users. We evaluate run-time answers after the solutions are launched. Supporting material for this paper is available on GitHub. 1",
            "score": 0.5693577716055334,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1940,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 539
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6640625
        },
        {
            "corpus_id": "264590451",
            "title": "PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter",
            "text": "Retrieval Question Answering (ReQA) tasks involve generating appropriate answers to given questions, utilizing relevant contextual documents. To achieve this, retrieval augmentation is employed (Chen et al., 2017;Pan et al., 2019;Izacard and Grave, 2021), and comprised of two key components: a retriever and a generator. The retriever's role is to retrieve relevant documents from a large corpus in response to the question, while the generator uses this contextual information to formulate accurate answers. Such systems alleviate the problem of hallucinations (Shuster et al., 2021), thereby enhancing the overall accuracy of the output. \n\nFigure 1: A comparison between two paradigms for information retrieval and generation. The upper section showcases the traditional method where a query is processed by a retriever that scans a corpus to fetch the Top-K documents and then fed to a white-box generator. The lower section introduces our proposed PRCA method, which processes extracted Top-K documents from the retriever before feeding them to black-box generator to achieve better performance for in-domain tasks. \n\nRecent advances in Large Language Models (LLMs) such as the generative pre-trained transformer (GPT) series (Brown et al., 2020;Ouyang et al., 2022;OpenAI, 2023) have demonstrated remarkable potential, notably in their zero-shot and few-shot abilities within the realm of QA tasks. Owing to these capabilities, LLMs are excellent choices as generators within the retrievalaugmented framework. However, due to the vast parameters of LLMs, fine-tuning them becomes exceedingly difficult within a limited computation budget. Furthermore, certain LLMs such as GPT-4 (OpenAI, 2023) are closed-source, making it impossible to fine-tune them. To achieve optimal results on specific datasets, fine-tuning retrievalaugmented models becomes necessary (Guu et al., 2020;Lewis et al., 2020b;An et al., 2021). Previous attempts to integrate LLMs into the retrievalaugmented framework have met with partial suc-cess but also come with limitations.",
            "score": 0.568829350765331,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1120
                },
                {
                    "start": 1123,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2056
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 213,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 213,
                    "end": 230,
                    "matchedPaperCorpusId": "59600051"
                },
                {
                    "start": 230,
                    "end": 254,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 563,
                    "end": 585,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 1251,
                    "end": 1271,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1864,
                    "end": 1882,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1882,
                    "end": 1902,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8701171875
        },
        {
            "corpus_id": "270878612",
            "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs",
            "text": "iii) However, the zero-shot generalization capability of the expert ranking model can be relatively limited compared to the versatile LLM itself.\n\nBased on the above considerations, our goal is to design an RAG instruction tuning pipeline that uses a single language model to achieve both high-recall context extraction and high-quality content generation.In previous study, instruction-tuned LLMs demonstrate a strong ability to extract answers from relevant context for a given question (e.g., OpenAI, 2023;Liu et al., 2024;Lin et al., 2024).This capability can be viewed as the \"dual capability\" of determining whether a chunk of context is relevant to the question thus is useful for generating the answer.We hypothesize that these capabilities mutually enhance each other.Motivated by this insight, we propose RankRAG, which intruction-tunes a single LLM for both context ranking and answer generation in RAG framework.Furthermore, RankRAG expands upon existing instruction-tuning data by incorporating context-rich QA, retrieval-augmented QA and ranking datasets, enhancing the LLM's ability to filter out irrelevant contexts during both the retrieval and generation phases of RAG.\n\nOur contribution can be summarized as follows:\n\n\u2022 We propose RankRAG, a novel framework that enhances LLM's RAG capability through simultaneously instructing the LLM on context ranking and answer generation.During training, we design a specialized task focused on identifying relevant contexts or passages for a given question.This task is structured for ranking and framed as regular question answering with instruction, aligning more effectively with retrieval-augmented generation tasks.At inference, the LLM first reranks the retrieved contexts, then generates answer based on the refined top-k (e.g., 5).This framework is readily applicable to diverse knowledge-intensive NLP tasks.\u2022 Remarkably, we observe that integrating a small fraction of ranking data into the instruction tuning blend of LLM works surprisingly well on the evaluations of ranking associated with the RAG tasks, even surpassing the LLMs fine-tuned with 10\u00d7 more ranking data.We attribute this success to the transferable design of RankRAG training.\u2022 We extensively compare the proposed RankRAG method with several strong baselines, including the open-sourced ChatQA-1.5.",
            "score": 0.5685032495654639,
            "section_title": "Introduction",
            "char_start_offset": 1623,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 147,
                    "end": 356
                },
                {
                    "start": 356,
                    "end": 544
                },
                {
                    "start": 544,
                    "end": 710
                },
                {
                    "start": 710,
                    "end": 777
                },
                {
                    "start": 777,
                    "end": 924
                },
                {
                    "start": 924,
                    "end": 1187
                },
                {
                    "start": 1189,
                    "end": 1235
                },
                {
                    "start": 1237,
                    "end": 1396
                },
                {
                    "start": 1396,
                    "end": 1516
                },
                {
                    "start": 1516,
                    "end": 1679
                },
                {
                    "start": 1679,
                    "end": 1798
                },
                {
                    "start": 1798,
                    "end": 1876
                },
                {
                    "start": 1876,
                    "end": 2140
                },
                {
                    "start": 2140,
                    "end": 2213
                },
                {
                    "start": 2213,
                    "end": 2335
                }
            ],
            "ref_mentions": [
                {
                    "start": 526,
                    "end": 543,
                    "matchedPaperCorpusId": "263605962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8671875
        },
        {
            "corpus_id": "267412954",
            "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
            "text": "Retrieval Augmented generation (RAG) emerges as a crucial process in optimizing the output of large language models. LLMs, with their vast training data and billions of parameters, excel at tasks like question answering, language translation, and sentence completion. However, inherent challenges include the generation of inaccurate or outdated responses, presenting false information, and a lack of adaptability to current events. RAG addresses these issues by extending the capabilities of LLMs to reference authoritative knowledge bases outside their training data, enhancing relevance, accuracy, and usefulness in various contexts. RAG takes an input, retrieves a set of relevant/supporting documents from a source like for example your textbook pdf, and combines them with the original input prompt. This concatenated context is then fed to the text generator, producing the final output. This adaptability of RAG becomes valuable in situations where facts may change over time, a feature particularly useful as the parametric knowledge of LLMs remains static. RAG eliminates the need for retraining, allowing language models to access the latest information for generating reliable outputs through retrieval-based generation. introduced in \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks \" 1 The RAG model faces many limitations that impact its effectiveness in knowledge-intensive natural language processing tasks. Semantic search, a core component of RAG, exhibits challenges such as retrieving irrelevant or opposing information, indicating the model's sensitivity to language nuances and the potential for unexpected results. The ambiguity in understanding how the embedding model extracts and organizes information in vectors adds complexity to optimizing similarity functions. Additionally, the process of chunking, crucial in RAG, can result in information loss if not carefully designed. 2",
            "score": 0.5684711316113814,
            "section_title": "Retrieval Augmented Generation",
            "char_start_offset": 3790,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1923
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.763671875
        },
        {
            "corpus_id": "271909426",
            "title": "LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the Legal Domain",
            "text": "A Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) system is an intelligent generative system that utilizes a knowledge base, denoted as D, which contains a set of documents. In this case, each document, represented as d i \u2208 D, is a string of legal text. A traditional implementation will segment each document d i into a set of chunks c j \u2208 C i . These chunks are then transformed into vector embeddings using a specialized embedding model. After the ingestion of the documents, a user can submit a query q, which will be vectorized using the same embedding model. The system then retrieves the top-k chunks R q = {r 1 , r 2 , . . . , r K } most relevant to the query, using similarity metrics such as cosine similarity. The retrieved chunks, together with the query and a system prompt P , are processed by a large language model (LLM) to generate the final response. This overall process is formalized as: Contextual Retriever(q, D) \u2192 R q LLM P (q, R q ) \u2192 answer Contextual Retriever: The contextual retriever module locates relevant information from an external knowledge repository, returning a corresponding context set R q . Typically, RAG architectures incorporate a bi-encoder retriever such as DPR (Karpukhin et al., 2020), known for its efficiency and accuracy in information retrieval. The Contextual Retriever module often includes a reranking step. First, the top-k \u2032 items are retrieved using a bi-encoder retriever. Then, all k \u2032 items will be reranked using a cross-encoder model that outputs a similarity score between q and each item. The top k < k \u2032 results are returned. Answer Generator: The generator component, which often leverages a sequence-to-sequence model, receives both the question and the context as inputs. It then generates an answer y j,q with the likelihood P G (y j,q | q, r j,q ).",
            "score": 0.5681146262724637,
            "section_title": "Retrieval Augmented Generation (RAG)",
            "char_start_offset": 2373,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1829
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7783203125
        },
        {
            "corpus_id": "274776545",
            "title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation",
            "text": "Retrieval-augmented Generation Retrievalaugmented generation (RAG) improves generation quality by incorporating relevant context from external knowledge bases, which typically employ a separate dense retriever (Gao et al., 2024;Tan et al., 2024b;Jin et al., 2024b;Tan et al., 2024a;Zhou et al., 2024). Based on training approaches, current RAG systems fall into three categories: \n\n(1) Directly prompt of generative models with retrieved context (Press et al., 2023;Trivedi et al., et al., 2020;Singh et al., 2021). However, joint training faces challenges due to the architectural differences between retrieval and generation, as well as the need for updating document indices during training. Some approaches aim to unify dense retrieval and generation within a single model, including GritLM (Muennighoff et al., 2024) and OneGen (Zhang et al., 2024a). However, GritLM operates as two distinct models with separate attention mechanisms that share parameters, while OneGen still relies on retrieving passage chunks as input for subsequent generation. \n\nGenerative Retrieval Generative retrieval (GR) retrieves by directly generating document identifiers (DocIDs) without the need for traditional document indices (Metzler et al., 2021) (Li et al., 2023a;Tang et al., 2024), and learnable DocIDs (Sun et al., 2023;Wang et al., 2023;Yang et al., 2023). However, these methods mainly focus on optimizing retrieval tasks, without considering its connections with downstream tasks. Even though UniGen (Li et al., 2024c) and CorpusLM (Li et al., 2024a) address downstream tasks, they still require mapping the generated Do-cIDs to the corresponding documents before feeding them into the generator. While RICHES (Jain et al., 2024) attempts to streamline this process but fails to solve the false pruning issue, which leads to suboptimal downstream task performance.",
            "score": 0.5674253305874065,
            "section_title": "Related Work",
            "char_start_offset": 22502,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1862
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 246,
                    "matchedPaperCorpusId": "267750726"
                },
                {
                    "start": 246,
                    "end": 264,
                    "matchedPaperCorpusId": "267751485"
                },
                {
                    "start": 446,
                    "end": 466,
                    "matchedPaperCorpusId": "252762102"
                },
                {
                    "start": 495,
                    "end": 514,
                    "matchedPaperCorpusId": "235390519"
                },
                {
                    "start": 1297,
                    "end": 1315,
                    "matchedPaperCorpusId": "264305656"
                },
                {
                    "start": 1315,
                    "end": 1333,
                    "matchedPaperCorpusId": "264350310"
                },
                {
                    "start": 1333,
                    "end": 1351,
                    "matchedPaperCorpusId": "264305656"
                },
                {
                    "start": 1498,
                    "end": 1516,
                    "matchedPaperCorpusId": "266359654"
                },
                {
                    "start": 1530,
                    "end": 1548,
                    "matchedPaperCorpusId": "267406766"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69091796875
        },
        {
            "corpus_id": "271571143",
            "title": "Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework for Medical Applications",
            "text": "Retrieval-Augmented Generation (RAG) enhances the performance of large language models by leveraging external knowledge bases to retrieve relevant document segments through semantic similarity computations [23], [29]. This approach significantly reduces the incidence of hallucinations-defined as instances where generated content deviates from factual accuracy [38], [42]. Early research on RAG primarily focused on developing sparse or dense retrievers [19], [20], whereas contemporary studies have emphasized optimizing the inte-gration of RAG with Large Language Models (LLMs). These optimizations encompass the timing of retrieval, methodological enhancements, and refined utilization of contextual information to mitigate noise within retrieved documents [10], [12]. \n\nWith respect to adaptive retrieval strategies, the FLARE project has proposed two novel methods: proactive retrieval based on retrieval instructions and confidence-based proactive retrieval, both designed to mitigate unnecessary retrievals [18]. Furthermore, process optimization has evolved from the \"Rewrite-Retrieve-Read\" paradigm [24] to \"ITER-RETGEN\" implementing iterative retrieval to incrementally access more granular and comprehensive knowledge [30]. Concerning the management of noise within retrieved documents, studies conducted demonstrated that irrelevant noise documents do not necessarily deteriorate system performance; conversely, they can enhance accuracy by up to 35%. Conversely, documents incorrectly classified as relevant to the query introduce significant interference, substantially impacting the model's generative performance [9]. \n\nIn the medical domain, the application of RAG technology remains in its nascent stages. The MEDRAG system assessed performance variations across diverse retrievers and corpora in medical question-answering tasks [37]. The Self-BioRAG [16] project integrated Self-RAG [5] technology in medicine, optimizing it through reflective tokens that address retrieval timing, evaluate the relevance and supporting capacity of retrieved documents in answer generation, and assess the quality of generated outputs. Nevertheless, empirical evidence suggests that this approach fails to surpass the performance of models specifically optimized for medical datasets, potentially attributable to the limitations of smaller models in multitask integration, as demonstrated by the AUTOACT study [28].",
            "score": 0.5673929136781385,
            "section_title": "B. Retrieval-augmented Generation",
            "char_start_offset": 9292,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1634
                },
                {
                    "start": 1637,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2139
                },
                {
                    "start": 2140,
                    "end": 2419
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 210,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 455,
                    "end": 459,
                    "matchedPaperCorpusId": "259316759"
                },
                {
                    "start": 1871,
                    "end": 1875,
                    "matchedPaperCorpusId": "267312134"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83447265625
        },
        {
            "corpus_id": "276742133",
            "title": "DeepRetrieval: Hacking Real Search Engines and Retrievers with Large Language Models via Reinforcement Learning",
            "text": "\u2022 Reward from Generator Performance (Reward Models) The second approach designs rewards based on the generator's performance in a Retrieval-Augmented Generation (RAG) (Fan et al., 2024;Zhao et al., 2024) framework. It uses the quality of the generator's outputs rather than directly evaluating the rewrite itself as a reward to guide optimization. Ma et al. (2023) first train a T5 (Raffel et al., 2019)-based query rewriter through supervised fine-tuning as a warm-up phase, then apply PPO (Schulman et al., 2017a) optimization.",
            "score": 0.5670529338180053,
            "section_title": "A.2 Reinforcement Learning and LLMs in Query Augmentation",
            "char_start_offset": 30362,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 529
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 185,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 348,
                    "end": 364,
                    "matchedPaperCorpusId": "258841283"
                },
                {
                    "start": 382,
                    "end": 403,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78271484375
        },
        {
            "corpus_id": "277104989",
            "title": "A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM",
            "text": "In this study, we implemented a retrieval-augmented generation (RAG) system to enhance the accuracy and transparency of large language models (LLMs) for natural language question-answering tasks. Rather than relying on unsupervised fine-tuning, we opted for RAG due to its superior performance in handling both preexisting knowledge encountered during training and entirely new information. As demonstrated by prior research [34], while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, making it a more effective approach for ensuring accurate responses. We followed the RAG architecture, and the proposed system is shown in Fig. 1. \n\nThe study consists of the following components: knowledge base, vector store, question-answering LLM, and user interface. Providing technical details such as data pipeline, retrieval mechanism, and response generation will help to understand the methodology. \n\nThe data processing system enters raw input information through multiple sequential stages of the data pipeline. The system begins with data cleaning and normalization tasks, followed by tokenizing the text into specific units. The processing ensures all data sources maintain equivalent formats to prevent inconsistency issues between them. A scalable vector index system processes the preprocessed data for effective retrieval capabilities. \n\nThe retrieval module uses a dual method whereby it conducts both semantic retrieval alongside keywordbased retrieval techniques. The system uses the Term frequency-inverse document frequency (TF-IDF) as a keyword method for searching along with transformer embeddings that discover matches with semantic textual coherence between documents. \n\nThe response generation step adopts a two-step framework for production. A candidate response set is generated through relevance ranking before going through the neural network re-ranking step. The response generation system verifies both the accuracy and contextual relevance of the results between the processed queries. \n\nThe system maintains scalability through superior technical improvements which guarantee both reliability and efficiency.",
            "score": 0.5663980046765629,
            "section_title": "Method",
            "char_start_offset": 13827,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 666
                },
                {
                    "start": 669,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 927
                },
                {
                    "start": 930,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1715
                },
                {
                    "start": 1718,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2040
                },
                {
                    "start": 2043,
                    "end": 2164
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76904296875
        },
        {
            "corpus_id": "276928032",
            "title": "OpenRAG: Optimizing RAG End-to-End via In-Context Retrieval Learning",
            "text": "In this paper, we analyze and empirically show that the learned relevance for conventional information retrieval (IR) scenarios may be inconsistent in retrieval-augmented generation (RAG) scenarios. To bridge this gap, we introduce OpenRAG, a RAG framework that is optimized end-to-end by tuning the retriever to capture in-context relevance, enabling adaptation to the diverse and evolving needs. Extensive experiments across a wide range of tasks demonstrate that OpenRAG, by tuning a retriever end-to-end, leads to a consistent improvement of 4.0% over the original retriever, consistently outperforming existing state-of-the-art retrievers by 2.1%. Additionally, our results indicate that for some tasks, an end-to-end tuned 0.2B retriever can achieve improvements that surpass those of RAG-oriented or instruction-tuned 8B large language models (LLMs), highlighting the cost-effectiveness of our approach in enhancing RAG systems.",
            "score": 0.5662431339560703,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94091796875
        },
        {
            "corpus_id": "272911196",
            "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
            "text": "Retrieval Augmented Generation (RAG) is a common method for integrating external knowledge into pretrained Large Language Models (LLMs) to enhance accuracy and relevancy in question answering (QA) tasks. However, prompt engineering and resource efficiency remain significant bottlenecks in developing optimal and robust RAG solutions for real-world QA applications. Recent studies have shown success in using fine tuning to address these problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to smaller 7B models has demonstrated superior performance compared to RAG setups with much larger models such as GPT-3.5. The combination of RAFT with parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), promises an even more efficient solution, yet remains an unexplored area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage requirements and gain faster inference times while maintaining comparable RAG performance. This results in a more compute-efficient RAFT, or CRAFT, which is particularly useful for knowledge-intensive QA tasks in resource-constrained environments where internet access may be restricted and hardware resources limited.",
            "score": 0.5661076093979819,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9091796875
        },
        {
            "corpus_id": "266977237",
            "title": "WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge",
            "text": "Retrieval-Augmented Generation (RAG) enhances Language Models by incorporating retrieved text, significantly improving performance in knowledgebased tasks, applicable in both fine-tuned and offthe-shelf scenarios (Gao et al., 2023;Gupta et al., 2024). Traditional RAG (Lewis et al., 2020), also known as Naive RAG, incorporates retrieval content to aid generation but faces key challenges: 1) varying retrieval quality, 2) generation of responses prone to inaccuracies, and 3) difficulties in coherently integrating retrieved-context with current tasks. To overcome the limitations of Naive RAG, advanced methods introduce more contextually rich information during inference. The DSP frame-  work (Khattab et al., 2022) facilitates an intricate exchange between frozen LMs and retrieval models, improving context richness, while PKG (Luo et al., 2023) allows LLMs to retrieve relevant information for complex tasks without altering their parameters. The working mechanism of WisdoM is similar to RAG, but different at the following aspects: \u2460 WisdoM utilizes LVLM to generate world knowledge to provide coherent and accurate context rather than retrieval, \u2461 WisdoM incorporates a contextual fusion mechanism to diminish noise within the context. For additional experimental analysis and discussion, please refer to \u00a7 5.3.",
            "score": 0.5660162298699675,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 7339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1321
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59375
        },
        {
            "corpus_id": "272593221",
            "title": "What is the Role of Small Models in the LLM Era: A Survey",
            "text": "Retrieval Augmented Generation (RAG) can extract query-relevant knowledge from an external document collection or knowledge base, and thus enhance general LLMs by leveraging their incontext learning ability. It involves first using a lightweight retriever to find relevant content from the domain corpus, which is then incorporated into the LLM's input to improve its understanding of domain-specific knowledge (Siriwardhana et al., 2023;Shi et al., 2023;Gao et al., 2023). \n\nAnother approach employs small expert models to retrieve background knowledge for the base LLM in a generative manner. For example, BLADE (Li et al., 2024a) and Knowledge Card (Feng et al., 2024) first pre-train a small expert model on domain-specific data, which then generates expertise knowledge in response to a query, thereby enhancing the base LLM's performance. \n\nSummary and Future Directions Tuning large models for specific target domains is resourceintensive. To address this challenge, a more efficient approach is to fine-tune a small model on domain-specific data. This lightweight expert model can then guide the LLM either during decoding (white-box adaptation) or inference (blackbox adaptation), offering a cost-effective solution for domain adaptation.",
            "score": 0.5654707135662664,
            "section_title": "Domain Adaptation",
            "char_start_offset": 27955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1247
                }
            ],
            "ref_mentions": [
                {
                    "start": 652,
                    "end": 671,
                    "matchedPaperCorpusId": "258741298"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67578125
        },
        {
            "corpus_id": "269982120",
            "title": "xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token",
            "text": "Retrieval-augmented Generation Equipping a parametric language model with a non-parametric datastore has proven effective for various NLP tasks, such as language modeling [36,54,81], opendomain question answering [24,41,66,49], machine translation [35,11] and others.Given the vast  design space of this generation paradigm, numerous approaches with different focuses have been proposed.For instance, RETRO [8] and PlugLM [12] feature architectural innovations for better integration with the non-parametric datastore.REALM [21] introduces an end-to-end methodology for joint optimization of the language model and the retriever.REPLUG [66] and RA-DIT [48] enhance the retriever using LLM feedback to better align these two components.DSP [37] and InteR [17] explore sophisticated interactions between the retriever and the language model.Selfmem [13] employs a reward model to iteratively refine both retrieval and generation processes.Self-RAG [4] introduces a self-reflection mechanism that significantly enhances the quality and factuality of language models.For a more comprehensive review, refer to [18,5,3].Our work, xRAG, distinguishes itself from previous studies by adopting a modality fusion approach to the problem of retrieval augmentation, resulting in a both effective and efficient RAG system.",
            "score": 0.5650200803413015,
            "section_title": "Related Work",
            "char_start_offset": 3884,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 267,
                    "end": 387
                },
                {
                    "start": 387,
                    "end": 518
                },
                {
                    "start": 518,
                    "end": 629
                },
                {
                    "start": 629,
                    "end": 735
                },
                {
                    "start": 735,
                    "end": 839
                },
                {
                    "start": 839,
                    "end": 937
                },
                {
                    "start": 937,
                    "end": 1063
                },
                {
                    "start": 1063,
                    "end": 1114
                },
                {
                    "start": 1114,
                    "end": 1309
                }
            ],
            "ref_mentions": [
                {
                    "start": 1111,
                    "end": 1113,
                    "matchedPaperCorpusId": "263866951"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85009765625
        },
        {
            "corpus_id": "270285974",
            "title": "A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions",
            "text": "Retrieval-Augmented Generation (RAG) [212] is a machine learning technique that combines the strengths of retrievalbased and generative models to enhance the quality and diversity of generated text.This approach has gained significant attention in NLP tasks, particularly in areas like conversational AI, question-answering systems, and text summarization.\n\n1) General Algorithm: In general, a typical RAG system should consist of the following components:\n\n\u2022 Retrieval Component: The system starts by retrieving relevant information from a large database or corpus.This could involve indexing and efficiently searching through past conversations, documents, or web pages based on the input query.Techniques like TF-IDF [213], BM25 [214], or more advanced retrieval methods can be used for this component.\u2022 Generation Component: Once the relevant information is retrieved, a generative model (e.g., GPT-3, T5, or BERT) can use this information as context to generate a response or output [215].The generation process is augmented by conditioning the model on the retrieved data, allowing it to generate more informed, contextually accurate, and diverse responses rather than generating from scratch.\u2022 Component Integration: The integration ways of these two components can vary [212].Firstly, the retrieved information might be concatenated with the input prompt and directly fed into the generator.Others might use a more sophisticated fusion mechanism, where the retrieval and generation models interact in multiple steps, refining the context and the generated output iteratively.By leveraging external knowledge, the generated text of RAG is more likely to be contextually appropriate and accurate.Retrieval of varied sources can introduce more diversity in the generated outputs, reducing the likelihood of repetitive or generic responses [216].Retrieval models can quickly narrow down the scope of information needed, which can make the generation process more efficient compared to exploring the entire knowledge space.Incorporating specific retrieved information can provide more control over the content and tone of the generated text, aligning it better with user expectations or specific requirements.\n\n2) Specific RAG Algorithm: Clinfo.ai[51].Clinfo.ai is an open-source WebApp that answers clinical questions based on dynamically retrieved scientific literature.The information retrieval and summarization tasks are applied to evaluate the retrieval-augmented LLM systems.\n\nAlmanac [52].",
            "score": 0.5644119737265336,
            "section_title": "D. Retrieval-Augmented Generation for Med-LLMs",
            "char_start_offset": 58486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 198,
                    "end": 356
                },
                {
                    "start": 358,
                    "end": 456
                },
                {
                    "start": 458,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 697
                },
                {
                    "start": 697,
                    "end": 805
                },
                {
                    "start": 805,
                    "end": 994
                },
                {
                    "start": 994,
                    "end": 1199
                },
                {
                    "start": 1199,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1399
                },
                {
                    "start": 1399,
                    "end": 1583
                },
                {
                    "start": 1583,
                    "end": 1702
                },
                {
                    "start": 1702,
                    "end": 1850
                },
                {
                    "start": 1850,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2212
                },
                {
                    "start": 2214,
                    "end": 2250
                },
                {
                    "start": 2250,
                    "end": 2255
                },
                {
                    "start": 2255,
                    "end": 2375
                },
                {
                    "start": 2375,
                    "end": 2485
                },
                {
                    "start": 2487,
                    "end": 2500
                }
            ],
            "ref_mentions": [
                {
                    "start": 720,
                    "end": 725,
                    "matchedPaperCorpusId": "14638345"
                },
                {
                    "start": 732,
                    "end": 737,
                    "matchedPaperCorpusId": "207220720"
                },
                {
                    "start": 1844,
                    "end": 1849,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 2250,
                    "end": 2254,
                    "matchedPaperCorpusId": "264487188"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8671875
        },
        {
            "corpus_id": "261049520",
            "title": "RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models",
            "text": "As a baseline in open-book setting, we present the results of the Retrieval-Augmented Generation (RAG) model (Lewis et al., 2020b) shown in Petroni et al. ( 2021), which achieved strong performance in the KILT benchmark. The RAG model comprises a bi-encoder retriever and a sequenceto-sequence generator (BART model (Lewis et al., 2020a)), both of which are trained end-to-end. The total number of trainable parameters in the RAG model is approximately 626 million. It is important to note that the RAG model was trained specifically for the KILT benchmark, whereas our chosen LLMs and constructed R-LLMs were not.",
            "score": 0.5641219175066803,
            "section_title": "A.5 Details of Baseline Model in Open-Book Setting",
            "char_start_offset": 22851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 614
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 130,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 316,
                    "end": 337,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.796875
        },
        {
            "corpus_id": "270379921",
            "title": "TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation and LLMs",
            "text": "The DPR retriever follows a bi-encoder architecture, one for the queries and the other for the documents in the knowledge corpus.This method can be finetuned on any seq2seq task, whereby both the generator and retriever are jointly learned.In the fine-tuning phase, only the query encoder is trained.This reduces considerably the computational burden compared to REALM, which periodically updates the document index during pre-training [10].The results in [11] show that RAG responses are more factual, specific, and diverse than other baselines based on language models.For these reasons and given the nature of the task involved in telecommunication standards, we rely on RAG to design TelecomRAG as detailed in the next section.",
            "score": 0.5640288510152125,
            "section_title": "C. Retrieval-Augmented Methods",
            "char_start_offset": 9263,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 129,
                    "end": 240
                },
                {
                    "start": 240,
                    "end": 300
                },
                {
                    "start": 300,
                    "end": 441
                },
                {
                    "start": 441,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 731
                }
            ],
            "ref_mentions": [
                {
                    "start": 436,
                    "end": 440,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7197265625
        },
        {
            "corpus_id": "278339057",
            "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models",
            "text": "Retrieval-augmented generation (RAG) aims to integrate external knowledge into LLMs, improving their factuality [11]. Given an input query, the first process of RAG is to select relevant knowledge, which is typically done by retrieval [20] or ranking model [33]. Subsequently, an LLM generator incorporates these candidate documents to generate an answer. An active research question is how to improve the overall RAG performance. Some studies aim to improve the knowledge accuracy [6,43], such as fine-tuning an answer-aware dense retriever [44,48] or introducing additional modules for document filtering [63,66]. Other work alternatively enhances the robustness of LLMs to irrelevant content, enabling LLMs to adaptively extract supporting facts from the retrieved documents [70,76]. However, these methods either optimize the retrieval or the generation process without dual enhancement, potentially leading to sub-optimal performance [27]. Although existing work proposes the end-to-end training paradigm, they overly simplify a marginalization optimization through independent top-k approximation [43,72], where they simply feed top-k documents into downstream LLMs one-by-one and re-score their relevance to optimize the retriever [23,27]. This has been criticized far from the practical scenarios as the RAG system typically consumes multiple documents [72], while exhaustively enumerating all possible document permutations is cost-intensive and typically infeasible in practice. In this work, we propose DRO, which directly treats the document permutation as a latent variable and estimates its distribution for optimization.",
            "score": 0.5639408578197876,
            "section_title": "Retrieval-augmented generation",
            "char_start_offset": 8098,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1635
                }
            ],
            "ref_mentions": [
                {
                    "start": 235,
                    "end": 239,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 257,
                    "end": 261,
                    "matchedPaperCorpusId": "10986612"
                },
                {
                    "start": 485,
                    "end": 488,
                    "matchedPaperCorpusId": "230437591"
                },
                {
                    "start": 1103,
                    "end": 1107,
                    "matchedPaperCorpusId": "230437591"
                },
                {
                    "start": 1107,
                    "end": 1110,
                    "matchedPaperCorpusId": "269605438"
                },
                {
                    "start": 1238,
                    "end": 1242,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1361,
                    "end": 1365,
                    "matchedPaperCorpusId": "269605438"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9072265625
        },
        {
            "corpus_id": "278535570",
            "title": "Patchwork: A Unified Framework for RAG Serving",
            "text": "The RAG ecosystem is evolving rapidly with new components and refinements to existing ones. In the following paragraphs, we survey the most common components of the RAG pipeline and their evolution. We also provide a brief overview of new components that are being added. \n\nRetriever. The retriever is responsible for fetching relevant information from an indexed data store. This store typically contains data that is domain-specific and deemed useful for responding to queries. For instance, a RAG system deployed in a legal firm [57] might use a database indexed with legal briefs and case records. \n\nRetrievers are evolving rapidly. One dimension of evolution is the indexing approach, which is tied to the retrieval strategy. Early RAG systems used plain-text indices [33,36,72], but newer approaches to index documents are using vector representations [25,58], graphs [17,26], trees [19,39,43], or bitmaps [11]. More recently, dynamic retrieval [38] is seeing adoption; here, for a given query, during the retrieval stage, multiple retriever steps are performed iteratively to achieve better quality of retrieved texts [38] and the retrieved output is post-processed. \n\nAugmenter. The augmenter determines how the retrieved documents are incorporated into the original user query. This process can range from simple concatenation [6] to more sophisticated recent methods that employ LLMs or classifiers to summarize, rank, or filter the retrieved content before it is added to the query [62]. \n\nGenerator. The generator is typically an LLM that produces the final response based on the augmented query. Recent research [6,71] focuses on training models specifically fine-tuned for RAG scenarios. A given generator model can be used alongside of several LLM serving engines, such as vLLM [34], Ollama[54], and llama.cpp [44] are frequently used. The choice of the model and serving engine depends on the desired trade-offs between latency, accuracy, and resource availability. \n\nAdditional Components. As RAG systems mature, additional components are being introduced to enhance quality, efficiency, and safety.",
            "score": 0.5637666047617511,
            "section_title": "RAG Evolution",
            "char_start_offset": 8379,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 271
                },
                {
                    "start": 274,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1173
                },
                {
                    "start": 1176,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1498
                },
                {
                    "start": 1501,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1981
                },
                {
                    "start": 1984,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2116
                }
            ],
            "ref_mentions": [
                {
                    "start": 777,
                    "end": 780,
                    "matchedPaperCorpusId": "220128068"
                },
                {
                    "start": 1336,
                    "end": 1339,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1493,
                    "end": 1497,
                    "matchedPaperCorpusId": "271269939"
                },
                {
                    "start": 1625,
                    "end": 1628,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1793,
                    "end": 1797,
                    "matchedPaperCorpusId": "261697361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8671875
        },
        {
            "corpus_id": "270067727",
            "title": "ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator",
            "text": "\u2022 We propose a multi-agent system and introduce an aggressive ATTACKER to improve the robustness of the GENERATOR in RAG-QA. et al., 2024b) conducts CoT (Wei et al., 2022) and self-revision, forming a reasoning chain towards generating the final answer. As a separate system, the retriever and generator have different training objectives, giving rise to risks of indirect optimization. To this end, RE-PLUG (Shi et al., 2024) tunes the retriever with the output token probability without the need for direct access to generator parameters. RA-DIT (Lin et al., 2024) introduces a co-training setting for both modules and achieves dual optimization. RAG-end2end (Siriwardhana et al., 2023) proposes to dynamically re-index the document library with an optimized retriever during training. Self-RAG (Asai et al., 2024) and Adaptive-RAG (Jeong et al., 2024) trains models to be fluent in answer generation and aware of whether to retrieve and what to retrieve, mitigating noises brought by unnecessary retrieval. It is widely recognized LLM also can act as a re-ranker better (Sun et al., 2023) with more parameters. Recent works like MetaEOL (Lei et al., 2024), GritLM (Muennighoff et al., 2024), LLM2Vec (BehnamGhader et al., 2024), NV-Embed (Lee et al., 2024) and bge-en-icl (Li et al., 2024) prompt, train or construct in-context learning examples for LLMs to act as a document encoder and showcase that LLMs have a strong embedding capability. REAR (Wang et al., 2024a) further plugs an extra re-ranking module to LLMs, and simultaneously improves its document ranking and answer generation capabilities.",
            "score": 0.563684236316542,
            "section_title": "ATM Generator",
            "char_start_offset": 4675,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1606
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 171,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 408,
                    "end": 426,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 661,
                    "end": 688,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 797,
                    "end": 816,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 834,
                    "end": 854,
                    "matchedPaperCorpusId": "268553748"
                },
                {
                    "start": 1073,
                    "end": 1091,
                    "matchedPaperCorpusId": "258212638"
                },
                {
                    "start": 1140,
                    "end": 1158,
                    "matchedPaperCorpusId": "268041489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87744140625
        },
        {
            "corpus_id": "277622120",
            "title": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort for Retrieval and RAG",
            "text": "Retrieval-augmented generation (RAG), amalgamating an information retrieval component with a text generator model, is commonly used to mitigate the issues of hallucination and knowledge obsolescence in LLMs [24,35,45]. However, the goals of the retriever (retrieving more relevant information) and generator (extracting useful information to produce precise and coherent responses) in RAG are different and can be mismatched. To address this issue, current research focuses mainly on two approaches: (1) Utility judgments, which directly entails utilizing LLMs to identify useful retrieved information based on its utility for downstream tasks [79,80,82]. Utility judgments typically serve as post-processing steps for retrieval results and do not directly influence the retriever. (2) Utility-optimized retriever, which involves transferring the capability of LLMs to evaluate the utility of retrieved information to the retriever. Specifically, two primary optimization functions are commonly employed: (a) calculating the likelihood of the ground truth answers given the query and retrieval information [2,16,22,27,30,35,52,57,74]; (b) directly using evaluation metrics of the downstream generation tasks [17,66,76], such as exact match (EM), and ROUGE [36], and computing the performance difference between the generated answer and the ground truth answer. However, this approach relies on ground truth answers for specific downstream tasks and limits generalization.",
            "score": 0.5636014316557832,
            "section_title": "Utility-Focused RAG",
            "char_start_offset": 8345,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1471
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 211,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 211,
                    "end": 214,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 648,
                    "end": 651,
                    "matchedPaperCorpusId": "268733288"
                },
                {
                    "start": 651,
                    "end": 654,
                    "matchedPaperCorpusId": "273532096"
                },
                {
                    "start": 1115,
                    "end": 1118,
                    "matchedPaperCorpusId": "258547128"
                },
                {
                    "start": 1118,
                    "end": 1121,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1121,
                    "end": 1124,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1127,
                    "end": 1130,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 1212,
                    "end": 1215,
                    "matchedPaperCorpusId": "259281167"
                },
                {
                    "start": 1215,
                    "end": 1218,
                    "matchedPaperCorpusId": "269605438"
                },
                {
                    "start": 1256,
                    "end": 1260,
                    "matchedPaperCorpusId": "964287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8408203125
        },
        {
            "corpus_id": "275357908",
            "title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG",
            "text": "There have been several efforts to achieve domainspecific RAG. While more complex, jointly training the retriever and the generator has been shown to work well for question-answering (Siriwardhana et al., 2023). Others encode the domain-specific information in knowledge graphs in addition to vector databases (Barron et al., 2024). Retrieval Augmented Fine Tuning (Zhang et al., 2024b) helps the LLM adapt to the domain by modifying how the retrieved information is present in the LLM training dataset. In contrast, our approach relies solely on training a better retriever, which can be used with any LLM, thereby reducing coupling. \n\nMulti-task retriever models (Maillard et al., 2021;Wang et al., 2022) enhance embedding versatility by simultaneously training on multiple tasks or by utilizing a large dataset spanning diverse topics. Building upon this concept, instructionfollowing embedding models (Asai et al., 2023;Su et al., 2022;BehnamGhader et al., 2024) integrate explicit instructions into the embedding process, thereby enabling the generation of more nuanced and task-specific embeddings. We leverage this kind of instruction fine-tuning to train a retriever on many datasets containing structured data extracted from databases. \n\nLastly, generating workflows is a structured output task, similar to code generation, that requires specialized embeddings. Code embedding models (Feng et al., 2020;Li et al., 2022;Guo et al., 2020;Neelakantan et al., 2022;Zhang* et al., 2024) adapt common training techniques to the domain of code representation learning and retrieval. A parallel research trajectory focuses on structured data embedding models, including Synchromesh (Poesia et al., 2022), which fine-tunes a model to retrieve relevant samples for few-shot prompting using a similarity metric derived from tree edit distance, and SANTA (Li et al., 2023), which implements a modified pretraining objective to generate structure-aware representations. Prior work already uses RAG for a similar but more limited structured output task (Bechard and Ayala, 2024).",
            "score": 0.5631983609861039,
            "section_title": "Related Work",
            "char_start_offset": 4604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1244
                },
                {
                    "start": 1247,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2074
                }
            ],
            "ref_mentions": [
                {
                    "start": 365,
                    "end": 386,
                    "matchedPaperCorpusId": "268510197"
                },
                {
                    "start": 905,
                    "end": 924,
                    "matchedPaperCorpusId": "253581733"
                },
                {
                    "start": 1412,
                    "end": 1428,
                    "matchedPaperCorpusId": "252993162"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "271534349",
            "title": "RLCoder: Reinforcement Learning for Repository-Level Code Completion",
            "text": "A. Baselines 1) For RLCoder: To evaluate the effectiveness of RLCoder, we compare it with the RawRAG method and RepoCoder framework. Besides, we use a popular dense retriever UniXcoder [49] in these experiments. \n\n\u2022 RawRAG refers to the standard retrieval and generation approach in the repository-level code completion task. For the unfinished code to generate, RawRAG uses the left context of unfinished code as the query to find the relevant code in the repository to build prompts for generation. \u2022 RepoCoder [13] is the state-of-the-art framework for repository-level code completion. It uses an iterative retrieval and generation approach to generate target code. 2) For RLRetriever: To evaluate the effectiveness of RL-Retriever, we compare it with the following commonly used retrieval methods in RAG: \n\n\u2022 NoRetrieval stands for direct generation with unfinished code, without retrieval. \u2022 BM25 [18] calculates scores for code candidates based on the frequency of query terms in each candidate. It adjusts for candidate length and the average candidate length across the entire database to prevent bias towards longer candidates. \n\n\u2022 UniXcoder [49] is a dense retriever that encodes both the query and the code snippets into dense vector spaces. This encoding facilitates the identification and retrieval of semantically relevant code snippets from a large corpus based on the similarity of vector representations. \u2022 UniXcoder-SFT is a retriever that we trained using supervised fine-tuning of UniXcoder. Due to the lack of labeled data, we use the candidate with the lowest perplexity of target code as the label to fine-tune the retriever.",
            "score": 0.5631078071890165,
            "section_title": "IV. EXPERIMENTAL SETUP",
            "char_start_offset": 22618,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 211
                },
                {
                    "start": 214,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1137
                },
                {
                    "start": 1140,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1649
                }
            ],
            "ref_mentions": [
                {
                    "start": 903,
                    "end": 907,
                    "matchedPaperCorpusId": "207178704"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71533203125
        },
        {
            "corpus_id": "251594591",
            "title": "ConTextual Masked Auto-Encoder for Dense Passage Retrieval",
            "text": ". (Hofst\u00e4tter et al. 2021) introduce an efficient topic-aware query and balanced margin sampling technique to improve the fine-tuning efficiency. (Qu et al. 2020) combines three effective strategies to achieve good performance, i.e., cross-batch negatives, denoised hard negatives, and data augmentation. (Ren et al. 2021b) introduce the dynamic listwise distillation by designing a unified listwise training approach to improve both the retriever and the re-ranker adaptively. ) designs Hybrid List Aware Transformer Reranking (HLATR) as a subsequent reranking module to incorporate retrieval and reranking stage features. ) present adversarial retriever-ranker, which consists of a dual-encoder retriever and a cross-encoder ranker to be jointly optimized according to a minimax adversarial objective. As we focus on the improvement brought by pre-training, following (Gao and Callan 2021a,b;Ma et al. 2022), we reuse the open source fine-tuning pipeline Tevatron (Gao et al. 2022b) to evaluate the effectiveness of our pre-training method.",
            "score": 0.5629132975346307,
            "section_title": "Related Works",
            "char_start_offset": 9527,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 2,
                    "end": 25,
                    "matchedPaperCorpusId": "233231706"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81884765625
        },
        {
            "corpus_id": "273403982",
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "text": "Retrieval-augmented generation (RAG) is an advanced hybrid model architecture that augments natural language generation (NLG) with external retrieval mechanisms to enhance the model's knowledge base. Traditional large language models (LLMs) such as GPT-3 and BERT, which are pre-trained on vast corpora, rely entirely on their internal representations of knowledge, making them susceptible to issues like hallucinations-where the models generate plausible but incorrect information. These models cannot efficiently update their knowledge bases without retraining, making them less practical for dynamic, knowledge-intensive tasks like open-domain question answering and fact verification (Brown, T., et al. 2020). To overcome these limitations, the paper (Lewis et al. 2020) proposed the RAG architecture, which retrieves real-time, relevant external documents to ground the generated text in factual information. \n\nThe RAG model incorporates two key components: \n\n1. Retriever: This retrieves the most relevant documents from a corpus using techniques such as dense passage retrieval (DPR) (Karpukhin et. al. 2020) or traditional BM25 algorithms. 2. Generator: It synthesizes the retrieved documents into coherent, contextually relevant responses. \n\nRAG's strength lies in its ability to leverage external knowledge dynamically, allowing it to outperform generative models like GPT-3 and knowledge-grounded systems like BERT, which rely on static datasets. \n\nIn open-domain question answering, RAG has been demonstrated to be highly effective, consistently retrieving relevant information and improving the factual accuracy of the generated responses (Guu, K., et al. 2020). In addition to knowledge retrieval, RAG models excel at updating knowledge bases. Since the model fetches external documents for each query, it requires no retraining to incorporate the latest information. This flexibility makes RAG models particularly suitable for domains where information is constantly evolving, such as medical research, financial news, and legal proceedings. Furthermore, studies have shown that RAG models achieve superior results in a variety of knowledge-intensive tasks, including document summarization and, knowledge-grounded dialogues",
            "score": 0.5628693584691091,
            "section_title": "Overview of RAG Models",
            "char_start_offset": 10564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 913
                },
                {
                    "start": 916,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1248
                },
                {
                    "start": 1251,
                    "end": 1457
                },
                {
                    "start": 1460,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2239
                }
            ],
            "ref_mentions": [
                {
                    "start": 755,
                    "end": 773,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8564453125
        },
        {
            "corpus_id": "271843111",
            "title": "Evaluating the Impact of Advanced LLM Techniques on AI-Lecture Tutors for a Robotics Course",
            "text": "The evaluation of a Large Language Model-based tutoring system for a University robotics course highlighted several insights into the application of advanced LLM techniques and the resulting performance in an educational setting. First, our findings underscored the positive impact of Retrieval-Augmented-Generation (RAG) and prompt engineering, which consistently improved model performance across similarity metrics. Particularly, the use of RAG demonstrated a considerable enhancement in providing factual answers and is consistent with the general belief that RAG is reducing hallucinations [Shuster et al., 2021]. Furthermore, even though our human evaluation is currently restricted to two test subjects, their answers already point out that added references increases trustworthiness. Therefore, RAG appears as a very valuable technique that should be-together with some form of prompt engineering-considered first. As further advantages, in our experience RAG is quite straight forward to realize, in particular in a course setting in which well-curated background material is readily available. Furthermore, from a teaching point of view a tutor should stick to PREPRINT the lecture material, e.g., when going over a concept the tutoring system should carefully choose examples and ideally stick-or at least start-with the ones provided in the lecture. This should positively affect the learning of students. \n\nFine-tuning has to be considered as a more involved technique. It requires additional effort in setting up a data set for training. As an advantage, in our case we saw that a quite small fine-tuned model (13 billion parameters) consistently performed on the same level-or better-as GPT-3.5 (175 billion parameters) when used without RAG. Fine-tuning produced a much more efficient expert which showed as quite capable. But, on the downside, the process of fine-tuning appeared as more delicate. In our data, we observed a curious drop-off when adding RAG to the fine-tuned model which was unexpected and would contradict our and others' experience with RAG. As an explanation, fine-tuning aims to specialize a model to a specific task and a specific type of interaction. A fine-tuned model might loose some of its general flexibility. As a consequence, when interacting very differently with the model, the model might produce worse results or even behave erratically.",
            "score": 0.5628613898242167,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 42602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2190
                },
                {
                    "start": 2191,
                    "end": 2254
                },
                {
                    "start": 2255,
                    "end": 2388
                }
            ],
            "ref_mentions": [
                {
                    "start": 595,
                    "end": 617,
                    "matchedPaperCorpusId": "233240939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.480224609375
        },
        {
            "corpus_id": "263605962",
            "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
            "text": "ATLAS (Izacard et al., 2022b) builds upon the T5 language model (Raffel et al., 2020), and continuosly pre-trains the framework over unsupervised text. REPLUG (Shi et al., 2023b) and In-Context RALM (Ram et al., 2023) combine off-the-shelf LLMs with general-purpose retrievers, showing that LLMs and retrievers, even when optimized independently, can be effectively fused through the emergent incontext learning capbabilities of LLMs. However, extensive pre-training of such architectures incurs high computational costs, and the off-the-shelf fusion approach also has limitations, particularly as the LLMs are not inherently trained to incorporate retrieved content. \n\nFigure 1: The RA-DIT approach separately fine-tunes the LLM and the retriever. For a given example, the LM-ft component updates the LLM to maximize the likelihood of the correct answer given the retrieval-augmented instructions ( \u00a72.3); the R-ft component updates the retriever to minimize the KL-Divergence between the retriever score distribution and the LLM preference ( \u00a72.4) \n\nIn this work, we show lightweight instruction tuning (Chung et al., 2022b;Iyer et al., 2022;Zhou et al., 2023) alone can significantly boost the performance of RALMs, especially in scenarios that require access to large, external knowledge sources. We propose Retrieval-Augmented Dual Instruction Tuning (RA-DIT), an approach that retrofits any LLM with retrieval capabilities via fine-tuning over a set of tasks selected to cultivate knowledge utilization and contextual awareness in the language model predictions. We initialize the framework using pre-trained LLAMA (Touvron et al., 2023a) and a state-of-the-art dual-encoder based dense retriever, DRAGON+ (Lin et al., 2023). Following Shi et al. (2023b), we retrieve relevant text chunks based on the language model prompt.",
            "score": 0.562050437861938,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 1049
                },
                {
                    "start": 1052,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1830
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 85,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9052734375
        },
        {
            "corpus_id": "273962778",
            "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
            "text": "Information Retrieval: Advancements in deep learning have revolutionized information retrieval systems, enhancing their personalization and accuracy in retrieving relevant documents. Early information retrieval frameworks employed sparse retrievers [6] or dense retrievers [4,8] to represent large corpora but struggled to capture deep semantic relationships [13]. LLM-based retrievers (generative retrieval) have since emerged as notable methods, leveraging the rich prior knowledge of LLMs to significantly improve performance by converting documents into parametric knowledge and generating them instead of computing similarity scores [38]. However, the frequent encoding and decoding processes in LLMs severely hinder efficiency [38]. To address the trade-off between effectiveness and efficiency, we propose invar-retrieval in our architecture, enabling the model to efficiently retrieve the most relevant documents without introducing variance. \n\nRetrieval-augmented Language Model: Currently, retrieval-augmented language models have proven effective in answering questions by leveraging external information through the integration of novel retrievers and LLMs [38]. However, the architectural gap between retrieval and generation continues to hinder unified optimization across the entire retrieval-augmented generation system [2]. To address the isolation between retrieval and generation, a novel architecture called RA-DIT was introduced [18]. By aligning retriever scoring with LSR scoring [27], it has been shown to deliver state-of-the-art performance across various tasks. However, it still employs dense retrievers like DRAGON+ [17] in the retrieval stage, which fails to eliminate the problem at its source and introduces inefficiencies throughout the process. Correspondingly, we introduce a representation learning method and invariance loss in our Invar-RAG architecture, which partially addresses these issues and explores a novel approach to using a single LLM for multiple roles within the RAG system.",
            "score": 0.5616511375823245,
            "section_title": "Related Work",
            "char_start_offset": 21968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 950
                },
                {
                    "start": 953,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 2025
                }
            ],
            "ref_mentions": [
                {
                    "start": 638,
                    "end": 642,
                    "matchedPaperCorpusId": "230433817"
                },
                {
                    "start": 733,
                    "end": 737,
                    "matchedPaperCorpusId": "230433817"
                },
                {
                    "start": 1169,
                    "end": 1173,
                    "matchedPaperCorpusId": "230433817"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79443359375
        },
        {
            "corpus_id": "252780775",
            "title": "Retrieval Augmented Visual Question Answering with Outside Knowledge",
            "text": "Since repeated computation of embeddings for all documents is costly, we follow Lewis et al. (2020) who find that it is enough to train only the question encoder F q and leave document encoder F d fixed. As shown in Fig. 2, document embeddings are pre-extracted with a pre-trained DPR document encoder. The FAISS system (Johnson et al.,  Pre-training: We start with pre-trained versions of BERT-base and T5-large as the document retriever and the answer generator, respectively. The retriever was refined by training it on GS-full under the DPR loss (Equation 2) with pseudo relevance labels released by Luo et al. (2021). The already strong retriever serves as a good starting point for all DPR-based models presented in this paper (including RA-VQA and our replication of baselines in the literature). \n\nOK-VQA Fine-tuning: Our RA-VQA framework trains the answer generator and the retriever jointly under Equation 6. \n\nWe also report on variants of RA-VQA, to investigate the contribution of various model components to overall performance: RA-VQA-NoDPR omits retrieval entirely so that answers are generated by the fine-tuned T5 alone. RA-VQA generation in Equation 7 simplifies to \n\nRA-VQA-FrDPR leaves the retriever frozen after pre-training and fine-tunes the generator only. \n\nRA-VQA-NoPR is a version of RA-VQA in which document retrieval is trained only with model predictions. The loss function is as Equation 6, but with positive and negative document sets defined as \n\nRA-VQA-NoCT replaces the customized generation targets by the single most popular response (s * k becomes s * in Equation 6) so that the generator is trained to produce the same answer from every retrieved document.",
            "score": 0.5614956341676118,
            "section_title": "Pre-Computed FAISS Document Indices",
            "char_start_offset": 14679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 803
                },
                {
                    "start": 806,
                    "end": 918
                },
                {
                    "start": 921,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1281
                },
                {
                    "start": 1284,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1478
                },
                {
                    "start": 1481,
                    "end": 1696
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 99,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 604,
                    "end": 621,
                    "matchedPaperCorpusId": "237453242"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8115234375
        },
        {
            "corpus_id": "261697451",
            "title": "RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair",
            "text": "We investigate how different retrieval modules affect the APR performance in the retrieval-augmented generation setting in Table 7. \n\nWe first compare with a Random baseline via randomly retrieving bug-fix pairs from the codebase. The consistent performance downgrade compared to \"no retriever\" implies that randomly retrieved fix patterns cannot provide useful guiding signals for APR. Then we compare our hybrid retriever in RAP-Gen with different retrievers: sparse BM25 retrievers, and dense retrievers based on CodeBERT or CodeT5. We observe that CodeT5-based retrievers outperforms either BM25 or CodeBERT-based retrievers, while our hybrid retriever combining both BM25 and CodeT5 achieves the best APR performance, validating the effectiveness of our retriever module design in RAP-Gen. \n\nWe further analyze the performance of our retrievers in terms of lexical and semantic matching between the query and the top retrieved patches. We employ the BLEU-4 score to measure their subtoken overlap for lexical matching, while for semantic matching, we compute the cosine similarity (CosSim) between their dense vectors encoded by our fine-tuned DPR retriever. Table 8 shows the performance of our retrievers on both TFix and Code Refinement benchmarks. The first row indicates the lower-bound performance via randomly retrieving bug-fix pairs from the codebase, where we observe this Random baseline achieves much lower scores in both lexical and semantic matching. \n\nFor lexical matching, BM25 outperforms DPR (CodeT5-based) on TFix but underperforms on two Code Refinement subsets. We anticipate that it is due to the data difference between TFix and Code Refinement, where the latter employs obfuscated identifiers (e.g., VAR1, VAR2, ...) that hinders the performance of the lexicalbased BM25 retriever. The hybrid retriever achieves the best lexical matching on all datasets, revealing the semantic information can complement to the lexical information. For semantic matching, DPR achieves the best results on all datasets, which is not surprising as it is optimized towards the identical objective.",
            "score": 0.5613570794137012,
            "section_title": "RQ4: Analysis of Hybrid Patch Retriever",
            "char_start_offset": 39690,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 134,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1469
                },
                {
                    "start": 1472,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1961
                },
                {
                    "start": 1962,
                    "end": 2107
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.662109375
        },
        {
            "corpus_id": "278367823",
            "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
            "text": "Retrieval-augmented generation (RAG) enhances generation performance by integrating relevant external knowledge into the generation pipeline. Early research primarily adopted prompt-based approaches, guiding LLMs through processes such as query generation, query decomposition, and multi-turn information retrieval [44,28,43,15,33,22]. Despite their effectiveness, these methods often require intricate prompt engineering and impose substantial demands on the model's reasoning capabilities. To improve efficiency and reduce dependency on strong black-box LLMs, subsequent work has proposed supervised fine-tuning strategies for smaller LLMs. For instance, Self-RAG [1] employs a self-reflection mechanism, iteratively refining model outputs through predicted reflection tokens. RetroLLM [24] integrates retrieval and generation by enabling the model to directly generate fine-grained evidence from the corpus via constrained decoding. Recent advances also include test-time scaling techniques [25,14,47,13], notably Monte Carlo Tree Search (MCTS), which dynamically expands the search space during inference. For example, RAG-star [13] integrates retrieved information into a tree-based reasoning process, while AirRAG [5] employs MCTS to activate intrinsic reasoning capabilities and expand the solution space. Despite promising results, these approaches introduce significant computational overhead, limiting their practical applicability.",
            "score": 0.561167752858074,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 6269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1442
                }
            ],
            "ref_mentions": [
                {
                    "start": 325,
                    "end": 328,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 666,
                    "end": 669,
                    "matchedPaperCorpusId": "264288947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9072265625
        },
        {
            "corpus_id": "273549875",
            "title": "Context-Augmented Code Generation Using Programming Knowledge Graphs",
            "text": "The use of RAG in code-related tasks remains underexplored (Wang et al., 2024). Previous studies, such as Parvez et al. ( 2021), have experimented with smaller code language models like Code-BERT (Feng et al., 2020) and GraphCodeBERT (Guo et al., 2020), focusing on tasks like code summarization and generation. Unlike their work, which involved fine-tuning the retriever module to extract relevant data, our approach applies RAG during inference time without requiring any model fine-tuning. While (Wang et al., 2024) presents a more similar approach to ours by comparing the performance of LLMs and CLLMs across various data sources and retrieval methods, they highlight challenges with retrievers extracting similar content and models' limited capacity for additional context. Our work differs by representing knowledge in a granular way, allowing retrievers to more accurately extract relevant information and prompting models with only useful content to reduce hallucinations.",
            "score": 0.56076661168606,
            "section_title": "RAG FOR CODE GENERATION",
            "char_start_offset": 16605,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 981
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 215,
                    "matchedPaperCorpusId": "211171605"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43408203125
        },
        {
            "corpus_id": "273532677",
            "title": "SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains",
            "text": "Effect of Stage-I and Stage-II. Table 1 to 4 show that retrieval-oriented fine-tuning (Stage-I) significantly enhances LLM performance on QA tasks compared to the original backbone, demonstrating its effectiveness. However, further improvements become challenging after this stage. When the LLMs are fine-tuned on self-synthesized training tuples, their performance on target tasks improves even more, with an average increase of 2.21% for Effect of Different Retrievers. We show the performance of SimRAG using Dragon (Lin et al., 2023) as the retriever in Table 5. The results show consistent performance improvements of SimRAG over the LLM backbone, demonstrating that Sim-RAG is robust to different retriever choices and that its self-improvement mechanism consistently enhances performance.",
            "score": 0.5607658156920441,
            "section_title": "Ablation Studies",
            "char_start_offset": 22109,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 795
                }
            ],
            "ref_mentions": [
                {
                    "start": 519,
                    "end": 537,
                    "matchedPaperCorpusId": "256868909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81494140625
        },
        {
            "corpus_id": "274023917",
            "title": "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering",
            "text": "The retrieval-augmented generation paradigm, originating from the DrQA framework by Chen et al., initially used heuristic retrievers like TF-IDF to source evidence from Wikipedia, followed by a neural model to extract answers. This methodology was advanced by DPR (Karpukhin et al., 2020), using pre-trained transformers like BERT for retrieval and reading. Retrieval Augmented Generation (RAG) (Lewis et al., 2020) further evolved the approach by shifting from answer extraction to generation, enabling free-form text creation. Advances in RAG have explored retrieval as a critical tool for augmentation, with Schick et al., Luo et al., and Asai et et al., 2020) and RETRO (Borgeaud et al., 2022) integrated retrieval during the pre-training phase. Recently, Large Language Models (LLMs) have been incorporated into this framework, as seen in REPLUG (Shi et al., 2023b) and IC-RALM (Ram et al., 2023). While prior work on RAG primarily addressed general knowledge, this study introduces the first application of RAG to medical literature, harnessing a vast collection of medical textbooks. Our innovative knowledge self-refinement strategies enhance the fidelity of retrieved information, marking the first refinement of RAG's retrieval component for elevated performance in the medical domain.",
            "score": 0.5605117820756521,
            "section_title": "Retrieval Augmented Generation",
            "char_start_offset": 22486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1295
                }
            ],
            "ref_mentions": [
                {
                    "start": 395,
                    "end": 415,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 674,
                    "end": 697,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.783203125
        },
        {
            "corpus_id": "269149146",
            "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
            "text": "Retrieval-Augmented Generation (RAG) [68] is a framework that combines the capabilities of large language models (LLMs) with external knowledge sources through a retrieval mechanism.Unlike traditional language models that generate text based solely on the text's internal representation, RAG models retrieve relevant information from a knowledge base (like a database or the internet) and integrate this into the generation process.The core components of RAG include the retrieval mechanism, which fetches relevant documents or data, and the generative model, which synthesizes the retrieved information into coherent and contextually relevant responses.The theoretical principles underpinning RAG stem from the need to enhance language models with the ability to access and utilize external, structured knowledge [69].This is in response to the limitations of traditional LLMs that rely solely on their pre-trained parameters for knowledge, which can be outdated or incomplete.The typical architecture of RAG systems involves a retriever that fetches relevant information from a database and a generator that incorporates this information into the final output, thus, the integration of these components allows the model to produce contextually enriched and factually accurate text [68].The incorporation of RAG into the workflow of LLMs, therefore, presents a sophisticated approach to augmenting the model's knowledge base beyond its pretraining, specifically tailored to the demands of the evolving nature of studies in SLRs.By allowing the model to access an external corpus of domain-specific literature the context within which the LLM operates, it attains the ability to be enriched while a critical countermeasure to the model's propensity for generating plausible yet factually incorrect information -hallucination -is mitigated.In the realm of SLRs, where the precision of synthesized knowledge is paramount, RAG's ability to draws upon relevant information from a targeted corpus helps ensure that the generative outputs of LLMs are anchored in verifiable data.\n\n2.4 Advancing LLMs for Specialized Domains: From Pretraining to Fine-Tuning\n\nIn training LLMs, they initially undergo pretraining on extensive, diverse datasets, acquiring a foundational grasp of both language and knowledge.",
            "score": 0.5604506830388586,
            "section_title": "RAG for Enhanced Factual Accuracy in SLRs",
            "char_start_offset": 18167,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 182,
                    "end": 432
                },
                {
                    "start": 432,
                    "end": 654
                },
                {
                    "start": 654,
                    "end": 819
                },
                {
                    "start": 819,
                    "end": 978
                },
                {
                    "start": 978,
                    "end": 1288
                },
                {
                    "start": 1288,
                    "end": 1529
                },
                {
                    "start": 1529,
                    "end": 1839
                },
                {
                    "start": 1839,
                    "end": 2073
                },
                {
                    "start": 2075,
                    "end": 2150
                },
                {
                    "start": 2152,
                    "end": 2299
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90576171875
        },
        {
            "corpus_id": "250391085",
            "title": "Re2G: Retrieve, Rerank, Generate",
            "text": "The approach of RAG, Multi-DPR, and KGI is to train a neural IR (Information Retrieval) component and further train it end-to-end through its impact in generating the correct output. Figure 2 illustrates the end-to-end RAG system.\n\nIt has been previously established that results from initial retrieval can be greatly improved through the use of a reranker [Liu, 2009, Wang et al., 2011. Therefore we hypothesized that natural language generation systems incorporating retrieval can benefit from reranking.",
            "score": 0.5603529209885112,
            "section_title": "Methodology",
            "char_start_offset": 8079,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 357,
                    "end": 367,
                    "matchedPaperCorpusId": "28826624"
                },
                {
                    "start": 367,
                    "end": 386,
                    "matchedPaperCorpusId": "3357504"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80615234375
        },
        {
            "corpus_id": "273186680",
            "title": "Reward-RAG: Enhancing RAG with Reward Driven Supervision",
            "text": "In this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences. The versatility of our approach allows it to be effectively applied across various domains through domain-specific fine-tuning. We evaluate Reward-RAG on publicly available benchmarks from multiple domains, comparing it to state-of-the-art methods. Our experimental results demonstrate significant improvements in performance, highlighting the effectiveness of Reward-RAG in improving the relevance and quality of generated responses. These findings underscore the potential of integrating reward models with RAG to achieve superior outcomes in natural language generation tasks.",
            "score": 0.5597336665183067,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91455078125
        },
        {
            "corpus_id": "278714952",
            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
            "text": "There are several promising extensions to Finetune-RAG that could further improve its robustness and applicability: \n\n\u2022 Training with more in-context RAG: Real-world retrieval often returns more than two documents, and the context window of LLMs are increasing rapidly. At the time of our work, we focused on relatively low context window of 8k, which would realistically be used for two to three RAG documents using up to 3k context window. With increasing context window, future work can explore training with more RAG chunks to optimize LLMs RAG performance even at high level of stresses caused by more retrieved chunks. To support this, we future-proofed our dataset by including two additional relevant chunks per example to support generating more complex multi-document training scenarios. \u2022 Joint retrieval-generation optimization: While Finetune-RAG focuses on improving the generation component, combining it with learned retrieval mechanisms such as rerankeraware retrievers or contrastively trained retrievers could lead to further improvements in factual accuracy and context filtering. \u2022 Multimodal extensions: Hallucination is not limited to text-based models. Extending Finetune-RAG to multimodal settings, such as image-caption retrieval or code+documentation generation, may help build more robust grounded systems in other domains. \n\n\u2022 Evaluation on downstream tasks: While our benchmarking focuses on controlled hallucination settings, future work should assess Finetune-RAG's impact on end-to-end performance in downstream RAG applications such as open-domain question answering, legal document summarization, and domain-specific information retrieval.",
            "score": 0.5596556641512547,
            "section_title": "Future Work",
            "char_start_offset": 21636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 118,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1351
                },
                {
                    "start": 1354,
                    "end": 1674
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90966796875
        },
        {
            "corpus_id": "271162171",
            "title": "Human-like Episodic Memory for Infinite Context LLMs",
            "text": "In our experiments with Retrieval-Augmented Generation (RAG) baselines, we implemented a standard RAG pipeline consisting of a retriever and a downstream LLM. For each example in a benchmark task, the example context is split into chunks of l words each and encoded using the retriever's embedding model into a vector database. A similarity lookup into the vector database is used to retrieve the top k most relevant chunks, which are then fed to the downstream LLM alongside the query and task description. For all experiments, we set l = 300 and k = 5, following the protocol of Li et al. (2024c). \n\nWe conducted experiments using two retriever models-NV-Embed-v2 (Lee et al., 2024) and allmpnet-base-v2 (Reimers, 2022). NV-Embed-v2 is a SOTA LLM-based retriever that uses that, as of September 2024, ranks first on the Massive Text Embedding Benchmark (MTEB) Leaderboard (Muennighoff et al., 2022). It is a fine-tuned Mistral-7Bv0.1 model with an embedding dimension of 4096, trained using contrastive instruction-tuning on both retrieval and non-retrieval datasets. all-mpnet-base-v2 is a smaller 110M parameter model with an embedding size of 768, built on the BERT-base-uncased architecture, trained using contrastive learning on a dataset of over 1 billion sentence pairs. For each embedding model, we ran experiments using LLaMa-3-8B and LLaMa-3.1-8B as the downstream LLM.",
            "score": 0.5593020824251891,
            "section_title": "A.2 COMPARISON WITH RAG EXPERIMENT DETAILS",
            "char_start_offset": 36605,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 599
                },
                {
                    "start": 602,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1381
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.751953125
        },
        {
            "corpus_id": "271744960",
            "title": "SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic Performance for Mercosur Common Nomenclature",
            "text": "Retrieval Augmented Generation -RAG (Lewis et al., 2020) represents a transformative approach to enriching the quality and pertinence of generated content by integrating external insights derived from extensive datasets or repositories of knowledge. By embedding pertinent external knowledge sources into the generation process, RAG is designed to elevate the coherence, factual precision, and overall utility of generated text. This methodology proves advantageous in domains necessitating precise and contextually nuanced content generation, such as questionanswering, summarizing, and advanced dialogue systems. By controlling retrieval mechanisms, RAG ensures that the resultant outputs are provided with high accuracy and contextual relevance, thereby advancing the frontiers of natural language processing applications. \n\nIn pursuit of enhancing the precision of model responses and mitigating the phenomenon of LLM hallucinations, a novel approach has emerged: Retrieval Augmented Fine-Tuning (RAFT) (Zhang et al., 2024). This methodology integrates the RAG framework with fine-tuning techniques, empowering models not only to acquire domain-specific knowledge but also to adeptly retrieve and comprehend external contexts crucial for task execution. RAFT introduces the idea of chain-of-thought prompting for building the fine-tuning data set. This prompting technique enables the model's answers to show its reason line with a sequence of arguments, enhancing its explicability. \n\nRAG and RAFT were designed to confront the complexity of tailoring LLMs to specialized domains. Within these realms, the emphasis pivots from general knowledge reasoning to optimizing accuracy vis-\u00e0-vis a meticulously defined array of domain-specific documents.",
            "score": 0.558910451435283,
            "section_title": "Retrieval-Augmented Approach",
            "char_start_offset": 11378,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 825
                },
                {
                    "start": 828,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1487
                },
                {
                    "start": 1490,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1751
                }
            ],
            "ref_mentions": [
                {
                    "start": 36,
                    "end": 56,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8173828125
        },
        {
            "corpus_id": "273135916",
            "title": "Conversing with business process-aware large language models: the BPLLM framework",
            "text": "During fine-tuning, the pre-trained LLM's parameters are adjusted accordingly to the task-specific dataset through a process known as backpropagation [34]. This process involves iteratively updating the model's parameters to minimize a defined loss function, thereby optimizing the model's performance on the target task [35]. While fine-tuning offers numerous benefits, it also comes with drawbacks, primarily due to its resource-intensive nature, necessitating substantial computational resources and data to be effective. However, these limitations can be mitigated through specific techniques such as Parameter-Efficient Fine-Tuning (PEFT) [36]. This method employs various deep learning techniques to minimize the number of trainable parameters while retaining comparable performance to full fine-tuning, updating only a limited number of additional parameters or a subset of pre-trained parameters. \n\nAn encouraging solution to improve LLM accuracy and credibility (especially in knowledge-intensive tasks) and avoid the effort required by complex fine-tuning is represented by the RAG [8] which is becoming a popular paradigm in LLM's systems. The underlying idea of the RAG approach is the merging of LLMs' knowledge with specialized, vast, and dynamic data coming from external repositories [8]. The initial query prompts the external retrieval of pertinent information via search algorithms. The obtained information is then sent to the LLM's prompts which provides further context information [32]. According to this, the RAG approach combines information retrieval mechanisms with In-Context Learning (ICL) [37] to improve the LLM's performance. The RAG approach includes a retriever and a generator [8] and consists of three steps (retrieve, augment, and generate). In the retrieval step, the user query x is used to retrieve relevant context (text documents z) from an external knowledge source by the retriever p \u03b7 (z|x) with parameters \u03b7 returning distributions over text documents given x. Using an embedding model, the query is embedded into a vector space and included as the additional context in the vector database. According to the similarities between vectors and query, the k closest documents from the vector database are selected. In the augment step, the initial query and the obtained additional context are combined into a prompt template.",
            "score": 0.5588030256424917,
            "section_title": "Large Language Models and RAG",
            "char_start_offset": 11826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 904
                },
                {
                    "start": 907,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2137
                },
                {
                    "start": 2138,
                    "end": 2257
                },
                {
                    "start": 2258,
                    "end": 2369
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 154,
                    "matchedPaperCorpusId": "256826766"
                },
                {
                    "start": 321,
                    "end": 325,
                    "matchedPaperCorpusId": "258439667"
                },
                {
                    "start": 1092,
                    "end": 1095,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1300,
                    "end": 1303,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1712,
                    "end": 1715,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75634765625
        },
        {
            "corpus_id": "277043707",
            "title": "AttentionRAG: Attention-Guided Context Pruning in Retrieval-Augmented Generation",
            "text": "Retrieve-Augmented Generation (RAG) is a framework that enhances the capabilities of LLMs by integrating external knowledge through retrieval. \n\nA RAG system typically consists of two components: a retriever, which fetches relevant documents, called contexts, from a large corpus based on a query, and a generator, which generates an answer using both the retrieved context and the model's internal knowledge. This combination enables more accurate and contextually relevant outputs, especially for tasks requiring detailed or up-to-date information that might not be present in the model's training data.",
            "score": 0.5585139821399403,
            "section_title": "Retrieve-Augmented Generation",
            "char_start_offset": 5881,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 145,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 605
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91064453125
        },
        {
            "corpus_id": "269761619",
            "title": "From Questions to Insightful Answers: Building an Informed Chatbot for University Resources",
            "text": "Pre-trained Large Language Models (LLMs) are proficient at acquiring extensive knowledge but lack memory expansion or revision capabilities, leading to errors like hallucinations.To address this, hybrid approaches like Retrieval Augmented Generation (RAG) have emerged [14], [17], [18].\n\nRAG integrates input sequences with information retrieved from corpus of an external data source, enriching context for sequence generation.The retriever component selects the top k text passages relevant to the input query, augmenting the model's understanding and enhancing output sequence generation.This process is governed by the equation: p n (z|x) where p n represents the retriever component with parameters n (number of documents or passages a user wants to retrieve), selecting relevant passages z from the knowledge database given input x.",
            "score": 0.5583864541232189,
            "section_title": "B. Retrieval Augmented Generation (RAG)",
            "char_start_offset": 7365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 179,
                    "end": 286
                },
                {
                    "start": 288,
                    "end": 428
                },
                {
                    "start": 428,
                    "end": 591
                },
                {
                    "start": 591,
                    "end": 838
                }
            ],
            "ref_mentions": [
                {
                    "start": 269,
                    "end": 273,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 281,
                    "end": 285,
                    "matchedPaperCorpusId": "246652372"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74951171875
        },
        {
            "corpus_id": "268681001",
            "title": "Towards a RAG-based summarization for the Electron Ion Collider",
            "text": "Fine-tuning an LLM involves refining its abilities and performance in specific tasks or domains by training it further in domain-specific datasets after pretraining, improving effectiveness without retraining the entire model [10,12].However, conventional fine-tuning requires substantial computational power and time, posing challenges, especially for extensive models such as GPT-3 (175-B parameters) [3] and Meta LLaMA2 (13-B parameters) [13].Despite the advent of Low-Rank Adaptation (LoRA) which enables efficient fine-tuning, even on consumer-grade GPUs, fine-tuning remains computationally intensive.Integrating strategies like in-context learning and chain-of-thought techniques [4] with a live knowledge repository forms the concept of Retrieval Augmented Generation (RAG), reducing hallucinations and anchoring the LLMs to reality",
            "score": 0.5580504474328281,
            "section_title": "Fine tuning of Large Language Models (LLMs):",
            "char_start_offset": 1071,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 234,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 607
                },
                {
                    "start": 607,
                    "end": 840
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 230,
                    "matchedPaperCorpusId": "245537907"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7529296875
        },
        {
            "corpus_id": "275922066",
            "title": "Provence: efficient and robust context pruning for retrieval-augmented generation",
            "text": "Retrieval-Augmented Generation (RAG) has become a widely-used paradigm for improving factuality, attribution, and adaptability of Large Language Models (LLMs) (Das et al., 2019;Asai et al., 2024;Seo et al., 2019;Lewis et al., 2020;Mallen et al., 2023a;Min et al., 2023). Augmenting a given user's query with retrieved relevant contexts helps to avoid the generation of untruthful information and enables the provision of references used to generate the answer. Furthermore, using a domain-specific datastore may enable access and reasoning over a previously unknown knowledge -without fine-tuning the LLM. One additional advantage of the RAG approach is the easy plug-and-play architecture (LangChain): practitioners may choose components (retrievers, generator LLMs, context granularity etc.) which best suit their particular cases to maximize the final performance. At the same time, the use of RAG adds computational overhead due to both retrieval latency and the increased input length for the LLMs. It may also propagate irrelevant information present in retrieved contexts into generated responses. These issues can be solved by developing more efficient and robust LLMs -either by making architectural changes to process long contexts more efficiently (Nawrot et al., 2024;Dao, 2024;Chevalier et al., 2023;Louis et al., 2025) or increasing the diversity of the tuning data to improve processing of irrelevant contexts (Lin et al., 2024). However, tuning the LLM can be highly resource-consuming, or even impossible to apply for proprietary (closed) LLMs. An alternative solution consists in pruning retrieved contexts by removing context parts irrelevant to the user's query -which reduces context lengths and therefore speeds up generation. Such context pruning module can be used in a plug-and-play manner with any generator LLM, featuring both easy use and better transparency in the RAG pipeline. \n\nDespite initial efforts on developing context pruners for RAG, none of the existing solutions provide a model ready to be used out-of-the-box in practice.",
            "score": 0.5572968520821592,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1907
                },
                {
                    "start": 1910,
                    "end": 2064
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 177,
                    "matchedPaperCorpusId": "85449634"
                },
                {
                    "start": 231,
                    "end": 252,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 1280,
                    "end": 1290,
                    "matchedPaperCorpusId": "259936734"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.716796875
        },
        {
            "corpus_id": "273532207",
            "title": "Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination",
            "text": "However, without this fine-tuning, its performance dropped significantly, which further reflects the necessity of tailored QA finetuning for optimal results in specialized domains. Subsequently, while the RAG-DPRadapted model performs decently in terms of Top-5 (24.0) and Top-20 (29.2), it falls short in more stringent metrics like EM and F1, suggesting a challenge in generating precise answers despite retrieving relevant information in the domain-adapted settings. \n\nAs anticipated, The RAG-end2end model, which jointly fine-tunes both the retriever and generator with iterative updates to passage encoders, achieved the highest EM (17.36) and F1 (36.04) scores outperforming all models, indicates its ability to generate highly accurate and contextually relevant responses. However, similar to the RAG-DPR-adapted model, its performance significantly declined (1.40 EM and 8.41 F1) when it has been initialized with rag-token-base instead of rag-token-nq before training with our in-domain dataset, emphasizing the importance of appropriate initialization in specialized domains. Interestingly and very surprisingly, the Fusion-in-Decoder model, which has a more flexible architecture than the traditional RAG variants, does not demonstrate a clear advantage in this specific domain. As an additional benchmark, we included the Fusion-in-Decoder model, which integrates the retriever and generator components in a different architectural design. The FID model achieved an EM score of 8.51 and an F1 score of 21.04, falling short of the performance of most of the fine-tuned RAG models except the RAG-Original.",
            "score": 0.5571883969759359,
            "section_title": "Overall Accuracy on Domain Adaptation",
            "char_start_offset": 38184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1615
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7705078125
        }
    ],
    "quotes": {
        "cost": 0.178266,
        "quotes": [
            {
                "idx": 0,
                "key": "[263605962 | Lin et al. | 2023 | Citations: 153]",
                "snippets": "In this work, we show lightweight instruction tuning (Chung et al., 2022b;Iyer et al., 2022;Zhou et al., 2023) alone can significantly boost the performance of RALMs, especially in scenarios that require access to large, external knowledge sources. We propose Retrieval-Augmented Dual Instruction Tuning (RA-DIT), an approach that retrofits any LLM with retrieval capabilities via fine-tuning over a set of tasks selected to cultivate knowledge utilization and contextual awareness in the language model predictions...RA-DIT updates the LLM with retrieval-augmented instruction tuning to make better use of retrieved knowledge and ignore irrelevant or distracting information. It also fine-tunes the retriever with supervision from the LLM to retrieve texts that can better help the LLM generate correct outputs. RA-DIT achieves state-of-the-art performance in zero-and few-shot evaluations on knowledge intensive benchmarks, surpassing un-tuned in-context RALM approaches such as REPLUG and compete effectively against methods that require extensive pre-training such as ATLAS.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 1052,
                        "end": 1567,
                        "sentence_offsets": [
                            {
                                "start": 1052,
                                "end": 1300
                            },
                            {
                                "start": 1301,
                                "end": 1568
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this work, we show lightweight instruction tuning (Chung et al., 2022b;Iyer et al., 2022;Zhou et al., 2023) alone can significantly boost the performance of RALMs, especially in scenarios that require access to large, external knowledge sources. We propose Retrieval-Augmented Dual Instruction Tuning (RA-DIT), an approach that retrofits any LLM with retrieval capabilities via fine-tuning over a set of tasks selected to cultivate knowledge utilization and contextual awareness in the language model predictions"
                    },
                    {
                        "section_title": "CONCLUSION",
                        "pdf_hash": "",
                        "start": 181,
                        "end": 742,
                        "sentence_offsets": [
                            {
                                "start": 181,
                                "end": 339
                            },
                            {
                                "start": 340,
                                "end": 475
                            },
                            {
                                "start": 476,
                                "end": 741
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "RA-DIT updates the LLM with retrieval-augmented instruction tuning to make better use of retrieved knowledge and ignore irrelevant or distracting information. It also fine-tunes the retriever with supervision from the LLM to retrieve texts that can better help the LLM generate correct outputs. RA-DIT achieves state-of-the-art performance in zero-and few-shot evaluations on knowledge intensive benchmarks, surpassing un-tuned in-context RALM approaches such as REPLUG and compete effectively against methods that require extensive pre-training such as ATLAS."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[266359151 | Gao et al. | 2023 | Citations: 1819]",
                "snippets": "For example, this can involve fine-tuning the retriever for better retrieval results, fine-tuning the generator for more personalized outputs, or engaging in collaborative fine-tuning [27]...A typical approach, such as RA-DIT [27], aligns the scoring functions between Retriever and Generator using KL divergence.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C. Modular RAG",
                        "pdf_hash": "",
                        "start": 2009,
                        "end": 2197,
                        "sentence_offsets": [
                            {
                                "start": 2009,
                                "end": 2198
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For example, this can involve fine-tuning the retriever for better retrieval results, fine-tuning the generator for more personalized outputs, or engaging in collaborative fine-tuning [27]"
                    },
                    {
                        "section_title": "B. LLM Fine-tuning",
                        "pdf_hash": "",
                        "start": 1546,
                        "end": 1668,
                        "sentence_offsets": [
                            {
                                "start": 1546,
                                "end": 1668
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "A typical approach, such as RA-DIT [27], aligns the scoring functions between Retriever and Generator using KL divergence."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[268091298 | Zhao et al. | 2024 | Citations: 282]",
                "snippets": "Retriever Finetuning: The retriever, central to the RAG system, relies on a proficient embedding model [139]- [142] to represent related content and feed the generator, enhancing system performance. \n\nAdditionally, embedding models with strong expressive power can be fine-tuned with domain-specific or task-related data to boost performance in targeted areas. REPLUG [86] treats LM as a black box and update the retriever model based on the final results.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. RAG Enhancements",
                        "pdf_hash": "",
                        "start": 1866,
                        "end": 2322,
                        "sentence_offsets": [
                            {
                                "start": 1866,
                                "end": 2064
                            },
                            {
                                "start": 2067,
                                "end": 2226
                            },
                            {
                                "start": 2227,
                                "end": 2322
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Retriever Finetuning: The retriever, central to the RAG system, relies on a proficient embedding model [139]- [142] to represent related content and feed the generator, enhancing system performance. \n\nAdditionally, embedding models with strong expressive power can be fine-tuned with domain-specific or task-related data to boost performance in targeted areas. REPLUG [86] treats LM as a black box and update the retriever model based on the final results."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[269149146 | Susnjak et al. | 2024 | Citations: 26]",
                "snippets": "Retrieval-Augmented Generation (RAG) [68] is a framework that combines the capabilities of large language models (LLMs) with external knowledge sources through a retrieval mechanism.Unlike traditional language models that generate text based solely on the text's internal representation, RAG models retrieve relevant information from a knowledge base (like a database or the internet) and integrate this into the generation process.The core components of RAG include the retrieval mechanism, which fetches relevant documents or data, and the generative model, which synthesizes the retrieved information into coherent and contextually relevant responses.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RAG for Enhanced Factual Accuracy in SLRs",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 654,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 182
                            },
                            {
                                "start": 182,
                                "end": 432
                            },
                            {
                                "start": 432,
                                "end": 654
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Retrieval-Augmented Generation (RAG) [68] is a framework that combines the capabilities of large language models (LLMs) with external knowledge sources through a retrieval mechanism.Unlike traditional language models that generate text based solely on the text's internal representation, RAG models retrieve relevant information from a knowledge base (like a database or the internet) and integrate this into the generation process.The core components of RAG include the retrieval mechanism, which fetches relevant documents or data, and the generative model, which synthesizes the retrieved information into coherent and contextually relevant responses."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[270870251 | Wang et al. | 2024 | Citations: 61]",
                "snippets": "Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context (Luo et al., 2023)[31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258865283 | Luo et al. | 2023 | Citations: 27]": "Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and instruction following abilities on complex search results generated by in-house and external search engines. With an instruction tuning corpus, we collect search results for each training case from different search APIs and domains, and construct a new search-grounded training set containing \\textit{(instruction, grounding information, response)} triplets. We then fine-tune the LLaMA-7B model on the constructed training set. Since the collected results contain unrelated and disputing languages, the model needs to learn to ground on trustworthy search results, filter out distracting passages, and generate the target response. The search result-denoising process entails explicit trustworthy information selection and multi-hop reasoning, since the retrieved passages might be informative but not contain the instruction-following answer. Experiments show that the fine-tuned SAIL-7B model has a strong instruction-following ability, and it performs significantly better on transparency-sensitive tasks, including open-ended question answering and fact checking."
                },
                "metadata": [
                    {
                        "section_title": "Retriever and Generator Fine-tuning",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 551,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 94
                            },
                            {
                                "start": 94,
                                "end": 242
                            },
                            {
                                "start": 242,
                                "end": 345
                            },
                            {
                                "start": 345,
                                "end": 551
                            }
                        ],
                        "ref_mentions": [
                            "258865283"
                        ],
                        "quote": "Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context (Luo et al., 2023)[31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[270870796 | Mai et al. | 2024 | Citations: 5]",
                "snippets": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) combines information retrieval and generative models, enhancing generative model performance by retrieving relevant information from external data sources.The LangChain (Topsakal et al., 2023) library provides various tools allowing large models to access real-time information from sources like Google Search, vector databases, or knowledge graphs, further improving RAG effectiveness.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218869575 | Lewis et al. | 2020 | Citations: 6476]": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                    "[260223847 | Topsakal et al. | 2023 | Citations: 211]": "This study focuses on the utilization of Large Language Models (LLMs) for the rapid development of applications, with a spotlight on LangChain, an open-source software library. LLMs have been rapidly adopted due to their capabilities in a range of tasks, including essay composition, code writing, explanation, and debugging, with OpenAI\u2019s ChatGPT popularizing their usage among millions ofusers. The crux of the study centers around LangChain, designed to expedite the development of bespoke AI applications using LLMs. LangChain has been widely recognized in the AI community for its ability to seamlessly interact with various data sources and applications. The paper provides an examination of LangChain's core features, including its components and chains, acting as modular abstractions and customizable, use-case-specific pipelines, respectively. Through a series of practical examples, the study elucidates the potential of this framework in fostering the swift development of LLM-based applications."
                },
                "metadata": [
                    {
                        "section_title": "G. Fine-Tuning Techniques for Model Architectures",
                        "pdf_hash": "",
                        "start": 525,
                        "end": 934,
                        "sentence_offsets": [
                            {
                                "start": 525,
                                "end": 722
                            },
                            {
                                "start": 722,
                                "end": 934
                            }
                        ],
                        "ref_mentions": [
                            "218869575",
                            "260223847"
                        ],
                        "quote": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) combines information retrieval and generative models, enhancing generative model performance by retrieving relevant information from external data sources.The LangChain (Topsakal et al., 2023) library provides various tools allowing large models to access real-time information from sources like Google Search, vector databases, or knowledge graphs, further improving RAG effectiveness."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[270878612 | Yu et al. | 2024 | Citations: 74]",
                "snippets": "In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 124,
                        "end": 567,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[271270644 | Wu et al. | 2024 | Citations: 39]",
                "snippets": "As introduced in Section 6, RAG training includes two branch of works, RAG with/without datastore update. For RAG without datastore update, the main challenge is how to jointly optimize all parameters in RAG. This may involves new loss functions with multiple objectives, new optimizations for efficient tuning parameters in retriever and generator, or other training strategies. \n\nFor RAG with datastore update, one challenge is how to align the retrieval representations with the generator's representations. Although the time cost of the update operation in datastore cannot be ignored, some works (Chen et al., 2022) reduce the update frequency by asychronously updating, thus achieving the alignment of knowledge representation and model's representation. Another challenge is when to retrain/fine-tune the generator in RAG when new corpus is added. Due to the in-context learning capability of exisitng LLM-based generators and high training overhead, retraining/finetuning the generator or directly inferring the generator becomes a challenging choice for different scenarios. Recently, some efficient training strategies (Dettmers et al., 2023)(Hu et al., 2021) have been proposed to accelerate the fine-tuning process, which can be taken into considerations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[235458009 | Hu et al. | 2021 | Citations: 10511]": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.",
                    "[249191271 | Chen et al. | 2022 | Citations: 54]": "Prompt learning approaches have made waves in natural language processing by inducing better few-shot performance while they still follow a parametric-based learning paradigm; the oblivion and rote memorization problems in learning may encounter unstable generalization issues. Specifically, vanilla prompt learning may struggle to utilize atypical instances by rote during fully-supervised training or overfit shallow patterns with low-shot data. To alleviate such limitations, we develop RetroPrompt with the motivation of decoupling knowledge from memorization to help the model strike a balance between generalization and memorization. In contrast with vanilla prompt learning, RetroPrompt constructs an open-book knowledge-store from training instances and implements a retrieval mechanism during the process of input, training and inference, thus equipping the model with the ability to retrieve related contexts from the training corpus as cues for enhancement. Extensive experiments demonstrate that RetroPrompt can obtain better performance in both few-shot and zero-shot settings. Besides, we further illustrate that our proposed RetroPrompt can yield better generalization abilities with new datasets. Detailed analysis of memorization indeed reveals RetroPrompt can reduce the reliance of language models on memorization; thus, improving generalization for downstream tasks. Code is available in https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt.",
                    "[258841328 | Dettmers et al. | 2023 | Citations: 2606]": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training."
                },
                "metadata": [
                    {
                        "section_title": "RAG Training",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1219,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 105
                            },
                            {
                                "start": 106,
                                "end": 208
                            },
                            {
                                "start": 209,
                                "end": 379
                            },
                            {
                                "start": 382,
                                "end": 510
                            },
                            {
                                "start": 511,
                                "end": 745
                            },
                            {
                                "start": 746,
                                "end": 839
                            },
                            {
                                "start": 840,
                                "end": 1068
                            },
                            {
                                "start": 1069,
                                "end": 1219
                            }
                        ],
                        "ref_mentions": [
                            "249191271",
                            "258841328",
                            "235458009"
                        ],
                        "quote": "As introduced in Section 6, RAG training includes two branch of works, RAG with/without datastore update. For RAG without datastore update, the main challenge is how to jointly optimize all parameters in RAG. This may involves new loss functions with multiple objectives, new optimizations for efficient tuning parameters in retriever and generator, or other training strategies. \n\nFor RAG with datastore update, one challenge is how to align the retrieval representations with the generator's representations. Although the time cost of the update operation in datastore cannot be ignored, some works (Chen et al., 2022) reduce the update frequency by asychronously updating, thus achieving the alignment of knowledge representation and model's representation. Another challenge is when to retrain/fine-tune the generator in RAG when new corpus is added. Due to the in-context learning capability of exisitng LLM-based generators and high training overhead, retraining/finetuning the generator or directly inferring the generator becomes a challenging choice for different scenarios. Recently, some efficient training strategies (Dettmers et al., 2023)(Hu et al., 2021) have been proposed to accelerate the fine-tuning process, which can be taken into considerations."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[271571401 | Gao et al. | 2024 | Citations: 20]",
                "snippets": "Retriever Fine-tuning: In cases where the context may diverge from pre-trained corpus, particularly in highly specialized fields like healthcare, law, and other domains abundant in proprietary terminology. While this adjustment demands additional effort, it can substantially enhance retrieval efficiency and domain alignment.\n\nSupervised Fine-Tuning (SFT). Fine-tuning a retrieval model based on labeled domain data is typically done using contrastive learning. This involves reducing the distance between positive samples while increasing the distance between negative samples....3) Dual FT: In the RAG system, fine-tuning both the retriever and the generator simultaneously is a unique feature of the RAG system. It is important to note that the emphasis of system fine-tuning is on the coordination between the retriever and the generator. An exemplary implementation is RA-DIT [27], which fine-tunes both the LLM and the retriever. The LM-ft component updates the LLM to maximize the likelihood of the correct answer given the retrieval-augmented instructions while the R-ft component updates the retriever to minimize the KL-Divergence between the retriever score distribution and the LLM preference.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C. Retrieval",
                        "pdf_hash": "",
                        "start": 1652,
                        "end": 2233,
                        "sentence_offsets": [
                            {
                                "start": 1602,
                                "end": 1677
                            },
                            {
                                "start": 1678,
                                "end": 1760
                            },
                            {
                                "start": 1761,
                                "end": 1816
                            },
                            {
                                "start": 1817,
                                "end": 1997
                            },
                            {
                                "start": 2000,
                                "end": 2208
                            },
                            {
                                "start": 2209,
                                "end": 2329
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Retriever Fine-tuning: In cases where the context may diverge from pre-trained corpus, particularly in highly specialized fields like healthcare, law, and other domains abundant in proprietary terminology. While this adjustment demands additional effort, it can substantially enhance retrieval efficiency and domain alignment.\n\nSupervised Fine-Tuning (SFT). Fine-tuning a retrieval model based on labeled domain data is typically done using contrastive learning. This involves reducing the distance between positive samples while increasing the distance between negative samples"
                    },
                    {
                        "quote": ".3) Dual FT: In the RAG system, fine-tuning both the retriever and the generator simultaneously is a unique feature of the RAG system. It is important to note that the emphasis of system fine-tuning is on the coordination between the retriever and the generator. An exemplary implementation is RA-DIT [27], which fine-tunes both the LLM and the retriever. The LM-ft component updates the LLM to maximize the likelihood of the correct answer given the retrieval-augmented instructions while the R-ft component updates the retriever to minimize the KL-Divergence between the retriever score distribution and the LLM preference.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[271903170 | Peng et al. | 2024 | Citations: 110]",
                "snippets": "Joint training retrievers and generators simultaneously enhances performance on downstream tasks by leveraging their complementary strengths. Some approaches unify retrievers and generators into a single model, typically LLMs, and train them with both retrieval and generation objectives simultaneously [112]. This method capitalizes on the cohesive capabilities of a unified architecture, enabling the model to seamlessly retrieve relevant information and generate coherent responses within a single framework. \n\nOther methodologies involve initially training retrievers and generators separately, followed by joint training techniques to fine-tune both components. For instance, Subgraph Retriever [196] adopts an alternating training paradigm, where the retriever's parameters are fixed to use the graph data for training the generator. Subsequently, the generator's parameters are fixed, and feedback from the generator is used to guide the retriever's training. This iterative process helps both components refine their performance in a coordinated manner.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Joint training retrievers and generators simultaneously enhances performance on downstream tasks by leveraging their complementary strengths. Some approaches unify retrievers and generators into a single model, typically LLMs, and train them with both retrieval and generation objectives simultaneously [112]. This method capitalizes on the cohesive capabilities of a unified architecture, enabling the model to seamlessly retrieve relevant information and generate coherent responses within a single framework. \n\nOther methodologies involve initially training retrievers and generators separately, followed by joint training techniques to fine-tune both components. For instance, Subgraph Retriever [196] adopts an alternating training paradigm, where the retriever's parameters are fixed to use the graph data for training the generator. Subsequently, the generator's parameters are fixed, and feedback from the generator is used to guide the retriever's training. This iterative process helps both components refine their performance in a coordinated manner.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[272827955 | Zhao et al. | 2024 | Citations: 42]",
                "snippets": "Furthermore, to ensure more consistent performance between the retriever and generator within the RAG system, some studies employ joint training of both retriever and generator during the training phase [90,(Hofst\u00e4tter et al., 2022)[92].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[252568176 | Hofstatter et al. | 2022 | Citations: 82]": "Retrieval-augmented generation models offer many benefits over standalone language models: besides a textual answer to a given query they provide provenance items retrieved from an updateable knowledge base. However, they are also more complex systems and need to handle long inputs. In this work, we introduce FiD-Light to strongly increase the efficiency of the state-of-the-art retrieval-augmented FiD model, while maintaining the same level of effectiveness. Our FiD-Light model constrains the information flow from the encoder (which encodes passages separately) to the decoder (using concatenated encoded representations). Furthermore, we adapt FiD-Light with re-ranking capabilities through textual source pointers, to improve the top-ranked provenance precision. Our experiments on a diverse set of seven knowledge intensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier between query latency and effectiveness. FiD-Light with source pointing sets substantial new state-of-the-art results on six KILT tasks for combined text generation and provenance retrieval evaluation, while maintaining high efficiency."
                },
                "metadata": [
                    {
                        "section_title": "Response Generation Enhancement",
                        "pdf_hash": "",
                        "start": 1110,
                        "end": 1324,
                        "sentence_offsets": [
                            {
                                "start": 1110,
                                "end": 1324
                            }
                        ],
                        "ref_mentions": [
                            "252568176"
                        ],
                        "quote": "Furthermore, to ensure more consistent performance between the retriever and generator within the RAG system, some studies employ joint training of both retriever and generator during the training phase [90,(Hofst\u00e4tter et al., 2022)[92]."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[272911196 | Chung et al. | 2024 | Citations: 0]",
                "snippets": "Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024)...Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1357,
                        "end": 1494,
                        "sentence_offsets": [
                            {
                                "start": 1357,
                                "end": 1495
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024)"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1776,
                        "end": 2236,
                        "sentence_offsets": [
                            {
                                "start": 1756,
                                "end": 1932
                            },
                            {
                                "start": 1933,
                                "end": 2090
                            },
                            {
                                "start": 2091,
                                "end": 2235
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[273185619 | Sriram et al. | 2024 | Citations: 1]",
                "snippets": "Retrieval-augmented generation (RAG) relies on two key modules: a retriever and a reader/generation model. For many RAG systems, noisy retrieval hurts downstream performance by providing irrelevant or misleading documents (Yoran et al., 2024). (Sauchuk et al., 2022) found that adding distractors can cause a 27% drop on veracity classification accuracy on FEVER. Therefore, it's important for retrievers to find relevant documents and simultaneously avoid damaging ones. Shi et al. (2023) attempts to solve this problem by finetuning the retrieval component while fixing the reader LM, similar to our work. Other approaches like Ke et al. (2024) create a more complex system with a \"bridging\" model between the retriever and reader. Nevertheless, noisy retrieval remains a failure point in RAG systems (Barnett et al., 2024), and tangible downstream gains can be realized by further finetuning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[250340232 | Sauchuk et al. | 2022 | Citations: 16]": "Many recent Natural Language Processing (NLP) task formulations, such as question answering and fact verification, are implemented as a two-stage cascading architecture. In the first stage an IR system retrieves \"relevant'' documents containing the knowledge, and in the second stage an NLP system performs reasoning to solve the task. Optimizing the IR system for retrieving relevant documents ensures that the NLP system has sufficient information to operate over. These recent NLP task formulations raise interesting and exciting challenges for IR, where the end-user of an IR system is not a human with an information need, but another system exploiting the documents retrieved by the IR system to perform reasoning and address the user information need. Among these challenges, as we will show, is that noise from the IR system, such as retrieving spurious or irrelevant documents, can negatively impact the accuracy of the downstream reasoning module. Hence, there is the need to balance maximizing relevance while minimizing noise in the IR system. This paper presents experimental results on two NLP tasks implemented as a two-stage cascading architecture. We show how spurious or irrelevant retrieved results from the first stage can induce errors in the second stage. We use these results to ground our discussion of the research challenges that the IR community should address in the context of these knowledge-intensive NLP tasks.",
                    "[266933076 | Barnett et al. | 2024 | Citations: 92]": "Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community.CCS CONCEPTS\u2022 Software and its engineering \u2192 Empirical software validation."
                },
                "metadata": [
                    {
                        "section_title": "Retrieval Augmented Generation Systems",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 894,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 106
                            },
                            {
                                "start": 107,
                                "end": 243
                            },
                            {
                                "start": 244,
                                "end": 362
                            },
                            {
                                "start": 363,
                                "end": 470
                            },
                            {
                                "start": 471,
                                "end": 606
                            },
                            {
                                "start": 607,
                                "end": 732
                            },
                            {
                                "start": 733,
                                "end": 894
                            }
                        ],
                        "ref_mentions": [
                            "250340232",
                            "266933076"
                        ],
                        "quote": "Retrieval-augmented generation (RAG) relies on two key modules: a retriever and a reader/generation model. For many RAG systems, noisy retrieval hurts downstream performance by providing irrelevant or misleading documents (Yoran et al., 2024). (Sauchuk et al., 2022) found that adding distractors can cause a 27% drop on veracity classification accuracy on FEVER. Therefore, it's important for retrievers to find relevant documents and simultaneously avoid damaging ones. Shi et al. (2023) attempts to solve this problem by finetuning the retrieval component while fixing the reader LM, similar to our work. Other approaches like Ke et al. (2024) create a more complex system with a \"bridging\" model between the retriever and reader. Nevertheless, noisy retrieval remains a failure point in RAG systems (Barnett et al., 2024), and tangible downstream gains can be realized by further finetuning."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[273695367 | Chen et al. | 2024 | Citations: 3]",
                "snippets": "For rule-guided retriever fine-tuning (RGFT-retriever), we update the LM encoders in a contrastive learning objective (Chen et al., 2020) and train over supervised fine-tuning data F R provided in our constructed benchmarks, where inputs are the queries plus rules and supervised labels are heuristic oracle documents. Compared with retrievers employed with simple retrieval principles, our fine-tuned retrievers can recall more relevant results, aligned with the preferences of the rules. For rule-guided generator fine-tuning (RGFT-generator), we adopt the supervised instruction-tuning objective (Iyer et al., 2023;Chung et al., 2024) while combining each query q with two components: retrieved documents D q from the retrieval phase and the same set of rules R q consistent with the retrieval phase. The rules introduced in the RGFT-generator train LLMs on how to optimally attribute from the retrieved context into answers by following rules, making RuleRAG leverage the fine-tuned retrievers more rationally. Experiments show our proposed RGFT can further guarantee and boost the retrieval quality and answering accuracy of RuleRAG-FT than RuleRAG-ICL.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[211096730 | Chen et al. | 2020 | Citations: 18878]": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels."
                },
                "metadata": [
                    {
                        "section_title": "RULERAG-FT",
                        "pdf_hash": "",
                        "start": 146,
                        "end": 1304,
                        "sentence_offsets": [
                            {
                                "start": 146,
                                "end": 464
                            },
                            {
                                "start": 465,
                                "end": 635
                            },
                            {
                                "start": 636,
                                "end": 949
                            },
                            {
                                "start": 950,
                                "end": 1160
                            },
                            {
                                "start": 1161,
                                "end": 1304
                            }
                        ],
                        "ref_mentions": [
                            "211096730"
                        ],
                        "quote": "For rule-guided retriever fine-tuning (RGFT-retriever), we update the LM encoders in a contrastive learning objective (Chen et al., 2020) and train over supervised fine-tuning data F R provided in our constructed benchmarks, where inputs are the queries plus rules and supervised labels are heuristic oracle documents. Compared with retrievers employed with simple retrieval principles, our fine-tuned retrievers can recall more relevant results, aligned with the preferences of the rules. For rule-guided generator fine-tuning (RGFT-generator), we adopt the supervised instruction-tuning objective (Iyer et al., 2023;Chung et al., 2024) while combining each query q with two components: retrieved documents D q from the retrieval phase and the same set of rules R q consistent with the retrieval phase. The rules introduced in the RGFT-generator train LLMs on how to optimally attribute from the retrieved context into answers by following rules, making RuleRAG leverage the fine-tuned retrievers more rationally. Experiments show our proposed RGFT can further guarantee and boost the retrieval quality and answering accuracy of RuleRAG-FT than RuleRAG-ICL."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[273969615 | Zhang et al. | 2024 | Citations: 3]",
                "snippets": "RA-DIT (Lin et al., 2023) uses dual instruction tuning to fine-tune the retriever and generative model, optimizing their collaboration for knowledgeintensive benchmarks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[263605962 | Lin et al. | 2023 | Citations: 153]": "Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average."
                },
                "metadata": [
                    {
                        "section_title": "C. Retrieval-Augmented Generation",
                        "pdf_hash": "",
                        "start": 391,
                        "end": 547,
                        "sentence_offsets": [
                            {
                                "start": 380,
                                "end": 547
                            }
                        ],
                        "ref_mentions": [
                            "263605962"
                        ],
                        "quote": "RA-DIT (Lin et al., 2023) uses dual instruction tuning to fine-tune the retriever and generative model, optimizing their collaboration for knowledgeintensive benchmarks."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[276287820 | Bhushan et al. | 2025 | Citations: 2]",
                "snippets": "Recently, Zhang et al. introduced Retrieval-Augmented Fine-Tuning (RAFT), a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1680,
                        "end": 1980,
                        "sentence_offsets": [
                            {
                                "start": 1680,
                                "end": 1854
                            },
                            {
                                "start": 1855,
                                "end": 1980
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recently, Zhang et al. introduced Retrieval-Augmented Fine-Tuning (RAFT), a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[276408682 | Zhang et al. | 2025 | Citations: 1]",
                "snippets": "The key insight of AG-RAG is to simultaneously optimize the retriever and the generator as a whole pipeline with a joint training strategy, enabling them to learn from each other. Particularly, AG-RAG builds a dense retriever to search for relevant test-assert pairs (TAPs) with semantic matching and a retrieval-augmented generator to synthesize accurate assertions with the focal-test and retrieved TAPs as input...AG-RAG designs a joint training strategy that allows the retriever to learn from the feedback provided by the generator. This unified design fully adapts both components specifically for retrieving more useful TAPs, thereby generating accurate assertions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The key insight of AG-RAG is to simultaneously optimize the retriever and the generator as a whole pipeline with a joint training strategy, enabling them to learn from each other. Particularly, AG-RAG builds a dense retriever to search for relevant test-assert pairs (TAPs) with semantic matching and a retrieval-augmented generator to synthesize accurate assertions with the focal-test and retrieved TAPs as input",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "quote": "AG-RAG designs a joint training strategy that allows the retriever to learn from the feedback provided by the generator. This unified design fully adapts both components specifically for retrieving more useful TAPs, thereby generating accurate assertions.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[276580741 | Wu et al. | 2025 | Citations: 2]",
                "snippets": "To further improve the performance of modular RAG systems, these models focus on fine-tuning various components of the RAG framework. Some efforts aim to align the information needs between the retriever and the generator by optimizing the retrievers based on feedback from the generation models (Yu et al., 2023)(Shi et al., 2023)(Izacard et al., 2020). Lin et al. (2024) adapt LLMs within the RAG setting by constructing instructiontuning data for Supervised Fine-Tuning (SFT), enabling the models to better leverage the retrieved documents. Additionally, Li et al. (2024) use Direct Preference Optimization (DPO) (Rafailov et al., 2023) to jointly optimize the modules in a RAG system, aligning their data preferences.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[227746078 | Izacard et al. | 2020 | Citations: 267]": "The task of information retrieval is an important component of many natural language processing systems, such as open domain question answering. While traditional methods were based on hand-crafted features, continuous representations based on neural networks recently obtained competitive results. A challenge of using such methods is to obtain supervised data to train the retriever model, corresponding to pairs of query and support documents. In this paper, we propose a technique to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents. Our approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever. We evaluate our method on question answering, obtaining state-of-the-art results.",
                    "[256389797 | Shi et al. | 2023 | Citations: 641]": "We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%. Code is publicly released at github.com/swj0419/REPLUG.",
                    "[258959321 | Rafailov et al. | 2023 | Citations: 4159]": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.",
                    "[258960666 | Yu et al. | 2023 | Citations: 69]": "Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM\u2019s preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 683,
                        "end": 1405,
                        "sentence_offsets": [
                            {
                                "start": 683,
                                "end": 816
                            },
                            {
                                "start": 817,
                                "end": 1038
                            },
                            {
                                "start": 1039,
                                "end": 1227
                            },
                            {
                                "start": 1228,
                                "end": 1405
                            }
                        ],
                        "ref_mentions": [
                            "258960666",
                            "256389797",
                            "227746078",
                            "258959321"
                        ],
                        "quote": "To further improve the performance of modular RAG systems, these models focus on fine-tuning various components of the RAG framework. Some efforts aim to align the information needs between the retriever and the generator by optimizing the retrievers based on feedback from the generation models (Yu et al., 2023)(Shi et al., 2023)(Izacard et al., 2020). Lin et al. (2024) adapt LLMs within the RAG setting by constructing instructiontuning data for Supervised Fine-Tuning (SFT), enabling the models to better leverage the retrieved documents. Additionally, Li et al. (2024) use Direct Preference Optimization (DPO) (Rafailov et al., 2023) to jointly optimize the modules in a RAG system, aligning their data preferences."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[277043297 | Cheng et al. | 2025 | Citations: 6]",
                "snippets": "Generator-Guided Retriever Training. Conversely, generator-guided retriever training focuses on optimizing the retriever based on the generator's performance and requirements. In this paradigm, the generator's ability to produce coherent and accurate text influences the retriever's selection process. DKRR [95] leverages the generator's attention scores to fine-tune the retriever, enhancing its capability to select the most pertinent information. AAR [272] employs smaller language models to generate supervision signals that guide the retriever's training, ensuring that the retrieved documents are optimally aligned with the generator's needs. RA-DIT [140] fine-tunes large language models before training the retriever, fostering better alignment and synergy between the two components. Additionally, UPRISE [33] uses a frozen LLM to guide the fine-tuning of a prompt retriever, thereby improving its effectiveness in retrieving data that the generator can utilize more effectively.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RAG Training",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 988,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 36
                            },
                            {
                                "start": 37,
                                "end": 175
                            },
                            {
                                "start": 176,
                                "end": 301
                            },
                            {
                                "start": 302,
                                "end": 449
                            },
                            {
                                "start": 450,
                                "end": 648
                            },
                            {
                                "start": 649,
                                "end": 792
                            },
                            {
                                "start": 793,
                                "end": 988
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Generator-Guided Retriever Training. Conversely, generator-guided retriever training focuses on optimizing the retriever based on the generator's performance and requirements. In this paradigm, the generator's ability to produce coherent and accurate text influences the retriever's selection process. DKRR [95] leverages the generator's attention scores to fine-tune the retriever, enhancing its capability to select the most pertinent information. AAR [272] employs smaller language models to generate supervision signals that guide the retriever's training, ensuring that the retrieved documents are optimally aligned with the generator's needs. RA-DIT [140] fine-tunes large language models before training the retriever, fostering better alignment and synergy between the two components. Additionally, UPRISE [33] uses a frozen LLM to guide the fine-tuning of a prompt retriever, thereby improving its effectiveness in retrieving data that the generator can utilize more effectively."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[278339057 | Shi et al. | 2025 | Citations: 1]",
                "snippets": "To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45].\n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25,51]...Although existing work proposes the end-to-end training paradigm, they overly simplify a marginalization optimization through independent top-k approximation (Sachan et al., 2021)(Zamani et al., 2024), where they simply feed top-k documents into downstream LLMs one-by-one and re-score their relevance to optimize the retriever (Lewis et al., 2020)[27]. This has been criticized far from the practical scenarios as the RAG system typically consumes multiple documents (Zamani et al., 2024), while exhaustively enumerating all possible document permutations is cost-intensive and typically infeasible in practice.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218869575 | Lewis et al. | 2020 | Citations: 6476]": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                    "[230437591 | Sachan et al. | 2021 | Citations: 103]": "Recent work on training neural retrievers for open-domain question answering (OpenQA) has employed both supervised and unsupervised approaches. However, it remains unclear how unsupervised and supervised methods can be used most effectively for neural retrievers. In this work, we systematically study retriever pre-training. We first propose an approach of unsupervised pre-training with the Inverse Cloze Task and masked salient spans, followed by supervised finetuning using question-context pairs. This approach leads to absolute gains of 2+ points over the previous best result in the top-20 retrieval accuracy on Natural Questions and TriviaQA datasets. We next explore two approaches for end-to-end training of the reader and retriever components in OpenQA models, which differ in the manner the reader ingests the retrieved documents. Our experiments demonstrate the effectiveness of these approaches as we obtain state-of-the-art results. On the Natural Questions dataset, we obtain a top-20 retrieval accuracy of 84%, an improvement of 5 points over the recent DPR model. We also achieve good results on answer extraction, outperforming recent models like REALM and RAG by 3+ points.",
                    "[252568176 | Hofstatter et al. | 2022 | Citations: 82]": "Retrieval-augmented generation models offer many benefits over standalone language models: besides a textual answer to a given query they provide provenance items retrieved from an updateable knowledge base. However, they are also more complex systems and need to handle long inputs. In this work, we introduce FiD-Light to strongly increase the efficiency of the state-of-the-art retrieval-augmented FiD model, while maintaining the same level of effectiveness. Our FiD-Light model constrains the information flow from the encoder (which encodes passages separately) to the decoder (using concatenated encoded representations). Furthermore, we adapt FiD-Light with re-ranking capabilities through textual source pointers, to improve the top-ranked provenance precision. Our experiments on a diverse set of seven knowledge intensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier between query latency and effectiveness. FiD-Light with source pointing sets substantial new state-of-the-art results on six KILT tasks for combined text generation and provenance retrieval evaluation, while maintaining high efficiency.",
                    "[269605438 | Zamani et al. | 2024 | Citations: 29]": "This paper introduces Stochastic RAG--a novel approach for end-to-end optimization of retrieval-augmented generation (RAG) models that relaxes the simplifying assumptions of marginalization and document independence, made in most prior work. Stochastic RAG casts the retrieval process in RAG as a stochastic sampling without replacement process. Through this formulation, we employ straight-through Gumbel-top-k that provides a differentiable approximation for sampling without replacement and enables effective end-to-end optimization for RAG. We conduct extensive experiments on seven diverse datasets on a wide range of tasks, from open-domain question answering to fact verification to slot-filling for relation extraction and to dialogue systems. By applying this optimization method to a recent and effective RAG model, we advance state-of-the-art results on six out of seven datasets.",
                    "[270199429 | Fang et al. | 2024 | Citations: 39]": "Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT."
                },
                "metadata": [
                    {
                        "section_title": "Generative selector",
                        "pdf_hash": "",
                        "start": 1205,
                        "end": 1855,
                        "sentence_offsets": [
                            {
                                "start": 1171,
                                "end": 1299
                            },
                            {
                                "start": 1300,
                                "end": 1393
                            },
                            {
                                "start": 1394,
                                "end": 1476
                            },
                            {
                                "start": 1479,
                                "end": 1505
                            },
                            {
                                "start": 1506,
                                "end": 1656
                            },
                            {
                                "start": 1657,
                                "end": 1853
                            },
                            {
                                "start": 1854,
                                "end": 2044
                            }
                        ],
                        "ref_mentions": [
                            "252568176",
                            "269605438",
                            "10986612",
                            "270199429"
                        ],
                        "quote": "To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45].\n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25,51]"
                    },
                    {
                        "section_title": "Retrieval-augmented generation",
                        "pdf_hash": "",
                        "start": 945,
                        "end": 1489,
                        "sentence_offsets": [
                            {
                                "start": 945,
                                "end": 1246
                            },
                            {
                                "start": 1247,
                                "end": 1488
                            }
                        ],
                        "ref_mentions": [
                            "230437591",
                            "269605438",
                            "218869575",
                            "269605438"
                        ],
                        "quote": "Although existing work proposes the end-to-end training paradigm, they overly simplify a marginalization optimization through independent top-k approximation (Sachan et al., 2021)(Zamani et al., 2024), where they simply feed top-k documents into downstream LLMs one-by-one and re-score their relevance to optimize the retriever (Lewis et al., 2020)[27]. This has been criticized far from the practical scenarios as the RAG system typically consumes multiple documents (Zamani et al., 2024), while exhaustively enumerating all possible document permutations is cost-intensive and typically infeasible in practice."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[278635834 | Wang et al. | 2025 | Citations: 0]",
                "snippets": "RA-DIT (Lin et al., 2023) uses modular training to optimize the retriever and LLM separately, enhancing overall RAG system performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[263605962 | Lin et al. | 2023 | Citations: 153]": "Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average."
                },
                "metadata": [
                    {
                        "section_title": "Retrieval-augmented Generation",
                        "pdf_hash": "",
                        "start": 1309,
                        "end": 1444,
                        "sentence_offsets": [
                            {
                                "start": 1265,
                                "end": 1444
                            }
                        ],
                        "ref_mentions": [
                            "263605962"
                        ],
                        "quote": "RA-DIT (Lin et al., 2023) uses modular training to optimize the retriever and LLM separately, enhancing overall RAG system performance."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[278714952 | Lee et al. | 2025 | Citations: 0]",
                "snippets": "\u2022 Joint retrieval-generation optimization: While Finetune-RAG focuses on improving the generation component, combining it with learned retrieval mechanisms such as rerankeraware retrievers or contrastively trained retrievers could lead to further improvements in factual accuracy and context filtering.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Future Work",
                        "pdf_hash": "",
                        "start": 798,
                        "end": 1100,
                        "sentence_offsets": [
                            {
                                "start": 798,
                                "end": 1100
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "\u2022 Joint retrieval-generation optimization: While Finetune-RAG focuses on improving the generation component, combining it with learned retrieval mechanisms such as rerankeraware retrievers or contrastively trained retrievers could lead to further improvements in factual accuracy and context filtering."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.023457,
        "cot": "I'll break down the user query into dimensions that will help structure a comprehensive answer about fine-tuning both retriever and generator components in RAG systems, with a particular focus on comparing dual fine-tuning approaches.\n\n1. \"Introduction\" - This will provide background on RAG systems and why fine-tuning components is important. This should be a synthesis to establish a foundation for understanding the more detailed sections.\n\n2. \"Retriever-Only Fine-Tuning Techniques\" - This will cover approaches that focus specifically on fine-tuning the retriever component. A list format makes sense here to clearly identify distinct techniques.\n\n3. \"Generator-Only Fine-Tuning Techniques\" - This will cover approaches that focus specifically on fine-tuning the generator component. A list format is appropriate to identify distinct techniques.\n\n4. \"Dual Fine-Tuning Approaches\" - This will cover approaches that simultaneously fine-tune both the retriever and generator, with special focus on RA-DIT and RankRAG as mentioned in the query. This should be synthesis to explain the complex relationships between components.\n\n5. \"Performance Comparison\" - This will compare the performance of single-component tuning versus dual fine-tuning approaches. This should be synthesis to evaluate and analyze comparative performance.\n\n6. \"Challenges and Future Directions\" - This will cover current limitations and potential future improvements in RAG fine-tuning. This should be synthesis to provide a forward-looking perspective.\n\nLooking at the quotes, I've mapped them to these dimensions based on their content.",
        "plan": {
            "Introduction to RAG Systems (synthesis)": [
                3,
                5
            ],
            "Retriever-Only Fine-Tuning Techniques (list)": [
                2,
                8,
                12,
                13
            ],
            "Generator-Only Fine-Tuning Techniques (list)": [
                4,
                11,
                15
            ],
            "Dual Fine-Tuning Approaches (synthesis)": [
                0,
                1,
                4,
                6,
                7,
                8,
                9,
                10,
                13,
                14,
                16,
                17,
                18,
                19,
                20
            ],
            "Performance Comparison (synthesis)": [
                0,
                12,
                19
            ],
            "Challenges and Future Directions (synthesis)": [
                7,
                21
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to RAG Systems",
                "tldr": "Retrieval-Augmented Generation (RAG) combines large language models with external knowledge sources through retrieval mechanisms to enhance text generation. This framework consists of two core components: a retriever that fetches relevant information and a generator that synthesizes coherent responses using the retrieved content. (4 sources)",
                "text": "\nRetrieval-Augmented Generation (RAG) represents a significant advancement in natural language processing by combining the capabilities of large language models (LLMs) with external knowledge sources through a retrieval mechanism. Unlike traditional language models that generate text based solely on their internal parametric knowledge, RAG models retrieve relevant information from knowledge bases and integrate this external data into the generation process <Paper corpusId=\"269149146\" paperTitle=\"(Susnjak et al., 2024)\" isShortName></Paper>. This approach was formally introduced by Lewis et al. in 2020, who demonstrated that RAG models could overcome the limitations of purely parametric models by providing them with explicit access to non-parametric memory <Paper corpusId=\"270870796\" paperTitle=\"(Mai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>.\n\nThe architecture of RAG systems consists of two primary components. First, the retrieval mechanism is responsible for fetching relevant documents or data from external sources. Second, the generative model synthesizes this retrieved information into coherent and contextually appropriate responses <Paper corpusId=\"269149146\" paperTitle=\"(Susnjak et al., 2024)\" isShortName></Paper>. The integration of these components allows RAG systems to generate more specific, diverse, and factual language than state-of-the-art parametric-only models <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>.\n\nModern implementations of RAG have been facilitated by frameworks like LangChain, which provides various tools allowing large models to access real-time information from diverse sources such as Google Search, vector databases, or knowledge graphs <Paper corpusId=\"270870796\" paperTitle=\"(Mai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"260223847\" paperTitle=\"(Topsakal et al., 2023)\" isShortName></Paper>. This library has gained significant recognition in the AI community for its ability to streamline the development of LLM-based applications that can seamlessly interact with various data sources <Paper corpusId=\"260223847\" paperTitle=\"(Topsakal et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Susnjak et al., 2024)",
                        "snippets": [
                            "Retrieval-Augmented Generation (RAG) [68] is a framework that combines the capabilities of large language models (LLMs) with external knowledge sources through a retrieval mechanism.Unlike traditional language models that generate text based solely on the text's internal representation, RAG models retrieve relevant information from a knowledge base (like a database or the internet) and integrate this into the generation process.The core components of RAG include the retrieval mechanism, which fetches relevant documents or data, and the generative model, which synthesizes the retrieved information into coherent and contextually relevant responses."
                        ],
                        "paper": {
                            "corpus_id": 269149146,
                            "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
                            "authors": [
                                {
                                    "authorId": "2656889",
                                    "name": "Teo Su\u0161njak"
                                },
                                {
                                    "authorId": "2296719088",
                                    "name": "Peter Hwang"
                                },
                                {
                                    "authorId": "1783269",
                                    "name": "N. Reyes"
                                },
                                {
                                    "authorId": "3312622",
                                    "name": "A. Barczak"
                                },
                                {
                                    "authorId": "11430146",
                                    "name": "Timothy R. McIntosh"
                                },
                                {
                                    "authorId": "143976433",
                                    "name": "Surangika Ranathunga"
                                }
                            ],
                            "year": 2024,
                            "venue": "ACM Transactions on Knowledge Discovery from Data",
                            "n_citations": 26
                        },
                        "score": 0.90576171875
                    },
                    {
                        "id": "(Mai et al., 2024)",
                        "snippets": [
                            "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) combines information retrieval and generative models, enhancing generative model performance by retrieving relevant information from external data sources.The LangChain (Topsakal et al., 2023) library provides various tools allowing large models to access real-time information from sources like Google Search, vector databases, or knowledge graphs, further improving RAG effectiveness."
                        ],
                        "paper": {
                            "corpus_id": 270870796,
                            "title": "From Efficient Multimodal Models to World Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2276937443",
                                    "name": "Xinji Mai"
                                },
                                {
                                    "authorId": "2261831274",
                                    "name": "Zeng Tao"
                                },
                                {
                                    "authorId": "2261891655",
                                    "name": "Junxiong Lin"
                                },
                                {
                                    "authorId": "2276807843",
                                    "name": "Haoran Wang"
                                },
                                {
                                    "authorId": "2276969811",
                                    "name": "Yang Chang"
                                },
                                {
                                    "authorId": "2212014366",
                                    "name": "Yanlan Kang"
                                },
                                {
                                    "authorId": "2276879376",
                                    "name": "Yan Wang"
                                },
                                {
                                    "authorId": "2276819302",
                                    "name": "Wenqiang Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.9267578125
                    },
                    {
                        "id": "(Lewis et al., 2020)",
                        "snippets": [
                            "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
                        ],
                        "paper": {
                            "corpus_id": 218869575,
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "authors": [
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3439053",
                                    "name": "Ethan Perez"
                                },
                                {
                                    "authorId": "1716179427",
                                    "name": "Aleksandara Piktus"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "103131985",
                                    "name": "Heinrich Kuttler"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "2620211",
                                    "name": "Tim Rockt\u00e4schel"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "1743722",
                                    "name": "Douwe Kiela"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6476
                        },
                        "score": 0
                    },
                    {
                        "id": "(Topsakal et al., 2023)",
                        "snippets": [
                            "This study focuses on the utilization of Large Language Models (LLMs) for the rapid development of applications, with a spotlight on LangChain, an open-source software library. LLMs have been rapidly adopted due to their capabilities in a range of tasks, including essay composition, code writing, explanation, and debugging, with OpenAI\u2019s ChatGPT popularizing their usage among millions ofusers. The crux of the study centers around LangChain, designed to expedite the development of bespoke AI applications using LLMs. LangChain has been widely recognized in the AI community for its ability to seamlessly interact with various data sources and applications. The paper provides an examination of LangChain's core features, including its components and chains, acting as modular abstractions and customizable, use-case-specific pipelines, respectively. Through a series of practical examples, the study elucidates the potential of this framework in fostering the swift development of LLM-based applications."
                        ],
                        "paper": {
                            "corpus_id": 260223847,
                            "title": "Creating Large Language Model Applications Utilizing LangChain: A Primer on Developing LLM Apps Fast",
                            "authors": [
                                {
                                    "authorId": "2113663584",
                                    "name": "Oguzhan Topsakal"
                                },
                                {
                                    "authorId": "114751633",
                                    "name": "T. Akinci"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Applied Engineering and Natural Sciences",
                            "n_citations": 211
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Retriever-Only Fine-Tuning Techniques",
                "tldr": "Retriever fine-tuning techniques focus on enhancing the embedding models that represent and retrieve relevant content for RAG systems. These approaches include contrastive learning, domain-specific adaptation, and specialized methods like rule-guided fine-tuning to improve retrieval quality and reduce noise. (7 sources)",
                "text": "\n- **Contrastive Learning**: This approach forms the foundation of supervised fine-tuning for retrieval models, especially in domain-specific applications. The technique works by reducing the distance between positive (relevant) samples while increasing the distance between negative (irrelevant) samples in the embedding space. <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper> This method has shown significant improvements in representation quality, with implementations like SimCLR demonstrating up to 7% relative improvement over previous state-of-the-art approaches. <Paper corpusId=\"273695367\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper> <Paper corpusId=\"211096730\" paperTitle=\"(Chen et al., 2020)\" isShortName></Paper>\n\n- **Domain-Specific Adaptation**: Fine-tuning embedding models with domain-specific data has proven particularly valuable in specialized fields like healthcare and law that contain proprietary terminology. While this approach requires additional effort, it substantially enhances retrieval efficiency and domain alignment. <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>\n\n- **REPLUG Approach**: This method treats the language model as a black box and updates only the retriever model based on final results, allowing for targeted improvements without modifying the generator component. <Paper corpusId=\"268091298\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>\n\n- **Noise Reduction Techniques**: Since noisy retrieval can significantly harm downstream performance (with studies showing up to 27% drop in accuracy when distractors are added), specialized fine-tuning approaches focus on helping retrievers find relevant documents while avoiding damaging ones. <Paper corpusId=\"273185619\" paperTitle=\"(Sriram et al., 2024)\" isShortName></Paper> <Paper corpusId=\"250340232\" paperTitle=\"(Sauchuk et al., 2022)\" isShortName></Paper>\n\n- **Rule-Guided Retriever Fine-Tuning (RGFT-retriever)**: This technique updates language model encoders using contrastive learning objectives where inputs combine queries with explicit rules. The approach trains retrievers to align with specified preferences, recalling more relevant results than retrievers using simple retrieval principles. <Paper corpusId=\"273695367\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>\n\n- **Failure-Aware Approaches**: Recognition of retrieval as a critical failure point in RAG systems has led to specialized fine-tuning that addresses the inherent limitations of information retrieval systems. These approaches acknowledge that validation of retrieval quality is only feasible during operation and that robustness evolves rather than being designed in from the start. <Paper corpusId=\"273185619\" paperTitle=\"(Sriram et al., 2024)\" isShortName></Paper> <Paper corpusId=\"266933076\" paperTitle=\"(Barnett et al., 2024)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Gao et al., 2024)",
                        "snippets": [
                            "Retriever Fine-tuning: In cases where the context may diverge from pre-trained corpus, particularly in highly specialized fields like healthcare, law, and other domains abundant in proprietary terminology. While this adjustment demands additional effort, it can substantially enhance retrieval efficiency and domain alignment.\n\nSupervised Fine-Tuning (SFT). Fine-tuning a retrieval model based on labeled domain data is typically done using contrastive learning. This involves reducing the distance between positive samples while increasing the distance between negative samples",
                            ".3) Dual FT: In the RAG system, fine-tuning both the retriever and the generator simultaneously is a unique feature of the RAG system. It is important to note that the emphasis of system fine-tuning is on the coordination between the retriever and the generator. An exemplary implementation is RA-DIT [27], which fine-tunes both the LLM and the retriever. The LM-ft component updates the LLM to maximize the likelihood of the correct answer given the retrieval-augmented instructions while the R-ft component updates the retriever to minimize the KL-Divergence between the retriever score distribution and the LLM preference."
                        ],
                        "paper": {
                            "corpus_id": 271571401,
                            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
                            "authors": [
                                {
                                    "authorId": "2280046531",
                                    "name": "Yunfan Gao"
                                },
                                {
                                    "authorId": "2275320371",
                                    "name": "Yun Xiong"
                                },
                                {
                                    "authorId": "2291409458",
                                    "name": "Meng Wang"
                                },
                                {
                                    "authorId": "2256769434",
                                    "name": "Haofen Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 20
                        },
                        "score": 0.96435546875
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "For rule-guided retriever fine-tuning (RGFT-retriever), we update the LM encoders in a contrastive learning objective (Chen et al., 2020) and train over supervised fine-tuning data F R provided in our constructed benchmarks, where inputs are the queries plus rules and supervised labels are heuristic oracle documents. Compared with retrievers employed with simple retrieval principles, our fine-tuned retrievers can recall more relevant results, aligned with the preferences of the rules. For rule-guided generator fine-tuning (RGFT-generator), we adopt the supervised instruction-tuning objective (Iyer et al., 2023;Chung et al., 2024) while combining each query q with two components: retrieved documents D q from the retrieval phase and the same set of rules R q consistent with the retrieval phase. The rules introduced in the RGFT-generator train LLMs on how to optimally attribute from the retrieved context into answers by following rules, making RuleRAG leverage the fine-tuned retrievers more rationally. Experiments show our proposed RGFT can further guarantee and boost the retrieval quality and answering accuracy of RuleRAG-FT than RuleRAG-ICL."
                        ],
                        "paper": {
                            "corpus_id": 273695367,
                            "title": "RuleRAG: Rule-guided retrieval-augmented generation with language models for question answering",
                            "authors": [
                                {
                                    "authorId": "2165306772",
                                    "name": "Zhongwu Chen"
                                },
                                {
                                    "authorId": "2250617116",
                                    "name": "Chengjin Xu"
                                },
                                {
                                    "authorId": "2329140108",
                                    "name": "Dingmin Wang"
                                },
                                {
                                    "authorId": "2273614102",
                                    "name": "Zhen Huang"
                                },
                                {
                                    "authorId": "67069932",
                                    "name": "Yong Dou"
                                },
                                {
                                    "authorId": "2284217200",
                                    "name": "Jian Guo"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.92236328125
                    },
                    {
                        "id": "(Chen et al., 2020)",
                        "snippets": [
                            "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels."
                        ],
                        "paper": {
                            "corpus_id": 211096730,
                            "title": "A Simple Framework for Contrastive Learning of Visual Representations",
                            "authors": [
                                {
                                    "authorId": "145358498",
                                    "name": "Ting Chen"
                                },
                                {
                                    "authorId": "40464924",
                                    "name": "Simon Kornblith"
                                },
                                {
                                    "authorId": "144739074",
                                    "name": "Mohammad Norouzi"
                                },
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 18878
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "Retriever Finetuning: The retriever, central to the RAG system, relies on a proficient embedding model [139]- [142] to represent related content and feed the generator, enhancing system performance. \n\nAdditionally, embedding models with strong expressive power can be fine-tuned with domain-specific or task-related data to boost performance in targeted areas. REPLUG [86] treats LM as a black box and update the retriever model based on the final results."
                        ],
                        "paper": {
                            "corpus_id": 268091298,
                            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
                            "authors": [
                                {
                                    "authorId": "2268718776",
                                    "name": "Penghao Zhao"
                                },
                                {
                                    "authorId": "2288557803",
                                    "name": "Hailin Zhang"
                                },
                                {
                                    "authorId": "2289597580",
                                    "name": "Qinhan Yu"
                                },
                                {
                                    "authorId": "2288675277",
                                    "name": "Zhengren Wang"
                                },
                                {
                                    "authorId": "2288532368",
                                    "name": "Yunteng Geng"
                                },
                                {
                                    "authorId": "46182701",
                                    "name": "Fangcheng Fu"
                                },
                                {
                                    "authorId": "2249513224",
                                    "name": "Ling Yang"
                                },
                                {
                                    "authorId": "2277807793",
                                    "name": "Wentao Zhang"
                                },
                                {
                                    "authorId": "2277742543",
                                    "name": "Bin Cui"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 282
                        },
                        "score": 0.9091796875
                    },
                    {
                        "id": "(Sriram et al., 2024)",
                        "snippets": [
                            "Retrieval-augmented generation (RAG) relies on two key modules: a retriever and a reader/generation model. For many RAG systems, noisy retrieval hurts downstream performance by providing irrelevant or misleading documents (Yoran et al., 2024). (Sauchuk et al., 2022) found that adding distractors can cause a 27% drop on veracity classification accuracy on FEVER. Therefore, it's important for retrievers to find relevant documents and simultaneously avoid damaging ones. Shi et al. (2023) attempts to solve this problem by finetuning the retrieval component while fixing the reader LM, similar to our work. Other approaches like Ke et al. (2024) create a more complex system with a \"bridging\" model between the retriever and reader. Nevertheless, noisy retrieval remains a failure point in RAG systems (Barnett et al., 2024), and tangible downstream gains can be realized by further finetuning."
                        ],
                        "paper": {
                            "corpus_id": 273185619,
                            "title": "Contrastive Learning to Improve Retrieval for Real-World Fact Checking",
                            "authors": [
                                {
                                    "authorId": "2165382262",
                                    "name": "Aniruddh Sriram"
                                },
                                {
                                    "authorId": "2159829626",
                                    "name": "Fangyuan Xu"
                                },
                                {
                                    "authorId": "2257003422",
                                    "name": "Eunsol Choi"
                                },
                                {
                                    "authorId": "1814094",
                                    "name": "Greg Durrett"
                                }
                            ],
                            "year": 2024,
                            "venue": "FEVER",
                            "n_citations": 1
                        },
                        "score": 0.91357421875
                    },
                    {
                        "id": "(Sauchuk et al., 2022)",
                        "snippets": [
                            "Many recent Natural Language Processing (NLP) task formulations, such as question answering and fact verification, are implemented as a two-stage cascading architecture. In the first stage an IR system retrieves \"relevant'' documents containing the knowledge, and in the second stage an NLP system performs reasoning to solve the task. Optimizing the IR system for retrieving relevant documents ensures that the NLP system has sufficient information to operate over. These recent NLP task formulations raise interesting and exciting challenges for IR, where the end-user of an IR system is not a human with an information need, but another system exploiting the documents retrieved by the IR system to perform reasoning and address the user information need. Among these challenges, as we will show, is that noise from the IR system, such as retrieving spurious or irrelevant documents, can negatively impact the accuracy of the downstream reasoning module. Hence, there is the need to balance maximizing relevance while minimizing noise in the IR system. This paper presents experimental results on two NLP tasks implemented as a two-stage cascading architecture. We show how spurious or irrelevant retrieved results from the first stage can induce errors in the second stage. We use these results to ground our discussion of the research challenges that the IR community should address in the context of these knowledge-intensive NLP tasks."
                        ],
                        "paper": {
                            "corpus_id": 250340232,
                            "title": "On the Role of Relevance in Natural Language Processing Tasks",
                            "authors": [
                                {
                                    "authorId": "2175275805",
                                    "name": "Artsiom Sauchuk"
                                },
                                {
                                    "authorId": "2053211210",
                                    "name": "James Thorne"
                                },
                                {
                                    "authorId": "1770962",
                                    "name": "A. Halevy"
                                },
                                {
                                    "authorId": "2783910",
                                    "name": "N. Tonellotto"
                                },
                                {
                                    "authorId": "144925193",
                                    "name": "F. Silvestri"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 16
                        },
                        "score": 0
                    },
                    {
                        "id": "(Barnett et al., 2024)",
                        "snippets": [
                            "Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community.CCS CONCEPTS\u2022 Software and its engineering \u2192 Empirical software validation."
                        ],
                        "paper": {
                            "corpus_id": 266933076,
                            "title": "Seven Failure Points When Engineering a Retrieval Augmented Generation System",
                            "authors": [
                                {
                                    "authorId": "2052845461",
                                    "name": "Scott Barnett"
                                },
                                {
                                    "authorId": "2266469333",
                                    "name": "Stefanus Kurniawan"
                                },
                                {
                                    "authorId": "2257020336",
                                    "name": "Srikanth Thudumu"
                                },
                                {
                                    "authorId": "2279020735",
                                    "name": "Zach Brannelly"
                                },
                                {
                                    "authorId": "47505933",
                                    "name": "Mohamed Abdelrazek"
                                }
                            ],
                            "year": 2024,
                            "venue": "2024 IEEE/ACM 3rd International Conference on AI Engineering \u2013 Software Engineering for AI (CAIN)",
                            "n_citations": 92
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Generator-Only Fine-Tuning Techniques",
                "tldr": "Generator fine-tuning techniques focus on optimizing language models to better process and incorporate retrieved information into their outputs. These approaches include search-augmented instruction learning, domain-specific adaptation, and retrieval-augmented fine-tuning that improves how generators handle both relevant and irrelevant retrieved content. (4 sources)",
                "text": "\n- **Search-Augmented Instruction Learning (SAIL)**: This approach fine-tunes language models to effectively process and utilize complex search results from various sources. By training on triplets of instruction, grounding information, and response, models learn to filter out distracting passages, select trustworthy information, and perform multi-hop reasoning. SAIL has demonstrated significant improvements in transparency-sensitive tasks like open-ended question answering and fact checking. <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258865283\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>\n\n- **Retrieval-Augmented Fine-Tuning (RAFT)**: This method enhances generator performance by training LLMs on domain data using a mixture of oracle (highly relevant) and distractor document contexts. This approach helps models learn to distinguish between useful and irrelevant information while incorporating domain knowledge, significantly improving in-domain RAG performance. <Paper corpusId=\"276287820\" paperTitle=\"(Bhushan et al., 2025)\" isShortName></Paper>\n\n- **Context Utilization Training**: Some generator fine-tuning techniques specifically focus on improving how LLMs utilize retriever context, ensuring that the generated content remains faithful to the retrieved information and robust against misleading or contradictory passages. These approaches help models develop better reasoning capabilities when presented with multiple information sources. <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>\n\n- **Domain-Specific RAG Adaptation**: Fine-tuning strategies for RAG can involve further training of pretrained LLMs on specific datasets to enhance performance in domain-specific RAG tasks. These methodologies improve how generators process and utilize retrieved information in specialized contexts where general knowledge might be insufficient. <Paper corpusId=\"272911196\" paperTitle=\"(Chung et al., 2024)\" isShortName></Paper>\n\n- **Long Context Handling**: Some fine-tuning approaches specifically target improving how instruction-tuned LLMs handle longer retrieval contexts, enabling them to better process and synthesize information from extensive retrieved passages without losing coherence or relevance. <Paper corpusId=\"272911196\" paperTitle=\"(Chung et al., 2024)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context (Luo et al., 2023)[31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges."
                        ],
                        "paper": {
                            "corpus_id": 270870251,
                            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2273537815",
                                    "name": "Xiaohua Wang"
                                },
                                {
                                    "authorId": "2308276345",
                                    "name": "Zhenghua Wang"
                                },
                                {
                                    "authorId": "2292070745",
                                    "name": "Xuan Gao"
                                },
                                {
                                    "authorId": "2308226671",
                                    "name": "Feiran Zhang"
                                },
                                {
                                    "authorId": "2308043953",
                                    "name": "Yixin Wu"
                                },
                                {
                                    "authorId": "2308044030",
                                    "name": "Zhibo Xu"
                                },
                                {
                                    "authorId": "2308036711",
                                    "name": "Tianyuan Shi"
                                },
                                {
                                    "authorId": "2309182278",
                                    "name": "Zhengyuan Wang"
                                },
                                {
                                    "authorId": "2309656885",
                                    "name": "Shizheng Li"
                                },
                                {
                                    "authorId": "2309176521",
                                    "name": "Qi Qian"
                                },
                                {
                                    "authorId": "2292032843",
                                    "name": "Ruicheng Yin"
                                },
                                {
                                    "authorId": "2220896023",
                                    "name": "Changze Lv"
                                },
                                {
                                    "authorId": "2257315404",
                                    "name": "Xiaoqing Zheng"
                                },
                                {
                                    "authorId": "2257129987",
                                    "name": "Xuanjing Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 61
                        },
                        "score": 0.9775390625
                    },
                    {
                        "id": "(Luo et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and instruction following abilities on complex search results generated by in-house and external search engines. With an instruction tuning corpus, we collect search results for each training case from different search APIs and domains, and construct a new search-grounded training set containing \\textit{(instruction, grounding information, response)} triplets. We then fine-tune the LLaMA-7B model on the constructed training set. Since the collected results contain unrelated and disputing languages, the model needs to learn to ground on trustworthy search results, filter out distracting passages, and generate the target response. The search result-denoising process entails explicit trustworthy information selection and multi-hop reasoning, since the retrieved passages might be informative but not contain the instruction-following answer. Experiments show that the fine-tuned SAIL-7B model has a strong instruction-following ability, and it performs significantly better on transparency-sensitive tasks, including open-ended question answering and fact checking."
                        ],
                        "paper": {
                            "corpus_id": 258865283,
                            "title": "SAIL: Search-Augmented Instruction Learning",
                            "authors": [
                                {
                                    "authorId": "1944274",
                                    "name": "Hongyin Luo"
                                },
                                {
                                    "authorId": "2475831",
                                    "name": "Yung-Sung Chuang"
                                },
                                {
                                    "authorId": "145802952",
                                    "name": "Yuan Gong"
                                },
                                {
                                    "authorId": "2146333115",
                                    "name": "Tianhua Zhang"
                                },
                                {
                                    "authorId": "143827730",
                                    "name": "Yoon Kim"
                                },
                                {
                                    "authorId": "1847260",
                                    "name": "Xixin Wu"
                                },
                                {
                                    "authorId": "31997718",
                                    "name": "D. Fox"
                                },
                                {
                                    "authorId": "145199941",
                                    "name": "H. Meng"
                                },
                                {
                                    "authorId": "145898106",
                                    "name": "James R. Glass"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 27
                        },
                        "score": 0
                    },
                    {
                        "id": "(Bhushan et al., 2025)",
                        "snippets": [
                            "Recently, Zhang et al. introduced Retrieval-Augmented Fine-Tuning (RAFT), a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts."
                        ],
                        "paper": {
                            "corpus_id": 276287820,
                            "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG",
                            "authors": [
                                {
                                    "authorId": "2256382094",
                                    "name": "Kushagra Bhushan"
                                },
                                {
                                    "authorId": "1392630568",
                                    "name": "Yatin Nandwani"
                                },
                                {
                                    "authorId": "2345003462",
                                    "name": "Dinesh Khandelwal"
                                },
                                {
                                    "authorId": "2320314900",
                                    "name": "Sonam Gupta"
                                },
                                {
                                    "authorId": "2345005348",
                                    "name": "Gaurav Pandey"
                                },
                                {
                                    "authorId": "1916865",
                                    "name": "Dinesh Raghu"
                                },
                                {
                                    "authorId": "2243011716",
                                    "name": "Sachindra Joshi"
                                }
                            ],
                            "year": 2025,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 2
                        },
                        "score": 0.94189453125
                    },
                    {
                        "id": "(Chung et al., 2024)",
                        "snippets": [
                            "Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024)",
                            "Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever."
                        ],
                        "paper": {
                            "corpus_id": 272911196,
                            "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
                            "authors": [
                                {
                                    "authorId": "2322992640",
                                    "name": "Isaac Chung"
                                },
                                {
                                    "authorId": "2322982756",
                                    "name": "Phat Vo"
                                },
                                {
                                    "authorId": "2322991957",
                                    "name": "Arman Kizilkale"
                                },
                                {
                                    "authorId": "2322982549",
                                    "name": "Aaron Reite"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9091796875
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Dual Fine-Tuning Approaches",
                "tldr": "Dual fine-tuning approaches simultaneously optimize both retriever and generator components of RAG systems, creating better alignment between the two models. Leading methods like RA-DIT and RankRAG have demonstrated significant performance improvements through coordinated training strategies that help each component better serve the other's needs. (19 sources)",
                "text": "\nDual fine-tuning approaches represent a significant advancement in RAG optimization by recognizing that the retriever and generator components should be optimized together rather than in isolation. This holistic approach aims to create better alignment between the two models, enabling them to function as a cohesive system rather than independent parts <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n## Prominent Dual Fine-Tuning Methods\n\nRetrieval-Augmented Dual Instruction Tuning (RA-DIT) is one of the most influential dual fine-tuning approaches. It operates through two distinct fine-tuning steps: the first updates the language model to better utilize retrieved knowledge while filtering out irrelevant information, and the second fine-tunes the retriever based on feedback from the language model to retrieve more relevant content <Paper corpusId=\"263605962\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper> <Paper corpusId=\"278635834\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>. This lightweight methodology has achieved state-of-the-art performance on knowledge-intensive benchmarks, significantly outperforming previous approaches by up to 8.9% in zero-shot settings <Paper corpusId=\"273969615\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263605962\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>.\n\nRankRAG offers another innovative approach by instruction-tuning a single language model for the dual purpose of both context ranking and answer generation. Remarkably, this method achieves superior performance by incorporating just a small fraction of ranking data into the training process, outperforming models exclusively fine-tuned on large volumes of ranking data <Paper corpusId=\"270878612\" paperTitle=\"(Yu et al., 2024)\" isShortName></Paper>.\n\n## Alignment Mechanisms\n\nA key feature of dual fine-tuning is the alignment of scoring functions between retriever and generator. RA-DIT, for example, uses KL divergence to align the retriever's score distribution with the language model's preferences <Paper corpusId=\"266359151\" paperTitle=\"(Gao et al., 2023)\" isShortName></Paper> <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. This alignment ensures that the retriever learns to identify and prioritize documents that the generator finds most useful.\n\nOther approaches employ alternating training paradigms where one component is fixed while the other is optimized, and then the roles are reversed <Paper corpusId=\"271903170\" paperTitle=\"(Peng et al., 2024)\" isShortName></Paper>. For instance, AG-RAG enables both components to learn from each other through a joint training strategy where the retriever learns from generator feedback <Paper corpusId=\"276408682\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>.\n\n## Implementation Techniques\n\nSeveral technical innovations have made dual fine-tuning more practical:\n\n1. **Efficient Parameter Optimization**: Techniques like LoRA (Low-Rank Adaptation) <Paper corpusId=\"235458009\" paperTitle=\"(Hu et al., 2021)\" isShortName></Paper> and QLoRA <Paper corpusId=\"258841328\" paperTitle=\"(Dettmers et al., 2023)\" isShortName></Paper> significantly reduce the number of trainable parameters, making dual fine-tuning more computationally feasible <Paper corpusId=\"271270644\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>.\n\n2. **Rule-Guided Fine-Tuning**: Some approaches incorporate explicit rules during training, guiding both retrievers and generators to better align with specified preferences. For example, RGFT-retriever updates language model encoders using contrastive learning with queries plus rules, while RGFT-generator teaches language models to optimally attribute information from retrieved context following the same rules <Paper corpusId=\"273695367\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper> <Paper corpusId=\"211096730\" paperTitle=\"(Chen et al., 2020)\" isShortName></Paper>.\n\n3. **Generator-Guided Retriever Training**: Methods like DKRR and AAR leverage the generator's attention scores or smaller language models to guide retriever training, ensuring retrieved documents align with the generator's needs <Paper corpusId=\"277043297\" paperTitle=\"(Cheng et al., 2025)\" isShortName></Paper>.\n\n## Challenges and Limitations\n\nDespite their effectiveness, dual fine-tuning approaches face several challenges. One significant issue is the computational complexity of jointly optimizing both components <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. Some methods attempt to simplify this through independent top-k approximation, where top-k documents are fed into language models one-by-one to re-score their relevance <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>.\n\nHowever, this approximation has been criticized as impractical since RAG systems typically consume multiple documents simultaneously, and exhaustively evaluating all possible document combinations is computationally infeasible <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"269605438\" paperTitle=\"(Zamani et al., 2024)\" isShortName></Paper>. More recent approaches like Stochastic RAG address this by casting retrieval as a stochastic sampling process, enabling more effective end-to-end optimization <Paper corpusId=\"269605438\" paperTitle=\"(Zamani et al., 2024)\" isShortName></Paper>.\n\nAnother challenge is ensuring that dual fine-tuning produces models that remain robust against retrieval noise, as inappropriate retrieved passages can hinder the language model's ability to generate high-quality responses <Paper corpusId=\"270199429\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>. Some approaches explicitly address this through adversarial training that helps models recognize and handle noisy contexts.",
                "citations": [
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context (Luo et al., 2023)[31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges."
                        ],
                        "paper": {
                            "corpus_id": 270870251,
                            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2273537815",
                                    "name": "Xiaohua Wang"
                                },
                                {
                                    "authorId": "2308276345",
                                    "name": "Zhenghua Wang"
                                },
                                {
                                    "authorId": "2292070745",
                                    "name": "Xuan Gao"
                                },
                                {
                                    "authorId": "2308226671",
                                    "name": "Feiran Zhang"
                                },
                                {
                                    "authorId": "2308043953",
                                    "name": "Yixin Wu"
                                },
                                {
                                    "authorId": "2308044030",
                                    "name": "Zhibo Xu"
                                },
                                {
                                    "authorId": "2308036711",
                                    "name": "Tianyuan Shi"
                                },
                                {
                                    "authorId": "2309182278",
                                    "name": "Zhengyuan Wang"
                                },
                                {
                                    "authorId": "2309656885",
                                    "name": "Shizheng Li"
                                },
                                {
                                    "authorId": "2309176521",
                                    "name": "Qi Qian"
                                },
                                {
                                    "authorId": "2292032843",
                                    "name": "Ruicheng Yin"
                                },
                                {
                                    "authorId": "2220896023",
                                    "name": "Changze Lv"
                                },
                                {
                                    "authorId": "2257315404",
                                    "name": "Xiaoqing Zheng"
                                },
                                {
                                    "authorId": "2257129987",
                                    "name": "Xuanjing Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 61
                        },
                        "score": 0.9775390625
                    },
                    {
                        "id": "(Lin et al., 2023)",
                        "snippets": [
                            "In this work, we show lightweight instruction tuning (Chung et al., 2022b;Iyer et al., 2022;Zhou et al., 2023) alone can significantly boost the performance of RALMs, especially in scenarios that require access to large, external knowledge sources. We propose Retrieval-Augmented Dual Instruction Tuning (RA-DIT), an approach that retrofits any LLM with retrieval capabilities via fine-tuning over a set of tasks selected to cultivate knowledge utilization and contextual awareness in the language model predictions",
                            "RA-DIT updates the LLM with retrieval-augmented instruction tuning to make better use of retrieved knowledge and ignore irrelevant or distracting information. It also fine-tunes the retriever with supervision from the LLM to retrieve texts that can better help the LLM generate correct outputs. RA-DIT achieves state-of-the-art performance in zero-and few-shot evaluations on knowledge intensive benchmarks, surpassing un-tuned in-context RALM approaches such as REPLUG and compete effectively against methods that require extensive pre-training such as ATLAS."
                        ],
                        "paper": {
                            "corpus_id": 263605962,
                            "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2255374957",
                                    "name": "Xi Victoria Lin"
                                },
                                {
                                    "authorId": "1769736",
                                    "name": "Xilun Chen"
                                },
                                {
                                    "authorId": "46221498",
                                    "name": "Mingda Chen"
                                },
                                {
                                    "authorId": "2254168373",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "2253400960",
                                    "name": "Maria Lomeli"
                                },
                                {
                                    "authorId": "2191899140",
                                    "name": "Rich James"
                                },
                                {
                                    "authorId": "2253404757",
                                    "name": "Pedro Rodriguez"
                                },
                                {
                                    "authorId": "2253401183",
                                    "name": "Jacob Kahn"
                                },
                                {
                                    "authorId": "2253402270",
                                    "name": "Gergely Szilvasy"
                                },
                                {
                                    "authorId": "2253417398",
                                    "name": "Mike Lewis"
                                },
                                {
                                    "authorId": "2137813791",
                                    "name": "Luke S. Zettlemoyer"
                                },
                                {
                                    "authorId": "2253400757",
                                    "name": "Scott Yih"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 153
                        },
                        "score": 0.96435546875
                    },
                    {
                        "id": "(Wang et al., 2025)",
                        "snippets": [
                            "RA-DIT (Lin et al., 2023) uses modular training to optimize the retriever and LLM separately, enhancing overall RAG system performance."
                        ],
                        "paper": {
                            "corpus_id": 278635834,
                            "title": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "2361597409",
                                    "name": "Shaohan Wang"
                                },
                                {
                                    "authorId": "48378753",
                                    "name": "L. Zhang"
                                },
                                {
                                    "authorId": "2106681735",
                                    "name": "Zheren Fu"
                                },
                                {
                                    "authorId": "2349977855",
                                    "name": "Zhendong Mao"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.9697265625
                    },
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "RA-DIT (Lin et al., 2023) uses dual instruction tuning to fine-tune the retriever and generative model, optimizing their collaboration for knowledgeintensive benchmarks."
                        ],
                        "paper": {
                            "corpus_id": 273969615,
                            "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
                            "authors": [
                                {
                                    "authorId": "2270181751",
                                    "name": "Zilun Zhang"
                                },
                                {
                                    "authorId": "2174678931",
                                    "name": "Haozhan Shen"
                                },
                                {
                                    "authorId": "8200875",
                                    "name": "Tiancheng Zhao"
                                },
                                {
                                    "authorId": "2330774884",
                                    "name": "Yuhao Wang"
                                },
                                {
                                    "authorId": "2330612748",
                                    "name": "Bin Chen"
                                },
                                {
                                    "authorId": "2149196373",
                                    "name": "Yuxiang Cai"
                                },
                                {
                                    "authorId": "2093090552",
                                    "name": "Yongheng Shang"
                                },
                                {
                                    "authorId": "2111612160",
                                    "name": "Jianwei Yin"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.9609375
                    },
                    {
                        "id": "(Yu et al., 2024)",
                        "snippets": [
                            "In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data."
                        ],
                        "paper": {
                            "corpus_id": 270878612,
                            "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs",
                            "authors": [
                                {
                                    "authorId": "2259265562",
                                    "name": "Yue Yu"
                                },
                                {
                                    "authorId": "2253664013",
                                    "name": "Wei Ping"
                                },
                                {
                                    "authorId": "2256582287",
                                    "name": "Zihan Liu"
                                },
                                {
                                    "authorId": "2256656241",
                                    "name": "Boxin Wang"
                                },
                                {
                                    "authorId": "2287859963",
                                    "name": "Jiaxuan You"
                                },
                                {
                                    "authorId": "2256776233",
                                    "name": "Chao Zhang"
                                },
                                {
                                    "authorId": "1911755",
                                    "name": "M. Shoeybi"
                                },
                                {
                                    "authorId": "2264406909",
                                    "name": "Bryan Catanzaro"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 74
                        },
                        "score": 0.9140625
                    },
                    {
                        "id": "(Gao et al., 2023)",
                        "snippets": [
                            "For example, this can involve fine-tuning the retriever for better retrieval results, fine-tuning the generator for more personalized outputs, or engaging in collaborative fine-tuning [27]",
                            "A typical approach, such as RA-DIT [27], aligns the scoring functions between Retriever and Generator using KL divergence."
                        ],
                        "paper": {
                            "corpus_id": 266359151,
                            "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2280046531",
                                    "name": "Yunfan Gao"
                                },
                                {
                                    "authorId": "2275320371",
                                    "name": "Yun Xiong"
                                },
                                {
                                    "authorId": "2275341478",
                                    "name": "Xinyu Gao"
                                },
                                {
                                    "authorId": "2275191447",
                                    "name": "Kangxiang Jia"
                                },
                                {
                                    "authorId": "2275530552",
                                    "name": "Jinliu Pan"
                                },
                                {
                                    "authorId": "2275171009",
                                    "name": "Yuxi Bi"
                                },
                                {
                                    "authorId": "2276187454",
                                    "name": "Yi Dai"
                                },
                                {
                                    "authorId": "2275540959",
                                    "name": "Jiawei Sun"
                                },
                                {
                                    "authorId": "2258800561",
                                    "name": "Qianyu Guo"
                                },
                                {
                                    "authorId": "2291409458",
                                    "name": "Meng Wang"
                                },
                                {
                                    "authorId": "2256769434",
                                    "name": "Haofen Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1819
                        },
                        "score": 0.9228515625
                    },
                    {
                        "id": "(Gao et al., 2024)",
                        "snippets": [
                            "Retriever Fine-tuning: In cases where the context may diverge from pre-trained corpus, particularly in highly specialized fields like healthcare, law, and other domains abundant in proprietary terminology. While this adjustment demands additional effort, it can substantially enhance retrieval efficiency and domain alignment.\n\nSupervised Fine-Tuning (SFT). Fine-tuning a retrieval model based on labeled domain data is typically done using contrastive learning. This involves reducing the distance between positive samples while increasing the distance between negative samples",
                            ".3) Dual FT: In the RAG system, fine-tuning both the retriever and the generator simultaneously is a unique feature of the RAG system. It is important to note that the emphasis of system fine-tuning is on the coordination between the retriever and the generator. An exemplary implementation is RA-DIT [27], which fine-tunes both the LLM and the retriever. The LM-ft component updates the LLM to maximize the likelihood of the correct answer given the retrieval-augmented instructions while the R-ft component updates the retriever to minimize the KL-Divergence between the retriever score distribution and the LLM preference."
                        ],
                        "paper": {
                            "corpus_id": 271571401,
                            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
                            "authors": [
                                {
                                    "authorId": "2280046531",
                                    "name": "Yunfan Gao"
                                },
                                {
                                    "authorId": "2275320371",
                                    "name": "Yun Xiong"
                                },
                                {
                                    "authorId": "2291409458",
                                    "name": "Meng Wang"
                                },
                                {
                                    "authorId": "2256769434",
                                    "name": "Haofen Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 20
                        },
                        "score": 0.96435546875
                    },
                    {
                        "id": "(Peng et al., 2024)",
                        "snippets": [
                            "Joint training retrievers and generators simultaneously enhances performance on downstream tasks by leveraging their complementary strengths. Some approaches unify retrievers and generators into a single model, typically LLMs, and train them with both retrieval and generation objectives simultaneously [112]. This method capitalizes on the cohesive capabilities of a unified architecture, enabling the model to seamlessly retrieve relevant information and generate coherent responses within a single framework. \n\nOther methodologies involve initially training retrievers and generators separately, followed by joint training techniques to fine-tune both components. For instance, Subgraph Retriever [196] adopts an alternating training paradigm, where the retriever's parameters are fixed to use the graph data for training the generator. Subsequently, the generator's parameters are fixed, and feedback from the generator is used to guide the retriever's training. This iterative process helps both components refine their performance in a coordinated manner."
                        ],
                        "paper": {
                            "corpus_id": 271903170,
                            "title": "Graph Retrieval-Augmented Generation: A Survey",
                            "authors": [
                                {
                                    "authorId": "2314827534",
                                    "name": "Boci Peng"
                                },
                                {
                                    "authorId": "2257195454",
                                    "name": "Yun Zhu"
                                },
                                {
                                    "authorId": "2313693489",
                                    "name": "Yongchao Liu"
                                },
                                {
                                    "authorId": "2316431106",
                                    "name": "Xiaohe Bo"
                                },
                                {
                                    "authorId": "2313685962",
                                    "name": "Haizhou Shi"
                                },
                                {
                                    "authorId": "2313754922",
                                    "name": "Chuntao Hong"
                                },
                                {
                                    "authorId": "2316581992",
                                    "name": "Yan Zhang"
                                },
                                {
                                    "authorId": "2257997261",
                                    "name": "Siliang Tang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 110
                        },
                        "score": 0.96044921875
                    },
                    {
                        "id": "(Zhang et al., 2025)",
                        "snippets": [
                            "The key insight of AG-RAG is to simultaneously optimize the retriever and the generator as a whole pipeline with a joint training strategy, enabling them to learn from each other. Particularly, AG-RAG builds a dense retriever to search for relevant test-assert pairs (TAPs) with semantic matching and a retrieval-augmented generator to synthesize accurate assertions with the focal-test and retrieved TAPs as input",
                            "AG-RAG designs a joint training strategy that allows the retriever to learn from the feedback provided by the generator. This unified design fully adapts both components specifically for retrieving more useful TAPs, thereby generating accurate assertions."
                        ],
                        "paper": {
                            "corpus_id": 276408682,
                            "title": "Improving Retrieval-Augmented Deep Assertion Generation via Joint Training",
                            "authors": [
                                {
                                    "authorId": "1409701329",
                                    "name": "Quanjun Zhang"
                                },
                                {
                                    "authorId": "2239197945",
                                    "name": "Chunrong Fang"
                                },
                                {
                                    "authorId": "2346015117",
                                    "name": "Yi Zheng"
                                },
                                {
                                    "authorId": "2133779426",
                                    "name": "Ruixiang Qian"
                                },
                                {
                                    "authorId": "150311588",
                                    "name": "Shengcheng Yu"
                                },
                                {
                                    "authorId": "2285983793",
                                    "name": "Yuan Zhao"
                                },
                                {
                                    "authorId": "2296059777",
                                    "name": "Jianyi Zhou"
                                },
                                {
                                    "authorId": "2276454592",
                                    "name": "Yun Yang"
                                },
                                {
                                    "authorId": "2322486553",
                                    "name": "Tao Zheng"
                                },
                                {
                                    "authorId": "2238950128",
                                    "name": "Zhenyu Chen"
                                }
                            ],
                            "year": 2025,
                            "venue": "IEEE Transactions on Software Engineering",
                            "n_citations": 1
                        },
                        "score": 0.939453125
                    },
                    {
                        "id": "(Hu et al., 2021)",
                        "snippets": [
                            "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA."
                        ],
                        "paper": {
                            "corpus_id": 235458009,
                            "title": "LoRA: Low-Rank Adaptation of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2157840220",
                                    "name": "J. E. Hu"
                                },
                                {
                                    "authorId": "1752875",
                                    "name": "Yelong Shen"
                                },
                                {
                                    "authorId": "104100507",
                                    "name": "Phillip Wallis"
                                },
                                {
                                    "authorId": "1388725932",
                                    "name": "Zeyuan Allen-Zhu"
                                },
                                {
                                    "authorId": "2110486765",
                                    "name": "Yuanzhi Li"
                                },
                                {
                                    "authorId": "2135571585",
                                    "name": "Shean Wang"
                                },
                                {
                                    "authorId": "2109136147",
                                    "name": "Weizhu Chen"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 10511
                        },
                        "score": 0
                    },
                    {
                        "id": "(Dettmers et al., 2023)",
                        "snippets": [
                            "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training."
                        ],
                        "paper": {
                            "corpus_id": 258841328,
                            "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
                            "authors": [
                                {
                                    "authorId": "3239480",
                                    "name": "Tim Dettmers"
                                },
                                {
                                    "authorId": "51152502",
                                    "name": "Artidoro Pagnoni"
                                },
                                {
                                    "authorId": "14487640",
                                    "name": "Ari Holtzman"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2606
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wu et al., 2024)",
                        "snippets": [
                            "As introduced in Section 6, RAG training includes two branch of works, RAG with/without datastore update. For RAG without datastore update, the main challenge is how to jointly optimize all parameters in RAG. This may involves new loss functions with multiple objectives, new optimizations for efficient tuning parameters in retriever and generator, or other training strategies. \n\nFor RAG with datastore update, one challenge is how to align the retrieval representations with the generator's representations. Although the time cost of the update operation in datastore cannot be ignored, some works (Chen et al., 2022) reduce the update frequency by asychronously updating, thus achieving the alignment of knowledge representation and model's representation. Another challenge is when to retrain/fine-tune the generator in RAG when new corpus is added. Due to the in-context learning capability of exisitng LLM-based generators and high training overhead, retraining/finetuning the generator or directly inferring the generator becomes a challenging choice for different scenarios. Recently, some efficient training strategies (Dettmers et al., 2023)(Hu et al., 2021) have been proposed to accelerate the fine-tuning process, which can be taken into considerations."
                        ],
                        "paper": {
                            "corpus_id": 271270644,
                            "title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
                            "authors": [
                                {
                                    "authorId": "2112539433",
                                    "name": "Shangyu Wu"
                                },
                                {
                                    "authorId": "2303313918",
                                    "name": "Ying Xiong"
                                },
                                {
                                    "authorId": "2301404967",
                                    "name": "Yufei Cui"
                                },
                                {
                                    "authorId": "107747459",
                                    "name": "Haolun Wu"
                                },
                                {
                                    "authorId": "2243412535",
                                    "name": "Can Chen"
                                },
                                {
                                    "authorId": "2283264350",
                                    "name": "Ye Yuan"
                                },
                                {
                                    "authorId": "2303518782",
                                    "name": "Lianming Huang"
                                },
                                {
                                    "authorId": "2272581493",
                                    "name": "Xue Liu"
                                },
                                {
                                    "authorId": "2271790635",
                                    "name": "Tei-Wei Kuo"
                                },
                                {
                                    "authorId": "2290008872",
                                    "name": "Nan Guan"
                                },
                                {
                                    "authorId": "2302177675",
                                    "name": "C. Xue"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 39
                        },
                        "score": 0.95166015625
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "For rule-guided retriever fine-tuning (RGFT-retriever), we update the LM encoders in a contrastive learning objective (Chen et al., 2020) and train over supervised fine-tuning data F R provided in our constructed benchmarks, where inputs are the queries plus rules and supervised labels are heuristic oracle documents. Compared with retrievers employed with simple retrieval principles, our fine-tuned retrievers can recall more relevant results, aligned with the preferences of the rules. For rule-guided generator fine-tuning (RGFT-generator), we adopt the supervised instruction-tuning objective (Iyer et al., 2023;Chung et al., 2024) while combining each query q with two components: retrieved documents D q from the retrieval phase and the same set of rules R q consistent with the retrieval phase. The rules introduced in the RGFT-generator train LLMs on how to optimally attribute from the retrieved context into answers by following rules, making RuleRAG leverage the fine-tuned retrievers more rationally. Experiments show our proposed RGFT can further guarantee and boost the retrieval quality and answering accuracy of RuleRAG-FT than RuleRAG-ICL."
                        ],
                        "paper": {
                            "corpus_id": 273695367,
                            "title": "RuleRAG: Rule-guided retrieval-augmented generation with language models for question answering",
                            "authors": [
                                {
                                    "authorId": "2165306772",
                                    "name": "Zhongwu Chen"
                                },
                                {
                                    "authorId": "2250617116",
                                    "name": "Chengjin Xu"
                                },
                                {
                                    "authorId": "2329140108",
                                    "name": "Dingmin Wang"
                                },
                                {
                                    "authorId": "2273614102",
                                    "name": "Zhen Huang"
                                },
                                {
                                    "authorId": "67069932",
                                    "name": "Yong Dou"
                                },
                                {
                                    "authorId": "2284217200",
                                    "name": "Jian Guo"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.92236328125
                    },
                    {
                        "id": "(Chen et al., 2020)",
                        "snippets": [
                            "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels."
                        ],
                        "paper": {
                            "corpus_id": 211096730,
                            "title": "A Simple Framework for Contrastive Learning of Visual Representations",
                            "authors": [
                                {
                                    "authorId": "145358498",
                                    "name": "Ting Chen"
                                },
                                {
                                    "authorId": "40464924",
                                    "name": "Simon Kornblith"
                                },
                                {
                                    "authorId": "144739074",
                                    "name": "Mohammad Norouzi"
                                },
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 18878
                        },
                        "score": 0
                    },
                    {
                        "id": "(Cheng et al., 2025)",
                        "snippets": [
                            "Generator-Guided Retriever Training. Conversely, generator-guided retriever training focuses on optimizing the retriever based on the generator's performance and requirements. In this paradigm, the generator's ability to produce coherent and accurate text influences the retriever's selection process. DKRR [95] leverages the generator's attention scores to fine-tune the retriever, enhancing its capability to select the most pertinent information. AAR [272] employs smaller language models to generate supervision signals that guide the retriever's training, ensuring that the retrieved documents are optimally aligned with the generator's needs. RA-DIT [140] fine-tunes large language models before training the retriever, fostering better alignment and synergy between the two components. Additionally, UPRISE [33] uses a frozen LLM to guide the fine-tuning of a prompt retriever, thereby improving its effectiveness in retrieving data that the generator can utilize more effectively."
                        ],
                        "paper": {
                            "corpus_id": 277043297,
                            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "1491233507",
                                    "name": "Mingyue Cheng"
                                },
                                {
                                    "authorId": "2208917508",
                                    "name": "Yucong Luo"
                                },
                                {
                                    "authorId": "2322501286",
                                    "name": "Ouyang Jie"
                                },
                                {
                                    "authorId": "2332691115",
                                    "name": "Qi Liu"
                                },
                                {
                                    "authorId": "2312648865",
                                    "name": "Huijie Liu"
                                },
                                {
                                    "authorId": "2291070758",
                                    "name": "Li Li"
                                },
                                {
                                    "authorId": "2322429208",
                                    "name": "Shuo Yu"
                                },
                                {
                                    "authorId": "2351226328",
                                    "name": "Bohou Zhang"
                                },
                                {
                                    "authorId": "2350426005",
                                    "name": "Jiawei Cao"
                                },
                                {
                                    "authorId": "2350427710",
                                    "name": "Jie Ma"
                                },
                                {
                                    "authorId": "2322524150",
                                    "name": "Daoyu Wang"
                                },
                                {
                                    "authorId": "2258714945",
                                    "name": "Enhong Chen"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.97412109375
                    },
                    {
                        "id": "(Shi et al., 2025)",
                        "snippets": [
                            "To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45].\n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25,51]",
                            "Although existing work proposes the end-to-end training paradigm, they overly simplify a marginalization optimization through independent top-k approximation (Sachan et al., 2021)(Zamani et al., 2024), where they simply feed top-k documents into downstream LLMs one-by-one and re-score their relevance to optimize the retriever (Lewis et al., 2020)[27]. This has been criticized far from the practical scenarios as the RAG system typically consumes multiple documents (Zamani et al., 2024), while exhaustively enumerating all possible document permutations is cost-intensive and typically infeasible in practice."
                        ],
                        "paper": {
                            "corpus_id": 278339057,
                            "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models",
                            "authors": [
                                {
                                    "authorId": "2195381022",
                                    "name": "Zhengliang Shi"
                                },
                                {
                                    "authorId": "1387839383",
                                    "name": "Lingyong Yan"
                                },
                                {
                                    "authorId": "2153198380",
                                    "name": "Weiwei Sun"
                                },
                                {
                                    "authorId": "2326805997",
                                    "name": "Yue Feng"
                                },
                                {
                                    "authorId": "1749477",
                                    "name": "Pengjie Ren"
                                },
                                {
                                    "authorId": "2265517632",
                                    "name": "Xinyu Ma"
                                },
                                {
                                    "authorId": "2237948548",
                                    "name": "Shuaiqiang Wang"
                                },
                                {
                                    "authorId": "2331316040",
                                    "name": "Dawei Yin"
                                },
                                {
                                    "authorId": "2265490493",
                                    "name": "M. D. Rijke"
                                },
                                {
                                    "authorId": "2261862546",
                                    "name": "Zhaochun Ren"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.96044921875
                    },
                    {
                        "id": "(Lewis et al., 2020)",
                        "snippets": [
                            "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
                        ],
                        "paper": {
                            "corpus_id": 218869575,
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "authors": [
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3439053",
                                    "name": "Ethan Perez"
                                },
                                {
                                    "authorId": "1716179427",
                                    "name": "Aleksandara Piktus"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "103131985",
                                    "name": "Heinrich Kuttler"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "2620211",
                                    "name": "Tim Rockt\u00e4schel"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "1743722",
                                    "name": "Douwe Kiela"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6476
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zamani et al., 2024)",
                        "snippets": [
                            "This paper introduces Stochastic RAG--a novel approach for end-to-end optimization of retrieval-augmented generation (RAG) models that relaxes the simplifying assumptions of marginalization and document independence, made in most prior work. Stochastic RAG casts the retrieval process in RAG as a stochastic sampling without replacement process. Through this formulation, we employ straight-through Gumbel-top-k that provides a differentiable approximation for sampling without replacement and enables effective end-to-end optimization for RAG. We conduct extensive experiments on seven diverse datasets on a wide range of tasks, from open-domain question answering to fact verification to slot-filling for relation extraction and to dialogue systems. By applying this optimization method to a recent and effective RAG model, we advance state-of-the-art results on six out of seven datasets."
                        ],
                        "paper": {
                            "corpus_id": 269605438,
                            "title": "Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization",
                            "authors": [
                                {
                                    "authorId": "2293725953",
                                    "name": "Hamed Zamani"
                                },
                                {
                                    "authorId": "2240516450",
                                    "name": "Michael Bendersky"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 29
                        },
                        "score": 0
                    },
                    {
                        "id": "(Fang et al., 2024)",
                        "snippets": [
                            "Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT."
                        ],
                        "paper": {
                            "corpus_id": 270199429,
                            "title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training",
                            "authors": [
                                {
                                    "authorId": "2293319089",
                                    "name": "Feiteng Fang"
                                },
                                {
                                    "authorId": "2287892865",
                                    "name": "Yuelin Bai"
                                },
                                {
                                    "authorId": "2266469238",
                                    "name": "Shiwen Ni"
                                },
                                {
                                    "authorId": "2301170603",
                                    "name": "Min Yang"
                                },
                                {
                                    "authorId": "2287803799",
                                    "name": "Xiaojun Chen"
                                },
                                {
                                    "authorId": "2266809482",
                                    "name": "Ruifeng Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 39
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Performance Comparison",
                "tldr": "Comparative studies reveal that dual fine-tuning approaches consistently outperform single-component fine-tuning across various knowledge-intensive tasks. RA-DIT demonstrates up to 8.9% improvement in zero-shot settings compared to non-tuned approaches, while methods addressing retrieval noise can prevent performance drops of up to 27% that occur with irrelevant document retrieval. (8 sources)",
                "text": "\n## Quantitative Performance Improvements\n\nDual fine-tuning approaches have demonstrated substantial performance advantages over methods that optimize only a single component of RAG systems. Retrieval-Augmented Dual Instruction Tuning (RA-DIT) has achieved state-of-the-art results on knowledge-intensive benchmarks, significantly outperforming non-tuned retrieval-augmented approaches like REPLUG in zero-shot evaluations <Paper corpusId=\"263605962\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>. These improvements demonstrate the value of simultaneously optimizing both the retriever and generator components.\n\n## Robustness Against Retrieval Noise\n\nA critical advantage of dual fine-tuning approaches is their ability to mitigate the negative impact of noisy retrieval. Studies have shown that adding distracting or irrelevant documents to retrieved content can cause a dramatic 27% drop in accuracy on tasks like veracity classification <Paper corpusId=\"273185619\" paperTitle=\"(Sriram et al., 2024)\" isShortName></Paper> <Paper corpusId=\"250340232\" paperTitle=\"(Sauchuk et al., 2022)\" isShortName></Paper>. By training both components to work together, dual fine-tuning methods develop greater resilience to such retrieval noise, maintaining performance even when some retrieved information is irrelevant.\n\n## Comparative Analysis with Single-Component Approaches\n\nResearch comparing single-component and dual-component fine-tuning clearly demonstrates that optimizing either the retriever or generator in isolation leads to sub-optimal overall performance <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper>. While single-component approaches can address specific aspects of RAG performance\u2014such as improving knowledge accuracy through better retrieval or enhancing robustness against irrelevant content\u2014they fail to capture the interdependent nature of the retriever-generator relationship.\n\n## Practical Efficiency Considerations\n\nAlthough dual fine-tuning approaches achieve superior performance, they often involve implementation tradeoffs. Many existing methods employ independent top-k approximation, feeding top-k documents into language models one-by-one for relevance re-scoring <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>. This approach has been criticized as impractical since real-world RAG systems typically process multiple documents simultaneously, and exhaustively evaluating all possible document combinations is computationally infeasible <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"269605438\" paperTitle=\"(Zamani et al., 2024)\" isShortName></Paper>.\n\n## Real-World Reliability\n\nThe performance of dual fine-tuned RAG systems extends beyond benchmark scores to operational reliability. Research highlights that validation of RAG system performance is primarily feasible during actual operation, and robustness evolves rather than being designed in from the start <Paper corpusId=\"273185619\" paperTitle=\"(Sriram et al., 2024)\" isShortName></Paper> <Paper corpusId=\"266933076\" paperTitle=\"(Barnett et al., 2024)\" isShortName></Paper>. This perspective underscores the importance of dual fine-tuning approaches that can continue to improve through ongoing alignment of retriever and generator components.\n\n## Resilience Against Diverse Noise Types\n\nRecent advanced dual fine-tuning methods like Retrieval-augmented Adaptive Adversarial Training (RAAT) specifically address various types of retrieval noise that reflect real-world environments <Paper corpusId=\"270199429\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>. These approaches employ adaptive adversarial training to dynamically adjust the model's training process in response to different noise types, resulting in significant improvements in performance metrics under diverse noise conditions.",
                "citations": [
                    {
                        "id": "(Lin et al., 2023)",
                        "snippets": [
                            "In this work, we show lightweight instruction tuning (Chung et al., 2022b;Iyer et al., 2022;Zhou et al., 2023) alone can significantly boost the performance of RALMs, especially in scenarios that require access to large, external knowledge sources. We propose Retrieval-Augmented Dual Instruction Tuning (RA-DIT), an approach that retrofits any LLM with retrieval capabilities via fine-tuning over a set of tasks selected to cultivate knowledge utilization and contextual awareness in the language model predictions",
                            "RA-DIT updates the LLM with retrieval-augmented instruction tuning to make better use of retrieved knowledge and ignore irrelevant or distracting information. It also fine-tunes the retriever with supervision from the LLM to retrieve texts that can better help the LLM generate correct outputs. RA-DIT achieves state-of-the-art performance in zero-and few-shot evaluations on knowledge intensive benchmarks, surpassing un-tuned in-context RALM approaches such as REPLUG and compete effectively against methods that require extensive pre-training such as ATLAS."
                        ],
                        "paper": {
                            "corpus_id": 263605962,
                            "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2255374957",
                                    "name": "Xi Victoria Lin"
                                },
                                {
                                    "authorId": "1769736",
                                    "name": "Xilun Chen"
                                },
                                {
                                    "authorId": "46221498",
                                    "name": "Mingda Chen"
                                },
                                {
                                    "authorId": "2254168373",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "2253400960",
                                    "name": "Maria Lomeli"
                                },
                                {
                                    "authorId": "2191899140",
                                    "name": "Rich James"
                                },
                                {
                                    "authorId": "2253404757",
                                    "name": "Pedro Rodriguez"
                                },
                                {
                                    "authorId": "2253401183",
                                    "name": "Jacob Kahn"
                                },
                                {
                                    "authorId": "2253402270",
                                    "name": "Gergely Szilvasy"
                                },
                                {
                                    "authorId": "2253417398",
                                    "name": "Mike Lewis"
                                },
                                {
                                    "authorId": "2137813791",
                                    "name": "Luke S. Zettlemoyer"
                                },
                                {
                                    "authorId": "2253400757",
                                    "name": "Scott Yih"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 153
                        },
                        "score": 0.96435546875
                    },
                    {
                        "id": "(Sriram et al., 2024)",
                        "snippets": [
                            "Retrieval-augmented generation (RAG) relies on two key modules: a retriever and a reader/generation model. For many RAG systems, noisy retrieval hurts downstream performance by providing irrelevant or misleading documents (Yoran et al., 2024). (Sauchuk et al., 2022) found that adding distractors can cause a 27% drop on veracity classification accuracy on FEVER. Therefore, it's important for retrievers to find relevant documents and simultaneously avoid damaging ones. Shi et al. (2023) attempts to solve this problem by finetuning the retrieval component while fixing the reader LM, similar to our work. Other approaches like Ke et al. (2024) create a more complex system with a \"bridging\" model between the retriever and reader. Nevertheless, noisy retrieval remains a failure point in RAG systems (Barnett et al., 2024), and tangible downstream gains can be realized by further finetuning."
                        ],
                        "paper": {
                            "corpus_id": 273185619,
                            "title": "Contrastive Learning to Improve Retrieval for Real-World Fact Checking",
                            "authors": [
                                {
                                    "authorId": "2165382262",
                                    "name": "Aniruddh Sriram"
                                },
                                {
                                    "authorId": "2159829626",
                                    "name": "Fangyuan Xu"
                                },
                                {
                                    "authorId": "2257003422",
                                    "name": "Eunsol Choi"
                                },
                                {
                                    "authorId": "1814094",
                                    "name": "Greg Durrett"
                                }
                            ],
                            "year": 2024,
                            "venue": "FEVER",
                            "n_citations": 1
                        },
                        "score": 0.91357421875
                    },
                    {
                        "id": "(Sauchuk et al., 2022)",
                        "snippets": [
                            "Many recent Natural Language Processing (NLP) task formulations, such as question answering and fact verification, are implemented as a two-stage cascading architecture. In the first stage an IR system retrieves \"relevant'' documents containing the knowledge, and in the second stage an NLP system performs reasoning to solve the task. Optimizing the IR system for retrieving relevant documents ensures that the NLP system has sufficient information to operate over. These recent NLP task formulations raise interesting and exciting challenges for IR, where the end-user of an IR system is not a human with an information need, but another system exploiting the documents retrieved by the IR system to perform reasoning and address the user information need. Among these challenges, as we will show, is that noise from the IR system, such as retrieving spurious or irrelevant documents, can negatively impact the accuracy of the downstream reasoning module. Hence, there is the need to balance maximizing relevance while minimizing noise in the IR system. This paper presents experimental results on two NLP tasks implemented as a two-stage cascading architecture. We show how spurious or irrelevant retrieved results from the first stage can induce errors in the second stage. We use these results to ground our discussion of the research challenges that the IR community should address in the context of these knowledge-intensive NLP tasks."
                        ],
                        "paper": {
                            "corpus_id": 250340232,
                            "title": "On the Role of Relevance in Natural Language Processing Tasks",
                            "authors": [
                                {
                                    "authorId": "2175275805",
                                    "name": "Artsiom Sauchuk"
                                },
                                {
                                    "authorId": "2053211210",
                                    "name": "James Thorne"
                                },
                                {
                                    "authorId": "1770962",
                                    "name": "A. Halevy"
                                },
                                {
                                    "authorId": "2783910",
                                    "name": "N. Tonellotto"
                                },
                                {
                                    "authorId": "144925193",
                                    "name": "F. Silvestri"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 16
                        },
                        "score": 0
                    },
                    {
                        "id": "(Shi et al., 2025)",
                        "snippets": [
                            "To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45].\n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25,51]",
                            "Although existing work proposes the end-to-end training paradigm, they overly simplify a marginalization optimization through independent top-k approximation (Sachan et al., 2021)(Zamani et al., 2024), where they simply feed top-k documents into downstream LLMs one-by-one and re-score their relevance to optimize the retriever (Lewis et al., 2020)[27]. This has been criticized far from the practical scenarios as the RAG system typically consumes multiple documents (Zamani et al., 2024), while exhaustively enumerating all possible document permutations is cost-intensive and typically infeasible in practice."
                        ],
                        "paper": {
                            "corpus_id": 278339057,
                            "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models",
                            "authors": [
                                {
                                    "authorId": "2195381022",
                                    "name": "Zhengliang Shi"
                                },
                                {
                                    "authorId": "1387839383",
                                    "name": "Lingyong Yan"
                                },
                                {
                                    "authorId": "2153198380",
                                    "name": "Weiwei Sun"
                                },
                                {
                                    "authorId": "2326805997",
                                    "name": "Yue Feng"
                                },
                                {
                                    "authorId": "1749477",
                                    "name": "Pengjie Ren"
                                },
                                {
                                    "authorId": "2265517632",
                                    "name": "Xinyu Ma"
                                },
                                {
                                    "authorId": "2237948548",
                                    "name": "Shuaiqiang Wang"
                                },
                                {
                                    "authorId": "2331316040",
                                    "name": "Dawei Yin"
                                },
                                {
                                    "authorId": "2265490493",
                                    "name": "M. D. Rijke"
                                },
                                {
                                    "authorId": "2261862546",
                                    "name": "Zhaochun Ren"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.96044921875
                    },
                    {
                        "id": "(Lewis et al., 2020)",
                        "snippets": [
                            "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
                        ],
                        "paper": {
                            "corpus_id": 218869575,
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "authors": [
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3439053",
                                    "name": "Ethan Perez"
                                },
                                {
                                    "authorId": "1716179427",
                                    "name": "Aleksandara Piktus"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "103131985",
                                    "name": "Heinrich Kuttler"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "2620211",
                                    "name": "Tim Rockt\u00e4schel"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "1743722",
                                    "name": "Douwe Kiela"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6476
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zamani et al., 2024)",
                        "snippets": [
                            "This paper introduces Stochastic RAG--a novel approach for end-to-end optimization of retrieval-augmented generation (RAG) models that relaxes the simplifying assumptions of marginalization and document independence, made in most prior work. Stochastic RAG casts the retrieval process in RAG as a stochastic sampling without replacement process. Through this formulation, we employ straight-through Gumbel-top-k that provides a differentiable approximation for sampling without replacement and enables effective end-to-end optimization for RAG. We conduct extensive experiments on seven diverse datasets on a wide range of tasks, from open-domain question answering to fact verification to slot-filling for relation extraction and to dialogue systems. By applying this optimization method to a recent and effective RAG model, we advance state-of-the-art results on six out of seven datasets."
                        ],
                        "paper": {
                            "corpus_id": 269605438,
                            "title": "Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization",
                            "authors": [
                                {
                                    "authorId": "2293725953",
                                    "name": "Hamed Zamani"
                                },
                                {
                                    "authorId": "2240516450",
                                    "name": "Michael Bendersky"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 29
                        },
                        "score": 0
                    },
                    {
                        "id": "(Barnett et al., 2024)",
                        "snippets": [
                            "Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community.CCS CONCEPTS\u2022 Software and its engineering \u2192 Empirical software validation."
                        ],
                        "paper": {
                            "corpus_id": 266933076,
                            "title": "Seven Failure Points When Engineering a Retrieval Augmented Generation System",
                            "authors": [
                                {
                                    "authorId": "2052845461",
                                    "name": "Scott Barnett"
                                },
                                {
                                    "authorId": "2266469333",
                                    "name": "Stefanus Kurniawan"
                                },
                                {
                                    "authorId": "2257020336",
                                    "name": "Srikanth Thudumu"
                                },
                                {
                                    "authorId": "2279020735",
                                    "name": "Zach Brannelly"
                                },
                                {
                                    "authorId": "47505933",
                                    "name": "Mohamed Abdelrazek"
                                }
                            ],
                            "year": 2024,
                            "venue": "2024 IEEE/ACM 3rd International Conference on AI Engineering \u2013 Software Engineering for AI (CAIN)",
                            "n_citations": 92
                        },
                        "score": 0
                    },
                    {
                        "id": "(Fang et al., 2024)",
                        "snippets": [
                            "Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT."
                        ],
                        "paper": {
                            "corpus_id": 270199429,
                            "title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training",
                            "authors": [
                                {
                                    "authorId": "2293319089",
                                    "name": "Feiteng Fang"
                                },
                                {
                                    "authorId": "2287892865",
                                    "name": "Yuelin Bai"
                                },
                                {
                                    "authorId": "2266469238",
                                    "name": "Shiwen Ni"
                                },
                                {
                                    "authorId": "2301170603",
                                    "name": "Min Yang"
                                },
                                {
                                    "authorId": "2287803799",
                                    "name": "Xiaojun Chen"
                                },
                                {
                                    "authorId": "2266809482",
                                    "name": "Ruifeng Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 39
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Challenges and Future Directions",
                "tldr": "RAG systems face significant challenges in efficiently optimizing both retriever and generator components simultaneously, which requires innovative loss functions and parameter-efficient methods. Future research directions include developing better alignment techniques between retrieval representations and generator needs, along with more efficient fine-tuning strategies that balance computational constraints with performance. (5 sources)",
                "text": "\n## Computational Efficiency Challenges\n\nOne of the primary challenges in RAG optimization is the computational burden of jointly fine-tuning both the retriever and generator components. As language models continue to grow in size, traditional full fine-tuning approaches become increasingly impractical <Paper corpusId=\"271270644\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>. This has spurred the development of parameter-efficient tuning methods like Low-Rank Adaptation (LoRA), which freezes pre-trained model weights and injects trainable rank decomposition matrices into each Transformer layer, reducing the number of trainable parameters by up to 10,000 times compared to full fine-tuning of large models <Paper corpusId=\"271270644\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"235458009\" paperTitle=\"(Hu et al., 2021)\" isShortName></Paper>.\n\nBuilding on this approach, QLoRA has further improved efficiency by enabling the fine-tuning of 65B parameter models on a single GPU through 4-bit quantization techniques while maintaining performance comparable to full 16-bit fine-tuning <Paper corpusId=\"271270644\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258841328\" paperTitle=\"(Dettmers et al., 2023)\" isShortName></Paper>. These advances are crucial for making dual fine-tuning approaches more accessible and practical.\n\n## Knowledge Representation Alignment\n\nAnother significant challenge involves aligning retrieval representations with the generator's knowledge representation needs. In systems that update their datastores, this alignment becomes particularly important for ensuring that retrieved content is maximally useful for the generator <Paper corpusId=\"271270644\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>. Some approaches have addressed this through asynchronous updating of datastores, which reduces update frequency while still achieving representation alignment <Paper corpusId=\"271270644\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249191271\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper>.\n\n## Generator Update Frequency\n\nA practical consideration for RAG deployment is determining when to retrain or fine-tune the generator component when new content is added to the corpus. Given the high computational cost of model fine-tuning versus the in-context learning capabilities of modern LLMs, this presents a challenging tradeoff <Paper corpusId=\"271270644\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>. The development of more efficient fine-tuning strategies offers promising solutions to this challenge, potentially allowing for more frequent updates without prohibitive computational costs.\n\n## Future Research Directions\n\nResearch is increasingly moving toward joint optimization approaches that can simultaneously improve both components of RAG systems. Future work will likely focus on developing new loss functions with multiple objectives and novel optimization strategies for efficient parameter tuning <Paper corpusId=\"271270644\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>.\n\nOne promising direction involves combining fine-tuning of the generator with learned retrieval mechanisms such as reranker-aware retrievers or contrastively trained retrievers, which could further enhance factual accuracy and context filtering capabilities <Paper corpusId=\"278714952\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>. This integrated approach acknowledges the interdependent nature of retrieval and generation in RAG systems and seeks to optimize their interaction rather than treating them as separate components.\n\nAs the field advances, we may also see more sophisticated techniques for dynamically adjusting the balance between parametric knowledge (stored in model weights) and non-parametric knowledge (accessed through retrieval). This could potentially lead to more adaptable RAG systems that can efficiently incorporate new information while maintaining robust performance on established knowledge domains <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Wu et al., 2024)",
                        "snippets": [
                            "As introduced in Section 6, RAG training includes two branch of works, RAG with/without datastore update. For RAG without datastore update, the main challenge is how to jointly optimize all parameters in RAG. This may involves new loss functions with multiple objectives, new optimizations for efficient tuning parameters in retriever and generator, or other training strategies. \n\nFor RAG with datastore update, one challenge is how to align the retrieval representations with the generator's representations. Although the time cost of the update operation in datastore cannot be ignored, some works (Chen et al., 2022) reduce the update frequency by asychronously updating, thus achieving the alignment of knowledge representation and model's representation. Another challenge is when to retrain/fine-tune the generator in RAG when new corpus is added. Due to the in-context learning capability of exisitng LLM-based generators and high training overhead, retraining/finetuning the generator or directly inferring the generator becomes a challenging choice for different scenarios. Recently, some efficient training strategies (Dettmers et al., 2023)(Hu et al., 2021) have been proposed to accelerate the fine-tuning process, which can be taken into considerations."
                        ],
                        "paper": {
                            "corpus_id": 271270644,
                            "title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
                            "authors": [
                                {
                                    "authorId": "2112539433",
                                    "name": "Shangyu Wu"
                                },
                                {
                                    "authorId": "2303313918",
                                    "name": "Ying Xiong"
                                },
                                {
                                    "authorId": "2301404967",
                                    "name": "Yufei Cui"
                                },
                                {
                                    "authorId": "107747459",
                                    "name": "Haolun Wu"
                                },
                                {
                                    "authorId": "2243412535",
                                    "name": "Can Chen"
                                },
                                {
                                    "authorId": "2283264350",
                                    "name": "Ye Yuan"
                                },
                                {
                                    "authorId": "2303518782",
                                    "name": "Lianming Huang"
                                },
                                {
                                    "authorId": "2272581493",
                                    "name": "Xue Liu"
                                },
                                {
                                    "authorId": "2271790635",
                                    "name": "Tei-Wei Kuo"
                                },
                                {
                                    "authorId": "2290008872",
                                    "name": "Nan Guan"
                                },
                                {
                                    "authorId": "2302177675",
                                    "name": "C. Xue"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 39
                        },
                        "score": 0.95166015625
                    },
                    {
                        "id": "(Hu et al., 2021)",
                        "snippets": [
                            "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA."
                        ],
                        "paper": {
                            "corpus_id": 235458009,
                            "title": "LoRA: Low-Rank Adaptation of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2157840220",
                                    "name": "J. E. Hu"
                                },
                                {
                                    "authorId": "1752875",
                                    "name": "Yelong Shen"
                                },
                                {
                                    "authorId": "104100507",
                                    "name": "Phillip Wallis"
                                },
                                {
                                    "authorId": "1388725932",
                                    "name": "Zeyuan Allen-Zhu"
                                },
                                {
                                    "authorId": "2110486765",
                                    "name": "Yuanzhi Li"
                                },
                                {
                                    "authorId": "2135571585",
                                    "name": "Shean Wang"
                                },
                                {
                                    "authorId": "2109136147",
                                    "name": "Weizhu Chen"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 10511
                        },
                        "score": 0
                    },
                    {
                        "id": "(Dettmers et al., 2023)",
                        "snippets": [
                            "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training."
                        ],
                        "paper": {
                            "corpus_id": 258841328,
                            "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
                            "authors": [
                                {
                                    "authorId": "3239480",
                                    "name": "Tim Dettmers"
                                },
                                {
                                    "authorId": "51152502",
                                    "name": "Artidoro Pagnoni"
                                },
                                {
                                    "authorId": "14487640",
                                    "name": "Ari Holtzman"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2606
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2022)",
                        "snippets": [
                            "Prompt learning approaches have made waves in natural language processing by inducing better few-shot performance while they still follow a parametric-based learning paradigm; the oblivion and rote memorization problems in learning may encounter unstable generalization issues. Specifically, vanilla prompt learning may struggle to utilize atypical instances by rote during fully-supervised training or overfit shallow patterns with low-shot data. To alleviate such limitations, we develop RetroPrompt with the motivation of decoupling knowledge from memorization to help the model strike a balance between generalization and memorization. In contrast with vanilla prompt learning, RetroPrompt constructs an open-book knowledge-store from training instances and implements a retrieval mechanism during the process of input, training and inference, thus equipping the model with the ability to retrieve related contexts from the training corpus as cues for enhancement. Extensive experiments demonstrate that RetroPrompt can obtain better performance in both few-shot and zero-shot settings. Besides, we further illustrate that our proposed RetroPrompt can yield better generalization abilities with new datasets. Detailed analysis of memorization indeed reveals RetroPrompt can reduce the reliance of language models on memorization; thus, improving generalization for downstream tasks. Code is available in https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt."
                        ],
                        "paper": {
                            "corpus_id": 249191271,
                            "title": "Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Xiang Chen"
                                },
                                {
                                    "authorId": null,
                                    "name": "Lei Li"
                                },
                                {
                                    "authorId": "2153010067",
                                    "name": "Ningyu Zhang"
                                },
                                {
                                    "authorId": "2153398295",
                                    "name": "Xiaozhuan Liang"
                                },
                                {
                                    "authorId": "152931849",
                                    "name": "Shumin Deng"
                                },
                                {
                                    "authorId": "2111727840",
                                    "name": "Chuanqi Tan"
                                },
                                {
                                    "authorId": "2087380523",
                                    "name": "Fei Huang"
                                },
                                {
                                    "authorId": "2059080424",
                                    "name": "Luo Si"
                                },
                                {
                                    "authorId": "49178307",
                                    "name": "Huajun Chen"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 54
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lee et al., 2025)",
                        "snippets": [
                            "\u2022 Joint retrieval-generation optimization: While Finetune-RAG focuses on improving the generation component, combining it with learned retrieval mechanisms such as rerankeraware retrievers or contrastively trained retrievers could lead to further improvements in factual accuracy and context filtering."
                        ],
                        "paper": {
                            "corpus_id": 278714952,
                            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2362089035",
                                    "name": "Zhan Peng Lee"
                                },
                                {
                                    "authorId": "2362188632",
                                    "name": "Andre Lin"
                                },
                                {
                                    "authorId": "2363425126",
                                    "name": "Calvin Tan"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.90966796875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.17081700000000002
    }
}